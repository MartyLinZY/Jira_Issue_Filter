Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Fix Version/s,Fix Version/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Log Work,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Blocker),Inward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Blocker),Inward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Incorporates),Outward issue link (Incorporates),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Inward issue link (Required),Inward issue link (dependent),Outward issue link (dependent),Outward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Authors),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Review Date),Custom field (Reviewer),Custom field (Reviewer),Custom field (Reviewers),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
"login() request via Thrift/PHP fails with ""Unexpected authentication problem"" in cassandra log / ""Internal error processing login"" in Thrift",CASSANDRA-935,12460737,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,rschildmeijer,redsolar,redsolar,31/Mar/10 04:22,16/Apr/19 17:33,22/Mar/23 14:57,04/Apr/10 05:59,0.7 beta 1,,,,0,,,,,,"When issuing a login request via PHP Thrift with the following parameters:

$auth_request = new cassandra_AuthenticationRequest;
$auth_request->credentials = array (
    ""username"" => ""jsmith"",
     ""password"" => ""havebadpass"",
);
$client->login(""Keyspace1"", $auth_request);

I get an exception, with the following details

PHP Exception:
PHP Fatal error:  Uncaught exception 'TApplicationException' with message 'Internal error processing login' in /home/redsolar/html/includes/thrift/packages/cassandra/Cassandra.php:73

Cassandra log:

ERROR 13:00:53,823 Internal error processing login
java.lang.RuntimeException: Unexpected authentication problem
        at org.apache.cassandra.auth.SimpleAuthenticator.login(SimpleAuthenticator.java:113)
        at org.apache.cassandra.thrift.CassandraServer.login(CassandraServer.java:651)
        at org.apache.cassandra.thrift.Cassandra$Processor$login.process(Cassandra.java:1147)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:1125)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.NullPointerException
        at java.io.FileInputStream.<init>(FileInputStream.java:133)
        at java.io.FileInputStream.<init>(FileInputStream.java:96)
        at org.apache.cassandra.auth.SimpleAuthenticator.login(SimpleAuthenticator.java:82)
        ... 7 more

File contents (all chmod 777 for testing):

""conf/access.properties""
Keyspace1=jsmith,Elvis Presley,dilbert

""conf/password.properties""
jsmith=havebadpass
Elvis\ Presley=graceland4evar
dilbert=nomoovertime","PHP 5.3, Cassandra 0.6.0-rc1 (as in current vote), CentOS 5.4 x64, 4-node cluster",rschildmeijer,tzz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Apr/10 03:13;tzz;CASSANDRA-935-check-properties.patch;https://issues.apache.org/jira/secure/attachment/12440379/CASSANDRA-935-check-properties.patch","01/Apr/10 02:56;tzz;CASSANDRA-935-check-properties.patch;https://issues.apache.org/jira/secure/attachment/12440378/CASSANDRA-935-check-properties.patch","01/Apr/10 02:07;rschildmeijer;CASSANDRA-935-v2.patch;https://issues.apache.org/jira/secure/attachment/12440370/CASSANDRA-935-v2.patch","03/Apr/10 15:04;rschildmeijer;CASSANDRA-935-v3.patch;https://issues.apache.org/jira/secure/attachment/12440672/CASSANDRA-935-v3.patch","31/Mar/10 04:52;rschildmeijer;CASSANDRA-935.patch;https://issues.apache.org/jira/secure/attachment/12440275/CASSANDRA-935.patch",,,,,,,,,,5.0,rschildmeijer,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19925,,,Tue Apr 06 13:10:16 UTC 2010,,,,,,,,,,"0|i0g1zj:",91752,,,,,Low,,,,,,,,,,,,,,,,,"31/Mar/10 04:24;jbellis;are you setting -DPASSWD_FILENAME_PROPERTY=path/to/access.properties ?  because that is what SimpleAuthenticator looks for.  (it should probably just try to load from classpath, that seems more java-ish.);;;","31/Mar/10 04:29;rschildmeijer;In fact the property keys are called passwd.properties and access.properties. 
    -Dpasswd.properties=<PATH>/passwd.properties
    -Daccess.properties=<PATH>access.properties

(The PASSWD_FILENAME_PROPERTY is just the name of the static constant)

;;;","31/Mar/10 04:39;redsolar;Running cassandra with ""bin/cassandra -f -Dpasswd.properties=conf/passwd.properties -Daccess.properties=conf/access.properties"" solved the issue
;;;","31/Mar/10 04:51;rschildmeijer;We should probably add a check that everything was properly defined and ""bail out"" otherwise;;;","31/Mar/10 04:52;rschildmeijer;Verifies that access.properties and passwd.properties are defined if SimpleAuthenticator is used (in storage-conf.xml);;;","31/Mar/10 22:30;urandom;I think I'm -1 on this patch Roger. Those authenticators are pluggable, I'd rather that we didn't have implementation details leaking out of them.;;;","31/Mar/10 22:49;rschildmeijer;I agree. 

I was considering a solution that augmented the IAuthenticator interface with methods like isAuthenticationRequired(), getAccessFileName(), getPasswordFileName, but that felt like polluting the interface with implementation specific details.

What speaks for the submitted solution (atleast to some extent) is that we are handling faulty configuration more correctly when using the built in/shipped authentication (SimpleAuthenticator).   ;;;","31/Mar/10 22:57;urandom;I think the correct solution is just to make SimpleAuthenticator produce a clearer error message when it's been misconfigured.;;;","31/Mar/10 23:08;jbellis;Could we add a validateConfiguration method to IAuthenticator?;;;","31/Mar/10 23:28;urandom;works for me.;;;","01/Apr/10 02:56;tzz;What should validateConfiguration() do that login() should not do as well?

I'd rather add the necessary checks to login().  The performance penalty is negligible.  See attached patch.;;;","01/Apr/10 03:01;jbellis;> What should validateConfiguration() do that login() should not do as well? 

fail the startup rather than breaking things when you try to query, which as we've seen here is the wrong approach.;;;","01/Apr/10 03:13;tzz;How about checking in the constructor?  Attaching patch to do it this way.;;;","01/Apr/10 04:08;rschildmeijer;I totaly agree that we should do the validation as early as possible (prefer startup check instead of ""on query check"").
I think the IAuthenticator API is easier to get a correct implementation for using the two method version (see CASSANDRA-935-v2.patch) instead of relying on that implementors throws an exception in the ctor.;;;","01/Apr/10 04:25;tzz;Either validateConfiguration() or in the constructor works.  Assuming your patch goes in, I would distinguish between the two kinds of configuration exception for the two possible missing items or show the actual values in the exception message, e.g.

String.format(""When using %s, properties %s (currently %s) and %s (currently %s) must be defined, ...)
;;;","03/Apr/10 06:22;urandom;I think I prefer the validateConfiguration() approach.

Roger, can you use o.a.c.config.DatabaseDescriptor.ConfigurationException instead of javax.naming.ConfigurationException? I'm also having some trouble applying your patch to trunk/ (maybe it just needs to be rebased).
;;;","03/Apr/10 15:04;rschildmeijer;Patch rebased (v3);;;","04/Apr/10 05:59;urandom;committed; thanks Roger!;;;","06/Apr/10 21:10;hudson;Integrated in Cassandra #399 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/399/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra crashes with segmentation fault on Debian 5.0 and Ubuntu 10.10,CASSANDRA-2441,12503787,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jbellis,xedin,xedin,09/Apr/11 05:35,16/Apr/19 17:33,22/Mar/23 14:57,12/Apr/11 23:09,0.8 beta 1,,,,0,,,,,,"Last working commit is c8d1984bf17cab58f40069e522d074c7b0077bc1 (merge from 0.7), branch: trunk.

What I did is cloned git://git.apache.org/cassandra.git and did git reset each commit with `ant clean && ant && ./bin/cassandra -f` until I got cassandra started","Both servers have identical hardware configuration: Quad-Core AMD Opteron(tm) Processor 2374 HE, 4 GB RAM (rackspace servers)

Java version ""1.6.0_20""
OpenJDK Runtime Environment (IcedTea6 1.9.7) (6b20-1.9.7-0ubuntu1)
OpenJDK 64-Bit Server VM (build 19.0-b09, mixed mode)",alexiswilke,amram99,mauzhang,ralph@massrelevance.com,urandom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5517,,,,,,,,"12/Apr/11 04:51;jbellis;2441.txt;https://issues.apache.org/jira/secure/attachment/12476056/2441.txt","12/Apr/11 04:21;jbellis;2441.txt;https://issues.apache.org/jira/secure/attachment/12476053/2441.txt","12/Apr/11 03:28;jbellis;jamm-0.2.1.jar;https://issues.apache.org/jira/secure/attachment/12476042/jamm-0.2.1.jar",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20627,,,Mon Oct 15 06:18:30 UTC 2012,,,,,,,,,,"0|i0839r:",45129,,xedin,,xedin,Critical,,,,,,,,,,,,,,,,,"09/Apr/11 06:12;jbellis;are you using sun jdk?;;;","09/Apr/11 06:17;xedin;no, I installed OpenJDK using apt-get `apt-get install openjdk-6-jdk openjdk-6-jre`;;;","09/Apr/11 06:24;jbellis;give sun jdk a try.;;;","09/Apr/11 06:26;xedin;Ok, I will give it a try and comment back!;;;","09/Apr/11 07:14;xedin;Works with Sun JDK

{noformat}
java version ""1.6.0_24""
Java(TM) SE Runtime Environment (build 1.6.0_24-b07)
Java HotSpot(TM) 64-Bit Server VM (build 19.1-b02, mixed mode)
{noformat}

But this could be a problem if, for example, Whirr sets up openjdk instead of one from the Sun, I will check that ASAP and comment...;;;","09/Apr/11 07:23;jbellis;Looks like this isn't the only time someone has seen segfaults due to using javaagent with OpenJDK: http://liteforums.appdynamics.com/discussion/143/appagent-causing-segfault/p1

I would prefer to fix by changing our packaging to explicitly use Sun JDK instead of ""whatever.""

Would also be useful to try w/ the IBM JDK: http://www.ibm.com/developerworks/java/jdk/linux/download.html;;;","09/Apr/11 07:26;xedin;Thats a good idea, I agree! Let me test IBM JDK tomorrow...;;;","10/Apr/11 23:23;jbellis;Also jrockit: http://www.oracle.com/technetwork/middleware/jrockit/downloads/index.html;;;","11/Apr/11 01:44;xedin;Latest code (branch trunk) works with JRockit (jrockit-jdk1.6.0_22-R28.1.1-4.0.1) but note that JVM does not support following options: -Xmn<size> -XX:ThreadPriorityPolicy -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:SurvivorRatio=8  -XX:MaxTenuringThreshold=1 -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly;;;","11/Apr/11 02:23;xedin;using IBM JDK started without any changes to the conf/cassandra-env.sh and everything worked fine.

{noformat}
java version ""1.6.0""
Java(TM) SE Runtime Environment (build pxi3260sr9fp1-20110208_03(SR9 FP1))
IBM J9 VM (build 2.4, JRE 1.6.0 IBM J9 2.4 Linux x86-32 jvmxi3260sr9-20110203_74623 (JIT enabled, AOT enabled)
J9VM - 20110203_074623
JIT  - r9_20101028_17488ifx3
GC   - 20101027_AA)
JCL  - 20110203_01
{noformat}
;;;","11/Apr/11 11:11;jbellis;So, definitely an OpenJDK-only bug.

I'll see if I can give it a no-java-agent mode so we can limp along without it for OpenJDK.;;;","11/Apr/11 18:23;xedin;+1;;;","12/Apr/11 03:28;jbellis;attached.  (requires jamm 0.2.1 in lib/, also attached.)

(most of the patch is svn deleting the 0.2 jar.  silly svn.);;;","12/Apr/11 04:21;jbellis;On Brandon's advice I moved the entire warning into the log4j call, even though this makes it unwieldy and we don't have perfect information as to what the cause is at that point.;;;","12/Apr/11 04:32;xedin;Can you please re-attach git apply and patch both say that patch is corrupted at line 90?..;;;","12/Apr/11 04:51;jbellis;manually ripped the binary portion out of the patch.;;;","12/Apr/11 05:00;xedin;+1;;;","12/Apr/11 23:09;jbellis;committed;;;","13/Apr/11 00:42;hudson;Integrated in Cassandra-0.8 #2 (See [https://hudson.apache.org/hudson/job/Cassandra-0.8/2/])
    hack to allow OpenJDK to run w/o javaagent (otherwise it segfaults)
patch by jbellis and brandonwilliams; reviewed by Pavel Yaskevich for CASSANDRA-2441
;;;","12/Apr/12 17:42;richardlow;Tracked this down to the stack size setting.  In cassandra-env.sh, the stack size is set to 128k.  From running gdb, the segfault is tracked down to a stack overflow in parse_manifest.c:234 within openjdk.  It's clear what's going on: there's a huge statically allocated variable.

Setting the stack size to 256k means Cassandra can start up with javaagent.  So we could reenable this for openjdk if people are prepared to take the memory hit on stack size.;;;","02/Oct/12 23:22;amram99;Thanks Richard! Changing the stack size from 180k to 256k worked for me on Ubuntu 12.04 with Cassandra 1.1.5 and OpenJDK 64-Bit Server VM/1.6.0_24;;;","11/Oct/12 06:07;alexiswilke;I can confirm Bubba Gump comment. I have the same setup and increasing the stack solved the crash issue with OpenJDK.;;;","15/Oct/12 14:18;mauzhang;180k works fine with OpenJDK 1.7.0_07. So update OpenJDK could be an option;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
thread problem?,CASSANDRA-168,12425326,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,nk11,nk11,13/May/09 20:49,16/Apr/19 17:33,22/Mar/23 14:57,13/May/09 21:47,,,,,0,,,,,,"With the original code CASSANDRA-153 works fine, even for large values of max.
I change a little bit the test. 

		int max = 10000;
		Random rnd = new Random();
		for (int a = 0; a < max; a++) {
			System.out.println(a);
			client.insert(""Table1"", ""k1:"" + rnd.nextInt(), ""Super1:x:x"", new byte[] { (byte) 1 }, 0, false);
		}

		client.get_key_range(""Table1"", ""k1:0"", ""k1:1000"", 1000);

Now the log says just:

ERROR [pool-1-thread-1] 2009-05-13 15:41:01,908 Cassandra.java (line 1187) Internal error processing get_key_range
java.lang.RuntimeException: error reading keyrange RangeCommand(table='Table1', startWith='k1:0', stopAt='k1:1000', maxResults=1000)
	at org.apache.cassandra.service.StorageProxy.getKeyRange(StorageProxy.java:682)
	at org.apache.cassandra.service.CassandraServer.get_key_range(CassandraServer.java:527)
	at org.apache.cassandra.service.Cassandra$Processor$get_key_range.process(Cassandra.java:1183)
	at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:805)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:252)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.util.concurrent.TimeoutException: Operation timed out.
	at org.apache.cassandra.net.AsyncResult.get(AsyncResult.java:95)
	at org.apache.cassandra.service.StorageProxy.getKeyRange(StorageProxy.java:677)
	... 7 more

But also some of:

DEBUG [ROW-MUTATION-STAGE:1] 2009-05-13 15:40:56,970 RowMutationVerbHandler.java (line 70) Applying RowMutation(key='k1:-1907502342', modifications=[ColumnFamily(Super1 [SuperColumn(x [x:false:1@0])])])
ERROR [ROW-READ-STAGE:1] 2009-05-13 15:40:56,955 DebuggableThreadPoolExecutor.java (line 125) Error in ThreadPoolExecutor
java.lang.RuntimeException: java.util.ConcurrentModificationException
	at org.apache.cassandra.service.RangeVerbHandler.doVerb(RangeVerbHandler.java:27)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:46)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.util.ConcurrentModificationException
	at java.util.HashMap$HashIterator.nextEntry(Unknown Source)
	at java.util.HashMap$KeyIterator.next(Unknown Source)
	at java.util.AbstractQueue.addAll(Unknown Source)
	at org.apache.cassandra.db.Memtable.sortedKeyIterator(Memtable.java:424)
	at org.apache.cassandra.db.Table.getKeyRangeUnsafe(Table.java:912)
	at org.apache.cassandra.db.Table.getKeyRange(Table.java:883)
	at org.apache.cassandra.service.RangeVerbHandler.doVerb(RangeVerbHandler.java:23)
	... 4 more

If a put a Thread.sleep() after the inserts or run only the get_key_range call all is ok.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19582,,,Wed May 13 22:37:45 UTC 2009,,,,,,,,,,"0|i0fxaf:",90991,,,,,Normal,,,,,,,,,,,,,,,,,"13/May/09 21:47;jbellis;fixed in CASSANDRA-161.  you need to update to the latest trunk.;;;","14/May/09 00:03;nk11;I always check out before submitting a bug.
And I forgot to specify, I tested with only one node.
But, I've tested on a slower machine back home and it did not reproduce... On another faster machine it reproduced each time!;;;","14/May/09 00:08;jbellis;you are right, there is still a bug here.;;;","14/May/09 06:37;jbellis;fixed better (patch posted to CASSANDRA-161);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug in Range.contains(Token),CASSANDRA-236,12428082,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,stuhood,stuhood,stuhood,17/Jun/09 16:37,16/Apr/19 17:33,22/Mar/23 14:57,18/Jun/09 00:09,0.4,,,,0,,,,,,"There is an off-by-one error in Range.contains(Token): the endpoint (right) of the range is always supposed to be exclusive, but it was inclusive in the non-wrapping case.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jun/09 16:37;stuhood;range.patch;https://issues.apache.org/jira/secure/attachment/12410910/range.patch",,,,,,,,,,,,,,1.0,stuhood,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19604,,,Thu Jun 18 12:35:21 UTC 2009,,,,,,,,,,"0|i0fxpb:",91058,,,,,Low,,,,,,,,,,,,,,,,,"17/Jun/09 16:37;stuhood;Fixes the issue and test.;;;","18/Jun/09 00:09;jbellis;committed, thanks!;;;","18/Jun/09 20:35;hudson;Integrated in Cassandra #112 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/112/])
    fix off-by-one error in Range.contains(Token): the endpoint (right) of the range is always supposed to be exclusive, but it was inclusive in the non-wrapping case.  patch by Stu Hood; reviewed by jbellis for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Supercolumn deserialization bug,CASSANDRA-255,12428810,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,25/Jun/09 05:18,16/Apr/19 17:33,22/Mar/23 14:57,26/Jun/09 05:07,0.4,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Jun/09 05:20;jbellis;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-255-add-asserts.txt;https://issues.apache.org/jira/secure/attachment/12411709/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-255-add-asserts.txt","25/Jun/09 05:20;jbellis;ASF.LICENSE.NOT.GRANTED--0002-add-tests-for-supercolumnfamily-removal-fix-bugs.txt;https://issues.apache.org/jira/secure/attachment/12411710/ASF.LICENSE.NOT.GRANTED--0002-add-tests-for-supercolumnfamily-removal-fix-bugs.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19609,,,Fri Jun 26 13:45:36 UTC 2009,,,,,,,,,,"0|i0fxtj:",91077,,,,,Normal,,,,,,,,,,,,,,,,,"25/Jun/09 05:19;jbellis;    (both the assert removal and the ""if"" removal in 02 are bug fixes.)
;;;","26/Jun/09 05:00;sandeep_tata;+1;;;","26/Jun/09 05:07;jbellis;committed;;;","26/Jun/09 21:45;hudson;Integrated in Cassandra #120 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/120/])
    add tests for supercolumnfamily removal; fix bugs.
(both the assert removal and the ""if"" removal are bug fixes.)

patch by jbellis; reviewed by Sandeep Tata for 
add asserts.  patch by jbellis; reviewed by Sandeep Tata for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug in count columns.,CASSANDRA-729,12446189,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,gasolwu,gasolwu,21/Jan/10 11:14,16/Apr/19 17:33,22/Mar/23 14:57,06/Feb/10 03:03,0.5,,,,0,,,,,,"same as thrift api (get_count).

Welcome to cassandra CLI.

Type 'help' or '?' for help. Type 'quit' or 'exit' to quit.
cassandra> connect localhost/9160
Connected to localhost/9160
cassandra> del Keyspace1.Standard1['1']
row removed.
cassandra> set Keyspace1.Standard1['1']['foo'] = 'foo value'
Value inserted.
cassandra> set Keyspace1.Standard1['1']['bar'] = 'bar value'
Value inserted.
cassandra> get Keyspace1.Standard1['1']
=> (column=foo, value=foo value, timestamp=1264043095206)
=> (column=bar, value=bar value, timestamp=1264043106184)
Returned 2 results.
cassandra> count Keyspace1.Standard1['1']
2 columns
cassandra> del Keyspace1.Standard1['1']['foo']
column removed.
cassandra> get Keyspace1.Standard1['1']       
=> (column=bar, value=bar value, timestamp=1264043106184)
Returned 1 results.
cassandra> count Keyspace1.Standard1['1']     
2 columns
cassandra>","debian lenny, sun jdk 1.6",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19840,,,Fri Feb 05 19:03:23 UTC 2010,,,,,,,,,,"0|i0g0pr:",91546,,,,,Low,,,,,,,,,,,,,,,,,"21/Jan/10 11:20;jbellis;this sounds like CASSANDRA-647, are you testing 0.5.0 final or an earlier release?;;;","21/Jan/10 11:25;gasolwu;yes, testing in 0.5.0-rc3 and 0.5.0 final.;;;","22/Jan/10 17:23;gasolwu;ColumnFamily.getSortedColumns() (return this.columns_) contains deleted column, it's odd.
i don't know how to fix.

CassandraServer.java
382:     Map<String, Collection<IColumn>> columnsMap = multigetColumns(commands, consistency_level);

        for (ReadCommand command: commands)
        {
            Collection<IColumn> columns = columnsMap.get(command.key);
            if(columns == null)
            {
               columnFamiliesMap.put(command.key, 0);
            }
            else
            {
394:            columnFamiliesMap.put(command.key, columns.size()); // contains removed column,
            }
        }
        return columnFamiliesMap;;;;","05/Feb/10 01:41;jbellis;Looks to me like this was fixed by the patch for CASSANDRA-703; can you verify that it works for you in the 0.5 branch?;;;","06/Feb/10 03:03;jbellis;lmorchard tested the 0.5 branch and reports that this bug is fixed now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug removing supercolumn,CASSANDRA-84,12422909,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,16/Apr/09 01:14,16/Apr/19 17:33,22/Mar/23 14:57,20/Apr/09 22:26,0.3,,,,0,,,,,,The code to create a SuperColumn tombstone has a bug.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Apr/09 03:09;jbellis;0001-make-remove-test-code-use-the-same-api-that-the-thri.patch;https://issues.apache.org/jira/secure/attachment/12405554/0001-make-remove-test-code-use-the-same-api-that-the-thri.patch","16/Apr/09 03:10;jbellis;0002-generate-supercolumn-tombstone-when-a-2-tuple-delete.patch;https://issues.apache.org/jira/secure/attachment/12405555/0002-generate-supercolumn-tombstone-when-a-2-tuple-delete.patch",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19541,,,Mon Apr 20 14:26:39 UTC 2009,,,,,,,,,,"0|i0fwrz:",90908,,,,,Normal,,,,,,,,,,,,,,,,,"16/Apr/09 07:51;sandeep_tata;Looks good to me.

Nits:
1. I'd rather use  !columnFamily.isSuper() instead of DatabaseDescriptor.getColumnFamilyType(cfName).equals(""Standard"")
2. Messages from testng assertions seem a little friendlier than plain java assertions.

;;;","16/Apr/09 09:53;jbellis;1. Good point.  I have a patch already changing this globally.

2. Noted, but I'm not editing that code here.;;;","20/Apr/09 22:26;jbellis;committed (a while ago, forgot to update jira);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BUGS.txt cites bugs which are now fixed,CASSANDRA-354,12432536,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,markr,markr,07/Aug/09 23:57,16/Apr/19 17:33,22/Mar/23 14:57,12/Aug/09 01:13,0.4,,Legacy/Documentation and Website,,0,,,,,,"BUGS.txt should only list bugs which are still bugs, not those which are fixed.

It lists CASSANDRA-208 which is now fixed (I think)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Aug/09 01:02;jbellis;354.patch;https://issues.apache.org/jira/secure/attachment/12415867/354.patch",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19648,,,Wed Aug 12 13:08:27 UTC 2009,,,,,,,,,,"0|i0fyfb:",91175,,,,,Low,,,,,,,,,,,,,,,,,"08/Aug/09 01:02;jbellis;patch to update BUGS.txt attached;;;","12/Aug/09 00:39;urandom;+1;;;","12/Aug/09 00:54;euphoria;lgtm;;;","12/Aug/09 01:13;jbellis;committed;;;","12/Aug/09 21:08;hudson;Integrated in Cassandra #165 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/165/])
    update BUGS.txt for 0.4
patch by jbellis; reviewed by Eric Evans and Michael Greene for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bug with test,CASSANDRA-2063,12496928,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,urandom,amit71,amit71,27/Jan/11 19:34,16/Apr/19 17:33,22/Mar/23 14:57,28/Jan/11 06:44,0.7.1,,,,0,,,,,,"when executing nosetests (e.g: nosetests test/system/test_avro_system.py), you get the following error:

    mod = load_module(part_fqname, fh, filename, desc)
  File ""/tmp/apache-cassandra-0.7.0-src/test/system/test_avro_system.py"", line 19
    from . import AvroTester
         ^
SyntaxError: invalid syntax

All *.py scripts should be changed to be ""from __init__ import (AvroTester)""    instead of ""from . import AvroTester""


",RHL. Python 2.4.3,,,,,,,,,,,,,,,,,,,,,,600,600,,0%,600,600,,,,,,,,,,,,,,,,,,,,"28/Jan/11 05:27;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2063-Python2.4-friendly-imports.txt;https://issues.apache.org/jira/secure/attachment/12469599/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2063-Python2.4-friendly-imports.txt",,,,,,,,,,,,,,1.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20422,,,Fri Jan 28 08:49:46 UTC 2011,,,,,,,,,,"0|i0g93j:",92904,,,,,Low,,,,,,,,,,,,,,,,,"28/Jan/11 06:06;jbellis;i'm kind of surprised that import is the only thing keeping 2.4 from running, but +1;;;","28/Jan/11 06:44;urandom;Yeah, same here.  Maybe OP didn't make it past this (I don't have 2.4 handy to test with).  Anyway, committed.;;;","28/Jan/11 07:04;hudson;Integrated in Cassandra-0.7 #222 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/222/])
    CASSANDRA-2063 Python2.4-friendly imports

Patch by eevans for CASSANDRA-2063
;;;","28/Jan/11 16:49;amit71;why is it surprising you?
when you do the test, you will face one more issue. i dont remember where
exactly, but in one of the parameter declaration there is an extra 'b'
outside the value (instad of ""a='value'"" its written ""a= b'value' "".
anyway, i suggest also to add script that will be executed in the beginning
of each and every test that will do the following:
1)  delete the DB.
2) kill the existed instance of Cassandra in case there is a file of
existing Casandra in root diretory.
3) delete the existing file that prevent the test from being ran.

I can tell u that for me it was really confusing, especially that not much
documentation is written on it.
Cheers,
Amit


;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug in calculating QUORUM,CASSANDRA-1487,12473700,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jigneshdhruv,jigneshdhruv,jigneshdhruv,10/Sep/10 00:34,16/Apr/19 17:33,22/Mar/23 14:57,11/Sep/10 09:00,0.7 beta 2,,,,0,,,,,,"Hello,

It seems that there is a bug in calculating QUORUM in src/java/org/apache/cassandra/service/QuorumResponseHandler.java

Currently the QUORUM formula in place will return correct QUORUM if replication factor <= 3. However if you have a Replication Factor > 3, it will return incorrect result.

-----------------------
--- src/java/org/apache/cassandra/service/QuorumResponseHandler.java    (revision 995482)
+++ src/java/org/apache/cassandra/service/QuorumResponseHandler.java    (working copy)
@@ -109,7 +109,7 @@
             case ANY:
                 return 1;
             case QUORUM:
-                return (DatabaseDescriptor.getQuorum(table)/ 2) + 1;
+                return DatabaseDescriptor.getQuorum(table);
             case ALL:
                 return DatabaseDescriptor.getReplicationFactor(table);
             default:
-------------------
In QuorumResponseHandler:determineBlockFor()
DatabaseDescriptor.getQuorum(table) is already returning a quorum value which is further divided by 2 and a one is added.

So say if your RF=6, it is suppose to check 4 replicas, (6/2)+1=4 but it ends up checking only 3 replicas as DatabaseDescriptor.getQuorum returns 4, so determineBlockFor will return (4/2)+1=3.

Let me know if you have any questions.

Jignesh",,scode,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Sep/10 04:20;jigneshdhruv;QuorumResponseHandler.java.patch;https://issues.apache.org/jira/secure/attachment/12454235/QuorumResponseHandler.java.patch",,,,,,,,,,,,,,1.0,jigneshdhruv,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20163,,,Sat Sep 11 01:00:11 UTC 2010,,,,,,,,,,"0|i0g5d3:",92299,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"10/Sep/10 03:34;kingryan;You should attach patches rather than pasting them.;;;","10/Sep/10 04:20;jigneshdhruv;QuoromResponseHandler Patch.;;;","10/Sep/10 04:21;jigneshdhruv;Attached patch for QuorumResponseHandler.java.;;;","11/Sep/10 09:00;jbellis;committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
getRestrictedRanges bug where node owns minimum token,CASSANDRA-1901,12494015,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,stuhood,jbellis,jbellis,24/Dec/10 07:44,16/Apr/19 17:33,22/Mar/23 14:57,30/Dec/10 11:38,0.6.9,0.7.0,,,0,,,,,,"From the ML, there are two RF=1 nodes, 0 for the local node (17.224.36.17) and 85070591730234615865843651857942052864 for the remote node (17.224.109.80).  Debug log shows

{code}
DEBUG [pool-1-thread-4] 2010-12-23 12:54:26,958 CassandraServer.java (line 479) range_slice
DEBUG [pool-1-thread-4] 2010-12-23 12:54:26,958 StorageProxy.java (line 412) RangeSliceCommand{keyspace='Harvest', column_family='TestCentroids', super_column=null, predicate=SlicePredicate(slice_range:SliceRange(start:80 01 00 01 00 00 00 10 67 65 74 5F 72 61 6E 67 65 5F 73 6C 69 63 65 73 00 00 00 0C 0C 00 01 0B 00 03 00 00 00 0D 54 65 73 74 43 65 6E 74 72 6F 69 64 73 00 0C 00 02 0C 00 02 0B 00 01 00 00 00 00, finish:80 01 00 01 00 00 00 10 67 65 74 5F 72 61 6E 67 65 5F 73 6C 69 63 65 73 00 00 00 0C 0C 00 01 0B 00 03 00 00 00 0D 54 65 73 74 43 65 6E 74 72 6F 69 64 73 00 0C 00 02 0C 00 02 0B 00 01 00 00 00 00 0B 00 02 00 00 00 00, reversed:false, count:1)), range=[0,0], max_keys=11}
DEBUG [pool-1-thread-4] 2010-12-23 12:54:26,958 StorageProxy.java (line 597) restricted ranges for query [0,0] are [[0,0]]
DEBUG [pool-1-thread-4] 2010-12-23 12:54:26,959 StorageProxy.java (line 423) === endpoint: belize1.apple.com/17.224.36.17 for range.right 0
{code}

Thus, node 85070591730234615865843651857942052864 is left out.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Dec/10 02:19;stuhood;0.6-0001-Switch-minimum-token-for-RP-to-1-for-midpoint-purposes.txt;https://issues.apache.org/jira/secure/attachment/12467053/0.6-0001-Switch-minimum-token-for-RP-to-1-for-midpoint-purposes.txt","28/Dec/10 15:48;stuhood;0001-Switch-minimum-token-for-RP-to-1-for-midpoint-purposes.txt;https://issues.apache.org/jira/secure/attachment/12467031/0001-Switch-minimum-token-for-RP-to-1-for-midpoint-purposes.txt",,,,,,,,,,,,,2.0,stuhood,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20362,,,Thu Dec 30 03:38:32 UTC 2010,,,,,,,,,,"0|i0g83z:",92744,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"24/Dec/10 07:48;jbellis;I'm guessing this is related to the special cases on minimum tokens -- should we just disallow using that in a live node token?;;;","25/Dec/10 07:53;jbellis;Mike reports,

bq. it looks like the workaround of using an initial token of 1 works;;;","25/Dec/10 14:15;stuhood;The problem is that the minimum token plays double duty by being ""less than all possible tokens"", while also being a valid token for RP (not for OPP). Changing the minimum token for RP to -1 (which is impossible to generate any other way) might help minimize these bugs. I started playing around with this tonight, but need to adjust the midpoint calculation code a little bit. I'd like to avoid disallowing 0, so I'll try to attach a better solution before the end of the weekend.;;;","28/Dec/10 15:48;stuhood;Switches the minimum token for RP to -1. Fixes the case mentioned on the mailing list, but hasn't been subjected to the same scrutiny as the OPPs get in StorageProxyTest: the singletons make this excessively difficult.;;;","28/Dec/10 15:59;stuhood;Patch tested against trunk, but should apply cleanly to 0.6/0.7;;;","29/Dec/10 00:34;jbellis;I get

{noformat}
patching file src/java/org/apache/cassandra/dht/RandomPartitioner.java
...
Hunk #2 FAILED at 67.
{noformat}

against 0.6;;;","29/Dec/10 02:19;stuhood;Rebased for 0.6;;;","30/Dec/10 07:24;hudson;Integrated in Cassandra-0.6 #43 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/43/])
    change RandomPartitioner mintoken to -1 to avoid collision w/
tokens in the ring
patch by Stu Hood; reviewed by jbellis for CASSANDRA-1901
;;;","30/Dec/10 11:38;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A bug in BufferedRandomAccessFile,CASSANDRA-2213,12499317,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,leojay,leojay,leojay,22/Feb/11 17:15,16/Apr/19 17:33,22/Mar/23 14:57,23/Feb/11 02:04,0.7.3,,,,0,,,,,,"The first line of BufferedRandomAccessFile.readAtMost is
{code}if (length >= bufferEnd && hitEOF){code}

I think It should be "">"" instead of "">="",
Here is a test for this:{code}
    @Test
    public void testRead() throws IOException {
        File tmpFile = File.createTempFile(""readtest"", ""bin"");
        tmpFile.deleteOnExit();

        // Create the BRAF by filename instead of by file.
        BufferedRandomAccessFile rw = new BufferedRandomAccessFile(tmpFile.getPath(), ""rw"");
        rw.write(new byte[] {1});

        rw.seek(0);
        byte[] buffer = new byte[1];
        assert rw.read(buffer) == 1;
        assert buffer[0] == 1;
}
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,leojay,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20509,,,Tue Feb 22 18:57:54 UTC 2011,,,,,,,,,,"0|i0ga0f:",93052,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"23/Feb/11 02:04;jbellis;committed, thanks!;;;","23/Feb/11 02:57;hudson;Integrated in Cassandra-0.7 #304 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/304/])
    avoid EOFing on requests for the last bytes in a file
patch by Leo Jay; reviewed by jbellis for CASSANDRA-2213
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bug in CollatedOrderPreservingPartitioner.midpoint,CASSANDRA-519,12439223,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,stuhood,jbellis,jbellis,28/Oct/09 05:30,16/Apr/19 17:33,22/Mar/23 14:57,08/Dec/09 05:17,0.5,,,,0,,,,,,"            assert FBUtilities.isEqualBits(MINIMUM.token, rbytes);

fails frequently when running the test added for CASSANDRA-517.  just revert patch 03 and it will fail several times out of 10 test runs (when range3 is a wrapping range, presumably)",,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Dec/09 10:02;stuhood;519-biginteger-midpoint.diff;https://issues.apache.org/jira/secure/attachment/12427084/519-biginteger-midpoint.diff",,,,,,,,,,,,,,1.0,stuhood,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19733,,,Wed Dec 09 12:42:41 UTC 2009,,,,,,,,,,"0|i0fzfj:",91338,,,,,Low,,,,,,,,,,,,,,,,,"28/Oct/09 05:55;stuhood;IPartitioner.midpoint() doesn't currently require implementations to support wrapping ranges. I'll get to this eventually, but it doesn't look like midpoint is going to be necessary/helpful for 192, like I originally thought.;;;","02/Dec/09 07:30;stuhood;I'll try and get to this one this week.;;;","06/Dec/09 10:02;stuhood;The implementations of midpoint in RPP and OPP were fairly optimized, but we would have needed to reimplement more of the operations in BigInteger in order to support wrapping ranges.

Instead, this patch reuses the BigInteger based midpoint implementation, and moves it into FBUtilities.midpoint. The partitioners convert their tokens into bit arrays represented as BigIntegers.;;;","06/Dec/09 10:08;stuhood;Er, s/RPP/COPP/ in the previous comment.;;;","08/Dec/09 05:17;jbellis;committed;;;","09/Dec/09 20:42;hudson;Integrated in Cassandra #282 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/282/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hyphenated index names cause problems,CASSANDRA-2196,12499061,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,sgpope,sgpope,19/Feb/11 00:16,16/Apr/19 17:33,22/Mar/23 14:57,26/Feb/11 09:19,0.7.3,,,,0,,,,,,"When inserting a large number of entries with batch_insert (100000) using thrift compiled into C# there's a NumberFormatException that occurs.

The first logged entry that tipped me off was this:
 INFO 10:53:52,171 Writing Memtable-TransactionLogs.client-hostname@350930888(1171371 bytes, 32787 o
perations)
ERROR 10:53:52,171 Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.NumberFormatException: For input string: ""tmp""
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NumberFormatException: For input string: ""tmp""
        at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)
        at java.lang.Integer.parseInt(Integer.java:449)
        at java.lang.Integer.parseInt(Integer.java:499)
        at org.apache.cassandra.io.sstable.Descriptor.fromFilename(Descriptor.java:154)
        at org.apache.cassandra.io.sstable.Descriptor.fromFilename(Descriptor.java:119)
        at org.apache.cassandra.io.sstable.SSTableWriter.<init>(SSTableWriter.java:67)
        at org.apache.cassandra.db.Memtable.writeSortedContents(Memtable.java:156)
        at org.apache.cassandra.db.Memtable.access$000(Memtable.java:49)
        at org.apache.cassandra.db.Memtable$1.runMayThrow(Memtable.java:174)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more

 Which points to the suspect piece of code in Descriptor.java:154 (browse at https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/io/sstable/Descriptor.java)

 The file I believe it's trying to parse is mentioned in my logs as:

INFO 10:51:31,231 Compacted to C:\cassandra\apache-cassandra-0.7.2\bin\..\Storage\data\system\Index
Info-tmp-f-6-Data.db.  384 to 225 (~58% of original) bytes for 1 keys.  Time: 281ms.

 I'm new here, so I'm not sure what needs fixing here (the filename, or the parsing of it).","Cassandra 0.7.2

Windows 7 64-bit
java version ""1.6.0_23""
Java(TM) SE Runtime Environment (build 1.6.0_23-b05)
Java HotSpot(TM) 64-Bit Server VM (build 19.0-b09, mixed mode)
",,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,"19/Feb/11 03:12;jbellis;2196.txt;https://issues.apache.org/jira/secure/attachment/12471434/2196.txt","26/Feb/11 07:32;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2196-invoke-toString-instead-of-casting.txt;https://issues.apache.org/jira/secure/attachment/12471994/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2196-invoke-toString-instead-of-casting.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20496,,,Sat Feb 26 02:40:31 UTC 2011,,,,,,,,,,"0|i0g9wn:",93035,,gdusbabek,,gdusbabek,Low,,,,,,,,,,,,,,,,,"19/Feb/11 00:49;jbellis;The bug is that it shouldn't be trying to open a file with ""tmp"" in the name.

I wonder if this is another one of those bugs where we have unix-oriented assumptions about opened files, that don't hold under Windows.  Can you doublecheck that there are no earlier ERROR or WARN lines in the log?;;;","19/Feb/11 00:56;sgpope;No errors before that one, and the only warning before that is:
 WARN 10:50:25,748 Generated random token Token(bytes[c010b410364388921ed82a633849a3cc]). Random tok
ens will result in an unbalanced ring; see http://wiki.apache.org/cassandra/Operations
;;;","19/Feb/11 02:32;jbellis;I see the problem. Your CF is named client-hostname, but - is an illegal character in columnfamily names.  Apparently the code that checks that got broken at some point.  How did you create the CF, through the CLI?;;;","19/Feb/11 02:43;sgpope;I'm confused. My CFs are TransactionLogs and Terms. The CF in the log entry is the system-generated one. I created my CFs in code.;;;","19/Feb/11 02:52;jbellis;Ah, I thought ""TransactionLogs.client-hostname"" was KS.CF but it must be CF.indexname. ;;;","19/Feb/11 03:00;sgpope;Sorry, yeah. I should've mentioned that client-hostname is one of my indexes.;;;","19/Feb/11 03:12;jbellis;patch to keep invalid characters out of index names.

if you can afford to lose the data the easiest fix for this CF is to drop and recreate it.;;;","19/Feb/11 03:15;sgpope;I can afford to lose it. Thanks!;;;","19/Feb/11 03:28;gdusbabek;+1;;;","19/Feb/11 04:56;hudson;Integrated in Cassandra-0.7 #293 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/293/])
    validate index namesfor \w+
patch by jbellis; reviewed by gdusbabek for CASSANDRA-2196
;;;","22/Feb/11 05:15;jbellis;committed;;;","26/Feb/11 07:31;urandom;r1072123 broke the CQL system tests with the following logged exception:

{noformat}
java.lang.ClassCastException: org.apache.avro.util.Utf8 cannot be cast to java.lang.String
        at org.apache.cassandra.db.migration.UpdateColumnFamily.<init>(UpdateColumnFamily.java:52)
        at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:638)
        at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1209)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.process(Cassandra.java:4576)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:3235)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:188)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
{noformat}

(Trivial )patch attached.;;;","26/Feb/11 07:37;gdusbabek;+1;;;","26/Feb/11 10:40;hudson;Integrated in Cassandra #746 (See [https://hudson.apache.org/hudson/job/Cassandra/746/])
    invoke toString() instead of casting

Patch by eevans; reviewed by gdusbabek for CASSANDRA-2196
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug with get_range_slices,CASSANDRA-1505,12474185,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,aimran,aimran,15/Sep/10 23:36,16/Apr/19 17:33,22/Mar/23 14:57,15/Sep/10 23:51,,,,,0,,,,,,"I was testing cassandra and trying to get iterate over a bunch of rows using the get_range_slices method.
In a loop I create small amount of data with this code:
for(int i=0;i<14;i++){
  long timestamp = System.currentTimeMillis();
  ColumnPath colPathName = new ColumnPath(CF_NAME);
  colPathName.setColumn(propName.getBytes(UTF8));
  client.insert(CASSANDRA_KEYSPACE, ""my key ""+i, colPathName, value, timestamp, ConsistencyLevel.ONE);
}

Next I try to iterate over the rows using this code:
Map<String, Map<String, byte[]>> rows = cs.getRows("""", fetchSize);
		//System.out.println(rows.size()+""-ROWS=""+rows);
				
		while(!rows.isEmpty()){
			Iterator<String> it = rows.keySet().iterator();
			while(it.hasNext()){
				String key = it.next();
				System.out.println(key+""-printing row:""+rows.get(key));
				if(!it.hasNext()){
					System.out.println(""reached end of current page. getting next page:""+key);
					rows = cs.getRows(key, fetchSize);
					System.out.println(""obtained new row:""+rows);
				}
			}
		}

the getrows method basically calls the get slice, for the key and count passed [I have set the predicate as per the example posted on the cassandra wiki]:
ColumnParent parent = new ColumnParent(CF_NAME);
			List<ColumnOrSuperColumn> results = client.get_slice(CASSANDRA_KEYSPACE, key, parent, predicate, ConsistencyLevel.ONE);

This goes in an endless loop. The keys are returned randomly and I am never able to successfully finish the iteration.

Here's the output. It seems to move forward a little and them goes in a wild loop:
total slices:2
my key 0-printing row:{description=[B@1a786c3}
my key 1-printing row:{description=[B@18088c0}
reached end of current page. getting next page:my key 1
total slices:2
obtained new row:{my key 1={description=[B@1922221}, my key 10={description=[B@fec107}}
my key 1-printing row:{description=[B@1922221}
my key 10-printing row:{description=[B@fec107}
reached end of current page. getting next page:my key 10
total slices:2
obtained new row:{my key 10={description=[B@132e13d}, my key 11={description=[B@1617189}}
my key 10-printing row:{description=[B@132e13d}
my key 11-printing row:{description=[B@1617189}
reached end of current page. getting next page:my key 11
total slices:2
obtained new row:{my key 12={description=[B@64f6cd}, my key 11={description=[B@872380}}
my key 12-printing row:{description=[B@64f6cd}
my key 11-printing row:{description=[B@872380}
reached end of current page. getting next page:my key 11
total slices:2
obtained new row:{my key 12={description=[B@2bb514}, my key 11={description=[B@17d5d2a}}
my key 12-printing row:{description=[B@2bb514}
my key 11-printing row:{description=[B@17d5d2a}
reached end of current page. getting next page:my key 11
total slices:2
obtained new row:{my key 12={description=[B@16fa474}, my key 11={description=[B@95c083}}
my key 12-printing row:{description=[B@16fa474}
my key 11-printing row:{description=[B@95c083}
reached end of current page. getting next page:my key 11
total slices:2
obtained new row:{my key 12={description=[B@191d8c1}, my key 11={description=[B@2d9c06}}
my key 12-printing row:{description=[B@191d8c1}
my key 11-printing row:{description=[B@2d9c06}
reached end of current page. getting next page:my key 11
total slices:2
obtained new row:{my key 12={description=[B@5e5a50}, my key 11={description=[B@7b6889}}
my key 12-printing row:{description=[B@5e5a50}
my key 11-printing row:{description=[B@7b6889}
reached end of current page. getting next page:my key 11
total slices:2
.
.
.
Shouldn't iteration work normally like other common iteration patterns or do I need to do things differently with casssandra to get iteration working?",Java 1.6.0_21,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-1442,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20169,,,Wed Sep 15 15:51:29 UTC 2010,,,,,,,,,,"0|i0g5h3:",92317,,,,,Normal,,,,,,,,,,,,,,,,,"15/Sep/10 23:51;jbellis;dupe of CASSANDRA-1442;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bugs in tombstone handling in remove code,CASSANDRA-33,12421770,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,01/Apr/09 21:47,16/Apr/19 17:33,22/Mar/23 14:57,21/Apr/09 00:39,0.3,,,,0,,,,,,"[copied from dev list]

Avinash pointed out two bugs in my remove code.  One is easy to fix,
the other is tougher.

The easy one is that my code removes tombstones (deletion markers) at
the ColumnFamilyStore level, so when CassandraServer does read repair
it will not know about the tombstones and they will not be replicated
correctly.  This can be fixed by simply moving the removeDeleted call
up to just before CassandraServer's final return-to-client.

The hard one is that tombstones are problematic on GC (that is, major
compaction of SSTables, to use the Bigtable paper terminology).

One failure scenario: Node A, B, and C replicate some data.  C goes
down.  The data is deleted.  A and B delete it and later GC it.  C
comes back up.  C now has the only copy of the data so on read repair
the stale data will be sent to A and B.

A solution: pick a number N such that we are confident that no node
will be down (and catch up on hinted handoffs) for longer than N days.
 (Default value: 10?)  Then, no node may GC tombstones before N days
have elapsed.  Also, after N days, tombstones will no longer be read
repaired.  (This prevents a node which has not yet GC'd from sending a
new tombstone copy to a node that has already GC'd.)

Implementation detail: we'll need to add a 32-bit ""time of tombstone""
to ColumnFamily and SuperColumn.  (For Column we can stick it in the
byte[] value, since we already have an unambiguous way to know if the
Column is in a deleted state.)  We only need 32 bits since the time
frame here is sufficiently granular that we don't need ms.  Also, we
will use the system clock for these values, not the client timestamp,
since we don't know what the source of the client timestamps is.

Admittedly this is suboptimal compared to being able to GC immediately
but it has the virtue of being (a) easily implemented, (b) with no
extra components such as a coordination protocol, and (c) better than
not GCing tombstones at all (the other easy way to ensure
correctness).",,hammer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-34,CASSANDRA-29,CASSANDRA-87,,,,,,,,,,,,,"17/Apr/09 23:20;jbellis;0001-preserve-tombstones-until-a-GC-grace-period-has-elap.patch;https://issues.apache.org/jira/secure/attachment/12405773/0001-preserve-tombstones-until-a-GC-grace-period-has-elap.patch","17/Apr/09 23:20;jbellis;0002-omit-tombstones-from-column_t-and-supercolumn_t-retu.patch;https://issues.apache.org/jira/secure/attachment/12405774/0002-omit-tombstones-from-column_t-and-supercolumn_t-retu.patch","17/Apr/09 23:53;jbellis;0003-make-GC_GRACE_IN_SECONDS-customizable-in-storage.con.patch;https://issues.apache.org/jira/secure/attachment/12405779/0003-make-GC_GRACE_IN_SECONDS-customizable-in-storage.con.patch","18/Apr/09 03:10;jbellis;0004-and-5-v2.patch;https://issues.apache.org/jira/secure/attachment/12405798/0004-and-5-v2.patch","18/Apr/09 02:49;junrao;0004_expose_remove_bug.patch;https://issues.apache.org/jira/secure/attachment/12405793/0004_expose_remove_bug.patch","18/Apr/09 02:50;junrao;0005_fix_exposed_remove_bug.patch;https://issues.apache.org/jira/secure/attachment/12405795/0005_fix_exposed_remove_bug.patch","18/Apr/09 09:11;junrao;0006_fix_sequencefile_bug.patch;https://issues.apache.org/jira/secure/attachment/12405832/0006_fix_sequencefile_bug.patch","21/Apr/09 00:27;junrao;0007_fix_another_sequencefile_bug.patch;https://issues.apache.org/jira/secure/attachment/12405946/0007_fix_another_sequencefile_bug.patch",,,,,,,8.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19521,,,Mon Apr 20 16:39:34 UTC 2009,,,,,,,,,,"0|i0fwh3:",90859,,,,,Normal,,,,,,,,,,,,,,,,,"10/Apr/09 22:53;jbellis;Moving the tombstone removal up to the storageproxy level so read repair can see them first blocks on Avinash's multiget changes.;;;","17/Apr/09 23:24;jbellis;MultiGet doesn't seem to be coming any time soon.  Guess we'll just have to deal with conflict resolution when it does.

Re the patches provided, they follow the outline above except that it turns out we can just use a single removeDeleted to handle both tombstones (which still supress old data ""below"" them in the tree, e.g., a deleted supercolumn does not need to keep its subcolumn data around) and GC.  So CFS and compaction can still calls removeDeleted, and then CassandraServer just has to remove the tombstones themselves in thriftifyColumns and thriftifySuperColumns.

Patch to make GC_GRACE_IN_SECONDS configurable forthcoming.;;;","17/Apr/09 23:53;jbellis;added configuration patch.;;;","18/Apr/09 02:49;junrao;Haven't looked at the patch in details, but found another related remove bug. Patch 0004 adds two test cases and one of them testRemoveColumn1() exposes the problem and will fail.
;;;","18/Apr/09 02:50;junrao;Attach a patch that fixes the problem exposed in 0004 patch.;;;","18/Apr/09 02:52;junrao;Another issue is that if you look at testRemoveColumn1() and testRemoveColumn2(), retrieved.getColumn(""Column1"") returns null. I am wondering how that affects read repairs.

Also, you need to patch test/conf/storage-conf.xml for GC_GRACE_IN_SECONDS.
;;;","18/Apr/09 02:59;jbellis;+1 for Jun's patches.

IMO test/conf is fine using the default value.;;;","18/Apr/09 03:10;jbellis;fixed tests to make it more obvious what should be happening (in 4-and-5-v2).;;;","18/Apr/09 03:11;jbellis;we do need the ability to read-repair CF and SC tombstones.  I'll open a separate ticket for that.

the reason the behavior retrieved.getColumn(""Column1"") == null is correct is, if we've deleted the CF (more recently than the column!) then we don't care what data used to be there, it should just be gone.  what we need to RR is the CF tombstone.

of course if there is a column tombstone _more_ recent than the CF tombstone then it should be preserved.;;;","18/Apr/09 03:19;urandom;+1 to 0001-0003 and 0004-and-5-v2.patch;;;","18/Apr/09 04:09;jbellis;committed;;;","18/Apr/09 09:11;junrao;There is a serious bug in SequenceFille.java with the current fix. Attach a patch in 0006.

Unfortunately, none of the unit test captures the fact that many fields such as localDeletionTime, markedForDeleteAt, totalNumOfCols were read incorrectly. Existing test cases work just by luck.;;;","18/Apr/09 09:11;junrao;reopens the issue because of the new bug found.;;;","18/Apr/09 09:33;jbellis;applied, thanks for catching that;;;","18/Apr/09 09:42;jbellis;created CASSANDRA-89 to remind us to add a test covering these code paths;;;","21/Apr/09 00:27;junrao;Include patch for another bug in SequeceFile.java where the size of the row is not calculated correctly. At this moment, this bug is not exposed.since the rowkey and the size of row written to outBuf were simply read and discarded in SSTable.next().

We should probably open another issue to clean up the code such that the row key and row size are not written to outBuf.
;;;","21/Apr/09 00:28;junrao;reopen the issue because of the new bug found.;;;","21/Apr/09 00:39;jbellis;committed.

> We should probably open another issue to clean up the code such that the row key and row size are not written to outBuf. 

it's not clear to me what the right cleanup is for this specific piece since the context is so messy. :(;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ByteBuffer bug in ExpiringColumn.updateDigest() ,CASSANDRA-1679,12478619,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,tjake,tjake,tjake,29/Oct/10 04:40,16/Apr/19 17:33,22/Mar/23 14:57,29/Oct/10 05:31,0.7.0 rc 1,,,,0,,,,,,"The MessageDigest calls in ExpringColumn change the position of the bytebuffer.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Oct/10 04:41;tjake;1679_v1.txt;https://issues.apache.org/jira/secure/attachment/12458274/1679_v1.txt",,,,,,,,,,,,,,1.0,tjake,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20255,,,Thu Oct 28 21:31:22 UTC 2010,,,,,,,,,,"0|i0g6pz:",92519,,,,,Normal,,,,,,,,,,,,,,,,,"29/Oct/10 05:31;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
commitlog recover bug,CASSANDRA-1297,12469585,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jftan,jftan,jftan,19/Jul/10 17:02,16/Apr/19 17:33,22/Mar/23 14:57,19/Jul/10 20:42,0.6.4,,,,0,,,,,,"class CommitLog.java
when recover log files;
 if one log  file have no dirty , process is break;
{quote}
199  int lowPos = CommitLogHeader.getLowestPosition(clHeader);
 200 if (lowPos == 0)
 201    break;
{quote}

why not continue and read next log file
{quote}
199  int lowPos = CommitLogHeader.getLowestPosition(clHeader);
200 if (lowPos == 0)\{
201   reader.close();
202   continue;
203  \}
{quote}

i am not very sure about that. how can answer?


",,,,,,,,,,,,,,,,,,,,,,,259200,259200,,0%,259200,259200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jftan,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20066,,,Mon Jul 19 12:42:13 UTC 2010,,,,,,,,,,"0|i0g47b:",92111,,,,,Critical,,,,,,,,,,,,,,,,,"19/Jul/10 20:42;jbellis;you're right, it should continue to the next segment.  fixed in r965457.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
error with utf8 columnfamilies,CASSANDRA-493,12438228,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,16/Oct/09 02:44,16/Apr/19 17:33,22/Mar/23 14:57,20/Oct/09 08:29,0.4,,,,0,,,,,,"From the mailing list:

ERROR [ROW-MUTATION-STAGE:2935] 2009-10-15 17:32:52,518
DebuggableThreadPoolExecutor.java (line 85) Error in
ThreadPoolExecutor
java.lang.IllegalArgumentException: The name should match the name of
the current column or super column
       at org.apache.cassandra.db.SuperColumn.putColumn(SuperColumn.java:208)
       at org.apache.cassandra.db.ColumnFamily.addColumn(ColumnFamily.java:200)
       at org.apache.cassandra.db.ColumnFamily.addColumns(ColumnFamily.java:127)
       at org.apache.cassandra.db.Memtable.resolve(Memtable.java:156)
       at org.apache.cassandra.db.Memtable.put(Memtable.java:139)
       at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:450)
       at org.apache.cassandra.db.Table.apply(Table.java:608)
       at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:205)
       at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:79)
       at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:39)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:619)
ERROR [ROW-MUTATION-STAGE:2935] 2009-10-15 17:32:52,519
CassandraDaemon.java (line 71) Fatal exception in thread
Thread[ROW-MUTATION-STAGE:2935,5,main]
java.lang.IllegalArgumentException: The name should match the name of
the current column or super column
       at org.apache.cassandra.db.SuperColumn.putColumn(SuperColumn.java:208)
       at org.apache.cassandra.db.ColumnFamily.addColumn(ColumnFamily.java:200)
       at org.apache.cassandra.db.ColumnFamily.addColumns(ColumnFamily.java:127)
       at org.apache.cassandra.db.Memtable.resolve(Memtable.java:156)
       at org.apache.cassandra.db.Memtable.put(Memtable.java:139)
       at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:450)
       at org.apache.cassandra.db.Table.apply(Table.java:608)
       at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:205)
       at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:79)
       at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:39)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:619)

Stopping and starting the cluster gives me something similar:

ERROR - Error in executor futuretask
java.util.concurrent.ExecutionException:
java.lang.IllegalArgumentException: The name should match the name of
the current column or super column
at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
at java.util.concurrent.FutureTask.get(FutureTask.java:83)
at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.logFutureExceptions(DebuggableThreadPoolExecutor.java:95)
at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor.afterExecute(DebuggableScheduledThreadPoolExecutor.java:50)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.IllegalArgumentException: The name should match
the name of the current column or super column
at org.apache.cassandra.db.SuperColumn.putColumn(SuperColumn.java:208)
at org.apache.cassandra.db.ColumnFamily.addColumn(ColumnFamily.java:200)
at org.apache.cassandra.db.ColumnFamily.addColumns(ColumnFamily.java:127)
at org.apache.cassandra.db.ColumnFamily.resolve(ColumnFamily.java:408)
at org.apache.cassandra.db.ColumnFamilyStore.merge(ColumnFamilyStore.java:477)
at org.apache.cassandra.db.ColumnFamilyStore.doFileCompaction(ColumnFamilyStore.java:1078)
at org.apache.cassandra.db.ColumnFamilyStore.doCompaction(ColumnFamilyStore.java:689)
at org.apache.cassandra.db.MinorCompactionManager$1.call(MinorCompactionManager.java:165)
at org.apache.cassandra.db.MinorCompactionManager$1.call(MinorCompactionManager.java:162)
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
at java.util.concurrent.FutureTask.run(FutureTask.java:138)
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:207)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
... 2 more",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Oct/09 03:53;jbellis;493.patch;https://issues.apache.org/jira/secure/attachment/12422396/493.patch","16/Oct/09 02:45;jbellis;493.patch;https://issues.apache.org/jira/secure/attachment/12422258/493.patch",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19719,,,Tue Oct 20 00:29:42 UTC 2009,,,,,,,,,,"0|i0fz9r:",91312,,,,,Normal,,,,,,,,,,,,,,,,,"16/Oct/09 02:45;jbellis;Fixes using non-utf8-aware comparison as a sanity check in SC.putColumn (to make sure we're actually merging the same column name);;;","16/Oct/09 06:58;urandom;This patch causes several of the tests to fail.

    [junit] Testcase: testRemoveSuperColumnWithNewData(org.apache.cassandra.db.RemoveSuperColumnTest):	Caused an ERROR
    [junit] null
    [junit] java.nio.BufferUnderflowException
    [junit] 	at java.nio.Buffer.nextGetIndex(Buffer.java:497)
    [junit] 	at java.nio.HeapByteBuffer.getLong(HeapByteBuffer.java:406)
    [junit] 	at org.apache.cassandra.db.marshal.LongType.compare(LongType.java:40)
    [junit] 	at org.apache.cassandra.db.marshal.LongType.compare(LongType.java:27)
    [junit] 	at org.apache.cassandra.db.SuperColumn.putColumn(SuperColumn.java:206)
    [junit] 	at org.apache.cassandra.db.ColumnFamily.addColumn(ColumnFamily.java:200)
    [junit] 	at org.apache.cassandra.db.ColumnFamily.addAll(ColumnFamily.java:126)
    [junit] 	at org.apache.cassandra.db.Memtable.resolve(Memtable.java:162)
    [junit] 	at org.apache.cassandra.db.Memtable.put(Memtable.java:145)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:442)
    [junit] 	at org.apache.cassandra.db.Table.apply(Table.java:466)
    [junit] 	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:205)
    [junit] 	at org.apache.cassandra.db.RemoveSuperColumnTest.testRemoveSuperColumnWithNewData(RemoveSuperColumnTest.java:105);;;","16/Oct/09 07:04;jbellis;looks like those tests are using invalid supercolumn names and we weren't catching that before.  will fix.;;;","17/Oct/09 03:53;jbellis;My patch was at fault after all.  (Who woulda guessed. :)  The problem is that SC.getComparator is the comparator for the subcolumns, not the SC and its peers.  There is no easy way for the SC to get the comparator for its peers, so this patch just leaves that assert out.;;;","20/Oct/09 08:29;jbellis;From the mailing list:

> I'm no longer running into the issue anymore. [with the patch]

committed to 0.4 and trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
trivial bug in Marshal comparators,CASSANDRA-319,12431533,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,28/Jul/09 04:04,16/Apr/19 17:33,22/Mar/23 14:57,29/Jul/09 04:50,,,,,0,,,,,,"comparing X to [] where X is not [] should be 1, not -1
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Jul/09 04:08;jbellis;319.patch;https://issues.apache.org/jira/secure/attachment/12414655/319.patch",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19634,,,Tue Jul 28 20:50:06 UTC 2009,,,,,,,,,,"0|i0fy7j:",91140,,,,,Normal,,,,,,,,,,,,,,,,,"28/Jul/09 04:08;jbellis;this patch also:

removes special-casing of [] in UTF8, since [] already deserializes to """"

removes TODOs hoping to r/m special-casing []; we'll always need that to support slice/finish sentinel values in get_slice;;;","28/Jul/09 04:13;stuhood;Assuming that it passes the rest of the unit tests, this patch looks alright. But rather than removing the TODOs, I would turn them into comments, since they document confusing behaviour.;;;","28/Jul/09 04:17;jbellis;committed with comment to AbstractType:

 * Note that empty byte[] are used to represent ""start at the beginning""
 * or ""stop at the end"" arguments to get_slice, so the Comparator
 * should always handle those values even if they normally do not
 * represent a valid byte[] for the type being compared.
;;;","29/Jul/09 04:50;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Equality problem in schema updates,CASSANDRA-1962,12495222,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,stuhood,stuhood,stuhood,11/Jan/11 17:53,16/Apr/19 17:33,22/Mar/23 14:57,11/Jan/11 22:24,0.7.1,,,,0,,,,,,"CFMetaData.apply uses equals to compare objects that are not the same class: this may work now, but we shouldn't rely on that behaviour.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Jan/11 17:54;stuhood;0001-Strings-never-equal-UTFs.txt;https://issues.apache.org/jira/secure/attachment/12467990/0001-Strings-never-equal-UTFs.txt",,,,,,,,,,,,,,1.0,stuhood,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20384,,,Tue Jan 11 17:45:36 UTC 2011,,,,,,,,,,"0|i0g8gv:",92802,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"11/Jan/11 18:21;slebresne;+1;;;","11/Jan/11 22:24;jbellis;committed;;;","12/Jan/11 01:45;hudson;Integrated in Cassandra-0.7 #151 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/151/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Secondary index and index expression problems,CASSANDRA-2406,12502972,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,muga_nishizawa,muga_nishizawa,31/Mar/11 11:18,16/Apr/19 17:33,22/Mar/23 14:57,18/Apr/11 21:28,0.7.5,,Feature/2i Index,,0,,,,,,"When I iteratively get data with secondary index and index clause, result of data acquired by consistency level ""one"" is different from the one by consistency level ""quorum"".  The one by consistecy level ""one"" is correct result.  But the one by consistecy level ""quorum"" is incorrect and is dropped by Cassandra.  

You can reproduce the bug by executing attached programs.

- 1. Start Cassandra cluster.  It consists of 3 cassandra nodes and distributes data by ByteOrderedPartitioner.  Initial tokens of those nodes are [""31"", ""32"", ""33""].  
- 2. Create keyspace and column family, according to ""create_table.cli"",
- 3. Execute ""secondary_index_insertv2.py"", inserting a few hundred columns to cluster
- 4. Execute ""secondary_index_checkv2.py"" and get data with secondary index and index clause iteratively.  ""secondary_index_insertv2.py"" and ""secondary_index_checkv2.py"" require pycassa.

You will be able to execute  4th ""secondary_index_checkv2.py"" script with following option so that 
you get data with consistency level ""one"".  

% python ""secondary_index_checkv2.py"" -one

On the other hand, to acquire data with consistency level ""quorum"", you will need to use following option.  

% python ""secondary_index_checkv2.py"" -quorum

You can check that result of data acquired by consistency level ""one"" is different from one by consistency level ""quorum"".  ","CentOS 5.5 (64bit), JDK 1.6.0_23",cherro,skamio,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/11 21:20;skamio;CASSANDRA-2406-debug.patch;https://issues.apache.org/jira/secure/attachment/12475709/CASSANDRA-2406-debug.patch","13/Apr/11 21:20;xedin;CASSANDRA-2406.patch;https://issues.apache.org/jira/secure/attachment/12476234/CASSANDRA-2406.patch","31/Mar/11 11:19;muga_nishizawa;create_table.cli;https://issues.apache.org/jira/secure/attachment/12475053/create_table.cli","08/Apr/11 10:52;skamio;node-1.system.log;https://issues.apache.org/jira/secure/attachment/12475775/node-1.system.log","31/Mar/11 11:20;muga_nishizawa;secondary_index_checkv2.py;https://issues.apache.org/jira/secure/attachment/12475055/secondary_index_checkv2.py","31/Mar/11 11:20;muga_nishizawa;secondary_index_insertv2.py;https://issues.apache.org/jira/secure/attachment/12475054/secondary_index_insertv2.py",,,,,,,,,6.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20604,,,Mon Apr 18 04:51:21 UTC 2011,,,,,,,,,,"0|i0gb6n:",93242,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"07/Apr/11 21:20;skamio;an experimental patch;;;","07/Apr/11 21:32;skamio;I've attached an experimental patch. The problem is gone with this patch. But it's inefficient when a large number of rows are requested.

The main problem was that the rows collected in ColumnFamilyStore.scan() can have duplicates. So, it returns less unique rows than requested. Then, StorageProxy.scan() asks more results from next range. That means the last returned row gets wrong.

As for inefficiency of the patch, if the rows are added in order, the uniqueness check should be done only for the last row. But I don't known if I can assume the order or not. So, please improve the patch if so.
;;;","07/Apr/11 23:34;jbellis;Hmm. There are two ways we could get duplicate rows:

1. the ranges we iterate through overlap.  but if that were the bug, we would see it on ONE as well as QUORUM
2. the ReadCallback/RangeSliceResponseResolver object (probably the resolver) returns duplicates from incorrect merging of the quorum replies

If 2. is the problem we should fix it in callback/resolver instead of in StorageProxy.

Even with it narrowed down there the problem is not obvious to me -- each response should come back in sorted (token) order, and RSRR uses a collating + reducing iterator to merge duplicates, in theory.;;;","08/Apr/11 10:49;skamio;I forgot to say, but 1. is right. The duplicate problem is visible on CL.ONE as well.
The above test script returns rows of ""173"", ""174"", ""174"" (duplicate), ""175"", ""176"" in the first iteration. It has duplicate.
And if you see my debug log attached, ColumnFamilyStore collects the row ""174"" twice in CL.ONE.

The only case it works correcly is when I specify start_key. In the above script, set start_key = ""173"". The result is ""173"", ""174"", ""175"", ""176"", ""177"" in the first iteration. It is correct, no duplicate.
;;;","08/Apr/11 10:52;skamio;I've attached debug log (node-1.system.log) with various debug prints. This is debug log in running the first iteration of test script with CL.ONE. The result has duplicate.;;;","12/Apr/11 23:24;jbellis;Pavel, can you take a stab at figuring out why the ranges overlap?  They are not supposed to.

(I am using https://github.com/pcmanus/ccm for testing, it saves a lot of time.);;;","15/Apr/11 22:33;jbellis;Pavel's patch fixes a 3rd kind of bug: CFS.scan itself can return duplicate rows, even for a single node and range.  Added a unit test and committed.

Shotaro, does this fix what you are seeing?;;;","18/Apr/11 12:51;skamio;Yes, it fixes our problem. Thanks.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get_key_range problems when a node is down,CASSANDRA-440,12435431,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,simongsmith,simongsmith,11/Sep/09 21:37,16/Apr/19 17:33,22/Mar/23 14:57,15/Sep/09 10:54,0.4,0.5,,,0,,,,,,"I'm running Cassandra on 5 nodes using the
OrderPreservingPartitioner, and have populated Cassandra with 78
records, and I can use get_key_range via Thrift just fine.  Then, if I
manually kill one of the nodes (if I kill off node #5), the node (node
#1) which I've been using to call get_key_range will timeout and the
error:

 Thrift: Internal error processing get_key_range

The Cassandra output traceback:

ERROR - Encountered IOException on connection:
java.nio.channels.SocketChannel[closed]
java.net.ConnectException: Connection refused
       at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
       at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:592)
       at org.apache.cassandra.net.TcpConnection.connect(TcpConnection.java:349)
       at org.apache.cassandra.net.SelectorManager.doProcess(SelectorManager.java:131)
       at org.apache.cassandra.net.SelectorManager.run(SelectorManager.java:98)
WARN - Closing down connection java.nio.channels.SocketChannel[closed]
ERROR - Internal error processing get_key_range
java.lang.RuntimeException: java.util.concurrent.TimeoutException:
Operation timed out.
       at org.apache.cassandra.service.StorageProxy.getKeyRange(StorageProxy.java:573)
       at org.apache.cassandra.service.CassandraServer.get_key_range(CassandraServer.java:595)
       at org.apache.cassandra.service.Cassandra$Processor$get_key_range.process(Cassandra.java:853)
       at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:606)
       at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
       at java.lang.Thread.run(Thread.java:675)
Caused by: java.util.concurrent.TimeoutException: Operation timed out.
       at org.apache.cassandra.net.AsyncResult.get(AsyncResult.java:97)
       at org.apache.cassandra.service.StorageProxy.getKeyRange(StorageProxy.java:569)
       ... 7 more


The error starts as soon as the downed node #5 goes down and lasts
until I restart the downed node #5.

bin/nodeprobe cluster is accurate (it knows quickly when #5 is down,
and when it is up again)

Since I set the replication set to 3, I'm confused as to why (after
the first few seconds or so) there is an error just because one host
is down temporarily.

(Jonathan Ellis and I discussed this on the mailing list, let me know if more information is needed.)",64-bit 4GB Rackspace-cloud boxes running FC11 (saw problem on 32-bit platform as well),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Sep/09 23:36;jbellis;440.patch;https://issues.apache.org/jira/secure/attachment/12419312/440.patch",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19690,,,Tue Sep 15 02:54:29 UTC 2009,,,,,,,,,,"0|i0fyxz:",91259,,,,,Normal,,,,,,,,,,,,,,,,,"11/Sep/09 23:36;jbellis;refactor findSuitableEndpoint to throw UnavailableException (instead of letting callers error out with NPE) when no replica is alive.  also check for endpoint live-ness in getNextEndpoint.

patch is against trunk but should also apply to 0.4.;;;","15/Sep/09 06:37;simongsmith;Jonathan:

I tried out the patch you attached above, I applied it to 0.4, and it works for me.  Now, as soon as I take a node down, there may be one or two seconds of the thrift-internal error, the timeout (which I totally expect, and this is obviously OK) but as soon as the host doing the querying can see the node is down, the error stops, and valid output is given by the get_key_range query again.  And there isn't any disruption when the node comes back up.

Thanks! 

Simon Smith;;;","15/Sep/09 10:54;jbellis;committed to 0.4 and 0.5 branches;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BUG: secondaryIndexes AND multiple index expressions can cause timesouts,CASSANDRA-1623,12477501,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jasontanner,jasontanner,16/Oct/10 03:59,16/Apr/19 17:33,22/Mar/23 14:57,19/Oct/10 04:42,0.7 beta 3,,,,0,,,,,,"1. Given this Column Family definition

    Column Family Name: Requests
      Column Family Type: Standard
      Column Sorted By: org.apache.cassandra.db.marshal.UTF8Type
      Column Metadata:
        Column Name: requested
          Validation Class: org.apache.cassandra.db.marshal.UTF8Type
          Index Type: KEYS
        Column Name: requestor
          Validation Class: org.apache.cassandra.db.marshal.UTF8Type
          Index Type: KEYS

If I have an entry that has the following column/value pairs:

""request-uuid1"" : [  { ""requested"",""person-uuid1"" }, { ""requestor"",""person-uuid2""}, { ""is_confirmed"",""true"" } ]

If I do an index lookup (pseudo coded) :

get_index_slices( Connection,
                                 ColumnParent.column_family=""Requests"",
                                 [ { ""requested"",""eq"", ""person-uuid1"" }, { ""is_confirmed"",""eq"", ""false"" } ],      % Index Expressions
                                 """",100,   % StartKey, KeyCount
                                 """","""",false,100   % StartCol, EndCol, Reversed, ColCount )

for ""requested"" = ""person-uuid1"" and ""is_confirmed"" = false 

then I get the following entries in my log and the request times out along with all other requests on all clients.

DEBUG [pool-1-thread-10] 2010-10-15 19:00:27,878 CassandraServer.java (line 531) scan
DEBUG [pool-1-thread-10] 2010-10-15 19:00:27,897 StorageProxy.java (line 563) restricted single token match for query [0,0]
DEBUG [pool-1-thread-10] 2010-10-15 19:00:27,897 StorageProxy.java (line 649) scan ranges are [0,0]
DEBUG [pool-1-thread-10] 2010-10-15 19:00:27,925 StorageProxy.java (line 669) reading org.apache.cassandra.db.IndexScanCommand@42a6eb from 52@localhost/127.0.0.1
DEBUG [ReadStage:2] 2010-10-15 19:00:27,931 SliceQueryFilter.java (line 121) collecting 0 of 1: null:false:0@1287103291
DEBUG [ReadStage:2] 2010-10-15 19:00:27,933 SliceQueryFilter.java (line 121) collecting 0 of 2147483647: is_confirmed:false:4@1287103291
DEBUG [ReadStage:2] 2010-10-15 19:00:27,934 SliceQueryFilter.java (line 121) collecting 1 of 2147483647: request_type:false:6@1287103291
DEBUG [ReadStage:2] 2010-10-15 19:00:27,935 SliceQueryFilter.java (line 121) collecting 2 of 2147483647: requested:false:58@1287103291
DEBUG [ReadStage:2] 2010-10-15 19:00:27,935 SliceQueryFilter.java (line 121) collecting 3 of 2147483647: requested_network:false:57@1287103291
DEBUG [ReadStage:2] 2010-10-15 19:00:27,936 SliceQueryFilter.java (line 121) collecting 4 of 2147483647: requestor:false:58@1287103291
DEBUG [ReadStage:2] 2010-10-15 19:00:27,937 SliceQueryFilter.java (line 121) collecting 5 of 2147483647: requestor_network:false:57@1287103291
DEBUG [ReadStage:2] 2010-10-15 19:00:27,942 SliceQueryFilter.java (line 121) collecting 0 of 1: null:false:0@1287103291
DEBUG [ReadStage:2] 2010-10-15 19:00:27,943 SliceQueryFilter.java (line 121) collecting 0 of 1: null:false:0@1287103291
DEBUG [ReadStage:2] 2010-10-15 19:00:27,945 SliceQueryFilter.java (line 121) collecting 0 of 1: null:false:0@1287103291
DEBUG [ReadStage:2] 2010-10-15 19:00:27,946 SliceQueryFilter.java (line 121) collecting 0 of 1: null:false:0@1287103291
 this last line repeats forever until I stop the server.

If instead I do the lookup where both terms match or just the last term matches then nothing goes wrong, I get a valid (empty or otherwise) result set.

It only seems to happen if the 2nd expression does not match.

I am using the very latest code from trunk.


Jason
                                       ",centos 5.5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Oct/10 23:05;jbellis;1623.txt;https://issues.apache.org/jira/secure/attachment/12457452/1623.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20221,,,Tue Oct 19 14:15:04 UTC 2010,,,,,,,,,,"0|i0g6db:",92462,,jasontanner,,jasontanner,Normal,,,,,,,,,,,,,,,,,"18/Oct/10 23:05;jbellis;Patch attached with fix and unit test demonstrating the problem.;;;","19/Oct/10 04:26;jasontanner;Patch tested on my install and it resolved my issue.;;;","19/Oct/10 04:42;jbellis;committed;;;","19/Oct/10 22:15;hudson;Integrated in Cassandra #570 (See [https://hudson.apache.org/hudson/job/Cassandra/570/])
    fix potential infinite loop in 2ary index queries
patch by jbellis; tested by Jason Tanner for CASSANDRA-1623
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming error on bootstrap,CASSANDRA-546,12440508,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,rays,rays,13/Nov/09 00:17,16/Apr/19 17:33,22/Mar/23 14:57,13/Nov/09 10:32,0.5,,,,0,,,,,,"Received the following error while bootstrapping a new node:

DEBUG - Adding stream context /usr/local/cassandra-trunk/data/data/Mahalo/VideosInSection-tmp-1-Data.db:4096052 for /10.1.10.198 ...
DEBUG - Sending a stream initiate done message ...
WARN - Problem reading from socket connected to : java.nio.channels.SocketChannel[connected local=/10.2.4.114:7000 remote=/2xx.2xx.194.2xx:55515]
WARN - Exception was generated at : 11/12/2009 15:30:38 on thread MESSAGING-SERVICE-POOL:3
Streaming context has not been set.
java.lang.IllegalStateException: Streaming context has not been set.
	at org.apache.cassandra.net.io.StreamContextManager.getStreamContext(StreamContextManager.java:264)
	at org.apache.cassandra.net.io.ContentStreamState.<init>(ContentStreamState.java:47)
	at org.apache.cassandra.net.io.ProtocolHeaderState.morphState(ProtocolHeaderState.java:66)
	at org.apache.cassandra.net.io.StartState.doRead(StartState.java:48)
	at org.apache.cassandra.net.io.ProtocolHeaderState.read(ProtocolHeaderState.java:39)
	at org.apache.cassandra.net.io.TcpReader.read(TcpReader.java:96)
	at org.apache.cassandra.net.TcpConnection$ReadWorkItem.run(TcpConnection.java:427)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
	at java.lang.Thread.run(Thread.java:619)

full log ---> http://pastie.org/695593","FreeBSD 7.2-RELEASE amd64 
Diablo Java HotSpot(TM) 64-Bit Server VM (build 10.0-b23, mixed mode)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Nov/09 06:48;jbellis;546.patch;https://issues.apache.org/jira/secure/attachment/12424784/546.patch",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19748,,,Fri Nov 13 12:35:04 UTC 2009,,,,,,,,,,"0|i0fzlj:",91365,,,,,Normal,,,,,,,,,,,,,,,,,"13/Nov/09 02:58;jbellis;this was fixed by 538, but a new one was introduced: http://pastie.org/695859;;;","13/Nov/09 05:15;rays;applied patch and get the below error, which is (from what I can tell) the same as the first error.

WARN - Problem reading from socket connected to : java.nio.channels.SocketChannel[connected local=/10.2.4.114:7000 remote=/7x.6x.2xx.1xx:63447]
WARN - Exception was generated at : 11/12/2009 20:36:42 on thread MESSAGING-SERVICE-POOL:3
Streaming context has not been set.
java.lang.IllegalStateException: Streaming context has not been set.
	at org.apache.cassandra.net.io.StreamContextManager.getStreamContext(StreamContextManager.java:264)
	at org.apache.cassandra.net.io.ContentStreamState.<init>(ContentStreamState.java:47)
	at org.apache.cassandra.net.io.ProtocolHeaderState.morphState(ProtocolHeaderState.java:66)
	at org.apache.cassandra.net.io.StartState.doRead(StartState.java:48)
	at org.apache.cassandra.net.io.ProtocolHeaderState.read(ProtocolHeaderState.java:39)
	at org.apache.cassandra.net.io.TcpReader.read(TcpReader.java:96)
	at org.apache.cassandra.net.TcpConnection$ReadWorkItem.run(TcpConnection.java:427)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
	at java.lang.Thread.run(Thread.java:619)

svn diff output: http://pastie.org/696251;;;","13/Nov/09 05:35;jbellis;difficult to disentangle this from CASSANDRA-541.  moving fixes there.;;;","13/Nov/09 06:28;jbellis;ok, this is separate from the generic bootstrap/move bugs

the problem is, Ray has two interfaces and mostly Cassandra uses the internal one, but bootstrap is trying to stream over the external one which confuses things.

with the debug info from CASSANDRA-541 this is clear:

DEBUG - Sending BootstrapMetadataMessage to /10.1.10.198 for (68939025851256836916907001051563673941,85173388956504742541769293679392562704]
ERROR - java.lang.IllegalStateException: Streaming context has not been set for /74.6x.2xx.1xx
;;;","13/Nov/09 06:48;jbellis;attempt to force correct local address in bind;;;","13/Nov/09 10:32;jbellis;from irc:

raysl: looks like it's working

committed;;;","13/Nov/09 20:35;hudson;Integrated in Cassandra #257 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/257/])
    force bind to correct address
patch by jbellis; reviewed by Ray Slakinski for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Problems reading remote rows in trunk?,CASSANDRA-1012,12462739,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Duplicate,,erickt,erickt,22/Apr/10 15:48,16/Apr/19 17:33,22/Mar/23 14:57,28/Apr/10 03:19,0.7 beta 1,,,,0,,,,,,"I've run into a problem with my small 3 node cluster where quorum reads do not return anything. I'm inserting 10 rows into a fresh database with replication factor 3. I've inserted with QUORUM, but when I try to read back at QUORUM no data gets returned. However, if I use the consistency level ONE, I do get results.

I've dug a bit into this problem, and it appears that internally ThriftServer.getSlice is finding the column family for the given key, but it's not finding any of the columns, so thrift doesn't return anything.

In order to confirm there being a problem, I dropped the RF down to 1 and rebuilt the cluster. I then reinserted some data, and I can only read data off of one machine.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19956,,,Tue Apr 27 19:57:19 UTC 2010,,,,,,,,,,"0|i0g2gn:",91829,,,,,Critical,,,,,,,,,,,,,,,,,"22/Apr/10 20:30;jbellis;how are you going to read data off more than one machine with RF=1?;;;","23/Apr/10 00:33;erickt;Through the service.StorageProxy.weakReadRemote  code path? For example, I can use cassandra-cli to connect to machine 1. I run ""get Keyspace1.Standard1['1']"" and get results back. Connecting with machine 2, I run the same thing and get no results back. As best as I can tell, the nodes are talking to each other according to the log, so I'm not sure what's going on.;;;","23/Apr/10 00:46;jbellis;if you turn on DEBUG logging it will tell what machines are involved for both inserts and gets;;;","23/Apr/10 05:17;erickt;So here's what I'm seeing in my log files with my inserts and gets. And by saying ""inserting into machine1"", I mean that my client is connecting to machine1, but machine1, machine2, and machine3 are all in a ring together and appear to have normal communication with each other:

% nodetool -h machine1 ring
Address       Status     Load          Range                                      Ring
                                       mGdY41r8RNGYLXsF                           
10.0.0.1    Up         20.23 KB      A7fozEmmpyDXlE7e                           |<--|
10.0.0.3    Up         5.44 KB       fQKDOQPHr2x8n1zG                           |   |
10.0.0.2    Up         5.44 KB       mGdY41r8RNGYLXsF                           |-->|

% nodetool -h machine1 streams
Mode: Normal
Not sending any streams.
Not receiving any streams.

% nodetool -h machine2 streams
Mode: Normal
Not sending any streams.
Not receiving any streams.

% nodetool -h machine3 streams
Mode: Normal
Not sending any streams.
Not receiving any streams.


------

inserting into machine1:

machine1:

DEBUG [ROW-READ-STAGE:4] 2010-04-22 14:10:43,513 StorageProxy.java (line 757) weakreadlocal reading SliceByNamesReadCommand(table='Keyspace1', key=3131, columnParent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', columns=[6a6f626964,])
DEBUG [pool-1-thread-2] 2010-04-22 14:10:43,788 CassandraServer.java (line 417) batch_mutate
DEBUG [pool-1-thread-2] 2010-04-22 14:10:43,869 StorageProxy.java (line 299) insert writing local key [B@163b4b1e

machine2: (nothing in log)
machine3: (nothing in log)

------

getting against machine1:

machine1: (nothing in log)

machine2:
DEBUG [pool-1-thread-4] 2010-04-22 14:07:22,825 CassandraServer.java (line 232) get_slice
DEBUG [ROW-READ-STAGE:7] 2010-04-22 14:07:22,826 StorageProxy.java (line 757) weakreadlocal reading SliceByNamesReadCommand(table='Keyspace1', key=31, columnParent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', columns=[6a6f626964,])

machine3: (nothing in log)

------

getting against machine2:

machine1:
DEBUG [ROW-READ-STAGE:4] 2010-04-22 14:03:10,038 ReadVerbHandler.java (line 94) Read key [B@3dd06d02; sending response to 153934@/10.0.0.2

machine2:
DEBUG [Timer-0] 2010-04-22 14:03:03,747 LoadDisseminator.java (line 37) Disseminating load info ...
DEBUG [pool-1-thread-8] 2010-04-22 14:03:10,012 CassandraServer.java (line 232) get_slice
DEBUG [pool-1-thread-8] 2010-04-22 14:03:10,013 StorageProxy.java (line 341) weakreadremote reading SliceByNamesReadCommand(table='Keyspace1', key=31, columnParent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', columns=[6a6f626964,])
DEBUG [pool-1-thread-8] 2010-04-22 14:03:10,013 StorageProxy.java (line 352) weakreadremote reading SliceByNamesReadCommand(table='Keyspace1', key=31, columnParent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', columns=[6a6f626964,]) from 153934@/10.0.0.1
DEBUG [RESPONSE-STAGE:8] 2010-04-22 14:03:10,024 ResponseVerbHandler.java (line 44) Processing response on an async result from 153934@/10.0.0.1

machine3: (nothing in log);;;","23/Apr/10 05:30;jbellis;you're doing something wrong, because where you write ""getting against machine1"" if you really were making a request to machine1 there would always be _something_ in the log at DEBUG level.  (at a minimum, the name of the command you sent)

does nodeprobe ring on 2 and 3 agree w/ 1?;;;","23/Apr/10 05:40;erickt;You're right, I messed up copying. Here's the real results:

------

getting against machine1:

machine1 log:
DEBUG [pool-1-thread-9] 2010-04-22 14:35:59,972 CassandraServer.java (line 232) get_slice
DEBUG [ROW-READ-STAGE:7] 2010-04-22 14:35:59,972 StorageProxy.java (line 757) weakreadlocal reading SliceByNamesReadCommand(table='Keyspace1', key=31, columnParent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', columns=[6a6f626964,])

machine2 log: (nothing)
machine3 log: (nothing)

------

getting against machine2:

machine1 log:
DEBUG [ROW-READ-STAGE:8] 2010-04-22 14:38:06,078 ReadVerbHandler.java (line 94) Read key [B@488ddb93; sending response to 5414@/10.0.0.2

machine2 log:
DEBUG [pool-1-thread-6] 2010-04-22 14:38:32,488 CassandraServer.java (line 232) get_slice
DEBUG [pool-1-thread-6] 2010-04-22 14:38:32,488 StorageProxy.java (line 341) weakreadremote reading SliceByNamesReadCommand(table='Keyspace1', key=31, columnParent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', columns=[6a6f626964,])
DEBUG [pool-1-thread-6] 2010-04-22 14:38:32,489 StorageProxy.java (line 352) weakreadremote reading SliceByNamesReadCommand(table='Keyspace1', key=31, columnParent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', columns=[6a6f626964,]) from 5498@/10.0.0.1
DEBUG [RESPONSE-STAGE:6] 2010-04-22 14:38:32,491 ResponseVerbHandler.java (line 44) Processing response on an async result from 5498@/10.0.0.1

machine3 log: (nothing)

------

and nodetool does agree with machine1:

% nodetool -h machine1 ring
Address       Status     Load          Range                                      Ring
                                       mGdY41r8RNGYLXsF                           
10.0.0.1    Up         20.23 KB      A7fozEmmpyDXlE7e                           |<--|
10.0.0.3    Up         5.44 KB       fQKDOQPHr2x8n1zG                           |   |
10.0.0.2    Up         5.44 KB       mGdY41r8RNGYLXsF                           |-->|


% nodetool -h machine2 ring
Address       Status     Load          Range                                      Ring
                                       mGdY41r8RNGYLXsF                           
10.0.0.1    Up         20.23 KB      A7fozEmmpyDXlE7e                           |<--|
10.0.0.3    Up         5.44 KB       fQKDOQPHr2x8n1zG                           |   |
10.0.0.2    Up         5.44 KB       mGdY41r8RNGYLXsF                           |-->|

% nodetool -h machine3 ring
Address       Status     Load          Range                                      Ring
                                       mGdY41r8RNGYLXsF                           
10.0.0.1    Up         20.23 KB      A7fozEmmpyDXlE7e                           |<--|
10.0.0.3    Up         5.44 KB       fQKDOQPHr2x8n1zG                           |   |
10.0.0.2    Up         5.44 KB       mGdY41r8RNGYLXsF                           |-->|
;;;","23/Apr/10 05:46;jbellis;so these logs are saying:

you did a write to machine1,  it wrote the data to itself.

when you did a read from machine1 it read the data from itself.

when you did a read from machine2 it sent a request to machine1 for the data, and got a response back.;;;","23/Apr/10 05:53;erickt;Yes, I believe so. I did some instrumenting on the code, and as best as I can tell that when I read from machine2, it is getting a db.Row back from machine1, but the db.Row's db.ColumnFamily is being deserialized to null, and then the ThriftServer returns an empty result set to the client.;;;","23/Apr/10 06:03;jbellis;then you'd need to figure out what is different from machine1 doing the read for a request that it hands directly back to the client, and a request that it hands to machine2.;;;","26/Apr/10 21:57;jbellis;I wonder if this is caused by CASSANDRA-1020.;;;","28/Apr/10 01:07;jbellis;1020 is committed, can you test if that fixes the problem?;;;","28/Apr/10 03:19;jbellis;Erick reports that this does seem to be fixed post-1020.;;;","28/Apr/10 03:57;rschildmeijer;I managed to reproduce the behaviour (without #1020 applied and used simple insert/get).
I applied #1020 to all my nodes and the issue was gone.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
utf8 error in DEBUG output in CommitLog.java,CASSANDRA-1366,12470958,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,nickmbailey,jeromatron,jeromatron,06/Aug/10 10:12,16/Apr/19 17:33,22/Mar/23 14:57,18/Aug/10 04:53,0.7 beta 2,,,,0,,,,,,"Looks like the bug Johan saw a while back where debug output was throwing a UTF8 error has manifested itself in CommitLog.java on line 279.

INFO 18:32:40,951 Replaying /var/lib/cassandra/commitlog/CommitLog-1281058340642.log
DEBUG 18:32:40,953 Replaying /var/lib/cassandra/commitlog/CommitLog-1281058340642.log starting at 276
DEBUG 18:32:40,953 Reading mutation at 276
DEBUG 18:32:40,956 replaying mutation for system.[B@77fe4169: {ColumnFamily(LocationInfo [B:false:1@1281058340821,])}
DEBUG 18:32:40,965 Reading mutation at 424
 INFO 18:32:40,966 Finished reading /var/lib/cassandra/commitlog/CommitLog-1281058340642.log
ERROR 18:32:40,967 Exception encountered during startup.
org.apache.cassandra.db.marshal.MarshalException: invalid UTF8 bytes [-64, -88, 101, 51]
	at org.apache.cassandra.db.marshal.UTF8Type.getString(UTF8Type.java:43)
	at org.apache.cassandra.db.Column.getString(Column.java:247)
	at org.apache.cassandra.db.marshal.AbstractType.getColumnsString(AbstractType.java:85)
	at org.apache.cassandra.db.ColumnFamily.toString(ColumnFamily.java:379)
	at org.apache.commons.lang.ObjectUtils.toString(ObjectUtils.java:241)
	at org.apache.commons.lang.StringUtils.join(StringUtils.java:3073)
	at org.apache.commons.lang.StringUtils.join(StringUtils.java:3133)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:279)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:174)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:120)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:90)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
Exception encountered during startup.
org.apache.cassandra.db.marshal.MarshalException: invalid UTF8 bytes [-64, -88, 101, 51]
	at org.apache.cassandra.db.marshal.UTF8Type.getString(UTF8Type.java:43)
	at org.apache.cassandra.db.Column.getString(Column.java:247)
	at org.apache.cassandra.db.marshal.AbstractType.getColumnsString(AbstractType.java:85)
	at org.apache.cassandra.db.ColumnFamily.toString(ColumnFamily.java:379)
	at org.apache.commons.lang.ObjectUtils.toString(ObjectUtils.java:241)
	at org.apache.commons.lang.StringUtils.join(StringUtils.java:3073)
	at org.apache.commons.lang.StringUtils.join(StringUtils.java:3133)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:279)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:174)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:120)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:90)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
",,johanoskarsson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,nickmbailey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20107,,,Tue Aug 17 20:53:00 UTC 2010,,,,,,,,,,"0|i0g4m7:",92178,,,,,Normal,,,,,,,,,,,,,,,,,"06/Aug/10 10:13;jeromatron;This was part of the debug output for CASSANDRA-1365, but this is a separate issue.;;;","07/Aug/10 01:29;jbellis;either this is CASSANDRA-1274 (you had data before that revision in your CL) or we have a similar problem in another CF b/s the Hints one.;;;","17/Aug/10 07:05;nickmbailey;Jeremy's log indicates this is happening when replaying the LocationInfo CF so I think this is a different but similar problem.;;;","17/Aug/10 07:30;nickmbailey;Scratch that, that was just the last mutation read.  Not sure what mutation is causing it to break here.  Jeremy unless you have any tips to reproduce I'm thinking this was solved by 1274.;;;","17/Aug/10 07:35;jeromatron;I emailed the person who had this error log and CCed Nick on the email.  If he doesn't have any further information on how to reproduce the problem, then yeah - we should probably resolve this one as unable to reproduce the problem.;;;","18/Aug/10 04:53;jbellis;closing as duplicate.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
log replay bugs,CASSANDRA-264,12429099,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,29/Jun/09 23:29,16/Apr/19 17:33,22/Mar/23 14:57,30/Jun/09 12:06,0.4,,,,0,,,,,,"- OOM on log replay
- log is deleted even when replay fails
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Jun/09 01:49;jbellis;0002-v2.patch;https://issues.apache.org/jira/secure/attachment/12412089/0002-v2.patch","30/Jun/09 00:54;jbellis;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-264-don-t-rm-when-replay-fails.txt;https://issues.apache.org/jira/secure/attachment/12412081/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-264-don-t-rm-when-replay-fails.txt","30/Jun/09 00:54;jbellis;ASF.LICENSE.NOT.GRANTED--0002-add-test-catching-buggy-update-of-header-on-flush-ref.txt;https://issues.apache.org/jira/secure/attachment/12412082/ASF.LICENSE.NOT.GRANTED--0002-add-test-catching-buggy-update-of-header-on-flush-ref.txt",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19614,,,Tue Jun 30 13:00:43 UTC 2009,,,,,,,,,,"0|i0fxvj:",91086,,,,,Normal,,,,,,,,,,,,,,,,,"30/Jun/09 00:56;jbellis;02 reads
  add test catching buggy update of header on flush; refactor so there is only one version of code doing those writes (the correct one)

the bug was in the writeCommitLogHeader called by discard; it didn't write the length first so readLong() on replay would get basically garbage;;;","30/Jun/09 01:36;junrao;The patch looks fine. I don't see any new test cases added though and am wondering how this bug escaped the unit test before.;;;","30/Jun/09 01:49;jbellis;oops, forgot to add the new test to 0002.  here is the new version of that patch.

the old test doesn't catch it because it doesn't add enough data to cause a flush, which is where the commitlogheader corruption happened.;;;","30/Jun/09 02:01;junrao;The new patch looks good.;;;","30/Jun/09 12:06;jbellis;committed;;;","30/Jun/09 21:00;hudson;Integrated in Cassandra #124 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/124/])
    add test catching buggy update of header on flush; refactor so there is only one version of code doing those writes (the correct one).
patch by jbellis; reviewed by Jun Rao for 
don't remove commitlog files when replay fails; you lose the chance to fix a bug, as well as your data.
patch by jbellis; reviewed by Jun Rao for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra's internal state broke by getting column slices. Error org.apache.thrift.TApplicationException: Internal error processing get_slice afterwards.,CASSANDRA-1423,12472303,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Duplicate,,dccwilliams,dccwilliams,24/Aug/10 05:36,16/Apr/19 17:33,22/Mar/23 14:57,26/Aug/10 05:21,0.7 beta 2,,,,0,,,,,,"The attached program (with data that it can import) causes large column slices to be requested from Cassandra. 

The program itself uploads school address data to a Cassyndex full text index, and then allows you to search that. The program simulates someone typing a search into an active search box, which shows you the matches for the current term as you type. Thus when you enter a search term such as ""cherwell school oxford"" actually it performs the searches ""c"", ""ch"", ""che"", ""cher"", ""cherw"" etc

You can configure the delay between the ""keystrokes"". If your delay allows the searches to complete sequentially, you are ok. But if you have a short delay, and searches are created in parallel, pretty quickly this error will arise - ""org.apache.thrift.TApplicationException: Internal error processing get_slice"".

Once this has occurred all future attempts and getting slices of columns will return the same error, and your'e only option is to restart Cassandra.

This looks like some kind of concurrency edge condition bug caused by requesting sufficiently large intersecting slices in parallel. It may be in other versions too.

I've been testing on 0.7 B1 using an RP cluster. 

The attached maven project should pull down the scale7 libraries but if interested you can find the sources at http://github.com/s7",Ubuntu 9.04,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Aug/10 05:41;dccwilliams;fmm-add-schools.zip;https://issues.apache.org/jira/secure/attachment/12452859/fmm-add-schools.zip",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20129,,,Mon Aug 23 22:29:17 UTC 2010,,,,,,,,,,"0|i0g4yv:",92235,,,,,Critical,,,,,,,,,,,,,,,,,"24/Aug/10 05:45;dccwilliams;I should add that I've been testing on a single node RP cluster so far. I'll try it on our multi-node cluster asap;;;","24/Aug/10 05:59;jbellis;what is the internal error?  there will be a traceback in the system log;;;","24/Aug/10 06:25;dccwilliams;Looks like this bug might hopefully be a straightforward case of not cleaning up file handles (or at least, always opening files afresh, and then not handling the error when you can't open any more). I should have checked logs. 

From system.log:

ERROR [pool-1-thread-74] 2010-08-23 23:18:34,749 Cassandra.java (line 2651) Internal error processing get_slice
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.io.IOError: java.io.FileNotFoundException: /var/lib/cassandra/data/FightMyMonster/UK_Schools_FullTextIndex-e-2-Data.db (Too many open files)
	at org.apache.cassandra.service.StorageProxy.weakRead(StorageProxy.java:354)
	at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:297)
	at org.apache.cassandra.thrift.CassandraServer.readColumnFamily(CassandraServer.java:125)
	at org.apache.cassandra.thrift.CassandraServer.getSlice(CassandraServer.java:231)
	at org.apache.cassandra.thrift.CassandraServer.multigetSliceInternal(CassandraServer.java:309)
	at org.apache.cassandra.thrift.CassandraServer.get_slice(CassandraServer.java:270)
	at org.apache.cassandra.thrift.Cassandra$Processor$get_slice.process(Cassandra.java:2643)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2499)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.util.concurrent.ExecutionException: java.io.IOError: java.io.FileNotFoundException: /var/lib/cassandra/data/FightMyMonster/UK_Schools_FullTextIndex-e-2-Data.db (Too many open files)
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.service.StorageProxy.weakRead(StorageProxy.java:350)
	... 11 more
Caused by: java.io.IOError: java.io.FileNotFoundException: /var/lib/cassandra/data/FightMyMonster/UK_Schools_FullTextIndex-e-2-Data.db (Too many open files)
	at org.apache.cassandra.io.util.BufferedSegmentedFile.getSegment(BufferedSegmentedFile.java:68)
	at org.apache.cassandra.io.sstable.SSTableReader.getFileDataInput(SSTableReader.java:545)
	at org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:71)
	at org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:48)
	at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:64)
	at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:76)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:956)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:851)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:826)
	at org.apache.cassandra.db.Table.getRow(Table.java:330)
	at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:71)
	at org.apache.cassandra.service.StorageProxy$weakReadLocalCallable.call(StorageProxy.java:816)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	... 3 more
Caused by: java.io.FileNotFoundException: /var/lib/cassandra/data/FightMyMonster/UK_Schools_FullTextIndex-e-2-Data.db (Too many open files)
	at java.io.RandomAccessFile.open(Native Method)
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
	at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)
	at org.apache.cassandra.io.util.BufferedSegmentedFile.getSegment(BufferedSegmentedFile.java:62)
	... 16 more
;;;","24/Aug/10 06:29;jbellis;probably CASSANDRA-1416.  can you test trunk and see if that fixes it?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Thrift validation bugs,CASSANDRA-266,12429153,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,eweaver,eweaver,30/Jun/09 12:36,16/Apr/19 17:33,22/Mar/23 14:57,01/Jul/09 06:08,0.4,,,,0,,,,,,"Server does not raise on invalid insert into regular ColumnFamily

DEBUG - Applying RowMutation(table='Twitter', key='8', modifications=[ColumnFamily(Statuses [body:false:5@1246336092])])
DEBUG - RowMutation(table='Twitter', key='8', modifications=[ColumnFamily(Statuses [body:false:5@1246336092])]) applied. 

OK

DEBUG - Applying RowMutation(table='Twitter', key='8', modifications=[ColumnFamily(Statuses [fhwagads:body:false:5@1246336111])])
DEBUG - RowMutation(table='Twitter', key='8', modifications=[ColumnFamily(Statuses [fhwagads:body:false:5@1246336111])]) applied.  

Not ok... ""Statuses:fhwagads:body"" insert should have failed. For example:

java.lang.IllegalArgumentException: Column Family Statuses:fhwagads:fhwagads:body in invalid format. Must be in <column family>:<column> format.

Also:

You can request an array of values via get_slice_by_names, and you can request an array of columns via get_super_slice_by_names, but you can't request an array of values through a supercolumn via either one. ""Ideally"" get_slice_by_names should allow a supercolumn specification like below:

InvalidRequestException: Column Family StatusRelationships:user_timelines is invalid. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Jul/09 00:26;jbellis;266.patch;https://issues.apache.org/jira/secure/attachment/12412179/266.patch",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19615,,,Wed Jul 01 13:41:16 UTC 2009,,,,,,,,,,"0|i0fxvr:",91087,,,,,Normal,,,,,,,,,,,,,,,,,"30/Jun/09 23:42;jbellis;(unbork my copy/paste job);;;","01/Jul/09 06:00;eweaver;Looks ok except for two issues:

* validateColumnPathOrParent has no callpoints.

* in validateColumnPathOrParent,
   +        else if (values.length != 3)
  should be
  +        else if (values.length > 3)
;;;","01/Jul/09 06:08;jbellis;committed w/ suggested fixes;;;","01/Jul/09 21:41;hudson;Integrated in Cassandra #125 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/125/])
    more consistent checking of thrift parameters; fixes multiple bugs.
patch by jbellis; reviewed by Evan Weaver for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix counter bug (regression from svn commit r1068504),CASSANDRA-2155,12498391,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,11/Feb/11 23:45,16/Apr/19 17:33,22/Mar/23 14:57,12/Feb/11 00:43,0.8 beta 1,,,,0,,,,,,A line was mistakenly removed by the merge from 0.7 at r1068504,,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,"11/Feb/11 23:45;slebresne;0001-Fix-regression-from-svn-commit-1068504.patch;https://issues.apache.org/jira/secure/attachment/12470863/0001-Fix-regression-from-svn-commit-1068504.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20469,,,Fri Feb 11 17:49:48 UTC 2011,,,,,,,,,,"0|i0g9nj:",92994,,,,,Low,,,,,,,,,,,,,,,,,"12/Feb/11 00:43;jbellis;committed;;;","12/Feb/11 01:49;hudson;Integrated in Cassandra #724 (See [https://hudson.apache.org/hudson/job/Cassandra/724/])
    fix merge
patch by slebresne; reviewed by jbellis for CASSANDRA-2155
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix few minor problems in nodeprobe cfstats,CASSANDRA-646,12443829,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,rrabah,rrabah,19/Dec/09 09:52,16/Apr/19 17:33,22/Mar/23 14:57,14/Jan/10 00:46,0.5,,Legacy/Tools,,0,,,,,,"nodeprobe cfstats reports that readlatency/writelatency is NaN on the keyspace level although it obviously is not.

For example:
Keyspace: Keyspace1
        Read Count: 392
        Read Latency: NaN ms.
        Write Count: 262
        Write Latency: NaN ms.
        Pending Tasks: 0

                Column Family: MyCF
                Memtable Columns Count: 143
                Memtable Data Size: 123433
                Memtable Switch Count: 2
                Read Count: 392
                Read Latency: 0.533 ms.
                Write Count: 262
                Write Latency: 0.000 ms.
                Pending Tasks: 0

                Column Family: Standard2
                Memtable Columns Count: 0
                Memtable Data Size: 0
                Memtable Switch Count: 0
                Read Count: 0
                Read Latency: NaN ms.
                Write Count: 0
                Write Latency: NaN ms.
                Pending Tasks: 0

The problem here is that there is more than one cf, and one of them has read latency/writelatency NaN. This causes the keyspace readlatency/writelatency to be NaN instead of the average across all cfs. 

Another problem with cfstats is that it does not account for the delays when a read/write times out, so it does not accurately reflect the health of the system under too much stress. 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Jan/10 04:45;jbellis;646-05.patch;https://issues.apache.org/jira/secure/attachment/12430042/646-05.patch",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19798,,,Wed Jan 13 16:46:48 UTC 2010,,,,,,,,,,"0|i0g07r:",91465,,,,,Low,,,,,,,,,,,,,,,,,"19/Dec/09 09:57;jbellis;If it times out, there is no data.  Making something up would be nonsensical.

Remember, StorageProxy is the core of the fat client, as well as the server routing.;;;","19/Dec/09 10:44;jbellis;sorry, cfstats is not the StorageProxy stats.  cfstats _does_ include full time, even for operations that another node gives up on.;;;","20/Dec/09 03:52;rrabah;If we want readLatency to mean only read from disk time, but not include network delay time can we at least log the exception on the server when a timedout exception happens (like we used to in version 0.4). The reason I mention this is that we had some major TimedOutExceptions being thrown, and the system was suffering badly, but the server logs and cfstats showed everything to be perfectly running fine. It's only when we dug into the client logs that we started noticing that. It makes monitoring the health of the system harder, when you have many connected clients to the cassandra servers, and you need to look at each of their logs separately. ;;;","20/Dec/09 06:00;jbellis;You could do that from the SP side.;;;","21/Dec/09 00:16;rrabah;Fair enough. Should I log a separate enhancement to log the TimedOutException in the SP side, and leave this bug to fix NaN for a Keyspace in the presence of multiple cfs?;;;","10/Jan/10 03:02;jbellis;Sure.;;;","13/Jan/10 04:45;jbellis;patch to fix NaNs.  applies to 0.5 (does not apply to trunk, I will fix that on merge);;;","14/Jan/10 00:40;gdusbabek;+1;;;","14/Jan/10 00:46;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Assertion failure in MerkleTree,CASSANDRA-639,12443671,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,stuhood,gdusbabek,gdusbabek,18/Dec/09 02:46,16/Apr/19 17:33,22/Mar/23 14:57,18/Dec/09 23:10,0.5,,,,0,,,,,,I'm running three nodes (I'll attach storage.conf).  I have a simple test that writes 100 rows of 100 cols each (15-20 byte names and values).  The assertion in MerkleTree.inc() is consistently failing.,,kimtea,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Dec/09 04:48;stuhood;639-off-by-one.diff;https://issues.apache.org/jira/secure/attachment/12428344/639-off-by-one.diff","18/Dec/09 04:23;stuhood;639-off-by-one.diff;https://issues.apache.org/jira/secure/attachment/12428342/639-off-by-one.diff","18/Dec/09 04:09;gdusbabek;stacktrave.txt;https://issues.apache.org/jira/secure/attachment/12428341/stacktrave.txt","18/Dec/09 02:48;gdusbabek;storage-conf.xml;https://issues.apache.org/jira/secure/attachment/12428330/storage-conf.xml",,,,,,,,,,,4.0,stuhood,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19794,,,Fri Dec 18 15:10:30 UTC 2009,,,,,,,,,,"0|i0g067:",91458,,,,,Low,,,,,,,,,,,,,,,,,"18/Dec/09 02:48;gdusbabek;I should also point out that the insertions are coming from another node running in client-only mode:;;;","18/Dec/09 02:50;stuhood;What version of Cassandra are you running?;;;","18/Dec/09 03:01;gdusbabek;trunk.;;;","18/Dec/09 04:09;gdusbabek;Here is a stack trace.;;;","18/Dec/09 04:23;stuhood;Hey Gary: I think this is just an off-by-one error. Can you confirm with the attached patch?;;;","18/Dec/09 04:35;gdusbabek;+1 
That solved it. 

;;;","18/Dec/09 04:40;jbellis;Is this something that we can add a test for?;;;","18/Dec/09 04:48;stuhood;Add assertion to constructor, to catch the problem immediately.;;;","18/Dec/09 23:10;jbellis;committed to 0.5 and trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
lost+found directories cause problems for cassandra,CASSANDRA-1547,12475245,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,mdennis,mdennis,mdennis,28/Sep/10 05:40,16/Apr/19 17:33,22/Mar/23 14:57,05/Oct/10 22:06,0.6.6,,,,0,,,,,,ext3/4 make lost+found directories at the root of the file system.  if you then point C* at the root of the FS (e.g. you have a mount point of /cassandra_data and/or /cassandra_commitlog) C* thinks lost+found is a keyspace and spews.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Oct/10 00:13;mdennis;1547-cassandra-0.6.txt;https://issues.apache.org/jira/secure/attachment/12456293/1547-cassandra-0.6.txt",,,,,,,,,,,,,,1.0,mdennis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20193,,,Tue Oct 05 14:06:02 UTC 2010,,,,,,,,,,"0|i0g5wf:",92386,,gdusbabek,,gdusbabek,Low,,,,,,,,,,,,,,,,,"29/Sep/10 04:21;jbellis;didn't we used to have code to ignore files starting with . (mostly for the benefit of OS X)?;;;","29/Sep/10 04:24;gdusbabek;We still do, but I believe it is only inside the KS directories.;;;","04/Oct/10 22:53;gdusbabek;Matt, what error do you see.  To test, I created a lost+found dir inside my data dir and didn't have any problems.;;;","05/Oct/10 00:12;mdennis;I wasn't able to get a copy of the stack trace at the time, but it looks like it's just the commitlog directly and not the data directory too.

{code}
 INFO 10:32:24,958 JNA not found. Native methods will be disabled.
 INFO 10:32:25,152 DiskAccessMode 'auto' determined to be standard, indexAccessMode is standard
 INFO 10:32:25,607 Sampling index for /var/lib/cassandra/data/system/LocationInfo-1-Data.db
 INFO 10:32:25,639 Sampling index for /var/lib/cassandra/data/system/LocationInfo-2-Data.db
 INFO 10:32:25,702 Sampling index for /var/lib/cassandra/data/Keyspace1/Standard1-1-Data.db
 INFO 10:32:25,889 Replaying /var/lib/cassandra/commitlog/CommitLog-1286206104383.log, /var/lib/cassandra/commitlog/lost+found
java.io.FileNotFoundException: /var/lib/cassandra/commitlog/lost+found (Is a directory)
	at java.io.RandomAccessFile.open(Native Method)
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
	at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:144)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:186)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:173)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:114)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:214)
 INFO 10:32:25,913 Finished reading /var/lib/cassandra/commitlog/CommitLog-1286206104383.log
ERROR 10:32:25,916 Exception encountered during startup.
java.io.FileNotFoundException: /var/lib/cassandra/commitlog/lost+found (Is a directory)
	at java.io.RandomAccessFile.open(Native Method)
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
	at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:144)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:186)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:173)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:114)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:214)
Exception encountered during startup.
{code}
;;;","05/Oct/10 00:13;mdennis;it looks like 0.7 doesn't have this problem;;;","05/Oct/10 20:37;gdusbabek;We still want to avoid any hidden files.

EDIT: Duh.  And this patch does avoid them.;;;","05/Oct/10 20:39;gdusbabek;+1;;;","05/Oct/10 22:06;gdusbabek;+1 committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
read failure during flush,CASSANDRA-1040,12463432,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jbellis,jbellis,jbellis,30/Apr/10 21:33,16/Apr/19 17:33,22/Mar/23 14:57,08/May/10 05:31,0.7 beta 1,,,,2,,,,,,"Joost Ouwerkerk writes:
	
On a single-node cassandra cluster with basic config (-Xmx:1G)
loop {
  * insert 5,000 records in a single columnfamily with UUID keys and
random string values (between 1 and 1000 chars) in 5 different columns
spanning two different supercolumns
  * delete all the data by iterating over the rows with
get_range_slices(ONE) and calling remove(QUORUM) on each row id
returned (path containing only columnfamily)
  * count number of non-tombstone rows by iterating over the rows
with get_range_slices(ONE) and testing data.  Break if not zero.
}

while this is running, call ""bin/nodetool -h localhost -p 8081 flush KeySpace"" in the background every minute or so.  When the data hits some critical size, the loop will break.",,anty,brandon.williams,greglu,johanoskarsson,joosto,schubertzhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/May/10 05:14;jbellis;1040.txt;https://issues.apache.org/jira/secure/attachment/12443998/1040.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19969,,,Fri May 07 21:31:52 UTC 2010,,,,,,,,,,"0|i0g2mv:",91857,,,,,Critical,,,,,,,,,,,,,,,,,"06/May/10 11:28;jbellis;Brandon's code to reproduce:

{code}


#!/usr/bin/python
from telephus.protocol import ManagedCassandraClientFactory
from telephus.client import CassandraClient
from twisted.internet import defer

HOST = 'cassandra-6'
PORT = 9160
KEYSPACE = 'Keyspace1'
CF = 'Standard1'
SCF = 'Super1'
colname = 'foo'
scname = 'bar'

@defer.inlineCallbacks
def dostuff(client):
    while True:
        print ""inserting""
        for i in xrange(5000):
            yield client.insert(str(i), CF, 'test', column=colname)
        print ""removing""
        res = yield client.get_range_slice(CF, count=10000)
        for ks in res:
            if len(ks.columns) > 0:
                yield client.remove(ks.key, CF)
        print ""checking""
        res = yield client.get_range_slice(CF, count=10000)
        for ks in res:
            assert len(ks.columns) == 0
        print ""ok""

if __name__ == '__main__':
    from twisted.internet import reactor
    from twisted.python import log
    import sys
    log.startLogging(sys.stdout)

    f = ManagedCassandraClientFactory()
    c = CassandraClient(f, KEYSPACE)
    dostuff(c)
    reactor.connectTCP(HOST, PORT, f)
    reactor.run()
{code}
;;;","06/May/10 11:41;jbellis;Also:

{code}
for x in seq `1 1000`; do bin/nodetool -h `hostname` flush Keyspace1; sleep 5; done
{code};;;","08/May/10 02:32;stuhood;Independent of (but related to) this issue, ColumnFamilyStore.getRangeRows has a race condition in memtable handling. The order of operations that might trigger the problem is:
# Copy memtablesPendingFlush
# (new memtable becomes pending)
# Copy reference to current Memtable

Swapping 3. with 1. would prevent new memtables from being ignored, but would mean we might scan one memtable twice. Making 1. and 3. atomic would remove the race, but is a longer time to hold the lock than we are use to.

EDIT: this description only applies to trunk;;;","08/May/10 05:14;jbellis;Most of this was caused by the bug Stu found for CASSANDRA-1063, which has been committed separately.  Here is a patch to fix the trunk-only part explained above.  (We take the ""allow the original memtable to scanned twice occasionally"" approach, which is the one taken by getTopLevelColumns.);;;","08/May/10 05:26;stuhood;+1 for 1040.txt. Thanks!;;;","08/May/10 05:31;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-cli parsing error,CASSANDRA-738,12446624,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,urandom,johanhil,johanhil,26/Jan/10 09:21,16/Apr/19 17:33,22/Mar/23 14:57,25/Feb/10 00:39,0.6,,Legacy/Tools,,1,,,,,,"Steps to reproduce:
1. Download the 0.5 release
2. Start Cassandra
3. Start cassandra-cli
4. Execute ""set foo.bar['toot']='balls'""

Expected output:
An error message telling me I'm not doing it right.

Actual output:
cassandra> set foo.bar['toot']='balls'
Exception in thread ""main"" java.lang.AssertionError: serious parsing error (this is a bug).
	at org.apache.cassandra.cli.CliClient.executeSet(CliClient.java:367)
	at org.apache.cassandra.cli.CliClient.executeCLIStmt(CliClient.java:63)
	at org.apache.cassandra.cli.CliMain.processCLIStmt(CliMain.java:131)
	at org.apache.cassandra.cli.CliMain.main(CliMain.java:172)

Perhaps this is related to https://issues.apache.org/jira/browse/CASSANDRA-615 in a non-direct way.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Feb/10 09:36;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-738-cli-friendlier-error-messages.txt;https://issues.apache.org/jira/secure/attachment/12436796/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-738-cli-friendlier-error-messages.txt",,,,,,,,,,,,,,1.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19846,,,Wed Feb 24 16:39:00 UTC 2010,,,,,,,,,,"0|i0g0rr:",91555,,,,,Low,,,,,,,,,,,,,,,,,"21/Feb/10 02:32;jab_doa;I experienced exactly the same problem. Setup as described in http://wiki.apache.org/cassandra/GettingStarted.;;;","24/Feb/10 11:28;jbellis;+1 Eric's fixes;;;","25/Feb/10 00:39;urandom;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ERROR [MIGRATION-STAGE:1] Previous Version Mistmatch,CASSANDRA-1384,12471466,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,gdusbabek,arya,arya,13/Aug/10 06:04,16/Apr/19 17:33,22/Mar/23 14:57,17/Aug/10 03:26,0.7 beta 2,,,,0,,,,,,"I fired up a 3 node cluster. I created few keyspaces using API and inserted to them with no problem. Now I tried to add more CFs to one of those existing Keyspaces in a loop. I got the following exception:

ERROR [MIGRATION-STAGE:1] 2010-08-12 14:46:40,493 CassandraDaemon.java (line 82) Uncaught exception in thread Thread[MIGRATION-STAGE:1,5,main]
java.util.concurrent.ExecutionException: java.lang.RuntimeException: org.apache.cassandra.config.ConfigurationException: Previous version mismatch. cannot apply.
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
	at java.util.concurrent.FutureTask.get(FutureTask.java:111)
	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:87)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1118)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.RuntimeException: org.apache.cassandra.config.ConfigurationException: Previous version mismatch. cannot apply.
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	... 2 more
Caused by: org.apache.cassandra.config.ConfigurationException: Previous version mismatch. cannot apply.
	at org.apache.cassandra.db.migration.Migration.apply(Migration.java:101)
	at org.apache.cassandra.db.DefinitionsUpdateResponseVerbHandler$1.runMayThrow(DefinitionsUpdateResponseVerbHandler.java:70)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 6 more

The above exception is logged in the log of node in which I send the request to and not other seeds. It is noteworthy that my schem_agreement is stuck in a disagreed state:

Array
(
    [1775e847-a658-11df-960f-7d867dfef3ae] => Array
        (
            [0] => 10.50.26.134
        )

    [163d874a-a65b-11df-aef0-d73a63bafff3] => Array
        (
            [0] => 10.50.26.133
        )

    [14869031-a658-11df-8553-930ba61048ac] => Array
        (
            [0] => 10.50.26.132
        )

)

And this does not change. Affect is that some keyspaces would not respond to reads any more giving Internal Error:

ERROR [pool-1-thread-26] 2010-08-12 14:50:57,034 Cassandra.java (line 2988) Internal error processing batch_mutate
java.lang.AssertionError
	at org.apache.cassandra.locator.AbstractReplicationStrategy.getNaturalEndpoints(AbstractReplicationStrategy.java:91)
	at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:1289)
	at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:1277)
	at org.apache.cassandra.service.StorageProxy.mutateBlocking(StorageProxy.java:193)
	at org.apache.cassandra.thrift.CassandraServer.doInsert(CassandraServer.java:474)
	at org.apache.cassandra.thrift.CassandraServer.batch_mutate(CassandraServer.java:438)
	at org.apache.cassandra.thrift.Cassandra$Processor$batch_mutate.process(Cassandra.java:2980)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2499)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)

In my CF creation, I block for CF creation of the same name and not different names. 

Please advice.
","CentOS 5.2
Trunc August 12th, 2010 at 1:30pm",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Aug/10 00:06;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-trap-ConfigExceptions-so-they-don-t-become-RTEs.txt;https://issues.apache.org/jira/secure/attachment/12452032/ASF.LICENSE.NOT.GRANTED--v1-0001-trap-ConfigExceptions-so-they-don-t-become-RTEs.txt",,,,,,,,,,,,,,1.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20113,,,Tue Aug 17 12:59:51 UTC 2010,,,,,,,,,,"0|i0g4q7:",92196,,,,,Normal,,,,,,,,,,,,,,,,,"13/Aug/10 23:03;gdusbabek;I think the problem is that the exception is not being handled properly.  Cassandra is throwing a RuntimeException, which is bad because this is a totally recoverable situation.;;;","14/Aug/10 00:09;gdusbabek;Arya: I wasn't able to replicate your problem, but I think I understand it enough to provide this fix.  Can you please apply and test it?

Basically, a ConfigurationException which Cassandra can recover from is percolating up and getting re-thrown as a RuntimeException, which is bad.;;;","14/Aug/10 00:41;jbellis;what causes the first type?

+                                    logger.info(""Migration not applied "" + ex.getMessage());

should that be error instead of info?;;;","14/Aug/10 02:09;gdusbabek;>what causes the first type?
Trying to apply the same migration twice.  This happens as a result of gossip.  I was dropping them silently before, but figured a log message would be ok.;;;","14/Aug/10 03:27;jbellis;let's comment that and move it to debug then.  +1 otherwise;;;","14/Aug/10 03:46;gdusbabek;ok.  I'm going to hold off on committing this until I hear back from Arya.;;;","14/Aug/10 06:46;arya;I updated my trunc with revision #985305 which includes your change and looks good to me. I tried it few times and I could not get the exception any more. +1;;;","14/Aug/10 20:48;hudson;Integrated in Cassandra #514 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/514/])
    revert last change (committed wrong branch CASSANDRA-1384)
;;;","17/Aug/10 03:26;gdusbabek;committed.;;;","17/Aug/10 20:59;hudson;Integrated in Cassandra #516 (See [https://hudson.apache.org/hudson/job/Cassandra/516/])
    trap ConfigExceptions so they don't become RTEs. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1384
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Assertion failure loadbalance-ing a ByteOrderedPartitioner cluster,CASSANDRA-1008,12462713,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,erickt,erickt,erickt,22/Apr/10 06:42,16/Apr/19 17:33,22/Mar/23 14:57,25/May/10 01:51,0.7 beta 1,,,,1,,,,,,"This seems to be a similar problem to CASSANDRA-1006:

ERROR [GMFD:4] 2010-04-21 15:37:56,942 CassandraDaemon.java (line 77) Fatal exception in thread Thread[GMFD:4,5,main]
java.lang.NumberFormatException: For input string: ""To""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:481)
	at org.apache.cassandra.utils.FBUtilities.hexToBytes(FBUtilities.java:361)
	at org.apache.cassandra.dht.AbstractByteOrderedPartitioner$1.fromString(AbstractByteOrderedPartitioner.java:133)
	at org.apache.cassandra.service.StorageService.handleStateLeft(StorageService.java:622)
	at org.apache.cassandra.service.StorageService.onChange(StorageService.java:517)
	at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:705)
	at org.apache.cassandra.gms.Gossiper.applyApplicationStateLocally(Gossiper.java:695)
	at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:624)
	at org.apache.cassandra.gms.Gossiper$GossipDigestAckVerbHandler.doVerb(Gossiper.java:966)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:41)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
",,daniel.spilker@hamburg.de,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Apr/10 07:07;erickt;0001-Use-the-token-factory-to-convert-tokens-to-strings.-.patch;https://issues.apache.org/jira/secure/attachment/12442489/0001-Use-the-token-factory-to-convert-tokens-to-strings.-.patch",,,,,,,,,,,,,,1.0,erickt,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19952,,,Tue May 25 12:56:37 UTC 2010,,,,,,,,,,"0|i0g2fr:",91825,,,,,Low,,,,,,,,,,,,,,,,,"22/Apr/10 07:07;erickt;Potential patch to fix this bug.;;;","25/May/10 00:42;stuhood;+1
Thanks for catching this Erick... sorry it fell through the cracks for such a long time there.;;;","25/May/10 01:51;jbellis;committed, thanks;;;","25/May/10 20:56;hudson;Integrated in Cassandra #445 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/445/])
    convert byte tokens to strings correctly.  patch by Erick Tryzelaar; reviewed by Stu Hood for CASSANDRA-1008
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error running cqlsh from .tar file -- global name 'SchemaDisagreementException' is not defined,CASSANDRA-2501,12504612,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,cdaw,cdaw,19/Apr/11 06:51,16/Apr/19 17:33,22/Mar/23 14:57,19/Apr/11 21:33,0.8 beta 1,,,,0,cql,,,,,"*Error when running cqlsh*
{code}
[cassandra@cdaw-qa1 cql-1.0.0]$ cqlsh cdaw-qa1
Traceback (most recent call last):
  File ""/usr/bin/cqlsh"", line 212, in <module>
    password=options.password)
  File ""/usr/bin/cqlsh"", line 55, in __init__
    self.conn = cql.connect(hostname, port, user=username, password=password)
  File ""/usr/lib/python2.6/site-packages/cql/__init__.py"", line 51, in connect
    return connection.Connection(host, port, keyspace, user, password)
  File ""/usr/lib/python2.6/site-packages/cql/connection.py"", line 53, in __init__
    c.execute('USE %s;' % keyspace)
  File ""/usr/lib/python2.6/site-packages/cql/cursor.py"", line 126, in execute
    except SchemaDisagreementException, sde:
NameError: global name 'SchemaDisagreementException' is not defined
{code}


*Build*
* Install the cassandra binary from the nightly build
wget https://builds.apache.org/hudson/job/Cassandra/lastSuccessfulBuild/artifact/cassandra/build/apache-cassandra-2011-04-18_11-02-29-bin.tar.gz

* Install cql from .tar file on nightly build
wget https://builds.apache.org/hudson/job/Cassandra/lastSuccessfulBuild/artifact/cassandra/build/cql-1.0.0.tar.gz

*CQL Install Output*
{code}
[cassandra@cdaw-qa1 cql-1.0.0]$ sudo python2.6 ./setup.py install
[sudo] password for cassandra: 
running install
running build
running build_py
running build_scripts
creating build/scripts-2.6
copying and adjusting cqlsh -> build/scripts-2.6
changing mode of build/scripts-2.6/cqlsh from 644 to 755
running install_lib
creating /usr/lib/python2.6/site-packages/cql
copying build/lib/cql/results.py -> /usr/lib/python2.6/site-packages/cql
copying build/lib/cql/marshal.py -> /usr/lib/python2.6/site-packages/cql
copying build/lib/cql/connection.py -> /usr/lib/python2.6/site-packages/cql
copying build/lib/cql/cursor.py -> /usr/lib/python2.6/site-packages/cql
creating /usr/lib/python2.6/site-packages/cql/cassandra
copying build/lib/cql/cassandra/__init__.py -> /usr/lib/python2.6/site-packages/cql/cassandra
copying build/lib/cql/cassandra/Cassandra.py -> /usr/lib/python2.6/site-packages/cql/cassandra
copying build/lib/cql/cassandra/constants.py -> /usr/lib/python2.6/site-packages/cql/cassandra
copying build/lib/cql/cassandra/ttypes.py -> /usr/lib/python2.6/site-packages/cql/cassandra
copying build/lib/cql/decoders.py -> /usr/lib/python2.6/site-packages/cql
copying build/lib/cql/__init__.py -> /usr/lib/python2.6/site-packages/cql
copying build/lib/cql/errors.py -> /usr/lib/python2.6/site-packages/cql
copying build/lib/cql/connection_pool.py -> /usr/lib/python2.6/site-packages/cql
byte-compiling /usr/lib/python2.6/site-packages/cql/results.py to results.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/marshal.py to marshal.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/connection.py to connection.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/cursor.py to cursor.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/cassandra/__init__.py to __init__.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/cassandra/Cassandra.py to Cassandra.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/cassandra/constants.py to constants.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/cassandra/ttypes.py to ttypes.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/decoders.py to decoders.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/__init__.py to __init__.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/errors.py to errors.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/connection_pool.py to connection_pool.pyc
running install_scripts
copying build/scripts-2.6/cqlsh -> /usr/bin
changing mode of /usr/bin/cqlsh to 755
running install_egg_info
Writing /usr/lib/python2.6/site-packages/cql-1.0.0-py2.6.egg-info

{code}
","Running on 3-node Centos 5.5. The cql package was installed with Python 2.6 and prior to installation, I downloaded and installed thrift05-0.5.0.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Apr/11 07:18;jbellis;2501.txt;https://issues.apache.org/jira/secure/attachment/12476664/2501.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20658,,,Tue Apr 19 14:16:54 UTC 2011,,,,,,,,,,"0|i0gbqf:",93331,,cdaw,,cdaw,Normal,,,,,,,,,,,,,,,,,"19/Apr/11 07:18;jbellis;i think the SDE error is masking another problem -- try with this patch (""ant release"" to build the same artifacts hudson does) and see if that exposes something else.;;;","19/Apr/11 10:05;cdaw;The error still occurs after running ""ant release"".  I also upgraded to thrift 6.0.0 and that didn't resolve the issue either.;;;","19/Apr/11 10:12;jbellis;just to doublecheck: you applied the patch (patch -p0 < 2501.txt) before running ant release?;;;","19/Apr/11 10:41;cdaw;Sorry about that ... applied the patch and got a new error:

{code}

[cassandra@cdaw-qa1 cql-1.0.0]$ cqlsh cdaw-qa1
Traceback (most recent call last):
  File ""/usr/bin/cqlsh"", line 212, in <module>
    password=options.password)
  File ""/usr/bin/cqlsh"", line 55, in __init__
    self.conn = cql.connect(hostname, port, user=username, password=password)
  File ""/usr/lib/python2.6/site-packages/cql/__init__.py"", line 51, in connect
    return connection.Connection(host, port, keyspace, user, password)
  File ""/usr/lib/python2.6/site-packages/cql/connection.py"", line 53, in __init__
    c.execute('USE %s;' % keyspace)
  File ""/usr/lib/python2.6/site-packages/cql/cursor.py"", line 133, in execute
    raise cql.InternalError(""Internal application error"")
cql.InternalError: Internal application error
{code};;;","19/Apr/11 10:56;jbellis;Good, that's what I thought.

Cassandra always logs a stacktrace for ""internal application error,"" can you grab that from /var/log/cassandra?

(if i were to take a wild-ass guess I would say it's not handling USE correctly when given a keyspace that doesn't exist.);;;","19/Apr/11 13:08;cdaw;User Error.  I noticed in my log file that the Cassandra version was 0.74 but I was running from the trunk.  I had put $CASSANDRA_HOME in my .bashrc file, but when I removed it, everything was fine.

I would normally resolve this as Will Not Fix, but not sure about the patch you provided, and if you want to associate this with that.;;;","19/Apr/11 21:33;jbellis;Will mark Fixed since the missing SchemaDisagreementException import was a real bug.;;;","19/Apr/11 21:34;jbellis;(committed in r1095082);;;","19/Apr/11 22:16;hudson;Integrated in Cassandra-0.8 #21 (See [https://hudson.apache.org/hudson/job/Cassandra-0.8/21/])
    add SchemaDisagreementException import
patch by jbellis; tested by Cathy Daw for CASSANDRA-2501
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error deleting files during bootstrap,CASSANDRA-681,12444942,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,brandon.williams,brandon.williams,08/Jan/10 01:44,16/Apr/19 17:33,22/Mar/23 14:57,14/Jan/10 00:06,0.5,,,,0,,,,,,"I started a 3 node cluster and proceeded to bootstrap a 4th node.  On one of the existing nodes I began to see tracebacks like this:

ERROR - Error in executor futuretask
java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.io.IOException: Unable to delete /mnt/drive3/data/Keyspace1/stream/Standard1-158-Index.db after 10 tries
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
        at java.util.concurrent.FutureTask.get(FutureTask.java:111)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:53)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1118)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.RuntimeException: java.io.IOException: Unable to delete /mnt/drive3/data/Keyspace1/stream/Standard1-158-Index.db after 10 tries
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:13)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        ... 2 more
Caused by: java.io.IOException: Unable to delete /mnt/drive3/data/Keyspace1/stream/Standard1-158-Index.db after 10 tries
        at org.apache.cassandra.io.DeletionService$2.runMayThrow(DeletionService.java:45)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:9)
        ... 6 more

For various data, index, and filter files.","debian lenny amd64 OpenJDK 64-Bit Server VM (build 1.6.0_0-b11, mixed mode)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Jan/10 04:55;jbellis;681.patch;https://issues.apache.org/jira/secure/attachment/12430045/681.patch",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19816,,,Wed Jan 13 16:06:47 UTC 2010,,,,,,,,,,"0|i0g0fb:",91499,,,,,Low,,,,,,,,,,,,,,,,,"09/Jan/10 01:06;brandon.williams;I also received this while testing CASSANDRA-680, so it is not limited to bootstrap.;;;","12/Jan/10 04:00;jbellis;were there any other errors (like the ones in CASSANDRA-657) in the logs?;;;","13/Jan/10 04:55;jbellis;it looks like the problem was that Streaming.transferSSTables and StreamManager.finish were both attempting to delete the streamed file.  this patch removes the one from transferSSTables.;;;","13/Jan/10 07:06;brandon.williams;+1, error no longer appears;;;","14/Jan/10 00:06;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
removetoken after removetoken rf error fails to work,CASSANDRA-2129,12497902,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,mbulman,mbulman,08/Feb/11 04:56,16/Apr/19 17:33,22/Mar/23 14:57,15/Jul/11 05:26,0.8.2,,,,0,,,,,,"2 node cluster, a keyspace existed with rf=2.  Tried removetoken and got:

mbulman@ripcord-maverick1:/usr/src/cassandra/tags/cassandra-0.7.0$ bin/nodetool -h localhost removetoken 159559397954378837828954138596956659794
Exception in thread ""main"" java.lang.IllegalStateException: replication factor (2) exceeds number of endpoints (1)

Deleted the keyspace, and tried again:

mbulman@ripcord-maverick1:/usr/src/cassandra/tags/cassandra-0.7.0$ bin/nodetool -h localhost removetoken 159559397954378837828954138596956659794
Exception in thread ""main"" java.lang.UnsupportedOperationException: This node is already processing a removal. Wait for it to complete.",,,,,,,,,,,,,,,,,,,,,,,14400,14400,,0%,14400,14400,,,,,,,,,,,,,,,,,,,,"28/May/11 04:28;brandon.williams;2129-v2.txt;https://issues.apache.org/jira/secure/attachment/12480689/2129-v2.txt","17/Mar/11 03:33;brandon.williams;2129.txt;https://issues.apache.org/jira/secure/attachment/12473835/2129.txt",,,,,,,,,,,,,2.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20457,,,Thu Jul 14 22:14:00 UTC 2011,,,,,,,,,,"0|i0g9hj:",92967,,xedin,,xedin,Low,,,,,,,,,,,,,,,,,"08/Feb/11 05:55;jbellis;Does removetoken force clear up the second problem?

We should allow reducing the nodes in the cluster below the RF count, just as we allow creating a keyspace with RF greater than the node count.  (In both cases, writes will be rejected until more nodes are added or RF is reduced.);;;","09/Feb/11 00:26;mbulman;No.  force and status both report no token removals in process.

tags/cassandra-0.7.0# bin/nodetool -h localhost removetoken status                                 
RemovalStatus: No token removals in process.

Restarting the node that was processing the removal clears up the confusion/issue.;;;","17/Mar/11 03:33;brandon.williams;Patch to allow removing the token, and throw UE instead of an internal error when trying to insert and the number of endpoints is less than the RF.;;;","17/Mar/11 10:14;jbellis;SP.mutate should be throwing UAE already (assureSufficientLiveNodes).  Why isn't that working?;;;","18/Mar/11 04:13;brandon.williams;Because it calls rs.getNaturalEndpoints first, which throws ISE and SP.mutate only catches IOException.  I'm a little uneasy with catching both IOE and ISE, or modifying the RS to throw IOE, but I'm not sure what the best solution is.  The patch as-is still has ISE problems, at least on describe_keyspace.;;;","18/Mar/11 10:39;jbellis;what purpose does leaving the ISE in serve, at this point?  should we just remove it?;;;","07/Apr/11 00:59;nickmbailey;Note: a similar error occurs when trying to do describe_ring on a cluster where rf < N.;;;","30/Apr/11 03:14;jbellis;Possibly the same bug was reported on the user list: http://permalink.gmane.org/gmane.comp.db.cassandra.user/15803;;;","28/May/11 04:28;brandon.williams;Removing ISE almost got us all the way there, but there was a subtle bug in WRH.determineBlockFor being relative to the amount of endpoints, instead of the RF.  v2 removes ISE and addresses this problem.;;;","15/Jul/11 05:20;xedin;+1;;;","15/Jul/11 05:26;brandon.williams;Committed;;;","15/Jul/11 06:14;hudson;Integrated in Cassandra-0.8 #215 (See [https://builds.apache.org/job/Cassandra-0.8/215/])
    Allow RF to exceed the number of nodes (but disallow writes)
Patch by brandonwilliams, reviewed by Pavel Yaskevich for CASSANDRA-2129

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1146900
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/locator/OldNetworkTopologyStrategy.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/locator/SimpleStrategy.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/locator/NetworkTopologyStrategy.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/locator/AbstractReplicationStrategy.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/WriteResponseHandler.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spurious failures of o.a.c.db.NameSortTest:testNameSort100,CASSANDRA-1783,12491450,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,urandom,urandom,28/Nov/10 00:09,16/Apr/19 17:33,22/Mar/23 14:57,11/Dec/10 14:52,0.7.0 rc 3,,,,0,,,,,,"{noformat}
    [junit] Cobertura: Loaded information on 961 classes.
    [junit] Cobertura: Saved information on 961 classes.
    [junit] Testsuite: org.apache.cassandra.db.NameSortTest
    [junit] Testsuite: org.apache.cassandra.db.NameSortTest
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec
    [junit] 
    [junit] Testcase: org.apache.cassandra.db.NameSortTest:testNameSort100:	Caused an ERROR
    [junit] Timeout occurred. Please note the time in the report does not reflect the time until the timeout.
    [junit] junit.framework.AssertionFailedError: Timeout occurred. Please note the time in the report does not reflect the time until the timeout.
    [junit]
{noformat}

See also: https://hudson.apache.org/hudson/job/Cassandra-0.7/33/console","Hudson, ubuntu2 (vesta.apache.org)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20312,,,Sat Dec 11 06:52:24 UTC 2010,,,,,,,,,,"0|i0g7db:",92624,,,,,Low,,,,,,,,,,,,,,,,,"11/Dec/10 14:52;jbellis;fixed in r1044570;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error in ThreadPoolExecutor,CASSANDRA-2134,12497954,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,patrik.modesto,patrik.modesto,08/Feb/11 14:57,16/Apr/19 17:33,22/Mar/23 14:57,09/Feb/11 22:23,0.7.1,,,,0,,,,,,"On my two-node test setup I get repeatedly following error:

The 10.0.18.129 server log:
{noformat} 
 INFO 14:10:37,707 Node /10.0.18.99 has restarted, now UP again
 INFO 14:10:37,708 Checking remote schema before delivering hints
 INFO 14:10:37,708 Sleeping 45506ms to stagger hint delivery
 INFO 14:10:37,709 Node /10.0.18.99 state jump to normal
 INFO 14:11:23,215 Started hinted handoff for endpoint /10.0.18.99
ERROR 14:11:23,884 Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.IllegalArgumentException
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.IllegalArgumentException
       at java.nio.Buffer.position(Buffer.java:218)
       at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:117)
       at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:111)
       at org.apache.cassandra.db.HintedHandOffManager.getTableAndCFNames(HintedHandOffManager.java:237)
       at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:306)
       at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:88)
       at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:385)
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
       ... 3 more
ERROR 14:11:23,885 Fatal exception in thread Thread[HintedHandoff:1,1,main]
java.lang.RuntimeException: java.lang.IllegalArgumentException
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.IllegalArgumentException
       at java.nio.Buffer.position(Buffer.java:218)
       at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:117)
       at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:111)
       at org.apache.cassandra.db.HintedHandOffManager.getTableAndCFNames(HintedHandOffManager.java:237)
       at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:306)
       at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:88)
       at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:385)
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
       ... 3 more
{noformat} 

The 10.0.18.99 server log:
{noformat} 
 INFO 14:10:37,691 Binding thrift service to /0.0.0.0:9160
 INFO 14:10:37,693 Using TFastFramedTransport with a max frame size of
15728640 bytes.
 INFO 14:10:37,695 Listening for thrift clients...
 INFO 14:10:38,337 GC for ParNew: 954 ms, 658827608 reclaimed leaving
966732432 used; max is 4265607168
 INFO 14:11:27,142 Started hinted handoff for endpoint /10.0.18.129
ERROR 14:11:27,370 Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.IllegalArgumentException
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.IllegalArgumentException
       at java.nio.Buffer.position(Buffer.java:218)
       at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:117)
       at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:111)
       at org.apache.cassandra.db.HintedHandOffManager.getTableAndCFNames(HintedHandOffManager.java:237)
       at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:306)
       at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:88)
       at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:385)
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
       ... 3 more
ERROR 14:11:27,371 Fatal exception in thread Thread[HintedHandoff:1,1,main]
java.lang.RuntimeException: java.lang.IllegalArgumentException
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.IllegalArgumentException
       at java.nio.Buffer.position(Buffer.java:218)
       at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:117)
       at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:111)
       at org.apache.cassandra.db.HintedHandOffManager.getTableAndCFNames(HintedHandOffManager.java:237)
       at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:306)
       at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:88)
       at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:385)
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
       ... 3 more
{noformat}

It happen durring batch_mutate test or after restart, when there are commitlogs to replay. Using current 0.7.1 from cassandra-0.7 branch.",Linux 2.6.32-bpo.4edois1-openvz-amd64 #1 SMP x86_64 GNU/Linux,cburroughs,patrik.modesto,,,,,,,,,,,,,,,,,,,,60,60,,0%,60,60,,,,,,,,,,,,,,,,,,,,"08/Feb/11 22:57;slebresne;0001-Fix-BBUtil.string-offset-related-to-position-instead.patch;https://issues.apache.org/jira/secure/attachment/12470594/0001-Fix-BBUtil.string-offset-related-to-position-instead.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20460,,,Wed Feb 09 14:23:18 UTC 2011,,,,,,,,,,"0|i0g9in:",92972,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"08/Feb/11 22:57;slebresne;Attached patch should fix this.;;;","08/Feb/11 23:29;hudson;Integrated in Cassandra-0.7 #258 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/258/])
    fix ByteBufferUtil.string position
patch by slebresne; reviewed by jbellis for CASSANDRA-2134
;;;","08/Feb/11 23:54;patrik.modesto;I can still get exception in ThreadPoolExecutor.

1st server:

{noformat}
ERROR 16:50:34,349 Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.Table.createReplicationStrategy(Table.java:318)
        at org.apache.cassandra.db.Table.<init>(Table.java:258)
        at org.apache.cassandra.db.Table.open(Table.java:107)
        at org.apache.cassandra.db.HintedHandOffManager.sendMessage(HintedHandOffManager.java:131)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:313)
        at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:88)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:391)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
ERROR 16:50:34,351 Fatal exception in thread Thread[HintedHandoff:1,1,main]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.Table.createReplicationStrategy(Table.java:318)
        at org.apache.cassandra.db.Table.<init>(Table.java:258)
        at org.apache.cassandra.db.Table.open(Table.java:107)
        at org.apache.cassandra.db.HintedHandOffManager.sendMessage(HintedHandOffManager.java:131)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:313)
        at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:88)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:391)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
{noformat}

2nd server:

{noformat}
ERROR 16:50:37,532 Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.Table.createReplicationStrategy(Table.java:318)
        at org.apache.cassandra.db.Table.<init>(Table.java:258)
        at org.apache.cassandra.db.Table.open(Table.java:107)
        at org.apache.cassandra.db.HintedHandOffManager.sendMessage(HintedHandOffManager.java:131)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:313)
        at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:88)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:391)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
ERROR 16:50:37,547 Fatal exception in thread Thread[HintedHandoff:1,1,main]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.Table.createReplicationStrategy(Table.java:318)
        at org.apache.cassandra.db.Table.<init>(Table.java:258)
        at org.apache.cassandra.db.Table.open(Table.java:107)
        at org.apache.cassandra.db.HintedHandOffManager.sendMessage(HintedHandOffManager.java:131)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:313)
        at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:88)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:391)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
{noformat};;;","09/Feb/11 00:52;jbellis;If you haven't removed the corrupt HintsColumnFamily files, you need to do so.;;;","09/Feb/11 20:12;patrik.modesto;Deleting the old HintColumnFamily files helped. Thanks.;;;","09/Feb/11 22:23;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTable statistics causing intermittent CL test failures in trunk.,CASSANDRA-1430,12472490,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,gdusbabek,gdusbabek,25/Aug/10 23:40,16/Apr/19 17:33,22/Mar/23 14:57,07/Sep/10 06:51,0.7 beta 2,,,,0,,,,,,"    [junit] Testcase: testCleanup(org.apache.cassandra.db.CommitLogTest):	FAILED
    [junit] 2 != 1
    [junit] junit.framework.AssertionFailedError: 2 != 1
    [junit] 	at org.apache.cassandra.db.CommitLogTest.testCleanup(CommitLogTest.java:69)
    [junit] 
    [junit] 

I see this 1-2 times a day.",,johanoskarsson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Sep/10 09:08;brandon.williams;1430.txt;https://issues.apache.org/jira/secure/attachment/12453855/1430.txt",,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20132,,,Sun Sep 12 19:39:07 UTC 2010,,,,,,,,,,"0|i0g50f:",92242,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"26/Aug/10 04:40;gdusbabek;To prove that I'm not hallucinating: http://ci.apache.org/builders/cassandra-trunk/builds/437/steps/compile/logs/stdio;;;","28/Aug/10 04:42;jbellis;this is caused by sstable statistics being written after the flush (see ssTableWriter.closeAndOpenReader)

is it time to admit that storing sstable info, in sstables, wasn't a good idea?;;;","28/Aug/10 04:44;jbellis;(this isn't just a test issue, i'm pretty sure it could bork drain, too);;;","28/Aug/10 06:20;gdusbabek;>is it time to admit that storing sstable info, in sstables, wasn't a good idea?
What if statistics operations ran on its own stage that could be halted at critical moments (flushing, for example).

;;;","28/Aug/10 06:33;jbellis;it already feels like a rather ugly set of workarounds, and maybe it's time to cut our losses;;;","28/Aug/10 06:43;gdusbabek;I can't disagree.  CASSANDRA-1382 should be on the list of things that get reverted when we figure out a better way of statistics.;;;","28/Aug/10 06:57;jbellis;Let's just create an EstimatedHistogram serializer and write out one for each of the row sizes and column counts.  SSTable.components will have to be updated.;;;","04/Sep/10 07:04;brandon.williams;Patch to remove sstable stats from the system table, and make them their own sstable component.;;;","04/Sep/10 08:51;jbellis;I'm getting build errors in a couple test classes;;;","04/Sep/10 09:08;brandon.williams;Oops, I'd forgotten about more tests being added due to CASSANDRA-1155's problems.  Patch updated to fix tests.;;;","04/Sep/10 09:29;jbellis;+1;;;","04/Sep/10 11:32;brandon.williams;Committed.;;;","07/Sep/10 03:30;jbellis;this broke StreamingTransferTest (easy to miss in the ant output because it is a timeout rather than a failure);;;","07/Sep/10 06:51;jbellis;fixed by CASSANDRA-1471;;;","13/Sep/10 03:39;hudson;Integrated in Cassandra #533 (See [https://hudson.apache.org/hudson/job/Cassandra/533/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
intermittent OneCompactionTest failure,CASSANDRA-184,12425622,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,16/May/09 04:55,16/Apr/19 17:33,22/Mar/23 14:57,20/May/09 22:40,0.4,,,,0,,,,,,"[junit] junit.framework.AssertionFailedError                                              
    [junit]     at org.apache.cassandra.io.SSTable.delete(SSTable.java:223)                   
    [junit]     at org.apache.cassandra.db.ColumnFamilyStore.doFileCompaction(ColumnFamilyStore.java:1454)
    [junit]     at org.apache.cassandra.db.ColumnFamilyStore.doCompaction(ColumnFamilyStore.java:896)     
    [junit]     at org.apache.cassandra.db.OneCompactionTest.testCompaction(OneCompactionTest.java:47)    
    [junit]     at org.apache.cassandra.db.OneCompactionTest.testCompaction2(OneCompactionTest.java:60)   
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/May/09 23:47;jbellis;184-0.3.patch;https://issues.apache.org/jira/secure/attachment/12408387/184-0.3.patch","18/May/09 23:48;jbellis;184-trunk.patch;https://issues.apache.org/jira/secure/attachment/12408388/184-trunk.patch",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19587,,,Thu May 21 13:10:36 UTC 2009,,,,,,,,,,"0|i0fxdr:",91006,,,,,Normal,,,,,,,,,,,,,,,,,"18/May/09 23:47;jbellis;OneCompactionTest is failing occasionally because 500 keys per CFS is actually triggering anautomatic compaction (since test flush threshold is only 20) and we were doing a non-threadsafe doCompaction for convenience: the failure occurs when our manual compaction begins mid-run of an automatic one, and the automatic deletesthe original sstable file first.  Fix by (a) dropping the number of keys so that OneCompactionTest lives up to its name (more are tested in ""CompactionsTest"") and (b) making the compactions call threadsafe by refactoring to allow a threshold parameter to MCM.submit.;;;","18/May/09 23:48;jbellis;patch for trunk that applies on top of the one for 0.3.  additional cleanup.;;;","19/May/09 02:51;junrao;The patch looks fine to me. 

The following comment in CFS.storeLocation() confuses me:
        /* it's ok if compaction gets submitted multiple times while one is already in process.
           worst that happens is, compactor will count the sstable files and decide there are
           not enough to bother with. */

With this patch, there can't be mutiple ongoing compactions, right?
;;;","19/May/09 03:11;jbellis;only one compaction can be in progress at a time, but flushes and compactions can happen concurrently (different executors) so a thread flushing CF A can schedule A for a compaction (i.e. submit the op on the MCM) even if A is currently being compacted.  the comment is explaining that when the MCM gets to that redundant compact the impact will be minimal.;;;","20/May/09 22:40;jbellis;guess we are good here.  committed.;;;","21/May/09 21:10;hudson;Integrated in Cassandra #83 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/83/])
    more cleanup of compaction code.
patch by jbellis; reviewed by Jun Rao for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
error reading key until first use of the HTTP interface,CASSANDRA-156,12424927,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,markr,markr,08/May/09 20:55,16/Apr/19 17:33,22/Mar/23 14:57,13/May/09 04:47,0.3,,,,0,,,,,,"After startup, but before the first access to the HTTP interface, thrift command get_slice returns the following error:

./Cassandra-remote -h tst04o:9160 get_slice Messages 305 base 0 1
Traceback (most recent call last):
  File ""./Cassandra-remote"", line 96, in ?
    pp.pprint(client.get_slice(args[0],args[1],args[2],eval(args[3]),eval(args[4]),))
  File ""/opt/mailcontrol/gen-py/org/apache/cassandra/Cassandra.py"", line 213, in get_slice
    return self.recv_get_slice()
  File ""/opt/mailcontrol/gen-py/org/apache/cassandra/Cassandra.py"", line 233, in recv_get_slice
    raise x
thrift.Thrift.TApplicationException: Internal error processing get_slice

Error message on the log file:

ERROR [pool-1-thread-1] 2009-05-08 14:49:36,977 Cassandra.java (line 823) Internal error processing get_slice
java.lang.RuntimeException: error reading key 305
        at org.apache.cassandra.service.StorageProxy.weakReadRemote(StorageProxy.java:256)
        at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:363)
        at org.apache.cassandra.service.CassandraServer.readColumnFamily(CassandraServer.java:112)
        at org.apache.cassandra.service.CassandraServer.get_slice(CassandraServer.java:191)
        at org.apache.cassandra.service.Cassandra$Processor$get_slice.process(Cassandra.java:817)
        at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:805)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:252)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.util.concurrent.TimeoutException: Operation timed out.
        at org.apache.cassandra.net.AsyncResult.get(AsyncResult.java:95)
        at org.apache.cassandra.service.StorageProxy.weakReadRemote(StorageProxy.java:252)
        ... 9 more

After first access to the HTTP interface, the get_slice method now succeeds.","java version ""1.6.0_13"" Linux tst04o 2.6.18-128.1.6.el5 #1 SMP Wed Apr 1 09:10:25 EDT 2009 x86_64 x86_64 x86_64 GNU/Linux
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/May/09 01:19;jbellis;156.patch;https://issues.apache.org/jira/secure/attachment/12407642/156.patch",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19573,,,Wed May 13 09:26:38 UTC 2009,,,,,,,,,,"0|i0fx7r:",90979,,,,,Low,,,,,,,,,,,,,,,,,"08/May/09 22:57;jbellis;how many nodes are you running?  can you reproduce this at will on a fresh start after wiping /var/cassandra/* ?;;;","09/May/09 00:38;markr;Thread dump as requested

Full thread dump Java HotSpot(TM) 64-Bit Server VM (11.3-b02 mixed mode):

""pool-1-thread-2"" prio=10 tid=0x00002aaafc0e6800 nid=0x33bb waiting on condition [0x0000000041804000..0x0000000041804b00]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab24e2680> (a java.util.concurrent.SynchronousQueue$TransferStack)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:422)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:323)
        at java.util.concurrent.SynchronousQueue.take(SynchronousQueue.java:857)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""pool-1-thread-1"" prio=10 tid=0x00002aaafc0dc000 nid=0x33ba waiting on condition [0x0000000041703000..0x0000000041703a80]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab24e2680> (a java.util.concurrent.SynchronousQueue$TransferStack)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:422)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:323)
        at java.util.concurrent.SynchronousQueue.take(SynchronousQueue.java:857)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""GMFD:1"" prio=10 tid=0x00002aaafc0da000 nid=0x33b9 waiting on condition [0x00000000457be000..0x00000000457bea00]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2422968> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""Timer-1"" prio=10 tid=0x00002aaafc0d8800 nid=0x33b8 in Object.wait() [0x00000000456bd000..0x00000000456bdd80]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        - waiting on <0x00002aaab23fcdf8> (a java.util.TaskQueue)
        at java.util.TimerThread.mainLoop(Timer.java:509)
        - locked <0x00002aaab23fcdf8> (a java.util.TaskQueue)
        at java.util.TimerThread.run(Timer.java:462)

""Timer thread for monitoring AnalyticsContext"" daemon prio=10 tid=0x00002aaafc0d7000 nid=0x33b7 in Object.wait() [0x00000000455bc000..0x00000000455bcd00]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        - waiting on <0x00002aaab23b7480> (a java.util.TaskQueue)
        at java.util.TimerThread.mainLoop(Timer.java:509)
        - locked <0x00002aaab23b7480> (a java.util.TaskQueue)
        at java.util.TimerThread.run(Timer.java:462)

""UDP Selector Manager"" prio=10 tid=0x00002aaafc0d5800 nid=0x33b6 runnable [0x00000000454bb000..0x00000000454bbc80]
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:215)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
        - locked <0x00002aaab23a0398> (a sun.nio.ch.Util$1)
        - locked <0x00002aaab23a0380> (a java.util.Collections$UnmodifiableSet)
        - locked <0x00002aaab23a0020> (a sun.nio.ch.EPollSelectorImpl)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
        at org.apache.cassandra.net.SelectorManager.run(SelectorManager.java:92)

""TCP Selector Manager"" prio=10 tid=0x00002aaafc0d0800 nid=0x33b5 runnable [0x0000000041f54000..0x0000000041f54c00]
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:215)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
        - locked <0x00002aaab2397380> (a sun.nio.ch.Util$1)
        - locked <0x00002aaab2397368> (a java.util.Collections$UnmodifiableSet)
        - locked <0x00002aaab2396fe8> (a sun.nio.ch.EPollSelectorImpl)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
        at org.apache.cassandra.net.SelectorManager.run(SelectorManager.java:92)

""HINTED-HANDOFF-POOL:1"" prio=10 tid=0x00002aaafc0bf400 nid=0x33b4 waiting on condition [0x00000000453ba000..0x00000000453bab80]
   java.lang.Thread.State: TIMED_WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab22f6748> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:198)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1963)
        at java.util.concurrent.DelayQueue.take(DelayQueue.java:164)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:583)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:576)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""MINOR-COMPACTION-POOL:1"" prio=10 tid=0x00002aaafc0be400 nid=0x33b3 waiting on condition [0x00000000452b9000..0x00000000452b9b00]
   java.lang.Thread.State: TIMED_WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab22dc660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:198)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1963)
        at java.util.concurrent.DelayQueue.take(DelayQueue.java:164)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:583)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:576)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""MEMTABLE-POOL-HintsColumnFamily10:1"" prio=10 tid=0x00002aaafc0bb000 nid=0x33b2 waiting on condition [0x00000000451b8000..0x00000000451b8a80]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab226f360> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""MEMTABLE-POOL-StandardByTime29:1"" prio=10 tid=0x00002aaafc0b9800 nid=0x33b1 waiting on condition [0x00000000450b7000..0x00000000450b7a00]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab226ae38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""MEMTABLE-POOL-TableMetadata8:1"" prio=10 tid=0x00002aaafc0b8000 nid=0x33b0 waiting on condition [0x0000000044fb6000..0x0000000044fb6d80]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2266918> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""MEMTABLE-POOL-StandardByTime17:1"" prio=10 tid=0x00002aaafc0b3000 nid=0x33af waiting on condition [0x0000000044eb5000..0x0000000044eb5d00]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2262400> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""MEMTABLE-POOL-LocationInfo6:1"" prio=10 tid=0x00002aaafc0b1800 nid=0x33ae waiting on condition [0x0000000044db4000..0x0000000044db4c80]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab225df20> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""MEMTABLE-POOL-base5:1"" prio=10 tid=0x00002aaafc0afc00 nid=0x33ad waiting on condition [0x0000000044cb3000..0x0000000044cb3c00]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2259ae8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""MEMTABLE-POOL-Super24:1"" prio=10 tid=0x00002aaafc0ae000 nid=0x33ac waiting on condition [0x0000000044bb2000..0x0000000044bb2b80]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2255680> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""MEMTABLE-POOL-extra3:1"" prio=10 tid=0x00002aaafc0ac800 nid=0x33ab waiting on condition [0x0000000044ab1000..0x0000000044ab1b00]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2251188> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""MEMTABLE-POOL-Super12:1"" prio=10 tid=0x00002aaafc0abc00 nid=0x33aa waiting on condition [0x00000000449b0000..0x00000000449b0a80]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab224cd20> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""MEMTABLE-POOL-RecycleColumnFamily1:1"" prio=10 tid=0x00002aaafc0a8400 nid=0x33a9 waiting on condition [0x00000000448af000..0x00000000448afa00]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab223e860> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""HTTP-REQUEST:1"" prio=10 tid=0x00002aaafc0a1400 nid=0x33a8 waiting on condition [0x00000000447ae000..0x00000000447aed80]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2159ed8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""MAP-REDUCE-STAGE:4"" prio=10 tid=0x00002aaafc09f800 nid=0x33a7 waiting on condition [0x00000000446ad000..0x00000000446add00]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2157b70> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""MAP-REDUCE-STAGE:3"" prio=10 tid=0x00002aaafc09e000 nid=0x33a6 waiting on condition [0x00000000445ac000..0x00000000445acc80]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2157b70> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""MAP-REDUCE-STAGE:2"" prio=10 tid=0x00002aaafc09c400 nid=0x33a5 waiting on condition [0x00000000444ab000..0x00000000444abc00]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2157b70> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""MAP-REDUCE-STAGE:1"" prio=10 tid=0x00002aaafc09a800 nid=0x33a4 waiting on condition [0x00000000443aa000..0x00000000443aab80]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2157b70> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""ROW-READ-STAGE:8"" prio=10 tid=0x00002aaafc098c00 nid=0x33a3 waiting on condition [0x00000000442a9000..0x00000000442a9b00]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2154ca0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""ROW-READ-STAGE:7"" prio=10 tid=0x00002aaafc097400 nid=0x33a2 waiting on condition [0x00000000441a8000..0x00000000441a8a80]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2154ca0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""ROW-READ-STAGE:6"" prio=10 tid=0x00002aaafc095800 nid=0x33a1 waiting on condition [0x00000000440a7000..0x00000000440a7a00]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2154ca0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""ROW-READ-STAGE:5"" prio=10 tid=0x00002aaafc093c00 nid=0x33a0 waiting on condition [0x0000000043fa6000..0x0000000043fa6d80]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2154ca0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""ROW-READ-STAGE:4"" prio=10 tid=0x00002aaafc092000 nid=0x339f waiting on condition [0x0000000043ea5000..0x0000000043ea5d00]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2154ca0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""ROW-READ-STAGE:3"" prio=10 tid=0x00002aaafc090c00 nid=0x339e waiting on condition [0x0000000043da4000..0x0000000043da4c80]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2154ca0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""ROW-READ-STAGE:2"" prio=10 tid=0x00002aaafc08f800 nid=0x339d waiting on condition [0x0000000043ca3000..0x0000000043ca3c00]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2154ca0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""ROW-READ-STAGE:1"" prio=10 tid=0x00002aaafc08e400 nid=0x339c waiting on condition [0x0000000043ba2000..0x0000000043ba2b80]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2154ca0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""ROW-MUTATION-STAGE:4"" prio=10 tid=0x00002aaafc08c800 nid=0x339b waiting on condition [0x0000000043aa1000..0x0000000043aa1b00]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2152838> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""ROW-MUTATION-STAGE:3"" prio=10 tid=0x00002aaafc08ac00 nid=0x339a waiting on condition [0x00000000439a0000..0x00000000439a0a80]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2152838> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""ROW-MUTATION-STAGE:2"" prio=10 tid=0x00002aaafc089400 nid=0x3399 waiting on condition [0x000000004389f000..0x000000004389fa00]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2152838> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""ROW-MUTATION-STAGE:1"" prio=10 tid=0x00002aaafc087800 nid=0x3398 waiting on condition [0x000000004379e000..0x000000004379ed80]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2152838> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""CONSISTENCY-MANAGER:4"" prio=10 tid=0x00002aaafc085c00 nid=0x3397 waiting on condition [0x000000004369d000..0x000000004369dd00]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab21505d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""CONSISTENCY-MANAGER:3"" prio=10 tid=0x00002aaafc084400 nid=0x3396 waiting on condition [0x000000004359c000..0x000000004359cc80]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab21505d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""CONSISTENCY-MANAGER:2"" prio=10 tid=0x00002aaafc082800 nid=0x3395 waiting on condition [0x000000004349b000..0x000000004349bc00]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab21505d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""CONSISTENCY-MANAGER:1"" prio=10 tid=0x00002aaafc080c00 nid=0x3394 waiting on condition [0x000000004339a000..0x000000004339ab80]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab21505d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""RESPONSE-STAGE:4"" prio=10 tid=0x00002aaafc07e800 nid=0x3393 waiting on condition [0x0000000043299000..0x0000000043299b00]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab20f6e28> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""RESPONSE-STAGE:3"" prio=10 tid=0x00002aaafc07d000 nid=0x3392 waiting on condition [0x0000000043198000..0x0000000043198a80]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab20f6e28> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""RESPONSE-STAGE:2"" prio=10 tid=0x00002aaafc07b400 nid=0x3391 waiting on condition [0x0000000043097000..0x0000000043097a00]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab20f6e28> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""RESPONSE-STAGE:1"" prio=10 tid=0x00002aaafc079800 nid=0x3390 waiting on condition [0x0000000042f96000..0x0000000042f96d80]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab20f6e28> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""MESSAGE-STREAMING-POOL:1"" prio=10 tid=0x00002aaafc077c00 nid=0x338f waiting on condition [0x0000000042e95000..0x0000000042e95d00]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab20ec248> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""MESSAGE-DESERIALIZER-POOL:4"" prio=10 tid=0x00002aaafc076000 nid=0x338e waiting on condition [0x0000000042d94000..0x0000000042d94c80]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab20e9e70> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""MESSAGE-DESERIALIZER-POOL:3"" prio=10 tid=0x00002aaafc074400 nid=0x338d waiting on condition [0x0000000042c93000..0x0000000042c93c00]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab20e9e70> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""MESSAGE-DESERIALIZER-POOL:2"" prio=10 tid=0x00002aaafc072c00 nid=0x338c waiting on condition [0x0000000042b92000..0x0000000042b92b80]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab20e9e70> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""MESSAGE-DESERIALIZER-POOL:1"" prio=10 tid=0x00002aaafc071000 nid=0x338b waiting on condition [0x0000000042a91000..0x0000000042a91b00]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab20e9e70> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""MESSAGE-SERIALIZER-POOL:4"" prio=10 tid=0x00002aaafc06f400 nid=0x338a waiting on condition [0x0000000042990000..0x0000000042990a80]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab20e7bd0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""MESSAGE-SERIALIZER-POOL:3"" prio=10 tid=0x00002aaafc06d800 nid=0x3389 waiting on condition [0x000000004288f000..0x000000004288fa00]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab20e7bd0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""MESSAGE-SERIALIZER-POOL:2"" prio=10 tid=0x00002aaafc06c000 nid=0x3388 waiting on condition [0x000000004278e000..0x000000004278ed80]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab26a45e8> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:747)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:778)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1114)
        at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:186)
        at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:262)
        at org.apache.cassandra.net.TcpConnectionManager.getConnection(TcpConnectionManager.java:59)
        at org.apache.cassandra.net.MessagingService.getConnection(MessagingService.java:313)
        at org.apache.cassandra.net.MessageSerializationTask.run(MessageSerializationTask.java:66)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)

""MESSAGE-SERIALIZER-POOL:1"" prio=10 tid=0x00002aaafc06a400 nid=0x3387 waiting for monitor entry [0x000000004268d000..0x000000004268dd00]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at sun.nio.ch.SelectorImpl.register(SelectorImpl.java:115)
        - waiting to lock <0x00002aaab2397368> (a java.util.Collections$UnmodifiableSet)
        at java.nio.channels.spi.AbstractSelectableChannel.register(AbstractSelectableChannel.java:180)
        - locked <0x00002aaab26c39c0> (a java.lang.Object)
        at org.apache.cassandra.net.SelectorManager.register(SelectorManager.java:79)
        at org.apache.cassandra.net.TcpConnection.<init>(TcpConnection.java:91)
        at org.apache.cassandra.net.TcpConnectionManager.getConnection(TcpConnectionManager.java:64)
        at org.apache.cassandra.net.MessagingService.getConnection(MessagingService.java:313)
        at org.apache.cassandra.net.MessageSerializationTask.run(MessageSerializationTask.java:66)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)

""MESSAGING-SERVICE-POOL:4"" prio=10 tid=0x00002aaafc060000 nid=0x3386 waiting on condition [0x000000004258c000..0x000000004258cc80]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab20e58d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""MESSAGING-SERVICE-POOL:3"" prio=10 tid=0x00002aaafc05e400 nid=0x3385 waiting on condition [0x000000004248b000..0x000000004248bc00]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab20e58d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""MESSAGING-SERVICE-POOL:2"" prio=10 tid=0x00002aaafc05c800 nid=0x3384 waiting on condition [0x000000004238a000..0x000000004238ab80]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab20e58d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""MESSAGING-SERVICE-POOL:1"" prio=10 tid=0x00002aaafc05b400 nid=0x3383 waiting on condition [0x0000000042289000..0x0000000042289b00]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab20e58d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""CACHETABLE-TIMER-2"" daemon prio=10 tid=0x00002aaafc062000 nid=0x3382 in Object.wait() [0x0000000042188000..0x0000000042188a80]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        - waiting on <0x00002aaab20e4fc8> (a java.util.TaskQueue)
        at java.util.TimerThread.mainLoop(Timer.java:509)
        - locked <0x00002aaab20e4fc8> (a java.util.TaskQueue)
        at java.util.TimerThread.run(Timer.java:462)

""CACHETABLE-TIMER-1"" daemon prio=10 tid=0x00002aaafc060c00 nid=0x3381 in Object.wait() [0x0000000042087000..0x0000000042087a00]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        - waiting on <0x00002aaab20e44f8> (a java.util.TaskQueue)
        at java.util.TimerThread.mainLoop(Timer.java:509)
        - locked <0x00002aaab20e44f8> (a java.util.TaskQueue)
        at java.util.TimerThread.run(Timer.java:462)

""LOAD-BALANCER-STAGE:1"" prio=10 tid=0x00002aaafc04e800 nid=0x3380 waiting on condition [0x0000000041602000..0x0000000041602d80]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2077370> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""LB-TARGET:1"" prio=10 tid=0x00002aaafc050000 nid=0x337f waiting on condition [0x0000000041501000..0x0000000041501d00]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab20715b0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""BOOT-STRAPPER:1"" prio=10 tid=0x00002aaafc053800 nid=0x337e waiting on condition [0x0000000041400000..0x0000000041400c80]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab205cbf8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""Timer-0"" prio=10 tid=0x00002aaafc052400 nid=0x337d in Object.wait() [0x00000000412ff000..0x00000000412ffc00]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        - waiting on <0x00002aaab205bb28> (a java.util.TaskQueue)
        at java.util.TimerThread.mainLoop(Timer.java:509)
        - locked <0x00002aaab205bb28> (a java.util.TaskQueue)
        at java.util.TimerThread.run(Timer.java:462)

""FILEUTILS-DELETE-POOL:1"" prio=10 tid=0x00002aaafc04c000 nid=0x337c waiting on condition [0x00000000411fe000..0x00000000411feb80]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab29d07e0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""RMI TCP Accept-0"" daemon prio=10 tid=0x0000000057080c00 nid=0x337a runnable [0x0000000040e43000..0x0000000040e43a80]
   java.lang.Thread.State: RUNNABLE
        at java.net.PlainSocketImpl.socketAccept(Native Method)
        at java.net.PlainSocketImpl.accept(PlainSocketImpl.java:384)
        - locked <0x00002aaab29d0978> (a java.net.SocksSocketImpl)
        at java.net.ServerSocket.implAccept(ServerSocket.java:453)
        at java.net.ServerSocket.accept(ServerSocket.java:421)
        at sun.management.jmxremote.LocalRMIServerSocketFactory$1.accept(LocalRMIServerSocketFactory.java:34)
        at sun.rmi.transport.tcp.TCPTransport$AcceptLoop.executeAcceptLoop(TCPTransport.java:369)
        at sun.rmi.transport.tcp.TCPTransport$AcceptLoop.run(TCPTransport.java:341)
        at java.lang.Thread.run(Thread.java:619)

""RMI TCP Accept-8080"" daemon prio=10 tid=0x000000005706f800 nid=0x3379 runnable [0x00000000410fd000..0x00000000410fda00]
   java.lang.Thread.State: RUNNABLE
        at java.net.PlainSocketImpl.socketAccept(Native Method)
        at java.net.PlainSocketImpl.accept(PlainSocketImpl.java:384)
        - locked <0x00002aaab29d0b88> (a java.net.SocksSocketImpl)
        at java.net.ServerSocket.implAccept(ServerSocket.java:453)
        at java.net.ServerSocket.accept(ServerSocket.java:421)
        at sun.rmi.transport.tcp.TCPTransport$AcceptLoop.executeAcceptLoop(TCPTransport.java:369)
        at sun.rmi.transport.tcp.TCPTransport$AcceptLoop.run(TCPTransport.java:341)
        at java.lang.Thread.run(Thread.java:619)

""RMI TCP Accept-0"" daemon prio=10 tid=0x0000000057065c00 nid=0x3378 runnable [0x0000000040d42000..0x0000000040d42d80]
   java.lang.Thread.State: RUNNABLE
        at java.net.PlainSocketImpl.socketAccept(Native Method)
        at java.net.PlainSocketImpl.accept(PlainSocketImpl.java:384)
        - locked <0x00002aaab29d0d90> (a java.net.SocksSocketImpl)
        at java.net.ServerSocket.implAccept(ServerSocket.java:453)
        at java.net.ServerSocket.accept(ServerSocket.java:421)
        at sun.rmi.transport.tcp.TCPTransport$AcceptLoop.executeAcceptLoop(TCPTransport.java:369)
        at sun.rmi.transport.tcp.TCPTransport$AcceptLoop.run(TCPTransport.java:341)
        at java.lang.Thread.run(Thread.java:619)

""Low Memory Detector"" daemon prio=10 tid=0x0000000056d65400 nid=0x3377 runnable [0x0000000000000000..0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""CompilerThread1"" daemon prio=10 tid=0x0000000056d63000 nid=0x3376 waiting on condition [0x0000000000000000..0x0000000041d51480]
   java.lang.Thread.State: RUNNABLE

""CompilerThread0"" daemon prio=10 tid=0x0000000056d5f000 nid=0x3375 waiting on condition [0x0000000000000000..0x0000000041c503b0]
   java.lang.Thread.State: RUNNABLE

""JDWP Event Helper Thread"" daemon prio=10 tid=0x0000000056d49c00 nid=0x3374 runnable [0x0000000000000000..0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""JDWP Transport Listener: dt_socket"" daemon prio=10 tid=0x0000000056d46400 nid=0x3373 runnable [0x0000000000000000..0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""Signal Dispatcher"" daemon prio=10 tid=0x0000000056d38000 nid=0x3372 runnable [0x0000000000000000..0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""Surrogate Locker Thread (CMS)"" daemon prio=10 tid=0x0000000056d36400 nid=0x3371 waiting on condition [0x0000000000000000..0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""Finalizer"" daemon prio=10 tid=0x0000000056d13800 nid=0x3370 in Object.wait() [0x00000000405df000..0x00000000405dfd80]
   java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        - waiting on <0x00002aaab29d1220> (a java.lang.ref.ReferenceQueue$Lock)
        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:116)
        - locked <0x00002aaab29d1220> (a java.lang.ref.ReferenceQueue$Lock)
        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:132)
        at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:159)

""Reference Handler"" daemon prio=10 tid=0x0000000056d11c00 nid=0x336f in Object.wait() [0x00000000404de000..0x00000000404ded00]
   java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        - waiting on <0x00002aaab29d1258> (a java.lang.ref.Reference$Lock)
        at java.lang.Object.wait(Object.java:485)
        at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:116)
        - locked <0x00002aaab29d1258> (a java.lang.ref.Reference$Lock)

""main"" prio=10 tid=0x0000000056c91400 nid=0x336c runnable [0x000000004080b000..0x000000004080bec0]
   java.lang.Thread.State: RUNNABLE
        at java.net.PlainSocketImpl.socketAccept(Native Method)
        at java.net.PlainSocketImpl.accept(PlainSocketImpl.java:384)
        - locked <0x00002aaab24be1c8> (a java.net.SocksSocketImpl)
        at java.net.ServerSocket.implAccept(ServerSocket.java:453)
        at java.net.ServerSocket.accept(ServerSocket.java:421)
        at org.apache.thrift.transport.TServerSocket.acceptImpl(TServerSocket.java:118)
        at org.apache.thrift.transport.TServerSocket.acceptImpl(TServerSocket.java:34)
        at org.apache.thrift.transport.TServerTransport.accept(TServerTransport.java:31)
        at org.apache.thrift.server.TThreadPoolServer.serve(TThreadPoolServer.java:183)
        at org.apache.cassandra.service.CassandraDaemon.start(CassandraDaemon.java:97)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:133)

""VM Thread"" prio=10 tid=0x0000000056d0c400 nid=0x336e runnable

""Concurrent Mark-Sweep GC Thread"" prio=10 tid=0x0000000056cb0c00 nid=0x336d runnable
""VM Periodic Task Thread"" prio=10 tid=0x0000000057083000 nid=0x337b waiting on condition

JNI global references: 2577

Heap
 def new generation   total 19136K, used 18899K [0x00002aaab1930000, 0x00002aaab2df0000, 0x00002aaab2df0000)
  eden space 17024K,  99% used [0x00002aaab1930000, 0x00002aaab29bff78, 0x00002aaab29d0000)
  from space 2112K,  91% used [0x00002aaab29d0000, 0x00002aaab2bb4e40, 0x00002aaab2be0000)
  to   space 2112K,   0% used [0x00002aaab2be0000, 0x00002aaab2be0000, 0x00002aaab2df0000)
 concurrent mark-sweep generation total 109824K, used 0K [0x00002aaab2df0000, 0x00002aaab9930000, 0x00002aaaf1930000)
 concurrent-mark-sweep perm gen total 21248K, used 13606K [0x00002aaaf1930000, 0x00002aaaf2df0000, 0x00002aaaf6d30000)
;;;","09/May/09 01:18;jbellis;in the future, please add thread dumps as attachments, they're a bit long :)

this part is the key:

""MESSAGE-SERIALIZER-POOL:1"" prio=10 tid=0x00002aaafc06a400 nid=0x3387 waiting for monitor entry [0x000000004268d000..0x000000004268dd00]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at sun.nio.ch.SelectorImpl.register(SelectorImpl.java:115)
        - waiting to lock <0x00002aaab2397368> (a java.util.Collections$UnmodifiableSet)
        at java.nio.channels.spi.AbstractSelectableChannel.register(AbstractSelectableChannel.java:180)
        - locked <0x00002aaab26c39c0> (a java.lang.Object)
        at org.apache.cassandra.net.SelectorManager.register(SelectorManager.java:79)
        at org.apache.cassandra.net.TcpConnection.<init>(TcpConnection.java:91)
        at org.apache.cassandra.net.TcpConnectionManager.getConnection(TcpConnectionManager.java:64)
        at org.apache.cassandra.net.MessagingService.getConnection(MessagingService.java:313)
        at org.apache.cassandra.net.MessageSerializationTask.run(MessageSerializationTask.java:66)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)

that says that register() is blocking on a lock held by the SelectorManager.select():

                selector.select(100);

this is a bug in the jdk or your os.  (I'm not sure how to narrow it down further.)  The semantics of select(100) are,

     * @param  timeout  If positive, block for up to <tt>timeout</tt>
     *                  milliseconds, more or less, while waiting for a
     *                  channel to become ready; if zero, block indefinitely;
     *                  must not be negative

so each 100ms register() calls should be able to go through but here you are getting stuck indefinitely anyway.

we ran into this in CASSANDRA-97 too, there we were able to re-order things so that all the register()s happen before the first select() call but in this case that doesn't seem possible.

Try the attached patch and see if that helps.  If it doesn't, try changing select(100) to select().  (still post-patch-apply.);;;","13/May/09 04:38;markr;156.patch looks like it fixes it, although I've also made a new build off trunk, so something in there could have done instead.

Anyway it seems to have gone away ( the problem) after I used the patched version.;;;","13/May/09 04:50;jbellis;MarkR42: Ok, I reverted the patch and the bug has returned - that's good enough for me :)

committed;;;","13/May/09 17:26;hudson;Integrated in Cassandra #73 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/73/])
    another workaround for register/select wonkiness.  patch by jbellis; tested by Mark Robson for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Scrub resulting in ""bloom filter claims to be longer than entire row size"" error",CASSANDRA-2296,12500831,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,alienth,alienth,09/Mar/11 09:40,16/Apr/19 17:33,22/Mar/23 14:57,09/Mar/11 22:37,0.7.4,,Legacy/Tools,,0,,,,,,"Doing a scrub on a node which I upgraded from 0.7.1 (was previously 0.6.8) to 0.7.3. Getting this error multiple times:
{code}
 WARN [CompactionExecutor:1] 2011-03-08 18:33:52,513 CompactionManager.java (line 625) Row is unreadable; skipping to next
 WARN [CompactionExecutor:1] 2011-03-08 18:33:52,514 CompactionManager.java (line 599) Non-fatal error reading row (stacktrace follows)
java.io.IOError: java.io.EOFException: bloom filter claims to be longer than entire row size
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:117)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:590)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:56)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:195)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.EOFException: bloom filter claims to be longer than entire row size
        at org.apache.cassandra.io.sstable.IndexHelper.defreezeBloomFilter(IndexHelper.java:113)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:87)
        ... 8 more
 WARN [CompactionExecutor:1] 2011-03-08 18:33:52,515 CompactionManager.java (line 625) Row is unreadable; skipping to next
 INFO [CompactionExecutor:1] 2011-03-08 18:33:53,777 CompactionManager.java (line 637) Scrub of SSTableReader(path='/cassandra/data/reddit/Hide-f-671-Data.db') complete: 254709 rows in new sstable
 WARN [CompactionExecutor:1] 2011-03-08 18:33:53,777 CompactionManager.java (line 639) Unable to recover 1630 that were skipped.  You can attempt manual recovery from the pre-scrub snapshot.  You can also run nodetool repair to transfer the data from a healthy replica, if any
{code}",,mdennis,stuhood,,,,,,,,,,,,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,,,,,"09/Mar/11 11:33;jbellis;2296.txt;https://issues.apache.org/jira/secure/attachment/12473092/2296.txt","09/Mar/11 09:58;alienth;sstable_part1.tar.bz2;https://issues.apache.org/jira/secure/attachment/12473088/sstable_part1.tar.bz2","09/Mar/11 09:58;alienth;sstable_part2.tar.bz2;https://issues.apache.org/jira/secure/attachment/12473089/sstable_part2.tar.bz2",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20546,,,Thu Mar 10 19:08:11 UTC 2011,,,,,,,,,,"0|i0gajb:",93137,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"09/Mar/11 10:40;jbellis;With debug logging turned on it looks like this:

{noformat}
[lots of rows around 119 bytes long]
DEBUG [CompactionExecutor:1] 2011-03-08 20:34:12,251 CompactionManager.java (line 559) row 337a306f615f666c38756a is 119 bytes
DEBUG [CompactionExecutor:1] 2011-03-08 20:34:12,251 CompactionManager.java (line 588) Index doublecheck: row 337a306f615f666c38756a is 119 bytes
DEBUG [CompactionExecutor:1] 2011-03-08 20:34:12,251 CompactionManager.java (line 550) Reading row at 44385
DEBUG [CompactionExecutor:1] 2011-03-08 20:34:12,251 CompactionManager.java (line 559) row 34306536785f666f666b65 is 0 bytes
DEBUG [CompactionExecutor:1] 2011-03-08 20:34:12,252 CompactionManager.java (line 588) Index doublecheck: row 34306536785f666f666b65 is 0 bytes
 WARN [CompactionExecutor:1] 2011-03-08 20:34:12,253 CompactionManager.java (line 606) Non-fatal error reading row (stacktrace follows)
java.io.IOError: java.io.EOFException: bloom filter claims to be 734305 bytes, longer than entire row size 0
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:125)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:597)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:57)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:196)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:680)
Caused by: java.io.EOFException: bloom filter claims to be 734305 bytes, longer than entire row size 0
        at org.apache.cassandra.io.sstable.IndexHelper.defreezeBloomFilter(IndexHelper.java:113)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:95)
        ... 8 more
 WARN [CompactionExecutor:1] 2011-03-08 20:34:12,255 CompactionManager.java (line 632) Row is unreadable; skipping to next
DEBUG [CompactionExecutor:1] 2011-03-08 20:34:12,255 CompactionManager.java (line 550) Reading row at 44406
DEBUG [CompactionExecutor:1] 2011-03-08 20:34:12,255 CompactionManager.java (line 559) row 34616465655f66707a6178 is 119 bytes
DEBUG [CompactionExecutor:1] 2011-03-08 20:34:12,255 CompactionManager.java (line 588) Index doublecheck: row 34616465655f66707a6178 is 119 bytes
[lots more rows around 119 bytes]
{noformat}

In other words: there's an row that's empty except for the key, which is causing the problem because we're not supposed to write rows like that.  I checked with a hex editor and that's what it looks like.

The good news is that scrub is correctly skipping it and recovering everything else fine.

The bad news is we have (or possibly, had) a bug that was causing those empty rows to be written.;;;","09/Mar/11 10:59;jbellis;added asserts to catch zero-length rows in r1079650;;;","09/Mar/11 11:07;alienth;Got the following error while restarting *after* I ran the scrub on that same node:

{code}
ERROR [CompactionExecutor:1] 2011-03-08 19:54:48,023 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.io.IOError: java.io.EOFException
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:117)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:67)
        at org.apache.cassandra.io.sstable.SSTableScanner$KeyScanningIterator.next(SSTableScanner.java:179)
        at org.apache.cassandra.io.sstable.SSTableScanner$KeyScanningIterator.next(SSTableScanner.java:144)
        at org.apache.cassandra.io.sstable.SSTableScanner.next(SSTableScanner.java:136)
        at org.apache.cassandra.io.sstable.SSTableScanner.next(SSTableScanner.java:39)
        at org.apache.commons.collections.iterators.CollatingIterator.set(CollatingIterator.java:284)
        at org.apache.commons.collections.iterators.CollatingIterator.least(CollatingIterator.java:326)
        at org.apache.commons.collections.iterators.CollatingIterator.next(CollatingIterator.java:230)
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:68)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
        at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
        at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
        at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:449)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:124)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:94)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code};;;","09/Mar/11 11:12;jbellis;The first scrub ended with

{noformat}
 INFO [CompactionExecutor:1] 2011-03-08 20:45:39,174 CompactionManager.java (line 644) Scrub of SST\
ableReader(path='/var/lib/cassandra/data/KS1/Hide-f-671-Data.db') complete: 254709 rows in new ssta\
ble
 WARN [CompactionExecutor:1] 2011-03-08 20:45:39,174 CompactionManager.java (line 646) Unable to re\
cover 1630 rows that were skipped.  You can attempt manual recovery from the pre-scrub snapshot.  Y\
ou can also run nodetool repair to transfer the data from a healthy replica, if any
{noformat}

Scrubbing the scrubbed version again, ended with

{noformat}
 INFO 21:11:01,349 Scrub of SSTableReader(path='/var/lib/cassandra/data/KS1/Hide-f-672-Data.db') complete: 253308 rows in new sstable
 WARN 21:11:01,349 Unable to recover 1401 rows that were skipped.  You can attempt manual recovery from the pre-scrub snapshot.  You can also run nodetool repair to transfer the data from a healthy replica, if any
{noformat}

Scrub is eating rows.;;;","09/Mar/11 11:21;jbellis;Scrub writes a zero-length row when tombstones expire and there is nothing left, instead of writing no row at all.  So, as the clock rolls forwards and more tombstones expire, you will usually get a few more zero-length rows written, that will be cleaned out by the next scrub.;;;","09/Mar/11 11:22;hudson;Integrated in Cassandra-0.7 #362 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/362/])
    add asserts to make sure we don't write zero-length rows; see CASSANDRA-2296
;;;","09/Mar/11 11:33;jbellis;fix attached.  now skips tombstoned rows properly w/o leaving stubs in the new sstable.;;;","09/Mar/11 17:48;slebresne;In the retry part, the goodRows++ after the if should be removed to avoid counting rows twice.

Other than this, +1;;;","09/Mar/11 22:37;jbellis;committed w/ ++ fix;;;","09/Mar/11 23:02;hudson;Integrated in Cassandra-0.7 #365 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/365/])
    avoid writing empty rows when scrubbing tombstoned rows
patch by jbellis; reviewed by slebresne for CASSANDRA-2296
;;;","10/Mar/11 00:58;jbellis;also added test in r1079882;;;","10/Mar/11 08:32;alienth;I applied the patch and retried. Getting a new exception. Thousands of this:

{code}
 WARN [CompactionExecutor:1] 2011-03-09 17:29:59,752 CompactionManager.java (line 641) Row at 517805025 is unreadable; skipping to next
 INFO [CompactionExecutor:1] 2011-03-09 17:29:59,752 SSTableWriter.java (line 108) Last written key : DecoratedKey(125686934811414729670440675125192621396, 627975726c2833626333626339353363353762313133373331336461303233396438303534312c66692e676f73757065726d6f64656c2e636f6d2f70726f66696c65732f2f6170706c65747265713d3132373333393332313937363529)
 INFO [CompactionExecutor:1] 2011-03-09 17:29:59,752 SSTableWriter.java (line 109) Current key : DecoratedKey(11081980355438931816706032048128862258, 30303063623061323633313463653465376663333561303531326333653737363333663065646134)
 INFO [CompactionExecutor:1] 2011-03-09 17:29:59,752 SSTableWriter.java (line 110) Writing into file /var/lib/cassandra/data/reddit/permacache-tmp-f-168615-Data.db
 WARN [CompactionExecutor:1] 2011-03-09 17:29:59,752 CompactionManager.java (line 607) Non-fatal error reading row (stacktrace follows)
java.io.IOException: Keys must be written in ascending order.
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:111)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:128)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:598)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:56)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:195)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
{code}

Keys are getting written back improperly?;;;","10/Mar/11 09:36;jbellis;Looks like a different problem. What is the context at debug level?;;;","10/Mar/11 09:51;alienth;I'll check the debug output on it tomorrow. I should note that I ran a scrub on this same set of data yesterday on 0.7.3. I got two errors regarding another CF, but nothing for the CF which is now complaining.;;;","11/Mar/11 02:52;alienth;Here is the debug output. Going to get a comparison on the unpatched 0.7.3 to see if there is any difference.

{code}
DEBUG 11:50:52,510 Reading row at 504216964
DEBUG 11:50:52,510 row 636f6d6d656e74735f706172656e74735f3233383135363235 is 66 bytes
DEBUG 11:50:52,510 Index doublecheck: row 636f6d6d656e74735f706172656e74735f3233383135363235 is 66 bytes
 INFO 11:50:52,511 Last written key : DecoratedKey(125686934811414729670440675125192621396, 627975726c2833626333626339353363353762313133373331336461303233396438303534312c66692e676f73757065726d6f64656c2e636f6d2f70726f66696c65732f2f6170706c65747265713d3132373333393332313937363529)
 INFO 11:50:52,511 Current key : DecoratedKey(11047858886149374835950241979723972473, 636f6d6d656e74735f706172656e74735f3233383135363235)
 INFO 11:50:52,511 Writing into file /var/lib/cassandra/data/reddit/permacache-tmp-f-168492-Data.db
 WARN 11:50:52,511 Non-fatal error reading row (stacktrace follows)
java.io.IOException: Keys must be written in ascending order.
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:111)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:128)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:598)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:56)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:195)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
{code}

;;;","11/Mar/11 03:08;alienth;Disregard. Getting the same thing on unpatched 0.7.3. I'll create a separate bug report.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failure to flush commit log,CASSANDRA-694,12445429,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,ryandaum,ryandaum,14/Jan/10 03:11,16/Apr/19 17:33,22/Mar/23 14:57,15/Jan/10 04:09,0.5,,,,0,,,,,,"The following exception occurs consistently on at least node (note did not occur on other same-configured nodes) during startup:

INFO - Replaying /var/lib/cassandra/commitlog/CommitLog-1262855754427.log, /var/lib/cassandra/commitlog/CommitLog-1262832689989.log, /var/lib/cassandra/commitlog/CommitLog-1262885833186.log, /var/lib/cassandra/commitlog/CommitLog-1262900845019.log, /var/lib/cassandra/commitlog/CommitLog-1262913267844.log, /var/lib/cassandra/commitlog/CommitLog-1262927898170.log, /var/lib/cassandra/commitlog/CommitLog-1262961421039.log, /var/lib/cassandra/commitlog/CommitLog-1262977175175.log, /var/lib/cassandra/commitlog/CommitLog-1262989588783.log, /var/lib/cassandra/commitlog/CommitLog-1263000573676.log, /var/lib/cassandra/commitlog/CommitLog-1263013691393.log, /var/lib/cassandra/commitlog/CommitLog-1263044706108.log, /var/lib/cassandra/commitlog/CommitLog-1263060004191.log, /var/lib/cassandra/commitlog/CommitLog-1263071446342.log, /var/lib/cassandra/commitlog/CommitLog-1263082950154.log, /var/lib/cassandra/commitlog/CommitLog-1263095400814.log, /var/lib/cassandra/commitlog/CommitLog-1263118331046.log, /var/lib/cassandra/commitlog/CommitLog-1263143402963.log, /var/lib/cassandra/commitlog/CommitLog-1263155294308.log, /var/lib/cassandra/commitlog/CommitLog-1263166154352.log, /var/lib/cassandra/commitlog/CommitLog-1263178359247.log, /var/lib/cassandra/commitlog/CommitLog-1263202112017.log, /var/lib/cassandra/commitlog/CommitLog-1263230932274.log, /var/lib/cassandra/commitlog/CommitLog-1263250726505.log, /var/lib/cassandra/commitlog/CommitLog-1263264159438.log, /var/lib/cassandra/commitlog/CommitLog-1263289964249.log, /var/lib/cassandra/commitlog/CommitLog-1263317974387.log, /var/lib/cassandra/commitlog/CommitLog-1263331989090.log, /var/lib/cassandra/commitlog/CommitLog-1263344147667.log, /var/lib/cassandra/commitlog/CommitLog-1263359751527.log, /var/lib/cassandra/commitlog/CommitLog-1263395707008.log, /var/lib/cassandra/commitlog/CommitLog-1263397833524.log, /var/lib/cassandra/commitlog/CommitLog-1263398736183.log, /var/lib/cassandra/commitlog/CommitLog-1263399753707.log, /var/lib/cassandra/commitlog/CommitLog-1263401667504.log, /var/lib/cassandra/commitlog/CommitLog-1263404640782.log, /var/lib/cassandra/commitlog/CommitLog-1263405827234.log, /var/lib/cassandra/commitlog/CommitLog-1263406901115.log
INFO - LocationInfo has reached its threshold; switching in a fresh Memtable
INFO - Enqueuing flush of Memtable(LocationInfo)@25934689
INFO - HintsColumnFamily has reached its threshold; switching in a fresh Memtable
INFO - Enqueuing flush of Memtable(HintsColumnFamily)@4766820
INFO - AdXRequestStatistics has reached its threshold; switching in a fresh Memtable
INFO - Enqueuing flush of Memtable(AdXRequestStatistics)@21521158
INFO - TokenGoogleIDCF has reached its threshold; switching in a fresh Memtable
INFO - Enqueuing flush of Memtable(TokenGoogleIDCF)@22889075
java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:160)
Caused by: java.lang.AssertionError: Blocking serialized executor is not yet implemented
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1.rejectedExecution(DebuggableThreadPoolExecutor.java:84)
        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:767)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:658)
        at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:78)
        at org.apache.cassandra.db.ColumnFamilyStore.submitFlush(ColumnFamilyStore.java:1045)
        at org.apache.cassandra.db.ColumnFamilyStore.switchMemtable(ColumnFamilyStore.java:395)
        at org.apache.cassandra.db.ColumnFamilyStore.forceFlush(ColumnFamilyStore.java:448)
        at org.apache.cassandra.db.Table.flush(Table.java:464)
        at org.apache.cassandra.db.CommitLog.recover(CommitLog.java:397)
        at org.apache.cassandra.db.RecoveryManager.doRecovery(RecoveryManager.java:65)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:90)
        at org.apache.cassandra.service.CassandraDaemon.init(CassandraDaemon.java:135)
        ... 5 more

And the same exception occurs intermittently on other node (running) nodes during 'nodeprobe flush':

root@domU-12-31-38-00-26-31:~# nodeprobe -host localhost -port 8080 flush Logger
Exception in thread ""main"" java.lang.AssertionError: Blocking serialized executor is not yet implemented
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1.rejectedExecution(DebuggableThreadPoolExecutor.java:84)
        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:767)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:658)
        at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:78)
        at org.apache.cassandra.db.ColumnFamilyStore.submitFlush(ColumnFamilyStore.java:1045)
        at org.apache.cassandra.db.ColumnFamilyStore.switchMemtable(ColumnFamilyStore.java:395)
        at org.apache.cassandra.db.ColumnFamilyStore.forceFlush(ColumnFamilyStore.java:448)
        at org.apache.cassandra.service.StorageService.forceTableFlush(StorageService.java:984)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1426)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1264)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1359)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:788)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
        at sun.rmi.transport.Transport$1.run(Transport.java:159)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)","Linux  2.6.21.7-2.fc8xen #1 SMP Fri Feb 15 12:39:36 EST 2008 i686 GNU/Linux, ec2 small instance",ryandaum,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Jan/10 04:37;jbellis;694-0.5.txt;https://issues.apache.org/jira/secure/attachment/12430168/694-0.5.txt","14/Jan/10 05:09;jbellis;694-trunk.txt;https://issues.apache.org/jira/secure/attachment/12430173/694-trunk.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19823,,,Thu Jan 14 20:09:04 UTC 2010,,,,,,,,,,"0|i0g0i7:",91512,,,,,Normal,,,,,,,,,,,,,,,,,"14/Jan/10 03:19;ryandaum;Note that the error occurs with build off trunk (svn rev 898899) as well.;;;","14/Jan/10 04:37;jbellis;assumption that all single-threaded executors have an unbounded queue is no longer valid.  this patch provides a policy for dealing with single thread executors w/ a full queue.;;;","14/Jan/10 05:09;jbellis;version for trunk w/ more comments & a unit test.  will backport when i commit to 0.5.;;;","14/Jan/10 05:23;jbellis;from irc:

rdaum> that patch works
;;;","15/Jan/10 04:09;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spurious Gossip Up/Down and IO Errors,CASSANDRA-800,12456474,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,kingryan,kingryan,17/Feb/10 03:48,16/Apr/19 17:33,22/Mar/23 14:57,23/Feb/10 00:36,0.5,,,,0,,,,,,"We're seeing a lot of nodes flapping. It appears to possibly be a race condition in Gossip.

on 10.209.23.110

WARN [MESSAGING-SERVICE-POOL:2] 2010-02-13 01:18:22,976 TcpConnection.java (line 484) Problem reading from socket connected to : java.nio.channels.SocketChannel[connected local=/10.209.23.110:7000 remote=/10.209.23.80:52720]
WARN [MESSAGING-SERVICE-POOL:1] 2010-02-13 01:18:22,976 TcpConnection.java (line 484) Problem reading from socket connected to : java.nio.channels.SocketChannel[connected local=/10.209.23.110:7000 remote=/10.209.23.80:36128]
 WARN [MESSAGING-SERVICE-POOL:2] 2010-02-13 01:18:22,977 TcpConnection.java (line 485) Exception was generated at : 02/13/2010 01:18:22 on thread MESSAGING-SERVICE-POOL:2
Reached an EOL or something bizzare occured. Reading from: /10.209.23.80 BufferSizeRemaining: 16
java.io.IOException: Reached an EOL or something bizzare occured. Reading from: /10.209.23.80 BufferSizeRemaining: 16
    at org.apache.cassandra.net.io.StartState.doRead(StartState.java:44)
    at org.apache.cassandra.net.io.ProtocolState.read(ProtocolState.java:39)
    at org.apache.cassandra.net.io.TcpReader.read(TcpReader.java:95)
    at org.apache.cassandra.net.TcpConnection$ReadWorkItem.run(TcpConnection.java:445)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)


on 10.209.23.80 about the same time


ERROR [pool-1-thread-4751] 2010-02-13 01:17:12,261 Cassandra.java (line 1096) Internal error processing batch_insert
java.util.ConcurrentModificationException
    at java.util.HashMap$HashIterator.nextEntry(HashMap.java:848)
    at java.util.HashMap$KeyIterator.next(HashMap.java:883)
    at java.util.AbstractCollection.addAll(AbstractCollection.java:305)
    at java.util.HashSet.<init>(HashSet.java:100)
    at org.apache.cassandra.gms.Gossiper.getLiveMembers(Gossiper.java:173)
    at org.apache.cassandra.locator.AbstractReplicationStrategy.getHintedMapForEndpoints(AbstractReplicationStrategy.java:120)
    at org.apache.cassandra.locator.AbstractReplicationStrategy.getHintedEndpoints(AbstractReplicationStrategy.java:78)
    at org.apache.cassandra.service.StorageService.getHintedEndpointMap(StorageService.java:1186)
    at org.apache.cassandra.service.StorageProxy.insertBlocking(StorageProxy.java:169)
    at org.apache.cassandra.service.CassandraServer.doInsert(CassandraServer.java:466)
    at org.apache.cassandra.service.CassandraServer.batch_insert(CassandraServer.java:445)
    at org.apache.cassandra.service.Cassandra$Processor$batch_insert.process(Cassandra.java:1088)
    at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:817)
    at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)


just before that:

INFO [Timer-1] 2010-02-13 01:17:12,070 Gossiper.java (line 194) InetAddress /10.209.21.223 is now dead.
INFO [Timer-1] 2010-02-13 01:17:12,257 Gossiper.java (line 194) InetAddress /10.209.21.217 is now dead.
INFO [Timer-1] 2010-02-13 01:17:12,257 Gossiper.java (line 194) InetAddress /10.209.21.216 is now dead.
INFO [Timer-1] 2010-02-13 01:17:12,258 Gossiper.java (line 194) InetAddress /10.209.21.215 is now dead.
INFO [Timer-1] 2010-02-13 01:17:12,258 Gossiper.java (line 194) InetAddress /10.209.23.82 is now dead.


and just after that:

INFO [Timer-1] 2010-02-13 01:17:12,261 Gossiper.java (line 194) InetAddress /10.209.23.81 is now dead.
INFO [Timer-1] 2010-02-13 01:17:12,293 Gossiper.java (line 194) InetAddress /10.209.23.79 is now dead.
INFO [Timer-1] 2010-02-13 01:17:12,304 Gossiper.java (line 194) InetAddress /10.209.21.204 is now dead.
INFO [Timer-1] 2010-02-13 01:17:12,307 Gossiper.java (line 194) InetAddress /10.209.21.197 is now dead.
INFO [Timer-1] 2010-02-13 01:17:12,308 Gossiper.java (line 194) InetAddress /10.209.21.245 is now dead.
INFO [Timer-1] 2010-02-13 01:17:12,309 Gossiper.java (line 194) InetAddress /10.209.21.242 is now dead.
INFO [Timer-1] 2010-02-13 01:17:12,310 Gossiper.java (line 194) InetAddress /10.209.23.106 is now dead.
INFO [GMFD:1] 2010-02-13 01:17:26,780 Log4jLogger.java (line 41) 02/13/2010 01:17:26 - Remaining bytes zero. Stopping deserialization in EndPointState.
INFO [GMFD:1] 2010-02-13 01:17:26,784 Gossiper.java (line 543) InetAddress /10.209.21.204 is now UP
INFO [GMFD:1] 2010-02-13 01:17:26,785 Gossiper.java (line 543) InetAddress /10.209.23.106 is now UP
INFO [GMFD:1] 2010-02-13 01:17:26,786 Gossiper.java (line 543) InetAddress /10.209.21.197 is now UP
INFO [GMFD:1] 2010-02-13 01:17:26,800 Gossiper.java (line 543) InetAddress /10.209.21.216 is now UP
INFO [GMFD:1] 2010-02-13 01:17:41,808 Gossiper.java (line 543) InetAddress /10.209.21.217 is now UP
INFO [GMFD:1] 2010-02-13 01:17:41,823 Gossiper.java (line 543) InetAddress /10.209.21.223 is now UP
INFO [GMFD:1] 2010-02-13 01:17:41,823 Gossiper.java (line 543) InetAddress /10.209.21.215 is now UP


We're on 298a0e66ba66c5d2a1e5d4a70f2f619ae3fbf72a from git.apache.org, which claims to be:

git-svn-id: https://svn.apache.org/repos/asf/incubator/cassandra/branches/cassandra-0.5@9035",,johanoskarsson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Feb/10 20:30;jbellis;800.txt;https://issues.apache.org/jira/secure/attachment/12436488/800.txt",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19869,,,Mon Feb 22 16:36:00 UTC 2010,,,,,,,,,,"0|i0g15j:",91617,,,,,Normal,,,,,,,,,,,,,,,,,"17/Feb/10 03:53;jbellis;the IOException is the same as #657 and is harmless (fixed in trunk, not going to be fixed in 0.5).

the ConcurrentModificationException  may be causing the deadness problem.  (it also might be related to CASSANDRA-757 but the stacktrace is different.);;;","17/Feb/10 10:41;jbellis;Ryan added in IRC:

""this may be the root of the problems I was describing above -- some threads may be dying due to OOM""

I'm skeptical that OOM could cause CME though.;;;","17/Feb/10 11:40;kingryan;I'm skeptical about it too, but I've seen stranger effects from OOME's. We've made some config changes to (hopefully) reduce heap size pressure. I'll let you know if that improves the situation or now.;;;","18/Feb/10 04:15;kingryan;After reducing the heap pressure these errors appear to have gone away. I think it would be reasonable to attribute this behavior to hitting OOME's, which killed some threads, but not all of them.

I think it would be best to kill the server when we hit an OOME.;;;","18/Feb/10 04:46;gdusbabek;OOME would appear in the logs.  I think the JVM is loaded and isn't making the right decisions about which threads to service.  I've been able to duplicate these exact errors on my dev machine when I spin up 4 cassandra instances, set RF=3 and send it a heavy write load.

The gossip thread is rather important in that if it gets stalled when the node isn't really down and the cluster is under a heavy write load, it could lead to writes getting sent to other (already loaded) nodes causing a cascade of failures.  

We might want to see about putting it in it's own thread group and giving it a higher priority.;;;","18/Feb/10 05:04;jbellis;Gary: CME is a correctness issue though, priority shouldn't affect that.

Ryan: OOME is indeed configured to kill the server by default in CassandraDaemon:

            public void uncaughtException(Thread t, Throwable e)
            {
                logger.error(""Fatal exception in thread "" + t, e);
                if (e instanceof OutOfMemoryError)
                {
                    System.exit(100);
                }

if we have a catch-all statement somewhere that is neutering that, the stacktrace should make that obvious.;;;","18/Feb/10 05:35;kingryan;gary-

we did see OOME's in the logs:

ERROR [pool-1-thread-5016] 2010-02-13 02:50:05,872 CassandraDaemon.java (line 71) Fatal exception in thread Thread[pool-1-thread-5016,5,main]
java.lang.OutOfMemoryError: Java heap space
 WARN [MESSAGING-SERVICE-POOL:2] 2010-02-13 02:46:24,194 TcpConnection.java (line 484) Problem reading from socket connected to : java.nio.channels.SocketChannel[connected local=/10.209.23.111:7000 remote=/10.209.23.84:37322]
ERROR [pool-1-thread-4994] 2010-02-13 02:45:29,807 CassandraDaemon.java (line 71) Fatal exception in thread Thread[pool-1-thread-4994,5,main]
java.lang.OutOfMemoryError: Java heap space
ERROR [main] 2010-02-13 02:45:17,044 CassandraDaemon.java (line 184) Exception encountered during startup.
ERROR [MESSAGE-DESERIALIZER-POOL:1] 2010-02-13 02:45:17,044 DebuggableThreadPoolExecutor.java (line 162) Error in executor futuretask
java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: Java heap space
    at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
    at java.util.concurrent.FutureTask.get(FutureTask.java:83)
    at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:154)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.OutOfMemoryError: Java heap space;;;","21/Feb/10 20:30;jbellis;Fix for OOME not killing the server attached;;;","23/Feb/10 00:36;jbellis;i'm going to close this as a dupe of CASSANDRA-757 even though they are different errors, since the right fix for 757 will be using a concurrent structure, which will fix any other CMEs too.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NetworkTopologyStrategy allows mismatched RF resulting in obscure failures,CASSANDRA-1831,12492629,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,scode,scode,07/Dec/10 23:43,16/Apr/19 17:33,22/Mar/23 14:57,08/Dec/10 00:39,,,,,0,,,,,,"On today's 0.7 branch:

Creating a keyspace like this (not how to do it in production, but that's not the point):

   create keyspace MyKeySpace with replication_factor = 2 and placement_strategy = 'org.apache.cassandra.locator.NetworkTopologyStrategy';

This is accepted by Cassandra in spite of there being no strategy options. Describing the keyspace will then give output similar to:

Keyspace: MyKeySpace:
 Replication Strategy: org.apache.cassandra.locator.SimpleStrategy
null

Attempts to write and read respectively gives the errors included at the bottom of this comment.

What happens is that the NTS's getReplicationFactor() returns the sum of RF for each DC. But lacking any replicate placement options for DC:s, the sum will always be 0. The result is that NTS.calculateNaturalEndpoints() yields 0 endpoints thus triggering the assertion failures apparent in the strack traces.

This was caused by misconfiguration during testing but should be handled better. What are people's thoughts on the set of changes that would constitute a proper fix?

Is there a reason for NTS to ever conclude that RF is different than that of the CF def? If not, I would say that one fix is to make the NTS bail early if the calculated RF adding up the DC placements does not match the configured RF for the column family. (I'll submit a patch if people agree.)

Beyond that, what else, if anything should be done? Should the creation fail due to the RF being inconsistent with strategy options? Is it correct that code assumes that naturalEndPoints will never return fewer nodes than RF? It seems natural to me that the natural endpoint count should always match RF, unless the total number of nodes in the cluster is lacking. But this gets complicated with NTS since the requirement is suddenly that you have enough in each DC. This probably relates to previous discussions on whether or not to allow an RF which is higher than the number of nodes in a cluster.

In this case, we failed hard because we got exactly 0 endpoints and triggered assertions. In other cases we might have gotten say 1, in which case we may have successfully been able to read and write as if we had a lower RF even though the column family RF was set to 2. This seems dangerous.

ERROR [pool-1-thread-2] 2010-12-07 11:18:40,638 Cassandra.java (line
3044) Internal error processing batch_mutate
java.lang.AssertionError: invalid response count 1 for replication factor 0
       at org.apache.cassandra.service.WriteResponseHandler.determineBlockFor(WriteResponseHandler.java:98)
       at org.apache.cassandra.service.WriteResponseHandler.<init>(WriteResponseHandler.java:48)
       at org.apache.cassandra.service.WriteResponseHandler.create(WriteResponseHandler.java:61)
       at org.apache.cassandra.locator.AbstractReplicationStrategy.getWriteResponseHandler(AbstractReplicationStrategy.java:125)
       at org.apache.cassandra.locator.NetworkTopologyStrategy.getWriteResponseHandler(NetworkTopologyStrategy.java:166)
       at org.apache.cassandra.service.StorageProxy.mutate(StorageProxy.java:114)
       at org.apache.cassandra.thrift.CassandraServer.doInsert(CassandraServer.java:446)
       at org.apache.cassandra.thrift.CassandraServer.batch_mutate(CassandraServer.java:419)
       at org.apache.cassandra.thrift.Cassandra$Processor$batch_mutate.process(Cassandra.java:3036)
       at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2555)
       at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:662)
ERROR [pool-1-thread-3] 2010-12-07 11:18:50,474 Cassandra.java (line
2876) Internal error processing get_range_slices
java.lang.AssertionError
       at org.apache.cassandra.service.RangeSliceResponseResolver.<init>(RangeSliceResponseResolver.java:53)
       at org.apache.cassandra.service.StorageProxy.getRangeSlice(StorageProxy.java:450)
       at org.apache.cassandra.thrift.CassandraServer.get_range_slices(CassandraServer.java:507)
       at org.apache.cassandra.thrift.Cassandra$Processor$get_range_slices.process(Cassandra.java:2868)
       at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2555)
       at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:662)
 INFO [MigrationStage:1] 2010-12-07 11:24:09,220",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20331,,,Sat Jan 22 16:42:44 UTC 2011,,,,,,,,,,"0|i0g7of:",92674,,,,,Normal,,,,,,,,,,,,,,,,,"08/Dec/10 00:39;jbellis;The right fix is to not allow conflicting definitions in the first place.  CASSANDRA-1263 is open for this.;;;","23/Jan/11 00:42;mck;I hit this issue. It wasn't immediately obvious to me what was required configuration to get NetworkTopologyStrategy working. The best docs i can find is in http://svn.apache.org/repos/asf/cassandra/trunk/conf/cassandra.yaml
A wiki page on NetworkTopologyStrategy would help a lot. (Or someone just telling me to try first OldNetworkTopologyStrategy, why wasn't it called instead SimpleNetworkTopologyStrategy?)

(http://wiki.apache.org/cassandra/Operations#Network_topology still refers to the old RackAwareStrategy);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BRAF assertion error,CASSANDRA-2256,12499978,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,jbellis,jbellis,01/Mar/11 05:29,16/Apr/19 17:33,22/Mar/23 14:57,04/Mar/11 00:44,0.7.4,,,,0,,,,,,"While investigating CASSANDRA-2240 I ran into this:

{noformat}
java.lang.AssertionError
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.read(BufferedRandomAccessFile.java\
:230)
        at java.io.RandomAccessFile.readByte(RandomAccessFile.java:589)
        at org.apache.cassandra.utils.ByteBufferUtil.readShortLength(ByteBufferUtil.java:273)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:284)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:539)
{noformat}",,stuhood,,,,,,,,,,,,,,,,,,,,,14400,14400,,0%,14400,14400,,,,,,,,,,,,,,,,,,,,"03/Mar/11 06:09;xedin;CASSANDRA-2256-v2.patch;https://issues.apache.org/jira/secure/attachment/12472476/CASSANDRA-2256-v2.patch","03/Mar/11 03:30;xedin;CASSANDRA-2256.patch;https://issues.apache.org/jira/secure/attachment/12472452/CASSANDRA-2256.patch",,,,,,,,,,,,,2.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20529,,,Thu Mar 03 16:27:39 UTC 2011,,,,,,,,,,"0|i0gaaf:",93097,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"03/Mar/11 01:52;xedin;Test for the problem:
{code}
public void testAssertOnRead() throws IOException
    {
        BufferedRandomAccessFile file = createTempFile(""braf"");
        file.write(new byte[10]);
        file.sync();

        BufferedRandomAccessFile copy = new BufferedRandomAccessFile(file.getPath(), ""r"");

        copy.seek(15);
        copy.read();

        file.close();
        copy.close();
    }
{code}

This happens when you are trying to seek to position > file length on read-only file and then read (because fileLength is cached, method isEOF() does not work properly). I don't think that we should allow such seeks.;;;","03/Mar/11 03:02;jbellis;Technically RAF.seek allows seeking beyond EOF and writing there (presumably the intervening space would be filled with 0?) but Cassandra doesn't use this and it's kind of a weird corner case.  There's a pretty strong assumption in BRAF that current <= EOF.

So, I would be okay with throwing EOFException if you try to seek past EOF.;;;","03/Mar/11 03:36;jbellis;Is there a way to do this w/o calling channel.size() for each seek?  We seek twice for every row written, and channel.size() is fairly expensive.;;;","03/Mar/11 03:41;xedin;There is no way to skip this unless we will check this only for read-only files.;;;","03/Mar/11 03:50;tjake;throwing EOF is good.

Regarding the check of length on every seek for writable files, I think you could change it to see if the seek is < bufferOffset + buffer.length before calling length()

;;;","03/Mar/11 04:11;xedin;It is also half-solving, I think we can check that for read-only files and for writable we can leave this as is (makes perfect sense), any counter-argument?;;;","03/Mar/11 04:19;tjake;The counter argument is we call seek() 2 times for every row in SSTableWriter.

So we need to make sure we don't call stat() unless we have no other choice.
Another approach would be to set fileLength based on the total number of bytes from the starting size, or 0 for a new file...;;;","03/Mar/11 04:36;xedin;if we will be doing this for read-only condition will be 
{code}
if (fileLength != -1 && newPosition > fileLength)
    throw new EOFException();
{code}

this won't call channel.size() or do any expensive calculations like length();;;","03/Mar/11 04:45;tjake;Oh so you mean let the it throw an AssertionError if you try to seek beyond the end of a file in ""rw"" mode?;;;","03/Mar/11 04:48;jbellis;Not a fan of relying on assert when we really want EOFException.;;;","03/Mar/11 04:55;xedin;Seems that we don't understand each other here :)

seek method will look like this:

{code}
    public void seek(long newPosition) throws IOException
    {
        if (newPosition < 0)
            throw new IllegalArgumentException(""new position should not be negative"");

        if (fileLength != -1 && newPosition > fileLength)
            throw new EOFException(""unable to seek past the end of the file in read-only mode."");

        current = newPosition;

        if (newPosition >= bufferOffset + validBufferBytes || newPosition < bufferOffset)
            reBuffer(); // this will set bufferEnd for us
    }
{code}

We set fileLength = -1 for ""rw"" mode and caching fileLength = channel.size() for ""r"" mode files, so condition ""fileLength != -1 && newPosition > fileLength"" will allow us to block seeking past the end of the file in ""r"" mode leaving ""rw"" untouched (which makes a good sense even if it's unused right now and lets us avoid calling length() for every seek()).;;;","03/Mar/11 05:00;tjake;Right, but what happens if you try to seek past the end of a file in ""rw"" mode?;;;","03/Mar/11 05:04;xedin;That will be allowed as it is right now but that won't create any problems because length of the file is dynamic in that case and isEOF() method will be working properly.;;;","03/Mar/11 05:23;tjake;If that's tested and proven then I don't see an issue with using the code snippet above.;;;","03/Mar/11 05:27;xedin;Yes, it is. I will attach v2 patch asap.;;;","04/Mar/11 00:06;jbellis;committed with additional test that writing past EOF actually works;;;","04/Mar/11 00:27;hudson;Integrated in Cassandra-0.7 #341 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/341/])
    throw EOFException when seeking past EOF in read-only mode
patch by Pavel Yaskevich; reviewed by tjake and jbellis for CASSANDRA-2256
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL: cqlsh error running batch update commands,CASSANDRA-2545,12505019,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,urandom,cdaw,cdaw,23/Apr/11 08:55,16/Apr/19 17:33,22/Mar/23 14:57,27/Apr/11 10:18,,,,,0,,,,,,"*CQL Test Case*
{code}
//TEST CASE #1
BEGIN BATCH
UPDATE users SET gender = 'm', birth_year = '1981' WHERE KEY = 'user1';
UPDATE users SET gender = 'm', birth_year = '1982' WHERE KEY = 'user2';
UPDATE users SET gender = 'm', birth_year = '1983' WHERE KEY = 'user3';
APPLY BATCH	

//TEST CASE #2
BEGIN BATCH USING CONSISTENCY ZERO
UPDATE users SET state = 'TX' WHERE KEY = 'user1';
UPDATE users SET state = 'TX' WHERE KEY = 'user2';
UPDATE users SET state = 'TX' WHERE KEY = 'user3';
APPLY BATCH	


//ERROR
Bad Request: line 0:-1 mismatched input '<EOF>' expecting K_APPLY
{code}

*Test Setup*
{code}
CREATE COLUMNFAMILY users (
  KEY varchar PRIMARY KEY,
  password varchar,
  gender varchar,
  session_token varchar,
  state varchar,
  birth_year bigint);

INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user1', 'ch@ngem3', 'f', 'CA', '1971');
INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user2', 'ch@ngem3', 'f', 'CA', '1972');
INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user3', 'ch@ngem3', 'f', 'CA', '1973');
{code}

*Documented Syntax*
{panel}
BEGIN BATCH [USING <CONSISTENCY>]
UPDATE CF1 SET name1 = value1, name2 = value2 WHERE KEY = keyname1;
UPDATE CF1 SET name3 = value3 WHERE KEY = keyname2;
UPDATE CF2 SET name4 = value4, name5 = value5 WHERE KEY = keyname3;
APPLY BATCH
{panel}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Apr/11 10:47;urandom;ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-2545-also-consider-APPLY-BATCH-for-terminati.txt;https://issues.apache.org/jira/secure/attachment/12477184/ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-2545-also-consider-APPLY-BATCH-for-terminati.txt",,,,,,,,,,,,,,1.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20688,,,Wed Apr 27 03:41:23 UTC 2011,,,,,,,,,,"0|i0gbzz:",93374,,,,,Normal,,,,,,,,,,,,,,,,,"23/Apr/11 10:32;urandom;This is a cqlsh bug.  The attached patch is an improvement, but could probably be improved upon.;;;","23/Apr/11 10:39;jbellis;i don't think ""cqlsh requires commands to end with semicolon"" is a bug;;;","23/Apr/11 10:52;urandom;bq. i don't think ""cqlsh requires commands to end with semicolon"" is a bug

It's not; That's not the bug.

What's buggy is the way that a multi-line statement is parsed from the input.  The attached patch should take care of it.  It does not require the semi-colon for the individual UPDATE statements (APPLY BATCH is the terminator here), but using them won't hurt anything.;;;","27/Apr/11 03:05;cdaw;The patch works fine.;;;","27/Apr/11 10:18;urandom;committed;;;","27/Apr/11 11:41;hudson;Integrated in Cassandra-0.8 #44 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/44/])
    CASSANDRA-2545 also consider APPLY BATCH for terminating statements

Patch by eevans for CASSANDRA-2545
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Warning error when a cache is empty,CASSANDRA-2224,12499427,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Duplicate,,j.casares,j.casares,23/Feb/11 08:24,16/Apr/19 17:33,22/Mar/23 14:57,23/Feb/11 22:09,,,,,0,,,,,,"I get this error after repair stalled and I ran 'sudo killall java' and started up Cassandra again. Most of my caches come with the same error and most of the files they are referencing are empty.

INFO 00:04:37,137 reading saved cache /var/lib/cassandra/saved_caches/system-IndexInfo-KeyCache
 WARN 00:04:37,139 error reading saved cache /var/lib/cassandra/saved_caches/system-IndexInfo-KeyCache
java.io.EOFException
	at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2297)
	at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2766)
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:797)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:297)
	at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:255)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:198)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:451)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:432)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:207)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:129)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)

===================================================

LocationInfo has this in the file: ^@^@^@^AL^@^@^@^AL
And reports this error:

 WARN 00:04:37,179 error reading saved cache /var/lib/cassandra/saved_caches/system-LocationInfo-KeyCache
java.io.StreamCorruptedException: invalid stream header: 00000001
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:800)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:297)
	at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:255)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:198)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:451)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:432)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:207)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:129)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20513,,,Wed Feb 23 14:09:09 UTC 2011,,,,,,,,,,"0|i0ga3b:",93065,,,,,Low,,,,,,,,,,,,,,,,,"23/Feb/11 14:57;mdennis;duplicate CASSANDRA-2174 ?;;;","23/Feb/11 22:09;jbellis;and/or CASSANDRA-2172;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Internal error processing get_range_slices,CASSANDRA-1781,12480961,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,stuhood,patrik.modesto,patrik.modesto,26/Nov/10 17:51,16/Apr/19 17:33,22/Mar/23 14:57,02/Dec/10 09:20,0.6.9,0.7.0 rc 2,,,0,,,,,,"Runnig mapreduce task on two or more Cassandra nodes gives following error:

DEBUG 16:51:48,653 range_slice
DEBUG 16:51:48,653 RangeSliceCommand{keyspace='TEST', column_family='Url', super_column=null, predicate=SlicePredicate(column_names:[java.nio.HeapByteBuffer[pos=57 lim=67 cap=177]]), range=(162950022446285318630909295651345252065,9481098247439719900692337295923514899], max_keys=4096}
DEBUG 16:51:48,653 restricted ranges for query (162950022446285318630909295651345252065,9481098247439719900692337295923514899] are [(162950022446285318630909295651345252065,9481098247439719900692337295923514899]]
DEBUG 16:51:48,653 local range slice
ERROR 16:51:48,653 Internal error processing get_range_slices
java.lang.AssertionError: (162950022446285318630909295651345252065,9481098247439719900692337295923514899]
at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1264)
at org.apache.cassandra.service.StorageProxy.getRangeSlice(StorageProxy.java:429)
at org.apache.cassandra.thrift.CassandraServer.get_range_slices(CassandraServer.java:514)
at org.apache.cassandra.thrift.Cassandra$Processor$get_range_slices.process(Cassandra.java:2868)
at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2555)
at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:619)
DEBUG 16:51:48,838 logged out: #<User allow_all groups=[]>

You can reproduce this by just running contrib/word_count example. Mapreduce last worked with Cassandra 0.7-beta2. Important is to run more than one node.","Debian Linux 2.6.32-openvz-amd64 x86_64 GNU/Linux, Cloudera hadoop CDH3",chrusty,jeromatron,patrik.modesto,shroman,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-1787,,,,,,,,,,,,"02/Dec/10 09:06;stuhood;1781.txt;https://issues.apache.org/jira/secure/attachment/12465094/1781.txt",,,,,,,,,,,,,,1.0,stuhood,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20310,,,Thu Dec 02 02:46:42 UTC 2010,,,,,,,,,,"0|i0g7cv:",92622,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"28/Nov/10 03:22;chrusty;I'm seeing exactly the same behaviour on 0.7-rc1. contrib/word_count (as well as my own map-reduce code based on contrib/word_count) works fine on a single-node, but as soon as i add more nodes to the cluster i get the same errors.

This seems identical to issue #1724 (https://issues.apache.org/jira/browse/CASSANDRA-1724), but that ticket has been ""resolved"".

I can provide logs if they're of use to anybody.


Chris;;;","02/Dec/10 09:06;stuhood;Ack! Posted this to CASSANDRA-1787 yesterday: we had another missing test case. Should be applied to 0.6, 0.7, trunk, etc.;;;","02/Dec/10 09:20;jbellis;committed, thanks!;;;","02/Dec/10 10:46;hudson;Integrated in Cassandra-0.6 #15 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/15/])
    fix range queries against wrapped range
patch by Stu Hood; reviewed by jbellis for CASSANDRA-1781
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
better error handling for CommitLogSyncDelay,CASSANDRA-349,12432405,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,urandom,urandom,06/Aug/09 23:03,16/Apr/19 17:33,22/Mar/23 14:57,08/Aug/09 01:03,0.4,,,,0,,,,,,"When CommitLogSyncDelay is missing from the config, cassandra throws a NumberFormatException. Since this required directive is new for 0.4, it should probably have better handling, (like CommitLogSync does for example).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Aug/09 00:45;jbellis;349.patch;https://issues.apache.org/jira/secure/attachment/12415759/349.patch",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19646,,,Sat Aug 08 12:35:18 UTC 2009,,,,,,,,,,"0|i0fye7:",91170,,,,,Low,,,,,,,,,,,,,,,,,"07/Aug/09 00:45;jbellis;patch;;;","08/Aug/09 00:26;urandom;+1;;;","08/Aug/09 01:03;jbellis;committed;;;","08/Aug/09 20:35;hudson;Integrated in Cassandra #161 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/161/])
    human-readable error for bad CommitLogSyncDelay.
patch by jbellis; reviewed by Eric Evans for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh errors on comments that end with a semicolon,CASSANDRA-2488,12504418,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,urandom,angryparsley,angryparsley,16/Apr/11 06:25,20/Aug/20 20:03,22/Mar/23 14:57,20/Apr/11 01:54,0.8 beta 1,,Legacy/Tools,,0,cql,,,,,"Commented-out lines that end in a semicolon cause an error.

Examples:

cqlsh> -- CREATE KEYSPACE ELE WITH replication_factor = 3 AND strategy_class = SimpleStrategy AND strategy_options:replication_factor=3;
Bad Request: line 0:-1 no viable alternative at input '<EOF>'
cqlsh> -- CREATE KEYSPACE ELE WITH replication_factor = 3 AND strategy_class = SimpleStrategy AND strategy_options:replication_factor=3
   ... 
   ... 
   ... ;
Bad Request: line 2:0 no viable alternative at input ';'
cqlsh> -- ;
Bad Request: line 0:-1 no viable alternative at input '<EOF>'
cqlsh> --;
Bad Request: line 0:-1 no viable alternative at input '<EOF>'

As long as there's a line with valid CQL before the semicolon, things work fine though.

I'm pretty sure the problem is on line 75 of cqlsh:
        if not line.endswith("";""):
            self.set_prompt(Shell.continue_prompt)
            return None

A quick workaround would be to kill the pretty continue prompt. A more involved fix would detect whether or not the semicolon was in a comment. This is harder than it sounds, since /* and */ allow multi-line comments.","OS X 10.6.7

$ java -version
java version ""1.6.0_24""
Java(TM) SE Runtime Environment (build 1.6.0_24-b07-334-10M3326)
Java HotSpot(TM) 64-Bit Server VM (build 19.1-b02-334, mixed mode)

(This stuff isn't really important. It's a bug in a Python script)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-15802,,,,,,,,"17/Apr/11 10:28;urandom;ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-2488-teach-cqlsh-to-ignore-comments.txt;https://issues.apache.org/jira/secure/attachment/12476545/ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-2488-teach-cqlsh-to-ignore-comments.txt",,,,,,,,,,,,,,1.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20650,,,Tue Apr 19 19:05:56 UTC 2011,,,,,,,,,,"0|i0gbnr:",93319,,gdusbabek,,gdusbabek,Low,,,,,,,,,,,,,,,,,"17/Apr/11 10:19;urandom;the attached patch should deal with any supported comment;;;","20/Apr/11 01:41;gdusbabek;+1;;;","20/Apr/11 01:54;urandom;committed;;;","20/Apr/11 03:05;hudson;Integrated in Cassandra-0.8 #22 (See [https://hudson.apache.org/hudson/job/Cassandra-0.8/22/])
    teach cqlsh to ignore comments

Patch by eevans; reviewed by gdusbabek for CASSANDRA-2488
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"hudson test failure: ""Forked Java VM exited abnormally.""",CASSANDRA-1834,12492681,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,urandom,urandom,08/Dec/10 07:04,16/Apr/19 17:33,22/Mar/23 14:57,12/Dec/10 00:12,0.7.0 rc 3,,,,0,,,,,,"https://hudson.apache.org/hudson/view/A-F/view/Cassandra/job/Cassandra-0.7/56/

{noformat}
    [junit] Testsuite: org.apache.cassandra.service.EmbeddedCassandraServiceTest
    [junit] Testsuite: org.apache.cassandra.service.EmbeddedCassandraServiceTest
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec
    [junit] 
    [junit] Testcase: org.apache.cassandra.service.EmbeddedCassandraServiceTest:BeforeFirstTest:	Caused an ERROR
    [junit] Forked Java VM exited abnormally. Please note the time in the report does not reflect the time until the VM exit.
    [junit] junit.framework.AssertionFailedError: Forked Java VM exited abnormally. Please note the time in the report does not reflect the time until the VM exit.
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.service.EmbeddedCassandraServiceTest FAILED (crashed)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20334,,,Sat Dec 11 16:12:28 UTC 2010,,,,,,,,,,"0|i0g7p3:",92677,,,,,Normal,,,,,,,,,,,,,,,,,"08/Dec/10 07:40;urandom;a bisect suggests it was introduced here: https://svn.apache.org/viewvc?view=revision&revision=1041951

""reads at ConsistencyLevel > 1 throwUnavailableException immediately if insufficient live nodes exist
patch by jbellis and tjake for CASSANDRA-1803"";;;","08/Dec/10 08:59;jbellis;usually when i've seen ""vm exited abnormally"" there is something in the log or on stdout, can we get the subprocess stdout from hudson?;;;","08/Dec/10 09:36;urandom;Is this not it? https://hudson.apache.org/hudson/view/A-F/view/Cassandra/job/Cassandra-0.7/56/console

That's everything that I see when it fails on me locally.;;;","08/Dec/10 09:43;jbellis;that's the console of the junit jvm, not the console of the jvm it forked to run the test;;;","08/Dec/10 12:27;urandom;{noformat}
org.apache.cassandra.config.ConfigurationException: Found system table files, but they couldn't be loaded. Did you change the partitioner?
        at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:236)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:105)
        at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:55)
        at org.apache.cassandra.service.AbstractCassandraDaemon.init(AbstractCassandraDaemon.java:183)
        at org.apache.cassandra.service.EmbeddedCassandraService.init(EmbeddedCassandraService.java:72)
        at org.apache.cassandra.service.EmbeddedCassandraServiceTest.setup(EmbeddedCassandraServiceTest.java:78)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:27)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:220)
        at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
        at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:422)
        at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:931)
        at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:785
{noformat};;;","08/Dec/10 13:54;jbellis;I bet having ECST extend CleanupHelper will clear that up;;;","08/Dec/10 14:56;tjake;When this happens to me I normally do remove the build/test dir to resolve (some kind of corruption)
;;;","12/Dec/10 00:12;jbellis;bq. I bet having ECST extend CleanupHelper will clear that up 

done.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error when read repair is disabled,CASSANDRA-2010,12496038,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,20/Jan/11 02:54,16/Apr/19 17:33,22/Mar/23 14:57,20/Jan/11 10:34,0.7.1,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,"20/Jan/11 02:56;jbellis;2010.txt;https://issues.apache.org/jira/secure/attachment/12468775/2010.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19350,,,Thu Jan 20 02:59:27 UTC 2011,,,,,,,,,,"0|i0g8rj:",92850,,tjake,,tjake,Low,,,,,,,,,,,,,,,,,"20/Jan/11 03:01;jbellis;the problem is that messages and endpoints in the sendRR need to be the same length.  fix attached.;;;","20/Jan/11 07:15;tjake;+1;;;","20/Jan/11 10:34;jbellis;committed;;;","20/Jan/11 10:59;hudson;Integrated in Cassandra-0.7 #181 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/181/])
    fix messages/endpoints mismatch when RR is disabled
patch by jbellis; reviewed by tjake for CASSANDRA-2010
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Attempting to mutate a non-existant CF does not propagate an error to the client,CASSANDRA-1036,12463365,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,brandon.williams,brandon.williams,30/Apr/10 03:33,16/Apr/19 17:33,22/Mar/23 14:57,07/May/10 21:58,0.7 beta 1,,,,0,,,,,,"An error gets logged on the server:

ERROR 15:23:21,035 Attempting to mutate non-existant column family Standard1

But nothing is raised on the client side, so it appears the request succeeded.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/May/10 20:35;slebresne;1036-Validate-CF-for-deletion-in-mutation-map.patch;https://issues.apache.org/jira/secure/attachment/12443958/1036-Validate-CF-for-deletion-in-mutation-map.patch","07/May/10 20:47;slebresne;1036-v2-Validate-CF-for-deletion-in-mutation-map.patch;https://issues.apache.org/jira/secure/attachment/12443960/1036-v2-Validate-CF-for-deletion-in-mutation-map.patch",,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19967,,,Fri May 07 13:58:04 UTC 2010,,,,,,,,,,"0|i0g2lz:",91853,,,,,Low,,,,,,,,,,,,,,,,,"07/May/10 20:35;slebresne;Attached patch should correct the problem.;;;","07/May/10 20:47;slebresne;Oups. Figured that I could add the validation to Avro too. Patch v2 attached.;;;","07/May/10 20:51;gdusbabek;Should we include a system test to verify?;;;","07/May/10 21:00;slebresne;You mean, for the avro part ? I'll have to admit that I'm not up to date on the avro part, 
but the avro system test for batch_mutate has a 
        # FIXME: still need to apply a mutation that deletes
so I was figuring that maybe that wasn't working yet. But I can have a look.;;;","07/May/10 21:14;gdusbabek;No.  I'm referring to test/system/test_thrift_server.py.  There are a few examples in there where a call is made and we expect an exception.  We should have a call like that where we attempt to mutate a non-existent CF and check to make sure an exception is received by the client.

See the usages of ""_expect_exception"".;;;","07/May/10 21:18;slebresne;The patch includes system tests for thrift. Or are those included not the ones you had in mind ?;;;","07/May/10 21:30;gdusbabek;My apologies.  For some reason I was expecting a git-style patch with a  listing of the modified files at the top.  I see the system test now.;;;","07/May/10 21:56;gdusbabek;+1.;;;","07/May/10 21:58;gdusbabek;Thanks for the patch!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error saving cache on Windows,CASSANDRA-2207,12499223,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,tantra,tantra,21/Feb/11 20:14,16/Apr/19 17:33,22/Mar/23 14:57,23/Feb/11 22:10,0.7.3,,,,0,,,,,,"I launch clean cassandra 7.2 instalation, and after few days i look at system.log follow error (more then 10 times):


ERROR [CompactionExecutor:1] 2011-02-19 02:56:17,965 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.RuntimeException: java.io.IOException: Unable to rename cache to F:\Cassandra\7.2\saved_caches\system-LocationInfo-KeyCache
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: Unable to rename cache to F:\Cassandra\7.2\saved_caches\system-LocationInfo-KeyCache
    at org.apache.cassandra.io.sstable.CacheWriter.saveCache(CacheWriter.java:85)
    at org.apache.cassandra.db.CompactionManager$9.runMayThrow(CompactionManager.java:746)
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
    ... 6 more
",WindowsXP(SP3) 32 bit,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,"22/Feb/11 05:27;jbellis;2207.txt;https://issues.apache.org/jira/secure/attachment/12471579/2207.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20504,,,Wed Feb 23 15:18:09 UTC 2011,,,,,,,,,,"0|i0g9z3:",93046,,mdennis,,mdennis,Low,,,,,,,,,,,,,,,,,"22/Feb/11 05:27;jbellis;Apparently Windows is won't let you rename over an existing file. Patch attached to explicitly delete the old one first.;;;","23/Feb/11 15:31;mdennis;+1;;;","23/Feb/11 22:10;jbellis;committed;;;","23/Feb/11 23:18;hudson;Integrated in Cassandra-0.7 #309 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/309/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deleting and re-inserting row causes error in get_slice count parameter,CASSANDRA-920,12460296,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,bflorian,bflorian,26/Mar/10 06:52,16/Apr/19 17:33,22/Mar/23 14:57,13/Apr/10 05:14,0.6.1,,,,0,,,,,,"I've found that when I delete an entire row in a column family with super columns, and then re-insert values with the same row and super column keys, the count parameter to the get_slice call no longer works properly.  Its like it is still counting the deleted columns, but only returning the new columns.

The following example uses the Ruby Cassandra client (see link below), but I've seen the same behavior with the Java Thrift interface.

Test code:
--------------
require 'rubygems'
require 'cassandra'
cc = Cassandra.new('Keyspace1')
cc.insert(:Super1,'test-key1',{'bucket1' => {'1' => 'Item 1', '2' => 'Item 2', '5' => 'Item 5'}})
items = cc.get(:Super1,'test-key1','bucket1')
puts ""returned #{items.size} items, should be 3""
cc.remove(:Super1,'test-key1')
items = cc.get(:Super1,'test-key1','bucket1')
puts ""returned #{items.size} items, should be 0""
cc.insert(:Super1,'test-key1',{'bucket1' => {'3' => 'Item 3', '4' => 'Item 4', '6' => 'Item 6'}})
items = cc.get(:Super1,'test-key1','bucket1')
puts ""returned #{items.size} items, should be 3""
items = cc.get(:Super1,'test-key1','bucket1',:count => 3)
puts ""returned #{items.size} items, should be 3""
items = cc.get(:Super1,'test-key1','bucket1',:count => 4)
puts ""returned #{items.size} items, should be 3""
items = cc.get(:Super1,'test-key1','bucket1',:count => 5)
puts ""returned #{items.size} items, should be 3""
items = cc.get(:Super1,'test-key1','bucket1',:count => 6)
puts ""returned #{items.size} items, should be 3""

Output:
returned 3 items, should be 3
returned 0 items, should be 0
returned 3 items, should be 3
returned 1 items, should be 3
returned 2 items, should be 3
returned 2 items, should be 3
returned 3 items, should be 3

Ruby library link:
http://blog.evanweaver.com/files/doc/fauna/cassandra/files/README_rdoc.html",Mac OS/ Java 6,brandon.williams,kingryan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Apr/10 03:47;brandon.williams;ASF.LICENSE.NOT.GRANTED--0001_add_system_test.txt;https://issues.apache.org/jira/secure/attachment/12441537/ASF.LICENSE.NOT.GRANTED--0001_add_system_test.txt","13/Apr/10 04:58;jbellis;ASF.LICENSE.NOT.GRANTED--920.txt;https://issues.apache.org/jira/secure/attachment/12441545/ASF.LICENSE.NOT.GRANTED--920.txt",,,,,,,,,,,,,2.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19918,,,Mon Apr 12 21:14:32 UTC 2010,,,,,,,,,,"0|i0g1w7:",91737,,,,,Low,,,,,,,,,,,,,,,,,"08/Apr/10 04:38;brandon.williams;Patch to reproduce with a system test.  Note that there is a commented get_count in here.  If you uncomment it, strangely enough, the test passes.;;;","08/Apr/10 08:04;brandon.williams;Oops, I had a bug in that patch.  Now with this one I'm unable to reproduce.;;;","09/Apr/10 03:37;jbellis;Bob, closing since it appears to work fine in Brandon's test.  (Maybe it's a timestamping problem?  I have no idea what resolution the rb client uses by default.);;;","09/Apr/10 03:52;jbellis;reopening to commit test;;;","09/Apr/10 05:03;kingryan;I've rewritten this in terms of our cassandra.gem test suite and can't seem to find a bug in the client. It fails on the last line.

{code}
    @twitter.insert(:StatusRelationships, key,  {'user_timelines' => {@uuids[0] => 'Item 0', @uuids[1] => 'Item 1', @uuids[2] => 'Item 2'}})
    @twitter.remove(:StatusRelationships, key)

    items = @twitter.get(:StatusRelationships, key, 'user_timelines')
    assert_equal 0, items.size

    @twitter.insert(:StatusRelationships, key,
        {'user_timelines' => {@uuids[2] => 'Item 2', @uuids[3] => 'Item 3', @uuids[4] => 'Item 4'}})

    items = @twitter.get(:StatusRelationships, key, 'user_timelines')
    assert_equal 3, items.size

    items = @twitter.get(:StatusRelationships,key,'user_timelines',:count => 3)
    assert_equal 3, items.size 
{code}

And the server logs for this whole test:

{code}
DEBUG - batch_mutate
DEBUG - insert writing local key test_get_super_sub_keys_with_count_after_remove
DEBUG - remove
DEBUG - insert writing local key test_get_super_sub_keys_with_count_after_remove
DEBUG - multiget_slice
DEBUG - weakreadlocal reading SliceFromReadCommand(table='Twitter', key='test_get_super_sub_keys_with_count_after_remove', column_parent='QueryPath(columnFamilyName='StatusRelationships', superColumnName='[B@746a63d3', columnName='null')', start='', finish='', reversed=false, count=100)
DEBUG - batch_mutate
DEBUG - insert writing local key test_get_super_sub_keys_with_count_after_remove
DEBUG - multiget_slice
DEBUG - weakreadlocal reading SliceFromReadCommand(table='Twitter', key='test_get_super_sub_keys_with_count_after_remove', column_parent='QueryPath(columnFamilyName='StatusRelationships', superColumnName='[B@3bd840d9', columnName='null')', start='', finish='', reversed=false, count=100)
DEBUG - collecting 93814000-b668-11b2-8e6a-06dc05f11207:false:6@1270760311002397
DEBUG - collecting 13814000-4eff-11b3-88b0-351600af16aa:false:6@1270760311002397
DEBUG - collecting 13814000-802c-11b4-8782-bede78b81183:false:6@1270760311069304
DEBUG - collecting 13814000-e286-11b6-9ea9-48a9edd974b6:false:6@1270760311069304
DEBUG - collecting 13814000-a73a-11bb-8a14-b8d289de6d53:false:6@1270760311069304
DEBUG - multiget_slice
DEBUG - weakreadlocal reading SliceFromReadCommand(table='Twitter', key='test_get_super_sub_keys_with_count_after_remove', column_parent='QueryPath(columnFamilyName='StatusRelationships', superColumnName='[B@6e37d490', columnName='null')', start='', finish='', reversed=false, count=3)
DEBUG - collecting 93814000-b668-11b2-8e6a-06dc05f11207:false:6@1270760311002397
DEBUG - collecting 13814000-4eff-11b3-88b0-351600af16aa:false:6@1270760311002397
DEBUG - collecting 13814000-802c-11b4-8782-bede78b81183:false:6@1270760311069304
DEBUG - GC for ParNew: 7 ms, 20659648 reclaimed leaving 19819056 used; max is 1211826176
{code}

And the thrift structures for the failing multiget_slice

{code}
<CassandraThrift::ColumnParent column_family:""StatusRelationships"", super_column:""user_timelines"">
<CassandraThrift::SlicePredicate slice_range:<CassandraThrift::SliceRange start:"""", finish:"""", reversed:false, count:3>>
{code}
;;;","09/Apr/10 06:38;brandon.williams;After much head scratching, Ryan and I figured out you have to remove the entire row, not just the subcolumns, to cause this.  Updated system test which does indeed fail.;;;","09/Apr/10 07:17;brandon.williams;Oh, and just to come full circle... if you put a get_count in between the the remove and subsequent slice, it passes.  I guess I had it right in the first patch after all. ;;;","13/Apr/10 02:44;jbellis;{code}
PYTHONPATH=test nosetests --tests=system.test_server:TestMutations.test_super_reinsert
.
----------------------------------------------------------------------
Ran 1 test in 2.432s

OK
{code};;;","13/Apr/10 03:47;brandon.williams;I must have attached the wrong patch, try this one.;;;","13/Apr/10 04:58;jbellis;(Brief) fix attached.

SuperColumns suck.;;;","13/Apr/10 05:03;brandon.williams;+1, both to the patch and SCs sucking.;;;","13/Apr/10 05:14;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Assertion error on quorum write,CASSANDRA-593,12442183,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dispalt,dispalt,dispalt,02/Dec/09 13:33,16/Apr/19 17:33,22/Mar/23 14:57,03/Dec/09 06:41,0.5,,,,0,,,,,,"In the middle of a quorum write I encountered this error.


2009-12-02_04:59:59.79647 java.lang.AssertionError: invalid response count 3
2009-12-02_04:59:59.79647       at org.apache.cassandra.service.WriteResponseHandler.<init>(WriteResponseHandler.java:47)
2009-12-02_04:59:59.79647       at org.apache.cassandra.locator.AbstractReplicationStrategy.getWriteResponseHandler(AbstractReplicationStrategy.java:61)
2009-12-02_04:59:59.79647       at org.apache.cassandra.service.StorageService.getWriteResponseHandler(StorageService.java:1081)
2009-12-02_04:59:59.79647       at org.apache.cassandra.service.StorageProxy.insertBlocking(StorageProxy.java:193)
2009-12-02_04:59:59.79647       at org.apache.cassandra.service.CassandraServer.doInsert(CassandraServer.java:466)
2009-12-02_04:59:59.79647       at org.apache.cassandra.service.CassandraServer.batch_insert(CassandraServer.java:445)
2009-12-02_04:59:59.79647       at org.apache.cassandra.service.Cassandra$Processor$batch_insert.process(Cassandra.java:983)
2009-12-02_04:59:59.79647       at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:712)
2009-12-02_04:59:59.79647       at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
2009-12-02_04:59:59.79647       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
2009-12-02_04:59:59.79647       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
2009-12-02_04:59:59.79647       at java.lang.Thread.run(Thread.java:636)",using trunk (r885572) while in the middle of a loadbalance,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,dispalt,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19770,,,Sat Dec 05 12:34:26 UTC 2009,,,,,,,,,,"0|i0fzvz:",91412,,,,,Low,,,,,,,,,,,,,,,,,"03/Dec/09 06:41;jbellis;one-line fix in r886332;;;","05/Dec/09 20:34;hudson;Integrated in Cassandra #278 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/278/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Assertion in logs: Internal error processing multiget_slice,CASSANDRA-1496,12473924,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,wr0ngway,wr0ngway,12/Sep/10 09:44,16/Apr/19 17:33,22/Mar/23 14:57,12/Sep/10 10:36,,,,,0,,,,,,"Seeing a number of these in my log when running a trunk build from 9/10/2010
No idea how to duplicate it, hopefully you can make sense of it from the stack trace


ERROR [Thread-127] 2010-09-11 17:07:29,719 CassandraDaemon.java (line 84) Uncaught exception in thread Thread[Thread-127,5,main]
java.lang.AssertionError
        at org.apache.cassandra.net.MessagingService.receive(MessagingService.java:374)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:82)
ERROR [pool-1-thread-51] 2010-09-11 17:07:47,047 Cassandra.java (line 2883) Internal error processing multiget_slice
java.lang.AssertionError
        at org.apache.cassandra.net.MessagingService.receive(MessagingService.java:374)
        at org.apache.cassandra.net.MessagingService.sendOneWay(MessagingService.java:285)
        at org.apache.cassandra.service.ReadResponseResolver.maybeScheduleRepairs(ReadResponseResolver.java:140)
        at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:108)
        at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:43)
        at org.apache.cassandra.service.QuorumResponseHandler.get(QuorumResponseHandler.java:89)
        at org.apache.cassandra.service.StorageProxy.strongRead(StorageProxy.java:387)
        at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:223)
        at org.apache.cassandra.thrift.CassandraServer.readColumnFamily(CassandraServer.java:114)
        at org.apache.cassandra.thrift.CassandraServer.getSlice(CassandraServer.java:220)
        at org.apache.cassandra.thrift.CassandraServer.multigetSliceInternal(CassandraServer.java:299)
        at org.apache.cassandra.thrift.CassandraServer.multiget_slice(CassandraServer.java:271)
        at org.apache.cassandra.thrift.Cassandra$Processor$multiget_slice.process(Cassandra.java:2875)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2646)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
","Ubuntu 10.04.1, 1.6.0_18-b18",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20168,,,Sun Sep 12 02:36:40 UTC 2010,,,,,,,,,,"0|i0g5f3:",92308,,,,,Normal,,,,,,,,,,,,,,,,,"12/Sep/10 10:36;jbellis;believe this is the same as CASSANDRA-1493 (which has patch);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""null"" error creating CF from cli",CASSANDRA-1835,12492695,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,jbellis,jbellis,08/Dec/10 09:37,16/Apr/19 17:33,22/Mar/23 14:57,11/Dec/10 00:37,0.7.0 rc 3,,Legacy/Tools,,0,,,,,,"This fails with only ""null"" as the failure message:

{code}
create column family test1 with column_type = 'Super' and comparator = 'LongType' and column_metadata=[{column_name:a,validation_class:LongType}];
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Dec/10 18:35;xedin;CASSANDRA-1835.patch;https://issues.apache.org/jira/secure/attachment/12465789/CASSANDRA-1835.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20335,,,Sat Dec 11 07:35:17 UTC 2010,,,,,,,,,,"0|i0g7pb:",92678,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"10/Dec/10 04:08;jbellis;should we require subcomparator instead of guessing?  that may be better.;;;","10/Dec/10 04:40;xedin;I think warning here will be a pretty good option but as you think the best...;;;","10/Dec/10 06:47;xedin;In other words I think we should keep it with warning, what do you say?;;;","11/Dec/10 00:37;jbellis;committed;;;","11/Dec/10 15:35;hudson;Integrated in Cassandra-0.7 #70 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/70/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Booting fails on Windows 7/Windows 2003 because of a file rename failure,CASSANDRA-1790,12491618,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jbellis,rockxwre,rockxwre,30/Nov/10 16:16,16/Apr/19 17:33,22/Mar/23 14:57,01/Dec/10 05:32,0.7.0 rc 2,,,,1,,,,,,"Cassandra 0.7.0 rc will not boot on Windows 7 and Windows 2003 because of a file rename failure. The logging:

{noformat}
 INFO [FlushWriter:1] 2010-11-25 17:12:43,796 Memtable.java (line 155) Writing Memtable-LocationInfo@691789110(435 bytes, 8 operations)
ERROR [FlushWriter:1] 2010-11-25 17:12:43,993 AbstractCassandraDaemon.java (line 90) Fatal exception in thread Thread[FlushWriter:1,5,main]
java.io.IOError: java.io.IOException: rename failed of d:\cassandra\data\system\LocationInfo-e-1-Data.db
	at org.apache.cassandra.io.sstable.SSTableWriter.rename(SSTableWriter.java:214)
	at org.apache.cassandra.io.sstable.SSTableWriter.closeAndOpenReader(SSTableWriter.java:184)
	at org.apache.cassandra.io.sstable.SSTableWriter.closeAndOpenReader(SSTableWriter.java:167)
	at org.apache.cassandra.db.Memtable.writeSortedContents(Memtable.java:161)
	at org.apache.cassandra.db.Memtable.access$000(Memtable.java:49)
	at org.apache.cassandra.db.Memtable$1.runMayThrow(Memtable.java:174)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.IOException: rename failed of d:\cassandra\data\system\LocationInfo-e-1-Data.db
	at org.apache.cassandra.utils.FBUtilities.renameWithConfirm(FBUtilities.java:359)
	at org.apache.cassandra.io.sstable.SSTableWriter.rename(SSTableWriter.java:210)
	... 12 more
 INFO [main] 2010-11-29 09:16:36,219 AbstractCassandraDaemon.java (line 73) Heap size: 1067253760/1067253760
{noformat}
","Windows 7 (64-bit), Windows 2003 (32-bit)",mdennis,rockxwre,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Dec/10 00:26;jbellis;1790.txt;https://issues.apache.org/jira/secure/attachment/12464967/1790.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20316,,,Sat Dec 11 07:35:11 UTC 2010,,,,,,,,,,"0|i0g7fb:",92633,,mdennis,,mdennis,Critical,,,,,,,,,,,,,,,,,"01/Dec/10 00:26;jbellis;patch to close file handle used for post-flush truncate, which will allow the rename to complete on Windows;;;","01/Dec/10 04:04;mdennis;+1;;;","01/Dec/10 05:32;jbellis;committed;;;","11/Dec/10 15:35;hudson;Integrated in Cassandra-0.7 #70 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/70/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
write operation will throw internal error if the bootstrapping node is down,CASSANDRA-742,12446654,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,david.pan,david.pan,26/Jan/10 17:58,16/Apr/19 17:33,22/Mar/23 14:57,26/Jan/10 21:34,0.6,,,,0,,,,,,"the opertions are that :
1) bootstrap a node A;
2) keep on inserting data while bootstrapping;
3) stop the service of the node A;
4) then the following exception was found:
ERROR [pool-1-thread-9] 2010-01-26 10:32:39,688 Cassandra.java (line 1064) Internal error processing insert
java.lang.AssertionError
at org.apache.cassandra.locator.TokenMetadata.getToken(TokenMetadata.java:213)
at org.apache.cassandra.locator.AbstractReplicationStrategy.getHintedMapForEndpoints(AbstractReplicationStrategy.java:142)
at org.apache.cassandra.locator.AbstractReplicationStrategy.getHintedEndpoints(AbstractReplicationStrategy.java:76)
at org.apache.cassandra.service.StorageService.getHintedEndpointMap(StorageService.java:1188)
at org.apache.cassandra.service.StorageProxy.insertBlocking(StorageProxy.java:169)
at org.apache.cassandra.service.CassandraServer.doInsert(CassandraServer.java:466)
at org.apache.cassandra.service.CassandraServer.insert(CassandraServer.java:417)
at org.apache.cassandra.service.Cassandra$Processor$insert.process(Cassandra.java:1056)
at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:817)
at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
at java.lang.Thread.run(Thread.java:619)

I traced the code and found that ""org.apache.cassandra.locator.AbstractReplicationStrategy.getHintedMapForEndpoints(Collection<InetAddress>)"" will select a hinted endpoint for a dead endpoint, no mater whether it's a normal node or a bootstrapping node. To get the tokenID of the endpoint, this method will call ""tokenMetadata_.getToken(ep);"", but getToken() asserts that the endpoint should be  a member of the ring only. Of course, the bootstrapping endpoint is not a member and a internal exception is throwed out.
This exception will always be throwed out until I re-boostrapping. This is really a big prolem for me, because the bootstrapping will last  30 hours and my machines are not very durable. I have to get up from bed at night to deal with this accident. :-(

",linux2.6,david.pan,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,"26/Jan/10 18:05;david.pan;742-write_failed_when_bootstrapping_down.patch;https://issues.apache.org/jira/secure/attachment/12431416/742-write_failed_when_bootstrapping_down.patch",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19848,,,Tue Jan 26 13:34:00 UTC 2010,,,,,,,,,,"0|i0g0sn:",91559,,,,,Normal,,,,,,,,,,,,,,,,,"26/Jan/10 18:05;david.pan;This patch is not a perfect solution for this issue, but I can have a sweet dream at night and I can deal with this accident the next morning.  :-)

This patch will remove the bootstrapping endpoint from the tokenMetadata if other nodes find this node is down.
The write opertion will be timeout before other nodes find the bootstrapping node is down, but it will be OK after other nodes remove the bootstrapping node from the pendingRanges.;;;","26/Jan/10 21:34;jbellis;fixed in CASSANDRA-722 for 0.5.1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Internal error from malformed remove with supercolumns,CASSANDRA-1866,12493352,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thobbs,jbellis,jbellis,16/Dec/10 01:56,16/Apr/19 17:33,22/Mar/23 14:57,19/Dec/10 11:40,0.6.9,0.7.0 rc 3,Legacy/CQL,,0,,,,,,"From the ML: 

""I just call ""remove"" method where ColumnPath structure has ""column_family"" and ""column"" members set (""super_column"" not set).""
{code}
ERROR 17:57:46,924 Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.ClassCastException: org.apache.cassandra.db.DeletedColumn cannot be cast to org.apache.cassandra.db.SuperColumn
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.ClassCastException: org.apache.cassandra.db.DeletedColumn cannot be cast to org.apache.cassandra.db.SuperColumn
        at org.apache.cassandra.db.SuperColumnSerializer.serialize(SuperColumn.java:318)
        at org.apache.cassandra.db.SuperColumnSerializer.serialize(SuperColumn.java:298)
        at org.apache.cassandra.db.ColumnFamilySerializer.serializeForSSTable(ColumnFamilySerializer.java:82)
        at org.apache.cassandra.db.ColumnFamilySerializer.serialize(ColumnFamilySerializer.java:68)
        at org.apache.cassandra.db.RowMutationSerializer.freezeTheMaps(RowMutation.java:344)
        at org.apache.cassandra.db.RowMutationSerializer.serialize(RowMutation.java:355)
        at org.apache.cassandra.db.RowMutationSerializer.serialize(RowMutation.java:333)
        at org.apache.cassandra.db.RowMutation.getSerializedBuffer(RowMutation.java:269)
        at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:194)
        at org.apache.cassandra.service.StorageProxy$1.runMayThrow(StorageProxy.java:197)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
ERROR 17:57:46,928 Fatal exception in thread Thread[MUTATION_STAGE:2,5,main]
java.lang.RuntimeException: java.lang.ClassCastException: org.apache.cassandra.db.DeletedColumn cannot be cast to org.apache.cassandra.db.SuperColumn
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.ClassCastException: org.apache.cassandra.db.DeletedColumn cannot be cast to org.apache.cassandra.db.SuperColumn
        at org.apache.cassandra.db.SuperColumnSerializer.serialize(SuperColumn.java:318)
        at org.apache.cassandra.db.SuperColumnSerializer.serialize(SuperColumn.java:298)
        at org.apache.cassandra.db.ColumnFamilySerializer.serializeForSSTable(ColumnFamilySerializer.java:82)
        at org.apache.cassandra.db.ColumnFamilySerializer.serialize(ColumnFamilySerializer.java:68)
        at org.apache.cassandra.db.RowMutationSerializer.freezeTheMaps(RowMutation.java:344)
        at org.apache.cassandra.db.RowMutationSerializer.serialize(RowMutation.java:355)
        at org.apache.cassandra.db.RowMutationSerializer.serialize(RowMutation.java:333)
        at org.apache.cassandra.db.RowMutation.getSerializedBuffer(RowMutation.java:269)
        at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:194)
        at org.apache.cassandra.service.StorageProxy$1.runMayThrow(StorageProxy.java:197)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
{code}

Fix: add system test to catch this & raise invalidrequestexception from ThriftValidation",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Dec/10 02:40;thobbs;1866-0.6-test.txt;https://issues.apache.org/jira/secure/attachment/12466481/1866-0.6-test.txt","18/Dec/10 02:40;thobbs;1866-0.6.txt;https://issues.apache.org/jira/secure/attachment/12466480/1866-0.6.txt","17/Dec/10 06:22;thobbs;1866-0.7-test.txt;https://issues.apache.org/jira/secure/attachment/12466413/1866-0.7-test.txt","17/Dec/10 06:22;thobbs;1866-0.7.txt;https://issues.apache.org/jira/secure/attachment/12466412/1866-0.7.txt",,,,,,,,,,,4.0,thobbs,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20350,,,Sun Dec 19 04:03:35 UTC 2010,,,,,,,,,,"0|i0g7vz:",92708,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"17/Dec/10 06:24;thobbs;Patches also apply to trunk.;;;","17/Dec/10 23:34;jbellis;committed, thanks!;;;","17/Dec/10 23:35;jbellis;can you backport to 0.6 too?;;;","18/Dec/10 01:17;hudson;Integrated in Cassandra-0.7 #94 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/94/])
    ;;;","18/Dec/10 02:40;thobbs;0.6 versions attached.;;;","19/Dec/10 11:40;jbellis;committed, thanks;;;","19/Dec/10 12:03;hudson;Integrated in Cassandra-0.6 #28 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/28/])
    backport CASSANDRA-1866 from 0.7
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
functional test errors on OSX,CASSANDRA-278,12429637,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,urandom,urandom,urandom,07/Jul/09 03:58,16/Apr/19 17:33,22/Mar/23 14:57,09/Jul/09 00:13,0.4,,Legacy/Tools,,0,,,,,,The -n argument to echo is (apparently )not portable. See http://gist.github.com/141615,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Jul/09 04:02;urandom;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-278-use-printf-instead-of-echo.txt;https://issues.apache.org/jira/secure/attachment/12412648/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-278-use-printf-instead-of-echo.txt",,,,,,,,,,,,,,1.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19616,,,Thu Jul 09 12:35:34 UTC 2009,,,,,,,,,,"0|i0fxyf:",91099,,,,,Normal,,,,,,,,,,,,,,,,,"08/Jul/09 12:38;euphoria;I seem to remember it being reported on IRC that this patch didn't work, but it's fixing the problem for me.  Before the patch, I get 22 failures on 10.5.7 and after the patch, all 22 pass OK.;;;","09/Jul/09 00:13;urandom;Thanks for the feedback. Committed.;;;","09/Jul/09 20:35;hudson;Integrated in Cassandra #132 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/132/])
    replace non-portable use of echo -n with printf

Patch by Eric Evans; reviewed by Michael Greene for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"pig contrib module not building, other errors",CASSANDRA-1150,12465974,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jeromatron,jeromatron,jeromatron,02/Jun/10 23:53,16/Apr/19 17:33,22/Mar/23 14:57,12/Jun/10 21:09,0.6.3,,,,0,,,,,,"Currently, the pig contrib module fails to build because of dependency issues - it looks like dependencies like hadoop that were at one time in the main cassandra dependency list.

Also, once the dependencies are resolved, there are still errors when running the example pig query in the README.txt in the module.

This ticket would address both of those issues and getting it working both on 0.6.x as well as mainline trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-1162,,,,,,"03/Jun/10 01:32;jeromatron;1150-build-0_6.txt;https://issues.apache.org/jira/secure/attachment/12446160/1150-build-0_6.txt","03/Jun/10 01:35;jeromatron;1150-build-trunk.txt;https://issues.apache.org/jira/secure/attachment/12446162/1150-build-trunk.txt","09/Jun/10 04:09;jeromatron;1150-runtime-0_6-patch.txt;https://issues.apache.org/jira/secure/attachment/12446622/1150-runtime-0_6-patch.txt","09/Jun/10 06:06;jeromatron;1150-runtime-trunk.txt;https://issues.apache.org/jira/secure/attachment/12446631/1150-runtime-trunk.txt",,,,,,,,,,,4.0,jeromatron,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20010,,,Sun Jun 13 12:45:40 UTC 2010,,,,,,,,,,"0|i0g3av:",91965,,,,,Low,,,,,,,,,,,,,,,,,"03/Jun/10 00:37;jeromatron;This patch is just for the build portion of the bug.;;;","03/Jun/10 01:29;jbellis;I think I'd rather ""fix"" this by adding the instruction ""rename or symlink your pig jar, to pig.jar"" so we're not pinned to a specific pig version.;;;","03/Jun/10 01:32;jeromatron;Updated to use a wildcard for the pig version, also attaching build fixes for both 0.6 branch and trunk.;;;","03/Jun/10 01:35;jeromatron;Bah - updated the trunk build fix - wishing IDEA wouldn't be buggy about what it included in patches...;;;","04/Jun/10 10:53;jbellis;committed;;;","05/Jun/10 01:20;jeromatron;It is now building at least - resolving this issue as CASSANDRA-1162 will address the other concerns - making it work properly within core and using a demo usage of it in the contrib section.;;;","08/Jun/10 06:51;jeromatron;Reopening since integrating with core will take a while (no mvn repo with pig).;;;","08/Jun/10 07:29;jeromatron;Adding patches for 0.6 branch and trunk.;;;","08/Jun/10 07:31;jeromatron;Adds a default pig script to run based on the README suggestion.  Adds more to the README indicating how to run in local mode since that's the least volatile mode to run in.

Also took out a couple of references to 0.7.0-dev -> 0.7.0.;;;","09/Jun/10 04:09;jeromatron;For both runtime patches:
Fixed the runtime problems with pig. Updated the README.txt for pig and added an example-script.pig. Fixed a few places where it referred to pig 0.7.0-dev.

For trunk:
It was calling the comparator's newInstance where it should use the singleton version.  Fixed that in a few other places as well.  Fixed a couple of comments in WordCountSetup.  Used cassandra.yaml instead of storage-conf.xml.  
Also enabled the executable bit on the word_count, word_count_setup, and pig_cassandra scripts.;;;","09/Jun/10 06:06;jeromatron;Added a bit to the NEWS.txt to let people know about the singleton model for AbstractType extensions.;;;","12/Jun/10 21:09;johanoskarsson;Committed to 0.6 branch and trunk.;;;","13/Jun/10 20:45;hudson;Integrated in Cassandra #464 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/464/])
    Remove references to -dev version of pig, add example script, use comparators singletons. Patch by Jeremy Hanna, review by johan. CASSANDRA-1150
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
errors reading while bootstrapping,CASSANDRA-1534,12474870,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,23/Sep/10 04:56,16/Apr/19 17:33,22/Mar/23 14:57,24/Sep/10 03:28,0.7 beta 2,,,,0,,,,,,"I loaded a 4 node cluster with 1M rows from stress.py, decommissioned a node, and then began bootstrapping it while performing constant reads against the others with stress.py.  After sleeping for 90s, the bootstrapping node started throwing many errors like this:

ERROR 16:51:48,667 Fatal exception in thread Thread[READ_STAGE:1270,5,main]
java.lang.RuntimeException: Cannot service reads while bootstrapping!
        at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:67)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)

And I began receiving timeout errors with stress.py.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Sep/10 00:21;jbellis;1534.txt;https://issues.apache.org/jira/secure/attachment/12455385/1534.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20182,,,Fri Sep 24 12:47:05 UTC 2010,,,,,,,,,,"0|i0g5tj:",92373,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"23/Sep/10 04:58;jbellis;has bootstrap-after-decom ever worked?  i think we leave data in the system table that's going to confuse things;;;","23/Sep/10 05:06;brandon.williams;I rm'd everything after decom.;;;","23/Sep/10 05:21;jbellis;does bs against a vanilla no-decom cluster work?;;;","23/Sep/10 05:33;brandon.williams;No, same problem.;;;","24/Sep/10 00:21;jbellis;patch attached that makes new node properly announce its bootstrap status.;;;","24/Sep/10 02:46;brandon.williams;+1;;;","24/Sep/10 03:28;jbellis;committed;;;","24/Sep/10 20:47;hudson;Integrated in Cassandra #545 (See [https://hudson.apache.org/hudson/job/Cassandra/545/])
    fix setting bootstrap status on startup.
patch by jbellis; reviewed by brandonwilliams for CASSANDRA-1534
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CQL: cqlsh does shows Exception, but not error message when running truncate while a node is down.",CASSANDRA-2539,12504937,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,urandom,cdaw,cdaw,22/Apr/11 08:28,16/Apr/19 17:33,22/Mar/23 14:57,23/Apr/11 08:18,0.8.0 beta 2,,,,0,cql,,,,,"This is really just a usability bug, but it would nice to bubble the error message that is printed in the log file up to the interface.

*cqlsh output*
{noformat}
cqlsh> truncate users;
Exception: UnavailableException()
{noformat}

*log file error*
{noformat}
 INFO [pool-2-thread-5] 2011-04-21 23:53:30,466 StorageProxy.java (line 1021) Cannot perform truncate, some hosts are down
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Apr/11 04:49;jbellis;2539-v3.txt;https://issues.apache.org/jira/secure/attachment/12477161/2539-v3.txt","23/Apr/11 03:47;urandom;ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-2539-print-friendlier-message-for-Unavailabl.txt;https://issues.apache.org/jira/secure/attachment/12477141/ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-2539-print-friendlier-message-for-Unavailabl.txt",,,,,,,,,,,,,2.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20684,,,Tue Apr 26 22:10:19 UTC 2011,,,,,,,,,,"0|i0gbyn:",93368,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"22/Apr/11 16:19;slebresne;UnavailableException does mean, by definition, ""some hosts are down"" (which means that we don't really have anything to bubble anything, we can just have cqlsh write it in plain english if we'd like).

But this made me think, maybe we could attach a String to the UnavailableException saying which nodes are down. Not sure what are the consequence in term of thrift compatibility though and if it has any, it's probably not worth the trouble.;;;","23/Apr/11 03:48;urandom;patch attached;;;","23/Apr/11 04:49;jbellis;v3 also adds a wrapper for TimedOutException;;;","23/Apr/11 05:51;urandom;+1;;;","23/Apr/11 08:18;jbellis;committed;;;","23/Apr/11 09:26;hudson;Integrated in Cassandra-0.8 #36 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/36/])
    more user-friendly messages for UE/TOE in cqlsh
patch by eevans and jbellis for CASSANDRA-2539
;;;","27/Apr/11 06:10;cdaw;Re-tested in current build.  cqlsh now returns the message:

Unable to complete request: one or more nodes were unavailable.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"ERROR 23:50:24,264 Uncaught exception in thread Thread[pool-1-thread-5,5,main]",CASSANDRA-1245,12468401,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Duplicate,,paixaop,paixaop,02/Jul/10 12:12,16/Apr/19 17:33,22/Mar/23 14:57,02/Jul/10 21:15,,,,,0,,,,,,"When connecting with cassandra-cli after enabling <ThriftFramedTransport>true</ThriftFramedTransport> in storage-conf.xml the cassandra daemon crashes  with 

java.lang.OutOfMemoryError: Java heap space
Dumping heap to java_pid95320.hprof ...
Heap dump file created [4638171 bytes in 0.542 secs]
ERROR 23:50:24,264 Uncaught exception in thread Thread[pool-1-thread-5,5,main]
java.lang.OutOfMemoryError: Java heap space
	at org.apache.thrift.protocol.TBinaryProtocol.readStringBody(TBinaryProtocol.java:296)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:203)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:1116)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:637)

if I use the --framed option in cassandra-cli it works. I know it is supposed to work only with the --framed option, but the daemon should not crash just because a user forgets to use the --framed option to cassandra-cli.

",Mac OS Snow Leopard 10.6.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20049,,,Fri Jul 02 13:15:11 UTC 2010,,,,,,,,,,"0|i0g3vr:",92059,,,,,Low,,,,,,,,,,,,,,,,,"02/Jul/10 21:15;jbellis;same as CASSANDRA-475;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Internal error processing insert java.lang.AssertionError  at org.apache.cassandra.service.StorageProxy.sendMessages(StorageProxy.java:219),CASSANDRA-1931,12494562,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,tjake,kmueller,kmueller,04/Jan/11 13:29,16/Apr/19 17:33,22/Mar/23 14:57,05/Jan/11 05:34,0.7.1,,,,0,,,,,,"ERROR [pool-1-thread-137] 2011-01-03 18:22:21,751 Cassandra.java (line 2960) Internal error processing insert
java.lang.AssertionError
        at org.apache.cassandra.service.StorageProxy.sendMessages(StorageProxy.java:219)
        at org.apache.cassandra.service.StorageProxy.mutate(StorageProxy.java:174)
        at org.apache.cassandra.thrift.CassandraServer.doInsert(CassandraServer.java:412)
        at org.apache.cassandra.thrift.CassandraServer.insert(CassandraServer.java:349)
        at org.apache.cassandra.thrift.Cassandra$Processor$insert.process(Cassandra.java:2952)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2555)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)",Linux Fedora 12 x86_64,,,,,,,,,,,,,,,,,,,,,,14400,14400,,0%,14400,14400,,,,,,,,,,,,,,CASSANDRA-1986,,,,,,"05/Jan/11 00:49;tjake;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-1931-change-sendMessages-to-handle-many-mess.txt;https://issues.apache.org/jira/secure/attachment/12467437/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-1931-change-sendMessages-to-handle-many-mess.txt",,,,,,,,,,,,,,1.0,tjake,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20374,,,Tue Jan 04 21:54:50 UTC 2011,,,,,,,,,,"0|i0g8af:",92773,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"04/Jan/11 13:33;jbellis;Looks like this was introduced by CASSANDRA-1530.;;;","05/Jan/11 00:53;tjake;sendMessages was incorrectly assuming one message would be sent to a DC at a time, which isn't the case for batch mutations.

This patch groups the enpoints to a message so many messages can be sent.;;;","05/Jan/11 05:34;jbellis;committed;;;","05/Jan/11 05:54;hudson;Integrated in Cassandra-0.7 #144 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/144/])
    fix batch mutations post-#1530
patch by tjake; reviewed by jbellis for CASSANDRA-1931
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DefsTest logs errors,CASSANDRA-921,12460349,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Duplicate,,jbellis,jbellis,26/Mar/10 21:30,16/Apr/19 17:33,22/Mar/23 14:57,26/Mar/10 22:53,0.7 beta 1,,,,0,,,,,,"    [junit] ERROR 08:28:33,650 Error in executor futuretask
    [junit] java.util.concurrent.ExecutionException: java.lang.RuntimeException: org.apache.cassandra.config.DatabaseDescriptor$ConfigurationException: Keyspace does not already exist.
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
    [junit] 	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
    [junit] 	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:87)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:619)
    [junit] Caused by: java.lang.RuntimeException: org.apache.cassandra.config.DatabaseDescriptor$ConfigurationException: Keyspace does not already exist.
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	... 2 more
    [junit] Caused by: org.apache.cassandra.config.DatabaseDescriptor$ConfigurationException: Keyspace does not already exist.
    [junit] 	at org.apache.cassandra.db.DefsTable$1.runMayThrow(DefsTable.java:66)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
    [junit] 	... 6 more
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19919,,,Fri Mar 26 14:25:22 UTC 2010,,,,,,,,,,"0|i0g1wf:",91738,,,,,Low,,,,,,,,,,,,,,,,,"26/Mar/10 22:25;gdusbabek;patches for 827 will cover this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool move throws Assertion Error,CASSANDRA-1732,12479762,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,tjake,arya,arya,12/Nov/10 08:34,16/Apr/19 17:33,22/Mar/23 14:57,19/Nov/10 22:34,0.7.0 rc 1,,,,0,,,,,,"Started from a clean slate 3 node cluster. I first started 1 node and bootstrapped the second and third node into the cluster. I created some Keyspaces and inserted some test data, I ended up with this ring:

[agoudarzi@cas-test1 ~]$ nodetool --host localhost ring
Address         Status State   Load            Token                                       
                                       142685436305748685139980028665762955655    
10.50.26.133    Up     Normal  160.51 KB       57614844575514069274136376807820902791      
10.50.26.134    Up     Normal  160.51 KB       100150140440631377207058202736791929223     
10.50.26.132    Up     Normal  165.48 KB       142685436305748685139980028665762955655     

Now I wanted to test manual moving nodes to balanced tokens:
stage1:agoudarzi:~:$ python test.py 3
56713727820156410577229101238628035242
113427455640312821154458202477256070484
170141183460469231731687303715884105727

So I did nodetool move on 10.50.26.132:
[agoudarzi@cas-test1 ~]$ nodetool --host localhost move 56713727820156410577229101238628035242

All went fine. 
[agoudarzi@cas-test1 ~]$ nodetool --host localhost ring
Address         Status State   Load            Token                                       
                                       100150140440631377207058202736791929223    
10.50.26.132    Up     Normal  603.03 KB       56713727820156410577229101238628035242      
10.50.26.133    Up     Normal  15.18 MB        57614844575514069274136376807820902791      
10.50.26.134    Up     Normal  15.19 MB        100150140440631377207058202736791929223     

Now I wanted to move the second node 10.50.26.133:

[agoudarzi@cas-test2 ~]$ nodetool --host localhost move 113427455640312821154458202477256070484
Exception in thread ""main"" java.lang.AssertionError
	at org.apache.cassandra.service.StorageService.getLocalToken(StorageService.java:1128)
	at org.apache.cassandra.service.StorageService.startLeaving(StorageService.java:1527)
	at org.apache.cassandra.service.StorageService.move(StorageService.java:1666)
	at org.apache.cassandra.service.StorageService.move(StorageService.java:1641)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:111)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:45)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:226)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:251)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:857)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:795)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1449)
	at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:90)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1284)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1382)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:807)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
	at sun.rmi.transport.Transport$1.run(Transport.java:177)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)

 I am attacking the logs for my 3 nodes. For your reference I refer to these IPs as these nodes:

node1: 10.50.26.132
node2: 10.50.26.133
node3: 10.50.26.134

I have seen similar exception being thrown in CASSANDRA-1670.

Please investigate.

-Arya
","CentOS 5.4
Cassandra Trunk SVN Revision #1034158",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Nov/10 09:52;tjake;1732_v1.txt;https://issues.apache.org/jira/secure/attachment/12459963/1732_v1.txt","12/Nov/10 08:36;arya;node1.log;https://issues.apache.org/jira/secure/attachment/12459409/node1.log","12/Nov/10 08:36;arya;node2.log;https://issues.apache.org/jira/secure/attachment/12459410/node2.log","12/Nov/10 08:36;arya;node3.log;https://issues.apache.org/jira/secure/attachment/12459411/node3.log",,,,,,,,,,,4.0,tjake,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20280,,,Fri Nov 19 15:12:34 UTC 2010,,,,,,,,,,"0|i0g71r:",92572,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"15/Nov/10 02:54;jbellis;Does this work on 0.6.8?;;;","19/Nov/10 09:52;tjake;Attached fix.

in 0.7 bootstrapping a clean node takes a slightly different code path due to the fact that there are no non-system CFs defined.  This new code wasn't setting the nodes token.

This seems to be the same issue as CASSANDRA-1738;;;","19/Nov/10 22:34;jbellis;committed, combining tokenMetadata_.updateNormalToken + SystemTable.updateToken into SS.setToken call;;;","19/Nov/10 23:12;hudson;Integrated in Cassandra-0.7 #16 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/16/])
    fix for bootstrap when no non-system tables are defined
patch by tjake; reviewed by jbellis for CASSANDRA-1732
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL: incorrect error message running truncate on CF that does not exist,CASSANDRA-2570,12505248,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,cdaw,cdaw,27/Apr/11 05:23,16/Apr/19 17:33,22/Mar/23 14:57,27/Apr/11 23:18,0.8.0 beta 2,,Legacy/CQL,,0,cql,,,,,"Run truncate on a CF that does not exist. The error message is misleading.

*CQLSH*
{code}
cqlsh> truncate aaaa;
Unable to complete request: one or more nodes were unavailable.
{code}

*cassandra-cli*
{code}
[default@cqldb] truncate aaaaaaaaa;
aaaaaaaaa not found in current keyspace.
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Apr/11 22:46;xedin;CASSANDRA-2570-v2-0.8.patch;https://issues.apache.org/jira/secure/attachment/12477548/CASSANDRA-2570-v2-0.8.patch","27/Apr/11 23:03;xedin;CASSANDRA-2570-v2-trunk.patch;https://issues.apache.org/jira/secure/attachment/12477551/CASSANDRA-2570-v2-trunk.patch","27/Apr/11 18:47;xedin;CASSANDRA-2570.patch;https://issues.apache.org/jira/secure/attachment/12477494/CASSANDRA-2570.patch",,,,,,,,,,,,3.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20703,,,Wed Apr 27 17:18:12 UTC 2011,,,,,,,,,,"0|i0gc5j:",93399,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"27/Apr/11 18:47;xedin;Working branch: trunk. QP now checks if the given CF exists in the current KS before executing ""truncate"" operation.;;;","27/Apr/11 22:46;xedin;patch for version 0.8 instead of trunk + test for truncate validation.;;;","27/Apr/11 23:18;jbellis;committed to 0.8 and trunk w/ some changes:

 - test uses assert_raises, combined w/ existing test_truncate method
 - server uses validateColumnFamily to test existance;;;","28/Apr/11 01:18;hudson;Integrated in Cassandra-0.8 #46 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/46/])
    validate cql TRUNCATE columnfamily before truncating
patch by Pavel Yaskevich and jbellis for CASSANDRA-2570
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Compaction errors: ""Keys must be written in ascending order""",CASSANDRA-1744,12479886,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,brandon.williams,brandon.williams,14/Nov/10 05:57,16/Apr/19 17:33,22/Mar/23 14:57,23/Nov/10 23:23,0.7.0 rc 1,,,,0,,,,,,"When compaction starts, many of these exceptions are thrown:

ERROR 16:53:00,252 non-fatal error during compaction
java.io.IOException: Keys must be written in ascending order.
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:134)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:151)
        at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:279)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:109)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:87)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)

Easily reproduced with stress.py",,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20286,,,Fri Nov 19 15:24:31 UTC 2010,,,,,,,,,,"0|i0g74f:",92584,,,,,Normal,,,,,,,,,,,,,,,,,"14/Nov/10 06:09;jbellis;Do you have r1034634 where I reverted a change to DK comparator?;;;","14/Nov/10 06:10;brandon.williams;Yes, this was against r1034653.;;;","14/Nov/10 06:17;jbellis;I don't get it.  Memtable.columnFamlies should be sorting by DK comparator which is what SSTW is using in the assert.;;;","14/Nov/10 08:49;stuhood;What was the partitioner? Were there secondary indexes involved?;;;","14/Nov/10 09:14;brandon.williams;RP, no indicies.  Just ran enough stress.py inserts to cause compaction.  I did 1M twice, so there were overwrites involved.;;;","15/Nov/10 01:24;jbellis;I haven't been able to reproduce:

- 1M inserts, followed by another 1M inserts, allowed minor compactions, forced major compaction when done w/ 2nd 1M
- wipe & restart; 500K inserts, major compact, 500K inserts, major compact

;;;","15/Nov/10 03:54;brandon.williams;I can reproduce just by inserting 1M rows.  I'm not triggering a major compaction, a minor is naturally occurring and causing the errors.;;;","15/Nov/10 06:34;jbellis;What do you get after updating to latest 0.7 branch (post-revert of CASSANDRA-1702)?  Is the ""ascending order"" the first exception thrown?  If so please gzip and attach the sstables involved.;;;","15/Nov/10 06:51;brandon.williams;Now the exception is fatal, but it still occurs and is the only exception thrown.  Even with bz2, the file is too large for jira (13 MB): http://67.23.43.173/~bwilliam/1744-sstables.tar.bz2;;;","19/Nov/10 23:24;tjake;This is caused by CASSANDRA-1743

TFastFramedTransport isn't thread safe.  I'll look at fixing this on the thrift side.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error starting up a cassandra cluster after creating a table in the system keyspace: Attempt to assign id to existing column family.,CASSANDRA-2563,12505230,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jbellis,cdaw,cdaw,27/Apr/11 02:50,16/Apr/19 17:33,22/Mar/23 14:57,27/Apr/11 23:55,0.8.0 beta 2,,,,0,,,,,,"*Repro Steps*
* rm -rf /var/lib/cassandra/*
* rm -rf /var/log/cassandra/*
* Start Cassandra
* In cqlsh, create a column family and insert data
{noformat}
cqlsh> CREATE COLUMNFAMILY users (
   ...   KEY varchar PRIMARY KEY,
   ...   password varchar,
   ...   gender varchar,
   ...   session_token varchar,
   ...   state varchar,
   ...   birth_year bigint);

cqlsh> INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user1', 'ch@ngem3', 'f', 'CA', '1971');
cqlsh> INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user2', 'ch@ngem3', 'f', 'CA', '1972');
cqlsh> INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user3', 'ch@ngem3', 'f', 'CA', '1973');
{noformat}

* Quit cqlsh
* Kill Cassandra
* Startup Cassandra and get error

{noformat}
 INFO 18:38:24,509 Loading schema version 087af100-7034-11e0-0000-242d50cf1fde
ERROR 18:38:24,774 Exception encountered during startup.
java.io.IOError: org.apache.cassandra.config.ConfigurationException: Attempt to assign id to existing column family.
	at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:489)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:138)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:313)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)
Caused by: org.apache.cassandra.config.ConfigurationException: Attempt to assign id to existing column family.
	at org.apache.cassandra.config.CFMetaData.map(CFMetaData.java:126)
	at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:485)
	... 3 more
Exception encountered during startup.
java.io.IOError: org.apache.cassandra.config.ConfigurationException: Attempt to assign id to existing column family.
	at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:489)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:138)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:313)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)
Caused by: org.apache.cassandra.config.ConfigurationException: Attempt to assign id to existing column family.
	at org.apache.cassandra.config.CFMetaData.map(CFMetaData.java:126)
	at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:485)
	... 3 more
{noformat}



*UPDATE:  This issue happens if I create the CF in the default keyspace.*

*Workaround*
{noformat}
cqlsh> CREATE KEYSPACE cqldb with 
   ...   strategy_class =  
   ...     'org.apache.cassandra.locator.SimpleStrategy' 
   ...   and strategy_options:replication_factor=1;
cqlsh> use cqldb;

The create the table and insert data.
{noformat}
","Branch: cassandra-0.8; git pull @ 11:30amPST on 4/26
Server: RHEL5.5 single node",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Apr/11 05:31;jbellis;2563.txt;https://issues.apache.org/jira/secure/attachment/12477442/2563.txt","27/Apr/11 03:01;cdaw;cassandra.2563.tar;https://issues.apache.org/jira/secure/attachment/12477431/cassandra.2563.tar",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20698,,,Wed Apr 27 17:18:12 UTC 2011,,,,,,,,,,"0|i0gc3z:",93392,,cdaw,,cdaw,Critical,,,,,,,,,,,,,,,,,"27/Apr/11 03:01;cdaw;The /var/lib/cassandra directory tarball.;;;","27/Apr/11 03:06;cdaw;I was able to reproduce this again ...  cleared out /var/lib/cassandra/* and then started up server and repeated steps.;;;","27/Apr/11 03:49;jbellis;is default keyspace = system?;;;","27/Apr/11 03:56;cdaw;The CF was created in the system keyspace:

{noformat}
Keyspace: system:
  Replication Strategy: org.apache.cassandra.locator.LocalStrategy
    Options: [replication_factor:1]
  Column Families:
    ColumnFamily: users
      Key Validation Class: org.apache.cassandra.db.marshal.UTF8Type
      Default column value validator: org.apache.cassandra.db.marshal.UTF8Type
      Columns sorted by: org.apache.cassandra.db.marshal.UTF8Type
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.140625/30/1440 (millions of ops/MB/minutes)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: true
      Built indexes: []
      Column Metadata:
        Column Name: session_token
          Validation Class: org.apache.cassandra.db.marshal.UTF8Type
        Column Name: state
          Validation Class: org.apache.cassandra.db.marshal.UTF8Type
        Column Name: birth_year
          Validation Class: org.apache.cassandra.db.marshal.LongType
        Column Name: password
          Validation Class: org.apache.cassandra.db.marshal.UTF8Type
        Column Name: gender
          Validation Class: org.apache.cassandra.db.marshal.UTF8Type
{noformat}
;;;","27/Apr/11 05:31;jbellis;patch to disallow screwing w/ system keyspace;;;","27/Apr/11 05:59;cdaw;The patch works good and gives this error when creating a new column family:
Bad Request: system keyspace is not user-modifiable

The only thing to think about is whether or not this fix limits the ability of support to fix corruption issues.
;;;","27/Apr/11 23:55;jbellis;committed.

note that this only disallows schema changes; modifying system *data* is still allowed (and still dangerous :);;;","28/Apr/11 01:18;hudson;Integrated in Cassandra-0.8 #46 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/46/])
    disallow making schema changes to system keyspace
patch by jbellis; tested by cdaw for CASSANDRA-2563
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"cli ""list"" gives unhelpful error when not authenticated",CASSANDRA-1731,12479746,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,jbellis,jbellis,12/Nov/10 04:32,16/Apr/19 17:33,22/Mar/23 14:57,12/Nov/10 05:03,0.7.0 rc 1,,Legacy/Tools,,0,,,,,,"{code}
[default@unknown] list Userline
Using default limit of 100
null
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Nov/10 04:44;xedin;CASSANDRA-1731.patch;https://issues.apache.org/jira/secure/attachment/12459379/CASSANDRA-1731.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20279,,,Thu Nov 11 21:03:39 UTC 2010,,,,,,,,,,"0|i0g71j:",92571,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"12/Nov/10 05:03;jbellis;committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
logging error on C* startup: Nodes /10.194.241.188 and /10.194.241.188 have the same token 85070591730234615865843651857942052863,CASSANDRA-1666,12478413,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,mdennis,mdennis,27/Oct/10 09:44,16/Apr/19 17:33,22/Mar/23 14:57,28/Oct/10 22:47,0.7 beta 3,,,,0,,,,,,"When restarting my cluster, I noticed that a

{code}Nodes /10.194.241.188 and /10.194.241.188 have the same token 85070591730234615865843651857942052863{code} error message.  Notice the IPs are the same.  The cluster came up fine and nodetool ring shows all nodes up so it's likely just a logging error.
",EC2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Oct/10 19:42;jbellis;1666.txt;https://issues.apache.org/jira/secure/attachment/12458245/1666.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20249,,,Thu Oct 28 14:47:46 UTC 2010,,,,,,,,,,"0|i0g6n3:",92506,,gdusbabek,,gdusbabek,Low,,,,,,,,,,,,,,,,,"28/Oct/10 20:44;gdusbabek;+1;;;","28/Oct/10 22:47;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
When the ANTLR code generation fails the build continues and ignores the failure,CASSANDRA-1850,12493071,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,stephenc,stephenc,stephenc,13/Dec/10 21:49,16/Apr/19 17:33,22/Mar/23 14:57,14/Dec/10 03:48,0.7.0 rc 3,,,,0,,,,,,"When trying to tweak the build, I broke the ANTLR code generation tasks, but as I was not doing a full clean build the generated code was still present, so I missed the fact that the code generation was broken. It would be nice if the antlr tasks had failonerror=""true""",,,,,,,,,,,,,,,,,,,,,,,0,0,,0%,0,0,,,,,,,,,,,,,,,,,,,,"13/Dec/10 21:50;stephenc;fail-build-if-codegen-fails.patch;https://issues.apache.org/jira/secure/attachment/12466133/fail-build-if-codegen-fails.patch",,,,,,,,,,,,,,1.0,stephenc,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20343,,,Mon Dec 13 20:16:14 UTC 2010,,,,,,,,,,"0|i0g7sn:",92693,,urandom,,urandom,Low,,,,,,,,,,,,,,,,,"13/Dec/10 21:50;stephenc;patch to fix this issue;;;","14/Dec/10 03:48;urandom;committed; thanks.;;;","14/Dec/10 04:08;hudson;Integrated in Cassandra-0.7 #74 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/74/])
    failonerror for Cli antlr generation target

Patch by Stephen Connolly; reviewed by eevans for CASSANDRA-1850
;;;","14/Dec/10 04:16;hudson;Integrated in Cassandra #625 (See [https://hudson.apache.org/hudson/job/Cassandra/625/])
    failonerror for Cql and Cli antlr generation targets

Patch by Stephen Connolly; reviewed by eevans for CASSANDRA-1850
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"renaming a keyspace, then trying to use original name again makes errorations",CASSANDRA-1548,12475250,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,gdusbabek,thepaul,thepaul,28/Sep/10 06:41,16/Apr/19 17:33,22/Mar/23 14:57,28/Sep/10 22:58,0.7 beta 2,,,,0,,,,,,"My test case does the following:

* Create a keyspace with at least one CF in it.
* Rename that keyspace
* Create a new keyspace with the same original name, containing a CF with the same name as earlier.

The second keyspace creation receives an error (although the keyspace does get created):

{{javax.management.InstanceAlreadyExistsException: org.apache.cassandra.db:type=ColumnFamilies,keyspace=keyspacename,columnfamily=cfname}}

After that point, trying to do almost anything with the new keyspace will generate the same error- even trying to drop it. This persists until cassandra itself is restarted.

One supposes that some JMX thing is lacking reregistration upon keyspace rename.","encountered on Debian squeeze, with cassandra from HEAD (r1001931).  effects can be seen with both Telephus and Pycassa as clients; probably any.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Sep/10 15:08;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-ensure-that-a-table-unloads-CFS-instances-when-they-ar.txt;https://issues.apache.org/jira/secure/attachment/12455809/ASF.LICENSE.NOT.GRANTED--v1-0001-ensure-that-a-table-unloads-CFS-instances-when-they-ar.txt","28/Sep/10 06:45;thepaul;test_case_1548.py;https://issues.apache.org/jira/secure/attachment/12455770/test_case_1548.py",,,,,,,,,,,,,2.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20194,,,Wed Sep 29 12:47:31 UTC 2010,,,,,,,,,,"0|i0g5wn:",92387,,,,,Low,,,,,,,,,,,,,,,,,"28/Sep/10 06:45;thepaul;a test case which tickles this bug, using pycassa;;;","28/Sep/10 21:48;jbellis;+1;;;","28/Sep/10 22:58;gdusbabek;committed.;;;","29/Sep/10 20:47;hudson;Integrated in Cassandra #550 (See [https://hudson.apache.org/hudson/job/Cassandra/550/])
    ensure that a table unloads CFS instances when they are cleared. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1548
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AntiEntropyService causes lots of errors to be logged during unit tests,CASSANDRA-657,12444335,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,stuhood,jbellis,jbellis,30/Dec/09 05:33,16/Apr/19 17:33,22/Mar/23 14:57,23/Jan/10 03:03,0.6,,,,0,,,,,,"Some examples from a successful test run on trunk:

java.io.IOException: Reached an EOL or something bizzare occured. Reading from: /127.0.0.1 BufferSizeRemaining: 16
	at org.apache.cassandra.net.io.StartState.doRead(StartState.java:44)
	at org.apache.cassandra.net.io.ProtocolState.read(ProtocolState.java:39)
	at org.apache.cassandra.net.io.TcpReader.read(TcpReader.java:96)
	at org.apache.cassandra.net.TcpConnection$ReadWorkItem.run(TcpConnection.java:444)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)

ERROR [TCP Selector Manager] 2009-12-29 16:27:53,244 TcpConnection.java (line 363) Encountered IOException on connection: java.nio.channels.SocketChannel[closed]
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:574)
	at org.apache.cassandra.net.TcpConnection.connect(TcpConnection.java:341)
	at org.apache.cassandra.net.SelectorManager.doProcess(SelectorManager.java:143)
	at org.apache.cassandra.net.SelectorManager.run(SelectorManager.java:107)

It's not obvious to me if these are errors in AES, errors in the streaming code, errors caused because the test environment insufficiently resembles a live one, or errors because the test is asking the streaming code to do something that doesn't make sense, but AES is definitely at least immediately responsible.",,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jan/10 12:43;stuhood;657-short-lived-connections-for-streaming.patch;https://issues.apache.org/jira/secure/attachment/12431095/657-short-lived-connections-for-streaming.patch",,,,,,,,,,,,,,1.0,stuhood,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19805,,,Sat Jan 23 12:35:50 UTC 2010,,,,,,,,,,"0|i0g0a7:",91476,,,,,Low,,,,,,,,,,,,,,,,,"30/Dec/09 06:49;stuhood;Ah yea. I've seen this one before: it seems to happen after successful streaming sessions, but because streaming is a bit of a black box, I've never looked into it too deeply. We might want to make this depend on CASSANDRA-579, since changes there will likely expose these inconsistencies.;;;","22/Jan/10 07:07;stuhood;Hmm, I haven't checked trunk, but this might have been fixed by #705. Mostly making this comment as a reminder to myself.;;;","22/Jan/10 07:10;jbellis;i checked, and the errors are slightly different (duh :) but still present.;;;","22/Jan/10 12:43;stuhood;The receive loop in IncomingTcpConnection expects that any type of connection will be long lived, but streaming connections only send a single header.;;;","22/Jan/10 13:14;jbellis;I'd rather just let it EOF; that is less fragile if we decide to pool streaming connections later.

We can move the log message to trace if it bothers you. :);;;","23/Jan/10 03:03;jbellis;moved to trace in r902220;;;","23/Jan/10 20:35;hudson;Integrated in Cassandra #332 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/332/])
    move eofexception ST down to trace.  patch by jbellis for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
When restarting Cassandra I get this error: java.io.IOError: java.io.IOException: Failed to delete  ..data\system\LocationInfo-e-1-Data.db,CASSANDRA-1852,12493087,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,aadel,aadel,14/Dec/10 00:06,16/Apr/19 17:33,22/Mar/23 14:57,14/Dec/10 00:23,0.7.0 rc 2,,,,0,,,,,,"java.io.IOError: java.io.IOException: Failed to delete C:\Documents and Settings\Bureaublad\cassandra\data\system\LocationInfo-e-1-Data.db
	at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:483)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:102)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:55)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:216)
	at com.remainsoftware.gravity.cassandra.internal.GCassandra$1.run(GCassandra.java:21)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.IOException: Failed to delete C:\Documents and Settings\Bureaublad\cassandra\data\system\LocationInfo-e-1-Data.db
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:54)
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:44)
	at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:479)
	... 7 more
ERROR 16:54:52,984 Exception encountered during startup.
java.io.IOError: java.io.IOException: Failed to delete C:\Documents and Settings\Bureaublad\cassandra\data\system\LocationInfo-e-1-Data.db
	at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:483)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:102)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:55)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:216)
	at com.remainsoftware.gravity.cassandra.internal.GCassandra$1.run(GCassandra.java:21)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.IOException: Failed to delete C:\Documents and Settings\Bureaublad\cassandra\data\system\LocationInfo-e-1-Data.db
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:54)
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:44)
	at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:479)
	... 7 more",Windows,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20344,,,Mon Dec 13 16:23:10 UTC 2010,,,,,,,,,,"0|i0g7t3:",92695,,,,,Normal,,,,,,,,,,,,,,,,,"14/Dec/10 00:23;aadel;I just downloaded  the latest beta version: 0.7.0 rc 2.  It seems to be fixed. Thanks folks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Thrift swallows certain classes of network errors,CASSANDRA-72,12422577,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,10/Apr/09 23:21,16/Apr/19 17:33,22/Mar/23 14:57,16/Apr/09 00:36,0.3,,,,0,,,,,,"Thrift logs network errors via java.util.logging, but we use log4j instead.  I've submitted a patch to make thrift use log4j too.  When that goes in we should upgrade.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,THRIFT-416,,,,,,,,,,,,,,,,,"15/Apr/09 05:45;jbellis;72.patch;https://issues.apache.org/jira/secure/attachment/12405467/72.patch","15/Apr/09 05:49;jbellis;libthrift.jar;https://issues.apache.org/jira/secure/attachment/12405468/libthrift.jar",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19534,,,Wed Apr 15 16:36:10 UTC 2009,,,,,,,,,,"0|i0fwpb:",90896,,,,,Normal,,,,,,,,,,,,,,,,,"15/Apr/09 05:45;jbellis;upgrade to thrift svn r763981.  (for Java, this should be identical to thrift 0.1.)  the only changes are regenerating service/ thrift code and s|com.facebook.thrift/org.apache/thrift|.;;;","16/Apr/09 00:22;urandom;This all looks good to me. +1;;;","16/Apr/09 00:36;jbellis;applied;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BinaryMemtable interface silently dropping data.,CASSANDRA-1093,12464558,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,tjungen,tjungen,15/May/10 04:13,16/Apr/19 17:33,22/Mar/23 14:57,26/Jul/10 22:47,0.6.4,,,,0,,,,,,"I've been attempting to use the Binary Memtable (BMT) interface to load a large number of rows. During my testing, I discovered that on larger loads (~1 million rows), occasionally some of the data never appears in the database. This happens in a non-deterministic manner, as sometimes all the data loads fine, and other times a significant chunk goes missing. No errors are ever logged to indicate a problem. I'm attaching some sample code that approximates my application's usage of Cassandra and explains this bug in more detail.","Linux Centos5, Fedora Core 4. Java HotSpot Server 1.6.0_14. See readme for more details.",hammer,johanoskarsson,stuhood,tjungen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Jul/10 21:42;jbellis;1093.txt;https://issues.apache.org/jira/secure/attachment/12449452/1093.txt","15/May/10 04:14;tjungen;cassandra_bmt_test.tar.gz;https://issues.apache.org/jira/secure/attachment/12444527/cassandra_bmt_test.tar.gz",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19989,,,Mon Jul 26 14:47:20 UTC 2010,,,,,,,,,,"0|i0g2yf:",91909,,,,,Low,,,,,,,,,,,,,,,,,"15/May/10 04:14;tjungen;Sample code and instructions for how to run. See readme.txt in the archive.;;;","16/May/10 07:56;lenn0x;I've never seen this happen and I've done many imports. At the very end of the import, are you calling nodetool flush <Keyspace> ? ;;;","16/May/10 13:10;lenn0x;Tonight I imported 1M rows and verified all rows existed.;;;","16/May/10 13:16;tjungen;Yes, I'm flushing each node after the import. I've also tried flushing the system keyspace (no effect). As noted in the readme, I would not be surprised if this problem is unique to my hardware/software configuration and isn't an inherent problem with Cassandra's BMT interface.

For what it's worth, I've hacked together a ""workaround"" for this problem by writing SSTables directly (using o.a.c.io.SSTableWriter), copying the generated files to appropriate directories on the nodes, and then restarting the nodes. This solution is bound to result in other bugs, but for now I've verified that there is no lost data with this method.;;;","29/May/10 00:33;jbellis;can you reproduce using Toby's code, Brandon?;;;","29/May/10 02:46;brandon.williams;I can't get it past the generate step without an OOM:


cassandra_bmt_test# java -jar -Xmx4096m build/cassandra-bmt-test.jar generate foo 1000000
Generating data...
Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space
        at java.util.ArrayList.<init>(ArrayList.java:132)
        at java.util.ArrayList.<init>(ArrayList.java:139)
        at CassandraBMTTest.generateData(Unknown Source)
        at CassandraBMTTest.main(Unknown Source)

I'm going to try with < 1M and see if that works.;;;","29/May/10 03:03;tjungen;I've been able to observe the error with a generate parameter of 25,000. Note that the generate step creates the entire randomized data set in memory before writing it to disk, so this test is limited by memory. With a parameter of 25,000 I ran fine with 512MB of heap space, at 100,000 I'd expect you to need around 2GB of heap space. 

The parameter for the generate step corresponds to a ""document"", and each document results in roughly 100 rows.;;;","29/May/10 04:17;brandon.williams;I used 100K ""documents"":

Processed 4547169 values.
Done.
        Missing documents: 0
        Mismatched documents: 0
        Missing index entries: 0
        Wrong-sized index entries: 0
        Mismatched index entries: 0;;;","29/May/10 04:24;tjungen;Looks like everything passed. You may want to re-run from start to finish one or two more times (my error didn't occur consistently), but if it still passes at that point then close this issue as CannotReproduce and I'll attribute the problem to my hardware setup. As mentioned I've found somewhat of a workaround. I'll gladly donate my test code as a possible unit test for BMT if needed. :);;;","29/May/10 05:38;brandon.williams;I did another 100K run and it passed:


Processed 4547169 values.
Done.
        Missing documents: 0
        Mismatched documents: 0
        Missing index entries: 0
        Wrong-sized index entries: 0
        Mismatched index entries: 0

Is it possible you aren't waiting long enough for the flush to complete? (nodetool doesn't block on the flush command, you have to watch the system.log);;;","29/May/10 06:55;tjungen;Yep, I'm waiting until I see the flush message in the log. It reads something like ""BinaryMemtable@7a82b flushed to disk"".

One thing I'm thinking may be causing problems is my nodes being out of sync time-wise. I'll have to verify their clocks, but is it possible that if the clocks differ significantly that values get lost?;;;","05/Jun/10 02:42;brandon.williams;No, that shouldn't happen since the timestamps for columns are supplied by the client.;;;","14/Jul/10 00:33;jbellis;BMT is a very fire-and-forget api, so any failure condition will cause messages to be dropped with no way of knowing.

Probably the most likely one is, under heavy load (network and/or cpu) it's reasonably common for one node in the cluster to be marked ""down"" incorrectly by other nodes in the cluster.  This causes any messages on the MessagingService queue to that node to be dropped summarily, and the pool connection to be re-attempted when the failure detector believes it is ""up"" again.  (See OutboundTcpConnectionPool.reset);;;","14/Jul/10 04:19;tjungen;Thanks for the insight Jonathan. That was my intuition as well, and I observed my cluster periodically marking nodes as down for a second or two. I figured it was random network hiccups, since our network hardware is rather old. It would make sense that these periodic interruptions caused the BMT to lose data.

While looking through the code, I did try to see if I could use BMT with the blocking MessagingService API (in the way the Thrift API works unless ConsistencyLevel.ZERO is specified), but it looks like BMT is hardcoded to be asynchronous. It might be nice for that option to be there, but since this issue appears to only affect me (and I no longer need to use BMT for my purposes), it's a super-low priority suggestion.;;;","14/Jul/10 04:35;brandon.williams;If it is a node is being errantly marked down, in 0.6.3 or later you can try increasing the PhiConvictThreshold configuration directive and see if that helps.  EC2 users are setting it to 10 or 11, 8 is the default.;;;","14/Jul/10 06:10;jbellis;As it happens, Riptano has a client that is running into this too, so I'll take a stab at fixing it. :);;;","14/Jul/10 21:42;jbellis;patch to add response from BinaryVerbHandler, and updates bmt_example to use sendRR;;;","24/Jul/10 10:18;tjungen;Applied and tested the patch, appears to solve the problem. Haven't run multiple tests yet to make sure, but looks good so far. Obviously, this slows down the write, but that's an acceptable loss. It's likely still faster and more efficient than using the thrift API.

I'll be out of my office for the next three weeks, but I'll try to test more when I get back. Feel free to mark as resolved in the mean time.;;;","26/Jul/10 22:47;jbellis;committed, with additional note that wait-for-acks can reduce throughput;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
drop/recreate column family race condition,CASSANDRA-1477,12473494,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,gdusbabek,btoddb,btoddb,08/Sep/10 05:21,16/Apr/19 17:33,22/Mar/23 14:57,16/Sep/10 04:39,0.7 beta 2,,,,0,,,,,,"using 0.7 latest from trunk as of few minutes ago.  1 client, 1 node

i have the scenario where i want to drop a column family and recreate it 
- unit testing for instance, is a good reason you may want to do this 
(always start fresh).

the problem i observe is that if i do the following:

1 - drop the column family
2 - recreate it
3 - read data from a key that existed before dropping, but doesn't exist now

if those steps happen fast enough, i will get the old row - definitely 
no good.

if they happen slow enough, get_slice throws:

""org.apache.thrift.TApplicationException: Internal error processing 
get_slice""

.. and on the server i see:

2010-09-07 13:53:48,086 ERROR 
[org.apache.cassandra.thrift.Cassandra$Processor] (pool-1-thread-4:) - 
Internal error processing get_slice
java.lang.RuntimeException: java.util.concurrent.ExecutionException: 
java.io.IOError: java.io.FileNotFoundException: 
cassandra-data/data/Queues/test_1283892789285_Waiting-e-1-Data.db (No 
such file or directory)
     at 
org.apache.cassandra.service.StorageProxy.weakRead(StorageProxy.java:275)
     at 
org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:218)
     at 
org.apache.cassandra.thrift.CassandraServer.readColumnFamily(CassandraServer.java:114)
     at 
org.apache.cassandra.thrift.CassandraServer.getSlice(CassandraServer.java:220)
     at 
org.apache.cassandra.thrift.CassandraServer.multigetSliceInternal(CassandraServer.java:299)
     at 
org.apache.cassandra.thrift.CassandraServer.get_slice(CassandraServer.java:260)
     at 
org.apache.cassandra.thrift.Cassandra$Processor$get_slice.process(Cassandra.java:2795)
     at 
org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2651)
     at 
org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
     at 
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
     at 
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
     at java.lang.Thread.run(Thread.java:619)
Caused by: java.util.concurrent.ExecutionException: java.io.IOError: 
java.io.FileNotFoundException: 
cassandra-data/data/Queues/test_1283892789285_Waiting-e-1-Data.db (No 
such file or directory)
     at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
     at java.util.concurrent.FutureTask.get(FutureTask.java:83)
     at 
org.apache.cassandra.service.StorageProxy.weakRead(StorageProxy.java:271)
     ... 11 more
Caused by: java.io.IOError: java.io.FileNotFoundException: 
cassandra-data/data/Queues/test_1283892789285_Waiting-e-1-Data.db (No 
such file or directory)
     at 
org.apache.cassandra.io.util.BufferedSegmentedFile.getSegment(BufferedSegmentedFile.java:68)
     at 
org.apache.cassandra.io.sstable.SSTableReader.getFileDataInput(SSTableReader.java:509)
     at 
org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:49)
     at 
org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:65)
     at 
org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:76)
     at 
org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:961)
     at 
org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:856)
     at 
org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:826)
     at org.apache.cassandra.db.Table.getRow(Table.java:321)
     at 
org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:63)
     at 
org.apache.cassandra.service.StorageProxy$weakReadLocalCallable.call(StorageProxy.java:737)
     at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
     at java.util.concurrent.FutureTask.run(FutureTask.java:138)
     ... 3 more
Caused by: java.io.FileNotFoundException: 
cassandra-data/data/Queues/test_1283892789285_Waiting-e-1-Data.db (No 
such file or directory)
     at java.io.RandomAccessFile.open(Native Method)
     at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
     at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
     at 
org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)
     at 
org.apache.cassandra.io.util.BufferedSegmentedFile.getSegment(BufferedSegmentedFile.java:62)
     ... 15 more

","1 Node cluster, latest code from 0.7 trunk",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Sep/10 03:34;gdusbabek;ASF.LICENSE.NOT.GRANTED--v4-0001-revert-r996974.-fix-by-adding-a-switch-to-files-to-all.txt;https://issues.apache.org/jira/secure/attachment/12454689/ASF.LICENSE.NOT.GRANTED--v4-0001-revert-r996974.-fix-by-adding-a-switch-to-files-to-all.txt","16/Sep/10 03:34;gdusbabek;ASF.LICENSE.NOT.GRANTED--v4-0002-move-directory-scrubbing-to-startup.txt;https://issues.apache.org/jira/secure/attachment/12454690/ASF.LICENSE.NOT.GRANTED--v4-0002-move-directory-scrubbing-to-startup.txt","16/Sep/10 03:34;gdusbabek;ASF.LICENSE.NOT.GRANTED--v4-0003-avro-system-tests.txt;https://issues.apache.org/jira/secure/attachment/12454691/ASF.LICENSE.NOT.GRANTED--v4-0003-avro-system-tests.txt","14/Sep/10 07:28;btoddb;RaceConditionTest.java;https://issues.apache.org/jira/secure/attachment/12454499/RaceConditionTest.java",,,,,,,,,,,4.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20159,,,Fri Sep 17 13:55:34 UTC 2010,,,,,,,,,,"0|i0g5av:",92289,,jbellis,,jbellis,Critical,,,,,,,,,,,,,,,,,"09/Sep/10 20:27;gdusbabek;On the ML you indicated you had a unit tests that was producing this failure.  Is it possible for you to attach it to this ticket?  I would like to include it in our tests.;;;","10/Sep/10 01:05;btoddb;it uses Pelops at the moment.  lemme see if i can rework it for raw thrift.;;;","14/Sep/10 07:28;btoddb;using latest trunk code as of now

The attached JUnit (method testRaceTooFast) illustrates the problem of data still existing even though the column family has been dropped and recreated, but no data inserted.

However, I cannot make the test fail by throwing ""org.apache.thrift.TApplicationException: Internal error processing get_slice"" as i indicate before because the first problem above never corrects itself.  The data file never goes away no matter how long i wait.  not sure what i was doing different in my app, or possibly code changed.  i no longer do it this way in my app so not sure.

in addition, i tried to drop the keyspace between tests to make sure the keyspace was ""clear"" and this returns ""InvalidRequestException(why:java.io.IOException: Unable to create compaction marker)"" when the keyspace does exist, but i guess i dropped the column family during the previous test and caused it grief.  not sure.

good luck!;;;","14/Sep/10 23:14;gdusbabek;I see now (I can reproduce this fwiw).  We changed the way to cleanup after dropped CFs at the end of August.  Prior to that, we blocked on deletion, but now just mark the CF compacted and wait for normal cleanup to do it's thing.

In hindsight, I'm not sure if this was the best approach.  If we allow creating, dropping, then recreating in rapid succession we should support it better.;;;","14/Sep/10 23:24;jbellis;why do sstables-marked-compacted have any effect on anything?  isn't that a relatively simple fix?;;;","14/Sep/10 23:29;gdusbabek;I'm not sure.  Is it as simple as changing CFS.files() to ignore those that have compacted markers?;;;","14/Sep/10 23:35;jbellis;either that, or have it clean out the compacted ones before doing its filtering;;;","14/Sep/10 23:35;jbellis;(ignoring is probably safer since it's possible for a file to be marked compacted, but still open for a reader in progress);;;","15/Sep/10 00:03;jbellis;+1;;;","15/Sep/10 00:49;btoddb;the patch fixes the first problem, but the second method, testRaceSlowEnoughToCauseException, in the attached unit test still throws the following from get_slice:

InvalidRequestException(why:java.io.IOException: Unable to create compaction marker)
    at org.apache.cassandra.thrift.Cassandra$system_drop_column_family_result.read(Cassandra.java:22872)
    at org.apache.cassandra.thrift.Cassandra$Client.recv_system_drop_column_family(Cassandra.java:1342)
    at org.apache.cassandra.thrift.Cassandra$Client.system_drop_column_family(Cassandra.java:1317)
    at RaceConditionTest.testRaceSlowEnoughToCauseException(RaceConditionTest.java:87)

on the server side i see this:

10/09/14 09:46:32 ERROR thrift.CassandraDaemon: Uncaught exception in thread Thread[MIGRATION_STAGE:1,5,main]
java.util.concurrent.ExecutionException: java.io.IOError: java.io.IOException: Unable to create compaction marker
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:87)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.IOError: java.io.IOException: Unable to create compaction marker
	at org.apache.cassandra.io.sstable.SSTableReader.markCompacted(SSTableReader.java:484)
	at org.apache.cassandra.io.sstable.SSTableTracker.replace(SSTableTracker.java:76)
	at org.apache.cassandra.db.ColumnFamilyStore.removeAllSSTables(ColumnFamilyStore.java:711)
	at org.apache.cassandra.db.Table.dropCf(Table.java:271)
	at org.apache.cassandra.db.migration.DropColumnFamily.applyModels(DropColumnFamily.java:94)
	at org.apache.cassandra.db.migration.Migration.apply(Migration.java:157)
	at org.apache.cassandra.thrift.CassandraServer$1.call(CassandraServer.java:644)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	... 2 more
Caused by: java.io.IOException: Unable to create compaction marker
	at org.apache.cassandra.io.sstable.SSTableReader.markCompacted(SSTableReader.java:480)
	... 11 more
;;;","15/Sep/10 01:01;btoddb;i should point out that testRaceSlowEnoughToCauseException runs fine in isolation, but run the junit class as a whole and it will fail.;;;","15/Sep/10 04:14;gdusbabek;Setting this to blocker. The bug is such that valid sstables can be deleted when the compaction markers stick around after a CF is dropped and recreated.

The fix is to check for a compaction marker when a sstable is created and delete it if it's there.;;;","15/Sep/10 04:58;jbellis;somehow we're getting into a situation where there is X.compacted but no other sign of X?;;;","15/Sep/10 05:48;gdusbabek;The exception happens because the compaction marker exists but the deletion executor hasn't done its thing.  It's easy to visualize:
* A keyspace is created along with a column family.  sstables are written.
* That keyspace is dropped. compacted markers are written.
* That keyspace and column family are recreated.  Meanwhile, no files have been deleted.
* As part of re-Table.open()ing it, CFS.scrubDataDirectories() is called but doesn't scrub the compacted markers because of those are now excluded by files() * (I'm pretty sure this wasn't working, or wasn't working like we expect, in the first place, since B.Todd was getting the error before applying the patch).
* Newly created sstables match the names of the compaction markers and will be possibly deleted if timing is bad.

Two fixes come to mind:
1. keep the first patch that has already been committed and make AddKeyspace ensure that data directories, if they exist, are empty.
2. revert the last patch and make AddColumnFamily ensure that its data directory, if it exists, is empty.

I favor option 2.;;;","15/Sep/10 05:56;jbellis;this seems like a race-prone thing to do in the first place since it's not valid to remove a data file that has a reader open to it...

verifying empty data directories on KS creation is fine but what is the analogue for CFs?  and how do you tell the client ""sorry, you're SOL"" if it's not empty?;;;","15/Sep/10 21:53;gdusbabek;You're right.  I was trying to take a shortcut.  

The real problem is that CFS.files() is being used for two things: determine valid files to read from and also count the files to determine generation.  It can't determine the right generation if it doesn't include the invalid files (compacted), but if it includes the invalid files, we read data we shouldn't.;;;","15/Sep/10 21:56;jbellis;I think you've nailed it.;;;","15/Sep/10 23:17;hudson;Integrated in Cassandra #536 (See [https://hudson.apache.org/hudson/job/Cassandra/536/])
    ensure that compacted sstables are excluded from newly instantiated readers. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1477
;;;","15/Sep/10 23:19;jbellis;don't we also need to change scrub to only run at server start to avoid race-with-reader?;;;","16/Sep/10 02:33;gdusbabek;Yes. v3 addresses that.;;;","16/Sep/10 02:42;jbellis;+1

(in patch 2, CFS.all() may offer a small simplification.);;;","16/Sep/10 03:37;gdusbabek;Your suggestion on patch 2 made me realize that the scrubbing should happen pior to populating the instances in CFS.  v4 fixes that.;;;","16/Sep/10 04:14;btoddb;seems like these patches are fixing the issues i found;;;","16/Sep/10 04:31;jbellis;+1;;;","16/Sep/10 04:39;gdusbabek;new and improved fix committed.;;;","17/Sep/10 21:55;hudson;Integrated in Cassandra #538 (See [https://hudson.apache.org/hudson/job/Cassandra/538/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed Streams Break Repair,CASSANDRA-2433,12503646,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,bcoverston,bcoverston,07/Apr/11 23:40,16/Apr/19 17:33,22/Mar/23 14:57,01/Sep/11 00:36,0.8.5,,,,5,repair,,,,,"Running repair in cases where a stream fails we are seeing multiple problems.

1. Although retry is initiated and completes, the old stream doesn't seem to clean itself up and repair hangs.
2. The temp files are left behind and multiple failures can end up filling up the data partition.

These issues together are making repair very difficult for nearly everyone running repair on a non-trivial sized data set.

This issue is also being worked on w.r.t CASSANDRA-2088, however that was moved to 0.8 for a few reasons. This ticket is to fix the immediate issues that we are seeing in 0.7.",,bcoverston,cherro,colinkuo,ijuma,jborgstrom,jeromatron,mdennis,stuhood,vijay2win@yahoo.com,yulinyen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-2610,,,"15/Jun/11 20:27;slebresne;0001-Put-repair-session-on-a-Stage-and-add-a-method-to-re-v4.patch;https://issues.apache.org/jira/secure/attachment/12482658/0001-Put-repair-session-on-a-Stage-and-add-a-method-to-re-v4.patch","15/Jun/11 20:27;slebresne;0002-Register-in-gossip-to-handle-node-failures-v4.patch;https://issues.apache.org/jira/secure/attachment/12482659/0002-Register-in-gossip-to-handle-node-failures-v4.patch","15/Jun/11 20:27;slebresne;0003-Report-streaming-errors-back-to-repair-v4.patch;https://issues.apache.org/jira/secure/attachment/12482660/0003-Report-streaming-errors-back-to-repair-v4.patch","15/Jun/11 20:27;slebresne;0004-Reports-validation-compaction-errors-back-to-repair-v4.patch;https://issues.apache.org/jira/secure/attachment/12482661/0004-Reports-validation-compaction-errors-back-to-repair-v4.patch","03/Aug/11 01:52;slebresne;2433.patch;https://issues.apache.org/jira/secure/attachment/12489094/2433.patch","30/Aug/11 23:25;slebresne;2433_v2.patch;https://issues.apache.org/jira/secure/attachment/12492251/2433_v2.patch","31/Aug/11 22:52;slebresne;2433_v3.patch;https://issues.apache.org/jira/secure/attachment/12492464/2433_v3.patch","31/Aug/11 23:46;slebresne;2433_v4.patch;https://issues.apache.org/jira/secure/attachment/12492468/2433_v4.patch",,,,,,,8.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20622,,,Wed Aug 31 17:16:01 UTC 2011,,,,,,,,,,"0|i0gbcn:",93269,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"21/Apr/11 19:45;slebresne;Attached patches are against 0.8.

This tries to catch what can go wrong with repair and reports it back to the user by making the full repair throw an exception. More precisely:
  * patch 0001: add a method to repair for reporting failure and propagate that up to the repair session. This puts repair session on a specific stage (instead of having RepairSession be a Thread) and use a future to allow waiting on completion. This allows a cleaner API to deal with errors (the Future.get() simply throw an ExecutionException) and this add the advantage of stage management to repair sessions.
  * patch 0002: Make repair session register through gossip to be informed of node dying and failing the session when that happens.
  * patch 0003: Reports errors during streaming to the repair session. This actually introduces a generic way to handle streaming failures and after that we should probably update the other user of streaming to deal correctly with failure too.
  * patch 004: Catch errors during validation compaction and push them up to repair (whether those happens on the coordinator of the repair or not).

Note that this includes streaming failures and thus includes stuffs from the patch of Aaron Morton attached on CASSANDRA-2088, but contrarily to that patch, it takes the approach of failing fast. This means that if streaming fails on a file, it fails the streaming altogether (same for repair). I think this is simpler code-wise and more useful from the point of view of the user, since a failure means the use will have to retry anyway.

Last but not least, this makes some modification to messages. So either this goes into 0.8.0 (which I think it should, because this really is a bug fix and fixes something that is a pain for users), or we should had a new messaging version for 0.8.0 and modify this to take it into account (we should probably add a 0.8.0 version to the messaging service anyway).
;;;","18/May/11 04:01;slebresne;Attaching rebased patch (against 0.8.1). It also change the behavior a little bit so as to not fail repair right away if a problem occur (it still throw an exception at the end if any problem had occured). It turns out to be slightly simpler that way. Especially for CASSANDRA-1610.;;;","24/May/11 04:09;stuhood;0001
* Since we're not trying to control throughput or monitor sessions, could we just use Stage.MISC?

0002
* I think RepairSession.exception needs to be volatile to ensure that the awoken thread sees it
* Would it be better if RepairSession implemented IEndpointStateChangeSubscriber directly?
* The endpoint set needs to be threadsafe, since it will be modified by the endpoint state change thread, and the AE_STAGE thread

0003
* Should StreamInSession.retries be volatile/atomic? (likely they won't retry quickly enough for it to be a problem, but...)

0004
* Playing devil's advocate: would sending a half-built tree in case of failure still be useful?
* success might need to be volatile as well

Thanks Sylvain!;;;","09/Jun/11 22:09;slebresne;Attaching v3 rebased (on 0.8).

bq. Since we're not trying to control throughput or monitor sessions, could we just use Stage.MISC?

The thing is that repair session are very long lived. And MISC is single threaded. So that would block other task that are not supposed to block. We could make MISC multi-threaded but even then it's not a good idea to mix short lived and long lived task on the same stage.

bq. I think RepairSession.exception needs to be volatile to ensure that the awoken thread sees it

Done in v3.

bq. Would it be better if RepairSession implemented IEndpointStateChangeSubscriber directly?

Good idea, it's slightly simpler, done in v3.

bq. The endpoint set needs to be threadsafe, since it will be modified by the endpoint state change thread, and the AE_STAGE thread

Done in v3. That will probably change with CASSANDRA-2610 anyway (which I have to update)

bq. Should StreamInSession.retries be volatile/atomic? (likely they won't retry quickly enough for it to be a problem, but...)

I did not change that, but if it's a problem for retries to not be volatile, I suspect having StreamInSession.current not volatile is also a problem. But really I'd be curious to see that be a problem.

bq. Playing devil's advocate: would sending a half-built tree in case of failure still be useful?

I don't think it is. Or more precisely, if you do send half-built tree, you'll have to be careful that the other doesn't consider what's missing as ranges not being in sync (I don't think people will be happy with tons of data being stream just because we happen to have a bug that make compaction throw an exception during the validation). So I think you cannot do much with a half-built tree, and it will add complication. For a case where people will need to restart a repair anyway once whatever happened is fixed

bq. success might need to be volatile as well

Done in v3.
;;;","15/Jun/11 20:27;slebresne;Attaching v4 that is rebased and simply set the reties variable in StreamInSession volatile after all (I've removed old version because it was a mess).;;;","18/Jul/11 16:08;stuhood;Hey Sylvain: sorry it took me so long to get back to this one. Would you mind rebasing it?;;;","03/Aug/11 01:52;slebresne;Attaching a rebase of the two previous first patches as '2433.patch'. That is, this patch adds registering in gossip so that repair fails and report it to the user when a node participating to the repair dies. Compared to the previous version, it fails fast because it's the easier thing to do now and a better option imho.

I should mention that while it is lame that repair get stuck when a node dies and we should fix it, this means that if a node is wrongly marked down, we will fail repair for no reason (but I suppose it's a failure detector problem).

Attached patch is against 0.8. This has no upgrade consequence of any sort and is a reasonably simple patch, so I think it could be worth committing in 0.8.
The rest of what was in previous patch 0003 and 0004 cannot go into 0.8 because it changes the wire protocol, so I will rebase against trunk directly, and maybe in another ticket. Having this first patch committed would help with that though :);;;","09/Aug/11 01:46;jbellis;Yuki, can you review this patch?;;;","30/Aug/11 23:25;slebresne;Attached v2 is rebased and use a higher conviction threshold before deciding to fail the repair, as the goal here is to avoid having a repair getting stuck for hours, but we want to avoid stopping a repair just because a node got into a longer than usual GC pause.

The threshold used is twice the configured phi_convict_threshold. This give 16 by default, which if I trust the original 'phi accrual failure detection' should give an order of magnitude less false positive than 8 (for about an order of magnitude in the detection time though). It feels reasonable to me but if a FD specialist want to voice his opinion, please do.;;;","30/Aug/11 23:42;jbellis;- Why do we need the new AE_SESSIONS stage?
- I prefer using WrappedRunnable to a Callable when you want to allow exceptions but don't care about a return value
- I think we can avoid a bunch of no-op onConvicts if RepairSession were to subscribe to FD directly instead of going through Gossip (i.e., leave IEndpointStateChangeSubscriber unchanged and expose convict in IFailureDetectionEventListener for when we need to go low-level).  Gossip is about high-level ""events"" which doesn't really fit here.;;;","31/Aug/11 00:37;slebresne;bq. Why do we need the new AE_SESSIONS stage?

If you mean ""why AE_SESSIONS when we already have the AE stage?"", then it is because repair push stuffs on the AE stage that it wait for, so we would deadlock. If you mean ""why a stage?"", it felt cleaner that just a Thread now that we want to check for exception at the end of the exception. If you mean ""why a stage rather than a simple ThreadExecutor?"", it is a good question. I guess it was just some reflex of mine to get a JMXEnabledThreadPool, but it's probably not worth a stage, not even the jmx enabledness maybe.

bq. I prefer using WrappedRunnable to a Callable when you want to allow exceptions but don't care about a return value

Agreed. I'll update the patch.

bq. I think we can avoid a bunch of no-op onConvicts if RepairSession were to subscribe to FD directly instead of going through Gossip

Yeah, I kind of started with that but the problem is that we must deal with the case of a node restarting before it has been convicted (especially if the conviction threshold is higher), which the FD won't see. We could deal of that last situation separately and have Gossip call some trigger into AntiEntropy on a gossip generation change to indicate to stop every started session involving the given endpoint, but creating a dependency of gossip to anti-entropy didn't felt like a good idea a priori.;;;","31/Aug/11 00:44;jbellis;bq. it's probably not worth a stage, not even the jmx enabledness maybe

Someone's probably going to want the JMX information but let's keep Stages for Verb-associated tasks.

bq. the problem is that we must deal with the case of a node restarting before it has been convicted (especially if the conviction threshold is higher), which the FD won't see

How about splitting onDead and onRestart in EndpointStateChange, then?  Then RS could implement convict and onRestart (ignoring onDead); other ESCS listeners could implement onRestart == onDead.  That would maintain the ""ESCS is about events, FDEL is low-level convict information"" separation of roles.;;;","31/Aug/11 22:46;slebresne;bq. Someone's probably going to want the JMX information but let's keep Stages for Verb-associated tasks

Sounds good, updated patch add a new executor directly into AntiEntropy.

bq. How about splitting onDead and onRestart in EndpointStateChange, then?

Done.

bq. I prefer using WrappedRunnable to a Callable

I changed to use WrappedRunnable. However, we still need to have access to both the repair session and the future from the executor so the implementation returns a pair of those two objects. I'm only marginally convinced this is cleaner than the previous solution...
;;;","31/Aug/11 22:52;slebresne;(Sorry, I had attached the wrong version of v3, corrected now);;;","31/Aug/11 22:56;jbellis;bq. we still need to have access to both the repair session and the future from the executor so the implementation returns a pair of those two objects

You can still use the RepairFuture approach, just use the FutureTask(Runnable, V) constructor;;;","31/Aug/11 23:46;slebresne;You're right, don't know why I got carried away like that. v4 ""fixes"" this.;;;","31/Aug/11 23:51;jbellis;+1;;;","01/Sep/11 00:36;slebresne;Committed, thanks.

This probably solves most of the case where repair was hanging infinitely. I've created CASSANDRA-3112 to handle the remaining cases, but it is much less urgent imho. Marking that one as resolved;;;","01/Sep/11 01:16;hudson;Integrated in Cassandra-0.8 #306 (See [https://builds.apache.org/job/Cassandra-0.8/306/])
    Make repair report failure when a participating node dies
patch by slebresne; reviewed by jbellis for CASSANDRA-2433

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1163677
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/FailureDetector.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/Gossiper.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/IEndpointStateChangeSubscriber.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/IFailureDetectionEventListener.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/AntiEntropyService.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/MigrationManager.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageLoadBalancer.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageService.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/service/AntiEntropyServiceTestAbstract.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"get_slice ignores the ""start"" parameter",CASSANDRA-81,12422833,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,sandeep_tata,sandeep_tata,sandeep_tata,15/Apr/09 06:54,16/Apr/19 17:33,22/Mar/23 14:57,23/Apr/09 06:54,0.3,,,,0,,,,,,"get_slice(string tablename, string key, string columnFamily_column, i32 start, i32 count) is expected is return all columns starting at offset ""start"" subject to a maximum of ""count"" columns. The current code does not do this.
Example interaction:

./Cassandra-remote insert 'Table1' 'key' 'DATA:c1' 'val1' 1
None
./Cassandra-remote insert 'Table1' 'key' 'DATA:c2' 'val2' 1
None
./Cassandra-remote insert 'Table1' 'key' 'DATA:c3' 'val3' 1
None

./Cassandra-remote get_slice 'Table1' 'key' 'DATA'  0 2

[ {'columnName': 'c1', 'value': 'val1', 'timestamp': 1},
  {'columnName': 'c2', 'value': 'val2', 'timestamp': 1}]

./Cassandra-remote get_slice 'Table1' 'key' 'DATA'  1 2

[ {'columnName': 'c1', 'value': 'val1', 'timestamp': 1},
  {'columnName': 'c2', 'value': 'val2', 'timestamp': 1}]]  <<---- Same as prev! ""start"" ignored

./Cassandra-remote  get_slice 'Table1' 'key' 'DATA'  0 1
[{'columnName': 'c1', 'value': 'val1', 'timestamp': 1}]

./Cassandra-remote get_slice 'Table1' 'key' 'DATA'  2 1
[{'columnName': 'c1', 'value': 'val1', 'timestamp': 1}]    <<---- Same as prev! ""start"" ignored




",all,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/09 02:46;jbellis;81-v5.patch;https://issues.apache.org/jira/secure/attachment/12405681/81-v5.patch","21/Apr/09 07:57;sandeep_tata;CASSANDRA-81-v6.patch;https://issues.apache.org/jira/secure/attachment/12405969/CASSANDRA-81-v6.patch","23/Apr/09 06:47;sandeep_tata;CASSANDRA-81-v7.patch;https://issues.apache.org/jira/secure/attachment/12406176/CASSANDRA-81-v7.patch","15/Apr/09 11:17;sandeep_tata;fix_for_get_slice.patch;https://issues.apache.org/jira/secure/attachment/12405487/fix_for_get_slice.patch","16/Apr/09 02:21;sandeep_tata;get_slice_fix_and_unit_tests_v2.patch;https://issues.apache.org/jira/secure/attachment/12405553/get_slice_fix_and_unit_tests_v2.patch","16/Apr/09 07:19;sandeep_tata;get_slice_fix_and_unit_tests_v3.patch;https://issues.apache.org/jira/secure/attachment/12405595/get_slice_fix_and_unit_tests_v3.patch","17/Apr/09 00:49;sandeep_tata;get_slice_fix_and_unit_tests_v4.patch;https://issues.apache.org/jira/secure/attachment/12405665/get_slice_fix_and_unit_tests_v4.patch","15/Apr/09 11:08;sandeep_tata;unit_tests_for_get_slice.patch;https://issues.apache.org/jira/secure/attachment/12405485/unit_tests_for_get_slice.patch",,,,,,,8.0,sandeep_tata,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19538,,,Wed Apr 22 22:54:18 UTC 2009,,,,,,,,,,"0|i0fwrb:",90905,,,,,Normal,,,,,,,,,,,,,,,,,"15/Apr/09 11:04;sandeep_tata;There's an another, more subtle problem with this bug. Even with start=0, the columns returned are not guaranteed to be in order.

Suppose that you:

write col2
write col3

flush -- the memtable is now empty, the ssTable contains col2 and col3
write col1
write col4
write col5

get_slice(table, colfam, 0, 3):

You should get col1, col2, col3 --> not col1, col4, col5 (from current memtable alone)
;;;","15/Apr/09 11:08;sandeep_tata;Added unit tests that produce errors for get_slice when
a) called with nonzero start
b) when data needs to be fetched from ssTables and Memtable;;;","15/Apr/09 11:17;sandeep_tata;Basic idea:

Instead of passing in a CountFilter, the getRow call resolves the entire row, drops the first ""offset"" cols, picks the next ""count"" cols and returns. For the given semantics, you can't get around resolving the full row.

;;;","15/Apr/09 11:41;sandeep_tata;The current patch only tests this with columns -- we'll need some tests for super columns next.;;;","16/Apr/09 00:15;jbellis;functionality patch looks ok.  (but watch bracing -- }else should be two lines to be consistent.  I can fix that up on apply though in this case.)

some comments on the tests --

1. does testGetRowSingleColumn add anything, coverage-wise?  If not, I don't want to maintain it. :)

2. don't catch test exceptions, let them get raised.  that's a lot more informative than assertTrue(False).

3. table.clearSnapshot() is a no-op here, so leave that out.  (if you inherit from ServerTest that will do a slightly better job of cleanup but the code relies too much on static structures for us to do it ""right.""  so don't worry too much about that.)
;;;","16/Apr/09 02:21;sandeep_tata;Thanks for the review. Attached a revised version.

>1. does testGetRowSingleColumn add anything, coverage-wise? If not, I don't want to maintain it. :) 
It isn't much, but It does test one of the simpler getRow calls in Table. 

2. don't catch test exceptions, let them get raised. that's a lot more informative than assertTrue(False). 
Fixed.

>3. table.clearSnapshot() is a no-op here, so leave that out. (if you inherit from ServerTest that will do a slightly better job of cleanup but the code relies too much on static structures for us to do it ""right."" so don't worry too much about that.) 
Removed. I like the idea of inheriting ServerTest -- I modified TableTest to do that. Everything still passes :-);;;","16/Apr/09 04:46;jbellis;oh, one more thing -- flush is asynchronous, so you'll want to call waitForFlush like the CFS tests.;;;","16/Apr/09 07:19;sandeep_tata;Changes with v3:

1. Added supercolumn tests
2. Moved the filtering code into CountFilter and called it from Table
3. Added call to waitForFlush ;;;","16/Apr/09 10:22;jbellis;hmm, patch does not apply cleanly against trunk for me.  which is weird since CountFilter hasn't been touched in a while.  Can you try regenerating after svn up just to be sure?;;;","17/Apr/09 00:49;sandeep_tata;Regenerated v4 after an svn up -- there were some changes to ColumnFamilyStoreTest that I hadn't updated to.;;;","17/Apr/09 02:46;jbellis;I cleaned up a couple things in CountFilter.

1. use this() for constructor overloading instead of pasting code

2. values.length == 1 is _not_ the same as !isSuper().  You could be slicing supercolumns of a super CF, or you could be slicing normal columns of a standard CF.  (This is an easy mistake to make because the API is designed poorly.  But fixing that is a job for 0.4 I think.)

testGetRowSuperColumnOffsetCount errors out b/c of cleanup problems.  When I applied it after CASSANDRA-85 (which improves cleanup considerably), the test fails.  (Possibly my fault for sloppy conflict resolution.)

I suggest we apply 85 first to at least get to a reproducible state in the test.;;;","17/Apr/09 04:13;sandeep_tata;Makes sense. I'll finish reviewing 85, and then redo this patch.;;;","21/Apr/09 07:57;sandeep_tata;Remade patch after #85.


> 2. values.length == 1 is _not_ the same as !isSuper(). You could be slicing supercolumns of a super CF, or you could be slicing normal columns of a standard CF. (This is an easy mistake to make because the API is designed poorly. But fixing that is a job for 0.4 I think.) 

Good catch. Fixed. 

I don't like that getColumnCount returns 1+number of subcolumns for supercolumns. That is, cf.getAllColumns.size() is not equal to cf.getColumnCount. If at some point we decide to change this, we'll have to fix these unit tests.;;;","23/Apr/09 06:32;jbellis;                if (count == Integer.MAX_VALUE && start == 0) //Don't need to filter

this test needs to be modified a bit since the defaults are -1, -1.  I scanned the rest of the code and I think the other tests on count and start are ok, can you doublecheck that?

                    IFilter filter = new CountFilter(count, start);
                    filteredCf = filter.filter(cf, columnFamily);

style nit: can we inline the filter creation?
;;;","23/Apr/09 06:47;sandeep_tata;1. fixed test.
2. inlined filter :-);;;","23/Apr/09 06:54;jbellis;applied;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"in a cluster, get_range_slice() does not return all the keys it should",CASSANDRA-781,12455694,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,bjc,bjc,09/Feb/10 06:32,16/Apr/19 17:33,22/Mar/23 14:57,24/Feb/10 01:06,0.5,,,,0,,,,,,"get_range_slice() does not return the same set of keys as get_key_range() in 0.5.0 final.

I posted a program to reproduce the behavior:

http://www.mail-archive.com/cassandra-dev@incubator.apache.org/msg01474.html

Apparently, you must have more than one node to get the behavior. Also, it may depend on the locations of the nodes on the ring.. I.e., if you don't generate enough keys randomly, then by chance they could all fall on the same host and you might not see the behavior, although I was able to get it to happen using only 2 nodes and 10 keys.

Here are the other emails describing the issue:

http://www.mail-archive.com/cassandra-user@incubator.apache.org/msg02423.html
","Debian 5 lenny on EC2, Gentoo linux, Windows XP",hbadenes,slebresne,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Feb/10 04:38;jbellis;781-backport.txt;https://issues.apache.org/jira/secure/attachment/12436624/781-backport.txt","16/Feb/10 01:03;jbellis;781.txt;https://issues.apache.org/jira/secure/attachment/12435887/781.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19861,,,Tue Feb 23 17:06:05 UTC 2010,,,,,,,,,,"0|i0g11b:",91598,,,,,Normal,,,,,,,,,,,,,,,,,"09/Feb/10 13:52;jbellis;Reproduced on trunk.  Can you try the attached patch there?  (We can backport to 0.5 afterwards.);;;","09/Feb/10 16:16;bjc;Before applying 781.txt to trunk I was getting an exception. The exception is now gone! Awesome. However, I still have the range scanning problem. Here are a few example runs of the test:

$ python test_bug.py get_range_slice
ebbde791748641be951802d64d48c62d not marked 0
9bfa7ad8abce48a9a45daccfa3772f29 not marked 0
$ python test_bug.py get_range_slice
c554dc532bfb462b950990b6824f11c1 not marked 0
e6c4a0100508451ea3a1d13088877dd9 not marked 0
$ python test_bug.py get_range_slice
$ python test_bug.py get_range_slice
$ python test_bug.py get_range_slice
27fc88c8e7ab489d96f4c749cc86aca1 not marked 0
$ python test_bug.py get_range_slice
b522cca4bd6f4282a507525598139f95 not marked 0
$ python test_bug.py get_range_slice
d045b3edabc949fea30242722a11587a not marked 0
$ 

Sigh, I just realized that under trunk my nodetool doesn't work. However, I got the tokens from the log:

INFO 08:10:50,338 Saved Token not found. Using 68054825649105441942293089893012253843
INFO 08:10:53,293 Saved Token not found. Using 44181284974408316254372647768836513112
;;;","09/Feb/10 20:53;hbadenes;I am hitting a similar problem on 3-node cluster I run. I do receive 5 rows in the get_range_slice query (from """" to """"), instead of an empty result set as described here (there exist more than just those 5 returned).

I am running 0.5.0 and could be able to test a patch for that version.;;;","09/Feb/10 21:30;slebresne;I add the same problem, range_slice on 2 nodes was missing results.
I updated to trunk and applied 781.txt. It fixes the missing results but
introduce a timeout exception (I don't believe it is the patch fault though).

The timeout happens with consistencyLevel.ONE (but I suspect it could happen
with QUORUM too). Looking a bit to the details, it seems that
RangeSliceResponseResolver wait for a response from every live natural
endpoints (in isDataPresent()). But in StorageProxy, the rangeSlice message is
only sent to 'responseCount' endpoints (which will be 1 for
consistencyLevel.ONE). Hence the timeout.

I'm not sure what would be the best way to deal with that though.;;;","09/Feb/10 22:27;jbellis;Sylvain: absolutely right, patch 01 has a fix for this.

Jack: updated patch 02 with more logging at INFO, maybe that will help.  I can't reproduce even w/ your tokens and several runs of 100 keys.  I am using a slightly simpler test, though:

        import uuid
        ks = ""Keyspace1""
        cf = ""Super1""
        path = ColumnPath(cf, ""foo"", ""is"")
        value = ""cool""

        # insert, record keys in `keys` set
        keys = set()
        for i in xrange(100):
            key = uuid.uuid4().hex
            client.insert(ks, key, path, value, 0, ConsistencyLevel.ONE)
            keys.add(key)
    
        # remove keys found from set
        parent = ColumnParent(column_family=cf)
        slice_range = SliceRange(start=""key"", finish=""key"")
        predicate = SlicePredicate(slice_range=slice_range)        
        result = client.get_range_slice(ks, parent, predicate, """", """", 1000, ConsistencyLevel.ONE)
        for row in result:
            keys.discard(row.key)

        # if there are any left over, there is a bug
        assert not keys, list(sorted(keys))

... this will of course only work until you insert more than 1000 keys.  (maybe your original test has a similar limitation, i don't remember.)

(Are you still testing w/ RF of 2?  If so maybe patch 01 will help you too.);;;","10/Feb/10 05:08;bjc;Ok, I checked out a fresh copy of trunk and applied both new patches (0001 and 0002). There were some errors for 0002, but upon looking at the code it seems parts of the patch have already been committed to the SVN repo, so what I ended up with is the right thing.

$ patch -p1 <0001-fix-timeout-bug.txt 
patching file src/java/org/apache/cassandra/service/StorageProxy.java
$ patch -p1 <0002-fix-slices-over-non-trivial-wrapped-ranges.txt
patching file src/java/org/apache/cassandra/db/ColumnFamilyStore.java
Hunk #1 succeeded at 43 (offset 1 line).
Hunk #2 succeeded at 1075 (offset 2 lines).
patching file src/java/org/apache/cassandra/dht/AbstractBounds.java
Hunk #1 FAILED at 29.
1 out of 1 hunk FAILED -- saving rejects to file src/java/org/apache/cassandra/dht/AbstractBounds.java.rej
patching file src/java/org/apache/cassandra/dht/Bounds.java
Hunk #1 FAILED at 1.
Hunk #2 FAILED at 11.
2 out of 2 hunks FAILED -- saving rejects to file src/java/org/apache/cassandra/dht/Bounds.java.rej
patching file src/java/org/apache/cassandra/dht/Range.java
patching file src/java/org/apache/cassandra/service/StorageProxy.java
patching file src/java/org/apache/cassandra/service/StorageService.java
Reversed (or previously applied) patch detected!  Assume -R? [n] 
Apply anyway? [n] 
Skipping patch.
3 out of 3 hunks ignored -- saving rejects to file src/java/org/apache/cassandra/service/StorageService.java.rej
patching file test/unit/org/apache/cassandra/dht/BoundsTest.java
patching file test/unit/org/apache/cassandra/dht/RangeTest.java
$ 

Copied my storage-conf.xml into place, which has only two changes: Seeds defined and binding addresses changed to null strings. Thus, my RF is 1 now, not 2 as it was before.

I now use your simpler test. Here is the test with all the import statements:

import uuid

from thrift import Thrift
from thrift.transport import TTransport
from thrift.transport import TSocket
from thrift.protocol.TBinaryProtocol import TBinaryProtocolAccelerated
from cassandra import Cassandra
from cassandra.ttypes import *

num_keys = 10

socket = TSocket.TSocket(""10.212.87.165"", 9160)
transport = TTransport.TBufferedTransport(socket)
protocol = TBinaryProtocol.TBinaryProtocolAccelerated(transport)
client = Cassandra.Client(protocol)

transport.open()

ks = ""Keyspace1""
cf = ""Super1""
path = ColumnPath(cf, ""foo"", ""is"")
value = ""cool""

# insert, record keys in `keys` set
keys = set()
for i in xrange(100):
    key = uuid.uuid4().hex
    client.insert(ks, key, path, value, 0, ConsistencyLevel.ONE)
    keys.add(key)

# remove keys found from set
parent = ColumnParent(column_family=cf)
slice_range = SliceRange(start=""key"", finish=""key"")
predicate = SlicePredicate(slice_range=slice_range)
result = client.get_range_slice(ks, parent, predicate, """", """", 1000, ConsistencyLevel.ONE)
for row in result:
    keys.discard(row.key)

# if there are any left over, there is a bug
assert not keys, list(sorted(keys))


I cleared out the data/commitlog dirs and launched both nodes. The tokens:

 INFO 21:02:08,768 Saved Token not found. Using 136351045523563703929320485474511375137
 INFO 21:02:09,509 Saved Token not found. Using 20118706661854036583649958139769313744

Now.. run the test!

$ python test_bug_simple.py
Traceback (most recent call last):
  File ""test_bug_simple.py"", line 36, in <module>
    result = client.get_range_slice(ks, parent, predicate, """", """", 1000, ConsistencyLevel.ONE) 
  File ""/usr/local/python//lib/python2.6/site-packages/cassandra/Cassandra.py"", line 486, in get_range_slice
  File ""/usr/local/python//lib/python2.6/site-packages/cassandra/Cassandra.py"", line 508, in recv_get_range_slice
thrift.Thrift.TApplicationException: Internal error processing get_range_slice


The log from the node I am querying:

 INFO 21:02:08,768 Saved Token not found. Using 13635104552356370392932048547451
1375137
 INFO 21:02:08,933 Starting up server gossip
 INFO 21:02:09,118 Cassandra starting up...
 INFO 21:02:10,686 Node /10.212.230.176 is now part of the cluster
 INFO 21:02:11,750 InetAddress /10.212.230.176 is now UP
 INFO 21:05:43,927 scanning node range (136351045523563703929320485474511375137,
20118706661854036583649958139769313744]
ERROR 21:05:43,927 Internal error processing get_range_slice
java.lang.AssertionError
        at org.apache.cassandra.dht.Bounds.<init>(Bounds.java:16)
        at org.apache.cassandra.dht.Bounds.restrictTo(Bounds.java:34)
        at org.apache.cassandra.service.StorageProxy.getRangeSlice(StorageProxy.
java:559)
        at org.apache.cassandra.thrift.CassandraServer.get_range_slice(Cassandra
Server.java:560)
        at org.apache.cassandra.thrift.Cassandra$Processor$get_range_slice.proce
ss(Cassandra.java:1189)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.jav
a:984)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadP
oolServer.java:253)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExec
utor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor
.java:908)
        at java.lang.Thread.run(Thread.java:619)



;;;","10/Feb/10 05:28;jbellis;Sorry, those patches that didn't apply were probably important. :)

I've rebased against r908163, can you revert and try again?;;;","10/Feb/10 05:44;bjc;Makes sense! Can you try one more time? I still can't apply the patch properly.. Or, maybe I'm doing something wrong? Here is the transcript.

$ svn checkout -r908163 https://svn.apache.org/repos/asf/incubator/cassandra/trunk cassandra
...
A    cassandra/README.txt
 U   cassandra
Checked out revision 908163.
$ cd cassandra
$ wget https://issues.apache.org/jira/secure/attachment/12435345/0001-fix-timeout-bug.txt
--2010-02-09 21:40:51--  https://issues.apache.org/jira/secure/attachment/124353
45/0001-fix-timeout-bug.txt
Resolving issues.apache.org... 140.211.11.140
Connecting to issues.apache.org|140.211.11.140|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 1873 (1.8K) [text/plain]
Saving to: `0001-fix-timeout-bug.txt'

100%[======================================>] 1,873       --.-K/s   in 0s      

2010-02-09 21:40:52 (47.0 MB/s) - `0001-fix-timeout-bug.txt' saved [1873/1873]

$ patch -p1 <0001-fix-timeout-bug.txt 
patching file src/java/org/apache/cassandra/service/StorageProxy.java
$ wget https://issues.apache.org/jira/secure/attachment/12435346/0002-fix-slices-over-non-trivial-wrapped-ranges.txt
--2010-02-09 21:41:13--  https://issues.apache.org/jira/secure/attachment/124353
46/0002-fix-slices-over-non-trivial-wrapped-ranges.txt
Resolving issues.apache.org... 140.211.11.140
Connecting to issues.apache.org|140.211.11.140|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 14776 (14K) [text/plain]
Saving to: `0002-fix-slices-over-non-trivial-wrapped-ranges.txt'

100%[======================================>] 14,776      74.8K/s   in 0.2s    

2010-02-09 21:41:14 (74.8 KB/s) - `0002-fix-slices-over-non-trivial-wrapped-ranges.txt' saved [14776/14776]

$ patch -p1 <0002-fix-slices-over-non-trivial-wrapped-ranges.txt 
patching file src/java/org/apache/cassandra/db/ColumnFamilyStore.java
patching file src/java/org/apache/cassandra/dht/AbstractBounds.java
Hunk #1 FAILED at 29.
1 out of 1 hunk FAILED -- saving rejects to file src/java/org/apache/cassandra/dht/AbstractBounds.java.rej
patching file src/java/org/apache/cassandra/dht/Bounds.java
Hunk #1 FAILED at 1.
Hunk #2 FAILED at 11.
2 out of 2 hunks FAILED -- saving rejects to file src/java/org/apache/cassandra/dht/Bounds.java.rej
patching file src/java/org/apache/cassandra/dht/Range.java
patching file src/java/org/apache/cassandra/service/StorageProxy.java
patching file src/java/org/apache/cassandra/service/StorageService.java
patching file test/unit/org/apache/cassandra/dht/BoundsTest.java
patching file test/unit/org/apache/cassandra/dht/RangeTest.java
$ 
;;;","10/Feb/10 06:22;jbellis;weird, happens for me too.  lame!

attached Bounds and AbstractBounds as they should look, post-patch.  just do what you showed in the transcript, then overwrite the local copies with these.;;;","10/Feb/10 07:24;bjc;Ok, got the patch applied properly and things look better! The simple test passes. Awesome!! :) However, the more complicated test uses a ""start"" offset after the first get_range_slice(), and that still causes an exception. From the log:

 INFO 23:15:52,425 scanning node range (20123910036548544936247138992367052936,67283373037552029587203789575295250400]
ERROR 23:15:52,425 Internal error processing get_range_slice
java.lang.AssertionError: [124451343962032323897724984972289130546,67283373037552029587203789575295250400]
        at org.apache.cassandra.dht.Bounds.<init>(Bounds.java:26)
        at org.apache.cassandra.dht.Bounds.getRangeOrBounds(Bounds.java:74)
        at org.apache.cassandra.dht.Bounds.restrictTo(Bounds.java:59)
        at org.apache.cassandra.service.StorageProxy.getRangeSlice(StorageProxy.java:559)
        at org.apache.cassandra.thrift.CassandraServer.get_range_slice(CassandraServer.java:560)
        at org.apache.cassandra.thrift.Cassandra$Processor$get_range_slice.process(Cassandra.java:1189)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:984)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)

If you have trouble duplicating this one I can give you more information, but I did not have difficulty getting it to happen. It happened every time I tried. For example, you can get it to happen just by modifying one line of the simple test:

result = client.get_range_slice(ks, parent, predicate, ""b37e14bb37304e0096e2e77a8fc88a5b"", """", 1000, ConsistencyLevel.ONE)

Of course, if you don't scan starting from """" then the simple test doesn't make sense, because you might specifically exclude keys you are looking for by starting from the string I put in.

However, the more complicated test I posted earlier makes sense and exercises the start and end ranges of get_range_slice(). So..can we go back to the complicated test and specifically make that one work? It's also nice because you should be able to run it over and over, since it removes the keys at the end. With the simple test you have to manually flush the data and restart the servers each time.

Thanks so much for fixing this! I am getting more familiar with the java so soon I might be able to fix some bugs like this.;;;","11/Feb/10 13:27;jbellis;committed patch 1, the timeout fix.

here is a new patch that should fix the remaining range issues.;;;","11/Feb/10 14:53;bjc;Almost there I think. I found that the keys were not being returned in sorted order as they were before, so my trick of taking the last key in a limited range, and using that to start the next limited range did not work. So, I modified the test to sort the keys, then take the last one. When I did this I found another bug: when there are fewer keys in the specified range, duplicates are returned. Also, when I played around with the start and end for the range the server starting giving AssertionErrors:

ERROR 06:25:41,032 Internal error processing get_range_slice
java.lang.AssertionError: [125358492461525499902293558181143752059,1244525150549
22950280650433865080672503]
        at org.apache.cassandra.dht.Bounds.<init>(Bounds.java:26)
        at org.apache.cassandra.dht.Bounds.<init>(Bounds.java:18)
        at org.apache.cassandra.thrift.CassandraServer.get_range_slice(Cassandra
Server.java:558)
        at org.apache.cassandra.thrift.Cassandra$Processor$get_range_slice.proce
ss(Cassandra.java:1189)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.jav
a:984)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadP
oolServer.java:253)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExec
utor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor
.java:908)
        at java.lang.Thread.run(Thread.java:619)


The modified test (below) gets stuck in an infinite loop. If you run it in ipython, you can control-c to get back to the interpretor, then type ""result"" to look at it's contents. It is a sorted list of keys. Look at the last key.. Here is the transcript showing the last two keys from result:

 'ff8bfa30777f455695bf934ac7cfedac',
 'ffb701ea740646b9955f0e339f8e3ee2']

In [70]: result2 = client.get_range_slice(ks, cparent, p, start, """", seg, cl)

In [71]: len(result2)
Out[71]: 1000

In [72]: result3 = client.get_range_slice(ks, cparent, p, start, """", seg, cl)

In [73]: len(result3)
Out[73]: 1000

In [74]: start
Out[74]: 'ffb701ea740646b9955f0e339f8e3ee2'

In [75]: result4 = client.get_range_slice(ks, cparent, p, start, start, seg, cl)

In [76]: len(result4)
Out[76]: 1000

In [77]: result5 = client.get_range_slice(ks, cparent, p, start, start, seg, cl)

In [78]: len(result5)
Out[78]: 1

In [79]: 

That can't be right. Here is the latest test..

import sys
import time
import uuid

from thrift import Thrift
from thrift.transport import TTransport
from thrift.transport import TSocket
from thrift.protocol.TBinaryProtocol import TBinaryProtocolAccelerated

import sys
sys.path.insert(0,'/usr/local/cassandra/interface/thrift/gen-py')

from cassandra import Cassandra
from cassandra.ttypes import *

num_keys = 10000

socket = TSocket.TSocket(""10.212.87.165"", 9160)
transport = TTransport.TBufferedTransport(socket)
protocol = TBinaryProtocol.TBinaryProtocolAccelerated(transport)
client = Cassandra.Client(protocol)

ks = ""Keyspace1""
cf = ""Super1""
cl = ConsistencyLevel.ONE

d = {}
    
transport.open()
    
if 1:
    ## insert keys using the raw thrift interface
    cpath = ColumnPath(cf, ""foo"", ""is"")
    value = ""cool""

    for i in xrange(num_keys):
        ts = time.time()
        key = uuid.uuid4().hex
        client.insert(ks, key, cpath, value, ts, cl)
        d[key] = 1

else:
    ## insert keys using pycassa!
    import pycassa

    client = pycassa.connect([""10.212.87.165:9160""])
    cf_test = pycassa.ColumnFamily(client, ks, cf, super=True)

    for i in xrange(num_keys):
        key = uuid.uuid4().hex
        cf_test.insert(key, { 'params' : { 'is' : 'cool' }})
        d[key] = 1


cparent = ColumnParent(column_family=cf)
slice_range = SliceRange(start=""key"", finish=""key"")
p = SlicePredicate(slice_range=slice_range)

done = False
seg = 1000
start = """"

## do a scan using either get_key_range() (deprecated) or get_range_slice()
## for every key returned that is in the dictionary, mark it as found
while not done:
    print ""start"", start
    result = client.get_range_slice(ks, cparent, p, start, """", seg, cl)

    def getkey(x):
        return x.key
    result = map(getkey, result)   
    result.sort()

    for r in result:
        if d.has_key(r): 
            d[r] = 0

    if len(result) < seg: done = True
    else: start = result[seg-1]

cpath = ColumnPath(column_family=cf, super_column='foo')

## get, remove all the keys
## print all the keys that were not marked 0
for k in d:
    result = client.get(ks, k, cpath, cl)
    #print result

    if d[k] == 1: 
        print k, ""not marked 0""
    #else:
    #    print k, ""was marked 0!""

    ts = time.time()
    client.remove(ks, k, cpath, ts, cl)




BTW, this time around my nodetool worked perfectly! When I first brought the two nodes up, they selected keys that were too close, and one node ended up with all the load. So I ran loadbalance, and it worked great! That was really awesome. The only thing I noticed was a single key that should have been found returned a NotFoundException. I'll keep an eye on this one, too. Best,

Jack;;;","12/Feb/10 06:17;jbellis;new patch attached.

> ERROR 06:25:41,032 Internal error processing get_range_slice 

added InvalidRequestException when start > end, which fixes this.

> The modified test (below) gets stuck in an infinite loop

Your test is buggy. :)

range_slice (like key_range) is start-INCLUSIVE, so if you pass a key that exists as start, you will always get at least one result, the start one.

> the keys were not being returned in sorted order 

This is working fine for me.  Not sure what you were seeing.

> when there are fewer keys in the specified range, duplicates are returned

Sounds like another illustration of start-inclusiveness.

If you still see problems, can you narrow it down to a specific set of keys, rather than relying on randomness to maybe reproduce it once in a while?  That would help a lot.  Thanks!
;;;","12/Feb/10 11:44;bjc;I don't think my test is buggy. I realize that the range is start inclusive, and it does pass a key that exists as start, but sets ""done = True"" if the range scan returns less keys than requested. Since it passes """" as the end/finish, this should return less keys than requested when you get to the end, provided you ask for more than one key (which I do).

I think the last remaining problem is with the sorting! I bet that is why using """" for my finish string doesn't work. Here's the problem I see now (transcript followed by test):

I put 10 random keys in, ask for them back. They aren't sorted, so I sort them and take the highest. I use that as start, and """" as finish. This should give me one key back, but instead I get 10. Could it be that my columnfamily definition is different than yours? Here's mine:

      <ColumnFamily ColumnType=""Super""
                    CompareWith=""UTF8Type""
                    CompareSubcolumnsWith=""UTF8Type""
                    Name=""Super1""
                    RowsCached=""1000""
                    KeysCachedFraction=""0""
                    Comment=""A column family with supercolumns, whose column and subcolumn names are UTF8 strings""/>


In [17]: run test_bug_simple2.py
result1 before sorting
af37b718213b4219897ea1564ebc8900
f196ad5537294840b2de0a636202dbd2
7578ba38b66d4708a38663717e020959
b5266af926a647c3a1a4d2f62dfe952c
d729d5181bac42a48ac3e49d9700047e
4d58b6fbea214d0c9c7a9f288feba2d8
41df7aee7d674a75a4943d89153f9bde
4e121f95459e4f67a6cd3c06b2d078e7
99b2b03675a8413f94e60e3d1bbded8c
66121c5c863f4c1f804a46b8c2136fe9
result1 after sorting
41df7aee7d674a75a4943d89153f9bde
4d58b6fbea214d0c9c7a9f288feba2d8
4e121f95459e4f67a6cd3c06b2d078e7
66121c5c863f4c1f804a46b8c2136fe9
7578ba38b66d4708a38663717e020959
99b2b03675a8413f94e60e3d1bbded8c
af37b718213b4219897ea1564ebc8900
b5266af926a647c3a1a4d2f62dfe952c
d729d5181bac42a48ac3e49d9700047e
f196ad5537294840b2de0a636202dbd2
start f196ad5537294840b2de0a636202dbd2
result2
f196ad5537294840b2de0a636202dbd2
7578ba38b66d4708a38663717e020959
b5266af926a647c3a1a4d2f62dfe952c
d729d5181bac42a48ac3e49d9700047e
4d58b6fbea214d0c9c7a9f288feba2d8
41df7aee7d674a75a4943d89153f9bde
4e121f95459e4f67a6cd3c06b2d078e7
99b2b03675a8413f94e60e3d1bbded8c
66121c5c863f4c1f804a46b8c2136fe9
b8b290a864464271ad30df1bbab2f2b7
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)

/k/jack/bridge/test_bug_simple2.py in <module>()
     54 for r in result2: print r.key
     55 
---> 56 assert len(result2) == 1
     57 
     58 

AssertionError: 
WARNING: Failure executing file: <test_bug_simple2.py>

In [18]: 





import uuid

from thrift import Thrift
from thrift.transport import TTransport
from thrift.transport import TSocket
from thrift.protocol.TBinaryProtocol import TBinaryProtocolAccelerated
 
import sys
sys.path.insert(0,'/usr/local/cassandra/interface/thrift/gen-py')
 
from cassandra import Cassandra
from cassandra.ttypes import *
 
socket = TSocket.TSocket(""10.212.87.165"", 9160)
transport = TTransport.TBufferedTransport(socket)
protocol = TBinaryProtocol.TBinaryProtocolAccelerated(transport)
client = Cassandra.Client(protocol)
 
transport.open()
 
ks = ""Keyspace1""
cf = ""Super1""
path = ColumnPath(cf, ""foo"", ""is"")
value = ""cool""
    
for i in xrange(100):
    key = uuid.uuid4().hex
    client.insert(ks, key, path, value, 0, ConsistencyLevel.ONE)
 
parent = ColumnParent(column_family=cf)
slice_range = SliceRange(start=""key"", finish=""key"")
predicate = SlicePredicate(slice_range=slice_range)
 
result1 = client.get_range_slice(ks, parent, predicate, """", """", 10, ConsistencyLevel.ONE)
 
print ""result1 before sorting""
for r in result1: print r.key
 
def getkey(x): return x.key
 
print ""result1 after sorting""
result1 = map(getkey, result1)
result1.sort()
 
for r in result1: print r
 
start = result1[-1]
 
print ""start"", start
 
result2 = client.get_range_slice(ks, parent, predicate, start, """", 10, ConsistencyLevel.ONE)
 
print ""result2""
for r in result2: print r.key
 
assert len(result2) == 1


;;;","12/Feb/10 11:48;jbellis;If you're using RP instead of OPP you will see that.;;;","12/Feb/10 12:18;bjc;Ahh!! Right you are, I was using RP instead of OPP. Ok, but now here is another problem: if I insert 10 keys and then ask for them back, it works. However, if I insert 10 more and do a range scan with start="""", I don't get the lowest key:

In [17]: run test_bug_simple3.py
insert aa3cf33059d64dac8aef4a250bc5ea9c
insert a7dbda2925eb4b439c89cd71d56b5113
insert f28e92d5e5554857940c9d3386bf4121
insert 2b8ec460e7d346cbaf3dcb00e1aaaf91
insert 7792c98f0c3948299c622c73b906df66
insert 37a8bfdb69b642ba8e96d33b060f789d
insert 38c18f5d3d2c46cbb4e44b603a8acdbd
insert bef8104ea9184abaa3f0788ef7b2e0db
insert 934fe04d30cc4a96b1f1a9e7930316b8
insert 1d3413e88af946349f148c4fafeb6bf7
result 1d3413e88af946349f148c4fafeb6bf7
result 2b8ec460e7d346cbaf3dcb00e1aaaf91
result 37a8bfdb69b642ba8e96d33b060f789d
result 38c18f5d3d2c46cbb4e44b603a8acdbd
result 7792c98f0c3948299c622c73b906df66
result a7dbda2925eb4b439c89cd71d56b5113
result aa3cf33059d64dac8aef4a250bc5ea9c
result bef8104ea9184abaa3f0788ef7b2e0db
result f28e92d5e5554857940c9d3386bf4121
start f28e92d5e5554857940c9d3386bf4121
result f28e92d5e5554857940c9d3386bf4121

In [18]: run test_bug_simple3.py
insert 4eb0300540ec4b4083fbaf33741fc4a5
insert 12b43ba967314b369faff7e59902d6c2
insert 5b4b729676bc4ea2816620c3b6dff080
insert cf2fda1b11d843f1ae7949dbbb7d179d
insert c9d0cf4a1e9a48caa143afd2b0268f70
insert 9a044cff59b940d5bfbeffd58b01ee8e
insert d2ee042f0b0b4f7ea86e6e2c0dfdcfdd
insert d239aee577684c27afea2fe7e3361bdf
insert 706b20976f974de49bda61d55b9c2a63
insert 36177455bc3b4469b7e6f51897c9f3ba
result a7dbda2925eb4b439c89cd71d56b5113
result aa3cf33059d64dac8aef4a250bc5ea9c
result bef8104ea9184abaa3f0788ef7b2e0db
result c9d0cf4a1e9a48caa143afd2b0268f70
result cf2fda1b11d843f1ae7949dbbb7d179d
start cf2fda1b11d843f1ae7949dbbb7d179d
result cf2fda1b11d843f1ae7949dbbb7d179d
result d239aee577684c27afea2fe7e3361bdf
result d2ee042f0b0b4f7ea86e6e2c0dfdcfdd
result f28e92d5e5554857940c9d3386bf4121

In [19]: 


See what I mean? In the first run I inserted ""1d3413e88af946349f148c4fafeb6bf7"" but the second range scan I get ""a7dbda2925eb4b439c89cd71d56b5113"" back first, even when I set start="""". Could this somehow be my fault too?

Test follows:


import uuid

from thrift import Thrift
from thrift.transport import TTransport
from thrift.transport import TSocket
from thrift.protocol.TBinaryProtocol import TBinaryProtocolAccelerated

import sys
sys.path.insert(0,'/usr/local/cassandra/interface/thrift/gen-py')

from cassandra import Cassandra
from cassandra.ttypes import *

socket = TSocket.TSocket(""10.212.87.165"", 9160)
transport = TTransport.TBufferedTransport(socket)
protocol = TBinaryProtocol.TBinaryProtocolAccelerated(transport)
client = Cassandra.Client(protocol)

transport.open()

ks = ""Keyspace1""
cf = ""Super1""
path = ColumnPath(cf, ""foo"", ""is"")
value = ""cool""

for i in xrange(10):
    key = uuid.uuid4().hex
    print ""insert"", key
    client.insert(ks, key, path, value, 0, ConsistencyLevel.ONE)

parent = ColumnParent(column_family=cf)
slice_range = SliceRange(start=""key"", finish=""key"")
predicate = SlicePredicate(slice_range=slice_range)


result = client.get_range_slice(ks, parent, predicate, """", """", 5, ConsistencyLevel.ONE)
for row in result:
    print ""result"", row.key

start = result[-1].key

print ""start"", start

result = client.get_range_slice(ks, parent, predicate, start, """", 10, ConsistencyLevel.ONE)
for row in result:
    print ""result"", row.key

;;;","13/Feb/10 12:28;jbellis;Ah, yes, definitely reintroduced a bug in picking the range to start scanning in.  Fix attached.;;;","14/Feb/10 05:04;bjc;Some of the patches didn't apply, some problem as before? I checked out freshly just now. Can you post the individual files again? Thanks.. Or..maybe this is the problem: why does your most recently attached patch have a Wednesday time stamp on it? Here is what I get at the top:

commit 630c33353647f062134d66afa3b487d95abe03fe
Author: Jonathan Ellis <jonathan.ellis@rackspace.com>
Date:   Wed Feb 10 18:04:08 2010 -0600

    fix range queries

Shouldn't that be Friday? Here's the transcript:

$ svn checkout https://svn.apache.org/repos/asf/incubator/cassan
dra/trunk cassandra
A    cassandra/test
A    cassandra/test/unit
...
$ cd cassandra
$ wget https://issues.apache.org/jira/secure/attachment/12435762
/781.txt
--2010-02-13 20:57:49--  https://issues.apache.org/jira/secure/attachment/124357
62/781.txt
Resolving issues.apache.org... 140.211.11.140
Connecting to issues.apache.org|140.211.11.140|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 47140 (46K) [text/plain]
Saving to: `781.txt'

100%[======================================>] 47,140       120K/s   in 0.4s    

2010-02-13 20:57:50 (120 KB/s) - `781.txt' saved [47140/47140]

$ patch -p1 <781.txt 
patching file src/java/org/apache/cassandra/db/ColumnFamilyStore.java
patching file src/java/org/apache/cassandra/db/RangeSliceReply.java
patching file src/java/org/apache/cassandra/dht/AbstractBounds.java
Hunk #1 FAILED at 1.
Hunk #2 FAILED at 25.
2 out of 2 hunks FAILED -- saving rejects to file src/java/org/apache/cassandra/
dht/AbstractBounds.java.rej
patching file src/java/org/apache/cassandra/dht/Bounds.java
Hunk #1 FAILED at 1.
Hunk #2 FAILED at 8.
2 out of 2 hunks FAILED -- saving rejects to file src/java/org/apache/cassandra/
dht/Bounds.java.rej
patching file src/java/org/apache/cassandra/dht/Range.java
patching file src/java/org/apache/cassandra/service/RangeSliceResponseResolver.j
ava
patching file src/java/org/apache/cassandra/service/StorageProxy.java
patching file src/java/org/apache/cassandra/service/StorageService.java
patching file src/java/org/apache/cassandra/thrift/CassandraServer.java
patching file src/java/org/apache/cassandra/thrift/ThriftValidation.java
patching file test/system/test_server.py
patching file test/unit/org/apache/cassandra/dht/BoundsTest.java
patching file test/unit/org/apache/cassandra/dht/RangeIntersectionTest.java
patching file test/unit/org/apache/cassandra/dht/RangeTest.java
$

;;;","14/Feb/10 05:43;jbellis;re-attached w/ line endings fixed.  ;;;","14/Feb/10 08:12;bjc;Thanks for fixing the patch, but I'm sorry...it still doesn't work right.. :( I found a couple of weird things. First, sometimes when I ask for 10, it gives more. Second, if I pass start="""" it still doesn't start at the beginning. 

However, this doesn't happen every time I start fresh. Maybe it's depedent on the tokens. Here are the tokens for the case I show below: 

 INFO 23:38:21,989 Saved Token not found. Using 0njMRYmU9KiXE80d 
 INFO 23:38:20,112 Saved Token not found. Using I8LW8J6h9UCuz0dC 

The first token belongs to the node I am attaching the client to.

This functionality seems surprisingly complicated. Maybe it would help to write down pseudo-code for the way it should work? I have to admit that I cannot piece it together by reading the comments in your patch.

Here is a transcript and test: 

First run, looks ok: 

ip-10-212-87-165$ python test_bug_simple3.py 
insert 10b470f49a7c46bd938d784ca4096b63 
insert 47f70aacb3e94e10acaf8e86edac7169 
insert 9a3b7d3b921345bebc4f2bedc1db7c01 
insert c1c9dab59abd4f4ca33ee79f71a179e9 
insert cff81b145faf4648ac8ae001973c6c75 
insert c752d33e5d344312908e5008e6cdae3e 
insert 6e9e32e8b89845bb935d993a9c8bcb13 
insert c286bf2711bc45c1ab033561112c2313 
insert 2dad487ddfa94c81b52c8b4d35d3cb5c 
insert 9c62c7dafdb94dfdbdf52b527bdd2b24 

result 10b470f49a7c46bd938d784ca4096b63 
result 2dad487ddfa94c81b52c8b4d35d3cb5c 
result 47f70aacb3e94e10acaf8e86edac7169 
result 6e9e32e8b89845bb935d993a9c8bcb13 
result 9a3b7d3b921345bebc4f2bedc1db7c01 
result 9c62c7dafdb94dfdbdf52b527bdd2b24 
result c1c9dab59abd4f4ca33ee79f71a179e9 
result c286bf2711bc45c1ab033561112c2313 
result c752d33e5d344312908e5008e6cdae3e 
result cff81b145faf4648ac8ae001973c6c75 
total_keys 10 

Second run, get 18 keys when I asked for 10: 

ip-10-212-87-165$ python test_bug_simple3.py 
insert f873d662dccf46c28080a01286e09ed8 
insert 903776c2f45740389aa52675bf47c7ec 
insert 0e80401a9052405a898d11e5ae874a13 
insert 398d51ba174b4c9db8c25ca6cd2c9454 
insert 50f1cd47dd284ee9b9573b4dfce39134 
insert 20fa43d2365b4dfab9b05a93992315d0 
insert e009d5b76e8840b784fe6b9b649ae1df 
insert 63497f9d63c74b99a681fa2fc52751ac 
insert 824bbcf997de48a99cad174e9e1f1eec 
insert 01c5a6506f4247068660c20338a03bb3 

result 01c5a6506f4247068660c20338a03bb3 
result 0e80401a9052405a898d11e5ae874a13 
result 10b470f49a7c46bd938d784ca4096b63 
result 20fa43d2365b4dfab9b05a93992315d0 
result 2dad487ddfa94c81b52c8b4d35d3cb5c 
result 398d51ba174b4c9db8c25ca6cd2c9454 
result 47f70aacb3e94e10acaf8e86edac7169 
result 50f1cd47dd284ee9b9573b4dfce39134 
result 63497f9d63c74b99a681fa2fc52751ac 
result 6e9e32e8b89845bb935d993a9c8bcb13 
result 824bbcf997de48a99cad174e9e1f1eec 
result 903776c2f45740389aa52675bf47c7ec 
result c1c9dab59abd4f4ca33ee79f71a179e9 
result c286bf2711bc45c1ab033561112c2313 
result c752d33e5d344312908e5008e6cdae3e 
result cff81b145faf4648ac8ae001973c6c75 
result e009d5b76e8840b784fe6b9b649ae1df 
result f873d662dccf46c28080a01286e09ed8 
total_keys 18 

Third run, start at ""ca.."" even though I pass start="""" and all the previous keys remain: 

ip-10-212-87-165$ python test_bug_simple3.py 
insert ca97d7efb63448f8a62d6f7f73044236 
insert 91363a713b714af88ac2191caeea5351 
insert 7b6756d0ab8e450b826b1abc7210d524 
insert e6c6765497af4078b93e1a1470bd3194 
insert e3457f26754c4e7cb7ef606f98e7bb78 
insert 99643eb237ea4ca8b50cac4bb4d58edd 
insert ec3e1f81359b4ae08cfed73899934a93 
insert ae2b990ceb044bf194a879059f823ecf 
insert 2c1494f0ad3d48d2bf4feb33f40cf38e 
insert 0cb1c2e906b64fee89f7729052e0810e 

result ae2b990ceb044bf194a879059f823ecf 
result c1c9dab59abd4f4ca33ee79f71a179e9 
result c286bf2711bc45c1ab033561112c2313 
result c752d33e5d344312908e5008e6cdae3e 
result ca97d7efb63448f8a62d6f7f73044236 
result cff81b145faf4648ac8ae001973c6c75 
result e009d5b76e8840b784fe6b9b649ae1df 
result e3457f26754c4e7cb7ef606f98e7bb78 
result e6c6765497af4078b93e1a1470bd3194 
result ec3e1f81359b4ae08cfed73899934a93 
total_keys 10 
ip-10-212-87-165$ 

Here is another run (different tokens):

 INFO 00:04:17,105 Saved Token not found. Using Iw1khrAgM5sd6WnX
 INFO 00:04:15,795 Saved Token not found. Using IgLbq912n2xEP99G

In this case I don't see the problem where I get back more keys than I asked for, but I don't get the 10 lowest keys in the second request. 10 are returned, but they are not ordered consistently with what I know is in the db.

First run, notice key ""893.."" is inserted:

ip-10-212-87-165$ python test_bug_simple3.py 
insert 7be5d87bc45843cfaffd36fd654aee53
insert 8ef4727d83474570aa2111bee3929a5f
insert 9a9a91b6b662430092db0209d63a5c9e
insert e45afe1f0e364012acd0dead5b75ea13
insert 10171c87634842aea4f16d46d611c435
insert 10b6f92ac6a447088a82c4ec13056f1e
insert c1acbde9ae454ea2819322975322206b
insert 89352cf117dd4cb9ab935cbb5f230ba0
insert 0b5d924f04174459969594d6293b9aca
insert e2471db4d8f445f2b0c36f3b2a5bb650

result 0b5d924f04174459969594d6293b9aca
result 10171c87634842aea4f16d46d611c435
result 10b6f92ac6a447088a82c4ec13056f1e
result 7be5d87bc45843cfaffd36fd654aee53
result 89352cf117dd4cb9ab935cbb5f230ba0
result 8ef4727d83474570aa2111bee3929a5f
result 9a9a91b6b662430092db0209d63a5c9e
result c1acbde9ae454ea2819322975322206b
result e2471db4d8f445f2b0c36f3b2a5bb650
result e45afe1f0e364012acd0dead5b75ea13
total_keys 10

Second run, notice the results start with ""0.."" but ""893.."" is not returned (though ""b2.."" is, and other higher keys):

ip-10-212-87-165$ python test_bug_simple3.py 
insert 85d282dfa03a466eb51d03f4eb5dacd5
insert a61e7757eaed4ef79fc7bf35f47843f7
insert 9b9b6e3f22994827b0dddcc16105ff7d
insert 880b1644636845d8b1c92faf1f6d8484
insert 5d1d7d7b26ec4540a89c027bccc17e06
insert 4df05d38950f44b29df604c165e1148f
insert 0c9f818aa28a47fb832b6a0929b94280
insert 7e7b829c136046a88b120a4a373d9a6b
insert b2d497713f9342de85bb31b5c0e69af6
insert 80e250253f1942aaab2e9b49880918c0

result 0b5d924f04174459969594d6293b9aca
result 0c9f818aa28a47fb832b6a0929b94280
result 10171c87634842aea4f16d46d611c435
result 10b6f92ac6a447088a82c4ec13056f1e
result 4df05d38950f44b29df604c165e1148f
result a61e7757eaed4ef79fc7bf35f47843f7
result b2d497713f9342de85bb31b5c0e69af6
result c1acbde9ae454ea2819322975322206b
result e2471db4d8f445f2b0c36f3b2a5bb650
result e45afe1f0e364012acd0dead5b75ea13
total_keys 10
ip-10-212-87-165$ 

I found another issue with ""nodetool ring"" which might be related to the patch: 

$ sudo bin/nodetool -h localhost ring 
Exception in thread ""main"" java.lang.reflect.UndeclaredThrowableException 
        at $Proxy0.getRangeToEndPointMap(Unknown Source) 
        at org.apache.cassandra.tools.NodeProbe.getRangeToEndPointMap(NodeProbe.java:151) 
        at org.apache.cassandra.tools.NodeCmd.printRing(NodeCmd.java:74) 
        at org.apache.cassandra.tools.NodeCmd.main(NodeCmd.java:403) 
Caused by: java.rmi.UnmarshalException: error unmarshalling return; nested exception is: 
        java.io.WriteAbortedException: writing aborted; java.io.NotSerializableException: org.apache.cassandra.dht.OrderPreservingPartitioner 
        at sun.rmi.server.UnicastRef.invoke(UnicastRef.java:173) 
        at com.sun.jmx.remote.internal.PRef.invoke(Unknown Source) 
        at javax.management.remote.rmi.RMIConnectionImpl_Stub.invoke(Unknown Source) 
        at javax.management.remote.rmi.RMIConnector$RemoteMBeanServerConnection.invoke(RMIConnector.java:993) 
        at javax.management.MBeanServerInvocationHandler.invoke(MBeanServerInvocationHandler.java:288) 
        ... 4 more 
Caused by: java.io.WriteAbortedException: writing aborted; java.io.NotSerializableException: org.apache.cassandra.dht.OrderPreservingPartitioner 
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1333) 
        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1947) 
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1871) 
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1753) 
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329) 
        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:351) 
        at java.util.HashMap.readObject(HashMap.java:1029) 
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 
        at java.lang.reflect.Method.invoke(Method.java:597) 
        at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:974) 
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1849) 
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1753) 
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329) 
        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:351) 
        at sun.rmi.server.UnicastRef.unmarshalValue(UnicastRef.java:306) 
        at sun.rmi.server.UnicastRef.invoke(UnicastRef.java:155) 
        ... 8 more 
Caused by: java.io.NotSerializableException: org.apache.cassandra.dht.OrderPreservingPartitioner 
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1156) 
        at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1509) 
        at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1474) 
        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1392) 
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1150) 
        at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:326) 
        at java.util.HashMap.writeObject(HashMap.java:1000) 
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 
        at java.lang.reflect.Method.invoke(Method.java:597) 
        at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:945) 
        at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1461) 
        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1392) 
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1150) 
        at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:326) 
        at sun.rmi.server.UnicastRef.marshalValue(UnicastRef.java:274) 
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:315) 
        at sun.rmi.transport.Transport$1.run(Transport.java:159) 
        at java.security.AccessController.doPrivileged(Native Method) 
        at sun.rmi.transport.Transport.serviceCall(Transport.java:155) 
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535) 
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790) 
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649) 
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) 
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) 
        at java.lang.Thread.run(Thread.java:619) 
$ 


Here is the test I ran above: 

import uuid 

from thrift import Thrift 
from thrift.transport import TTransport 
from thrift.transport import TSocket 
from thrift.protocol.TBinaryProtocol import TBinaryProtocolAccelerated 

import sys 
sys.path.insert(0,'/usr/local/cassandra/interface/thrift/gen-py') 

from cassandra import Cassandra 
from cassandra.ttypes import * 

socket = TSocket.TSocket(""10.212.87.165"", 9160) 
transport = TTransport.TBufferedTransport(socket) 
protocol = TBinaryProtocol.TBinaryProtocolAccelerated(transport) 
client = Cassandra.Client(protocol) 

transport.open() 

ks = ""Keyspace1"" 
cf = ""Super1"" 
path = ColumnPath(cf, ""foo"", ""is"") 
value = ""cool"" 

for i in xrange(10): 
    key = uuid.uuid4().hex 
    print ""insert"", key 
    client.insert(ks, key, path, value, 0, ConsistencyLevel.ONE) 

print 

parent = ColumnParent(column_family=cf) 
slice_range = SliceRange(start=""key"", finish=""key"") 
predicate = SlicePredicate(slice_range=slice_range) 

total_keys = 0 

result = client.get_range_slice(ks, parent, predicate, """", """", 10, ConsistencyLevel.ONE) 
for row in result: 
    total_keys += 1 
    print ""result"", row.key 

print ""total_keys"", total_keys 


;;;","16/Feb/10 01:03;jbellis;Yes, it's more complicated than it looks. :)

Attached version fixes regression w/ result set size, and also start key when it falls into a wrapped node range.

There's also a ton of debug logging if you turn that on in log4j, btw.;;;","16/Feb/10 07:18;bjc;Victory!! I think this patch works. :)

One last possible issue: if I remove keys, then do a get_range_slice(), they still show up. A get() on a removed key will return a ""not found"" exception. Should get_range_slice() be aware of the removal? I'm guessing this is an issue about not properly processing the ""tombstone"".

Due to the complexity of get_range_slice(), maybe it's not worth processing the tombstone?;;;","16/Feb/10 09:38;jbellis;right, for get_range_slice we changed the contract from get_key_range -- it can't tell the difference between ""this row has other data, but not data in the columns you requested"" and ""this row has been deleted entirely"" w/o a relatively expensive query, so we decided to just return the slice as-is.;;;","16/Feb/10 10:48;jbellis;fix committed to trunk.  backport to 0.5 pending.;;;","18/Feb/10 01:54;hudson;Integrated in Cassandra #357 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/357/])
    ;;;","22/Feb/10 20:46;herchu;Is this fix going to be backported to 0.5? (any ETA?)
Thanks!;;;","22/Feb/10 21:08;jbellis;Yes, that's at the top of my list.  (I've been busy with PyCon.);;;","22/Feb/10 21:20;herchu;Great, thank you. The testcases for the current fix seem thorough, but if it helps I can do the testing with my own data as soon as a new patch is available.;;;","23/Feb/10 04:38;jbellis;attached backport of fix to 0.5.;;;","23/Feb/10 22:04;herchu;+1 to 781-backport.txt, it solved the problem in my cluster. Thank you!;;;","24/Feb/10 01:06;jbellis;committed to 0.5, thanks for testing.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"ColumnFamilyRecordReader fails for a given split because a host is down, even if records could reasonably be read from other replica.",CASSANDRA-2388,12502417,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,pauloricardomg,eldondev,eldondev,26/Mar/11 04:40,16/Apr/19 17:33,22/Mar/23 14:57,20/Nov/15 21:54,2.1.12,2.2.4,Legacy/Tools,,6,hadoop,inputformat,,,,ColumnFamilyRecordReader only tries the first location for a given split. We should try multiple locations for a given split.,,alexliu68,bontempi,brandon.williams,eldondev,jeromatron,lannyripple,mck,mdennis,patrik.modesto,pauloricardomg,pkolaczk,scottfines,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-4886,,,,,,,,"26/Mar/11 09:32;eldondev;0002_On_TException_try_next_split.patch;https://issues.apache.org/jira/secure/attachment/12474683/0002_On_TException_try_next_split.patch","26/Jun/14 03:22;pauloricardomg;1.2-CASSANDRA-2388.patch;https://issues.apache.org/jira/secure/attachment/12652488/1.2-CASSANDRA-2388.patch","26/Jun/14 03:22;pauloricardomg;2.0-CASSANDRA-2388-v2.patch;https://issues.apache.org/jira/secure/attachment/12652489/2.0-CASSANDRA-2388-v2.patch","09/Jun/14 22:11;pauloricardomg;2.0-CASSANDRA-2388.patch;https://issues.apache.org/jira/secure/attachment/12649379/2.0-CASSANDRA-2388.patch","28/Jun/11 23:02;mck;CASSANDRA-2388-addition1.patch;https://issues.apache.org/jira/secure/attachment/12484458/CASSANDRA-2388-addition1.patch","04/Jul/11 22:01;mck;CASSANDRA-2388-extended.patch;https://issues.apache.org/jira/secure/attachment/12485146/CASSANDRA-2388-extended.patch","03/Jul/11 06:14;mck;CASSANDRA-2388.patch;https://issues.apache.org/jira/secure/attachment/12485071/CASSANDRA-2388.patch","22/Jun/11 21:46;mck;CASSANDRA-2388.patch;https://issues.apache.org/jira/secure/attachment/12483441/CASSANDRA-2388.patch","11/Jun/11 17:15;mck;CASSANDRA-2388.patch;https://issues.apache.org/jira/secure/attachment/12482140/CASSANDRA-2388.patch","09/Jun/11 14:29;mck;CASSANDRA-2388.patch;https://issues.apache.org/jira/secure/attachment/12481890/CASSANDRA-2388.patch",,,,,10.0,pauloricardomg,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20597,,,Fri Nov 20 13:54:28 UTC 2015,,,,,,,,,,"0|i07hqv:",41642,,pkolaczk,,pkolaczk,Low,,,,,,,,,,,,,,,,,"26/Mar/11 06:45;brandon.williams;I'm not sure special casing NoRouteToHostException to be blacklisted is the best thing to do.  I don't think connections are being setup so often that maintaining a blacklist for any reason is needed.;;;","27/Mar/11 02:24;brandon.williams;Unfortunately, I thought of another problem here.  If we go over the entire replica set, we're potentially going outside of the DC, which is bad since a lot of installations have a DC dedicated to analytics so it doesn't affect their app. It seems that the local address is preferred though, are your task trackers not on the same machines as Cassandra?;;;","18/May/11 05:07;jbellis;Eldon, are you planning to take another stab at this?;;;","18/May/11 05:09;tjake;We need to return the list of replicas in the same DC;;;","23/May/11 22:45;jbellis;Mck, do you want to take a stab at this?;;;","24/May/11 00:27;mck;I'm having a go currently at CASSANDRA-1125 so i might as well look at this too. (but you've caught me on a holiday-week...);;;","07/Jun/11 22:11;mck;How do i obtain the DataCenter name for a given address?

IEndpointSnitch.getDataCenter(inetAddress) would work nicely for me but how do i get the snitch client-side?;;;","08/Jun/11 04:41;mck;Initial attempt at solution. Although I'm a little apprehensive to the additions to cassandra.thrift
(describe_rack(..) isn't used anywhere, it just made sense to add describe_datacenter(..) and describe_rack(..) at the same time).

I've tested that existing hadoop jobs work but the new functionality hasn't been tested (as i currently don't have any RF=2 data setup).

This patch does not include the required re-generated Cassandra.java;;;","09/Jun/11 14:29;mck;Second attempt. (god only knows what i was trying to test last patch ;)
this patch:
 - adds describe_datacenter and describe_rack to cassandra.thrift
 - adds locations in ColumnFamilyRecordReader from the split's alternative endpoints if dc is the same

This patch does not include the required re-generated Cassandra.java
;;;","09/Jun/11 18:22;mck;I have tested this now on data w/ RF=2.
Seems to work ~ok as far as i can see.

One side-effect of this patch is where once one could configure ConfigHelper.setInitialAddress(conf, ""localhost"") this will no longer work for tasks trying to run on the down node.
ColumnFamilyRecordReader.getLocations() will ConnectException trying to call describe_datacenter(..). This will lead to the task failing. Hadoop re-runs the task then on another node and eventually the job will complete. But the fall back to replica never is used.

If the initialAddress is hardcoded to one node then we no longer have a decentralised job.

I would like to allow a comma-separated in initialAddress, for example it could be ""localhost, node01, node02, node03"". This would give preference to localhost and avoid any centralisation.

I would also like to make ColumnFamilyRecordReader.getLocations() return an iterator instead of an array.
The createConnection(..) and client.describe_datacenter(..) calls are an unnecessary overhead when all nodes (or first endpoint location) are up, and could be avoided by lazy-loading the list.;;;","11/Jun/11 17:15;mck;New patch. I think i'm at last happy with it.

getLocations() returns an iterator so client.describe_datacenter() is only called when necessary.

Rather than provide a list in initialAddress it was possible to use either the initialAddress OR the endpoint. This gave the benefit in not listing a location that can't actually be connected to.

The ""only use replica from same DC"" is an option now in ConfigHelper. By default it is true.

Again the re-generated Cassandra.java is not included in the patch.

I have tested this on normal jobs, and RF=2 jobs with a node down.;;;","13/Jun/11 23:23;tjake;The get_rack seems unused so it should be removed.

Also, it might be better to pass all locations in the get_datacenter thrift call since you can get the results in one shot and sort them by the dynamic snitch, filtering out the dead nodes:

{noformat}
  DatabaseDescriptor.getEndpointSnitch().sortByProximity(FBUtilities.getLocalAddress(), endpoints); 
{noformat}

{noformat}
  FailureDetector.instance.isAlive(endpoint)
{noformat}

;;;","14/Jun/11 00:15;mck;Then (if i understand you correctly) i would need in cassandra.thrift
{noformat}
      /** returns alive endpoints, sorted by proximity, that belong in the same datacenter as the given endpoint */
  list<string> get_endpoints_in_same_datacenter(1: string endpoint, 2: required list<string> endpoints)
    throws (1:InvalidRequestException ire)
{noformat}

Then the API becomes quite specific to this usecase. Is the performance gain worth it? What's the cost of each client.describe_datacenter(..) call, and probably more important what is the lost performance of writing to the furthest node that's within the same datacenter?;;;","14/Jun/11 00:40;mck;Just make sure i understand you T Jake, you would rather something like this in CassandraServer.java?
(I've renamed from the previous comment get_endpoints_in_same_datacenter(..) to sort_endpoints_by_proximity(..))
{noformat}
    public String[] sort_endpoints_by_proximity(String endpoint, String[] endpoints, boolean restrictToSameDC) 
        throws TException, InvalidRequestException
    {
        try
        {
            List<String> results = new ArrayList<String>();
            InetAddress address = InetAddress.getByName(endpoint);
            String datacenter = DatabaseDescriptor.getEndpointSnitch().getDatacenter(address);
            List<InetAddress> addresses = new ArrayList<InetAddress>();
            for(String ep : endpoints)
            {
                addresses.add(InetAddress.getByName(ep));
            }
            DatabaseDescriptor.getEndpointSnitch().sortByProximity(address, addresses);
            for(InetAddress ep : addresses)
            {
                String dc = DatabaseDescriptor.getEndpointSnitch().getDatacenter(ep);
                if(FailureDetector.instance.isAlive(ep) && (!restrictToSameDC || datacenter.equals(dc)))
                {
                    results.add(ep.getHostName());
                }
            }
            return results.toArray(new String[results.size()]);
        }
        catch (UnknownHostException e)
        {
            throw new InvalidRequestException(e.getMessage());
        }
    }
{noformat};;;","14/Jun/11 00:59;tjake;bq. what is the lost performance of writing to the furthest node that's within the same datacenter?

The benefit is really the DynamicSnitch. if a node it slow due to compaction then this would avoid sending requests there... 

bq. public String[] sort_endpoints_by_proximity(String endpoint, String[] endpoints, boolean restrictToSameDC)

I don't think it makes sense to send the client endpoint to this call since the endpoint might not be a cassandra node.  It's a reasonable assumption that the endpoint it's talking to is local enough to the client to use that.

;;;","14/Jun/11 01:52;mck;{quote}
bq.   public String[] sort_endpoints_by_proximity(String endpoint, String[] endpoints, boolean restrictToSameDC)
I don't think it makes sense to send the client endpoint to this call since the endpoint might not be a cassandra node. It's a reasonable assumption that the endpoint it's talking to is local enough to the client to use that.
{quote}
For the test set i was running against, RF=2, each split's has two endpoints always in different datacenters.

If the ""local"" endpoint is down then getLocations() will then call client.sort_endpoints_by_proximity(..) and this will fail (being the same endpoint).
It then makes a client connection through the ""other"" endpoint. \[see CFRR.describeDatacenter(..)].
This will presume the wrong datacenter and return itself as a valid endpoint. 
I need some way to know what the original datacenter is, even when it is down.;;;","14/Jun/11 02:19;tjake;ok but why not change the response to map<string,list<string>>  where key is DC and value are proximity sorted endpoints?;;;","14/Jun/11 03:19;mck;Won't the sorting still be wrong?
For the use-case above it will solve restricting to the correct datacenter, but the sorting will still be based on proximity to the wrong node?

bq. I don't think it makes sense to send the client endpoint to this call since the endpoint might not be a cassandra node. 
It might not be an alive cassandra node, but it should be a cassandra node. It comes from the split's list of endpoints. At least in this use-case, or are you referring to general usage for this new api?
bq. It's a reasonable assumption that the endpoint it's talking to is local enough to the client to use that.
I don't think so... The endpoint that it talks to is a completely random (just the next endpoint listed in the split's list). This is why i think that such sorting won't just be wrong but not even close. Does this make sense?;;;","14/Jun/11 21:07;tjake;I think the core issue is you can't assume the hadoop node is running on a cassandra node...

If it is then the logic is straight forward, if not then it's possible the connection could cross DC boundaries. One possibility is to use the ip octets like the RackInferringSnitch.  

How's this proposal then?  keep the sort_endpoints_by_proximity signature as is and pass the client endpoint along with the list of data endpoints and add the following logic:

1) sort the endpoints using the endpoint_snitch.
2) if client endpoint *is* a valid cassandra node get the nodes DC and prune nodes outside of this DC
3) if client endpoint *is not* a valid cassandra node try to infer the DC from its ip and prune dataendpoint nodes in a different DC. If no cassandra nodes are in the DC list goto 3).
4) all else fails return the sorted endpoint list
;;;","15/Jun/11 19:23;mck;bq. [snip] One possibility is to use the ip octets like the RackInferringSnitch. 

In our usecase we have three nodes defined via PropertyFileSnitch:{noformat}152.90.241.22=DC1:RAC1 #node1
152.90.241.23=DC2:RAC1 #node2
152.90.241.24=DC1:RAC1 #node3{noformat}
The only way to infer here is even addresses belong to one dc, odd to the other. This is not how RackInferringSnithc works.

When we make the connection through the ""other"" (node2) endpoint taking the rack inferring approach ""152.90."" will say it's in DC2. (again) this is the wrong DC and will return itself as a valid endpoint....

Step (3) seems to me to be too specific to be included here.
If i go only with steps (1),(2),and (4) we get this code:{noformat}    public String[] sort_endpoints_by_proximity(String endpoint, String[] endpoints, boolean restrictToSameDC) 
            throws TException, InvalidRequestException
    {
        try
        {
            List<String> results = new ArrayList<String>();
            InetAddress address = InetAddress.getByName(endpoint);
            boolean endpointValid = null != Gossiper.instance.getEndpointStateForEndpoint(address);
            String datacenter = DatabaseDescriptor
                    .getEndpointSnitch().getDatacenter(endpointValid ? address : FBUtilities.getLocalAddress());
            List<InetAddress> addresses = new ArrayList<InetAddress>();
            for(String ep : endpoints)
            {
                addresses.add(InetAddress.getByName(endpoint));
            }
            DatabaseDescriptor.getEndpointSnitch().sortByProximity(address, addresses);
            for(InetAddress ep : addresses)
            {
                String dc = DatabaseDescriptor.getEndpointSnitch().getDatacenter(ep);
                if(FailureDetector.instance.isAlive(ep) && (!restrictToSameDC || datacenter.equals(dc)))
                {
                    results.add(ep.getHostName());
                }
            }
            return results.toArray(new String[results.size()]);
        }
        catch (UnknownHostException e)
        {
            throw new InvalidRequestException(e.getMessage());
        }
    }{noformat}

I'm happy with this (except that {{Gossiper.instance.getEndpointStateForEndpoint(address)}} is only my guess on how to tell if an endpoint is valid as such).;;;","22/Jun/11 21:04;mck;Problem with the suggested approach is that sortByProximity(..) *only* works when address is the local address. See assert statement DynamicEndpointSnitch:134

I could hack this and rewrite the line to
{noformat}IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();
snitch = snitch instanceof DynamicEndpointSnitch ? ((DynamicEndpointSnitch)snitch).subsnitch : snitch;
snitch.sortByProximity(address, addresses);{noformat}
But this of course means that we always bypass DynamicEndpointSnitch's ""scores"".;;;","22/Jun/11 21:46;mck;Up to date patch.
Follows T Jake's points (1),(2), and (4).
And bypasses DynamicEndpointSnitch when sorting by proximity.;;;","24/Jun/11 23:43;tjake;committed with a change to use the dynamic snitch id the passed endpoint is valid.;;;","25/Jun/11 00:43;hudson;Integrated in Cassandra-0.8 #191 (See [https://builds.apache.org/job/Cassandra-0.8/191/])
    Change ColumnFamilyRecordReader to read split from replicas if primary is down

Patch by Mck SembWever; reviewed by tjake for CASSANDRA-2388

jake : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1139358
Files : 
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/Cassandra.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/hadoop/ConfigHelper.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/hadoop/ColumnFamilyInputFormat.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/hadoop/ColumnFamilyRecordReader.java
* /cassandra/branches/cassandra-0.8/interface/cassandra.thrift
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/CassandraServer.java
;;;","25/Jun/11 08:50;jeromatron;This patch applies to the current 0.7-branch with minimal problems - just some imports on CassandraServer that it couldn't resolve properly.  Can this be committed against 0.7-branch for inclusion in 0.7.7?;;;","25/Jun/11 09:05;jeromatron;I've done basic testing with the word count and pig examples to make sure that the basic hadoop integration isn't negatively affected by this.  I'll also try it against our dev cluster before and after the patch - killing one node to see if it fails over to another replica - to make sure it does what it should that way.;;;","25/Jun/11 09:15;jeromatron;Reopening for testing against 0.7.6.;;;","25/Jun/11 10:44;jbellis;Took a look at this belatedly.  I don't understand the contortions at all.  It looks like there's a ton of effort put in to avoiding making sortByProximity work w/ non-local nodes.  Why not just make that work instead?
;;;","25/Jun/11 10:48;jbellis;also: running hadoop on a non-cassandra node is dumb.  i don't see a point in supporting that really.  (yes, my fault it was written that way to begin with, mea culpa.);;;","25/Jun/11 10:58;jeromatron;Jonathan - is it possible to attach an updated patch based on your changes to 0.8 branch?  Not sure if that would be simple to extract.;;;","25/Jun/11 11:07;jbellis;with svn: svn diff -r 1139323:1139483 and hack out the OutboundTcpConnection change in the middle manually from the output.

with git: create a branch, rip out the offending OTC change and squash the other two;;;","25/Jun/11 11:07;jbellis;I think there's deep surgery to be done here still though.  Backporting is probably premature.;;;","25/Jun/11 11:12;jbellis;bq. It looks like there's a ton of effort put in to avoiding making sortByProximity work w/ non-local nodes

Wait, why do we even care?  ""local node"" IS the right host to sort against -- we want the split that is closest to the node running the job, this is not the same as some other C* node we contact.;;;","28/Jun/11 22:13;mck;bq. It looks like there's a ton of effort put in to avoiding making sortByProximity work w/ non-local nodes
Because it's only when that local node is down that we actually need to sort...
When/if DynamicEndpointSnitch's limitation is fixed (and it can sort by non-local nodes) then CassandraServer.java need not bypass it. But this won't simplify the code in CFRR. Now that CFIF supports multiple initialAddresses the method CFRR.sortEndpointsByProximity(..) can be rewritten (ie any connection to any initialAddress is all we need, no need to mess around with trying to connect through replica's to find information about replicas...)
bq. Wait, why do we even care? ""local node"" IS the right host to sort against
Depends on this is CFRR's ""local node"" or CassandraServer's ""local node""... 
CFRR's local node is the right and only node worth sorting against, it being the ""task tracker node"". 
But when c* on the ""task tracker node"" is down, then we randomly connect to another c* node so to find out of the replica we know about which are 1) up, 2) closest, and 3) in the same dc. Then it is a random c* node that becomes the ""local node"" and the call needs to be {{snitch.sortByProximity(initialAddress, addresses)}}.
But yes... the CFRR code is contorted. In many ways i prefer the simplicity of the first patch (both in api and in implementation) despite it not being ""as correct"". i thought of this ""fallback to replica"" as a last resort to keep the m/r job running, rather than an actively used feature where DynamicEndpointSnitch's scores will maximise performance. But then i'm only thinking in terms of a small c* cluster and i certainly am naive about what performance gains these scores can give...;;;","28/Jun/11 22:51;jbellis;bq. CFRR's local node is the right and only node worth sorting against, it being the ""task tracker node"". 

Right.

bq. Then it is a random c* node that becomes the ""local node""

We still want to sort by proxmity-to-TT, because CFRR connects directly to the split owner to do the reads.  initialAddress isn't involved post-split-discovery.

Again, all the complexity goes away if we just embed the snitch into CFIF/TT.

One wrinkle: ec2snitch requires gossip, so TT would need a separate local ip to participate in the gossip ring.  We could make that optional (and fall back to old ""recognize local data, otherwise you get a random replica"" behavior otherwise).
;;;","28/Jun/11 23:01;jbellis;Taking a step back: aren't we optimizing for (1) a corner case with (2) the wrong solution?

Here's what I mean:

1) CFRR already prioritizes the local replica.  So if you have >= one TT for each replica, this only helps if the local C* node dies, BUT the TT does not.  This doesn't happen often.

2) If we ARE in that situation, the ""right"" solution would be to send the job to a TT whose local replica IS live, not to read the data from a nonlocal replica.  How can we signal that?  ;;;","28/Jun/11 23:02;mck;CASSANDRA-2388-addition1.patch: Simplify CFRR now that multiple initialAddresses are supported.;;;","29/Jun/11 05:11;brandon.williams;bq. If we ARE in that situation, the ""right"" solution would be to send the job to a TT whose local replica IS live, not to read the data from a nonlocal replica. How can we signal that?

ISTM the right thing to do in that situation is just fail and let the JT reschedule somewhere else.;;;","29/Jun/11 12:58;mck; - This does happen already (i've seen it while testing initial patches that were no good).
Problem is that the TT is blacklisted, reducing hadoop's throughput for all jobs running.
I bet too that a fallback to a replica is faster than a fallback to another TT.

 - There is no guarantee that any given TT will have its split accessible via a local c* node - this is only a preference in CFRR. A failed task may just as likely go to a random c* node. At least now we can actually properly limit to the one DC and sort by proximity. 

 - One thing we're not doing here is applying this same DC limit and sort by proximity in the case when there isn't a localhost preference. See CFRR.initialize(..)
It would make sense to rewrite CFRR.getLocations(..) to
{noformat}    private Iterator<String> getLocations(final Configuration conf) throws IOException
    {
        return new SplitEndpointIterator(conf);
    }{noformat} and then to move the finding-a-preference-to-localhost code into SplitEndpointIterator...

 - A bug i can see in the patch that did get accepted already is in CassandraServer.java:763 when endpointValid is false and restrictToSameDC is true we end up restricting to a random DC. I could fix this so restrictToSameDC is disabled in such situations but this actually invalidates the previous point: we can't restrict to DC anymore and we can only sortByProximity to a random node... I think this supports Jonathan's point that it's overall a poor approach. I'm more and more in preference of my original approach using just client.getDatacenter(..) and not worrying about proximity within the datacenter.

 - Another bug is that, contray to my patch, the code committed
bq. committed with a change to use the dynamic snitch id the passed endpoint is valid.
 can call {{DynamicEndpointSnitch.sortByProximity(..)}} with an address that is not localhost and this breaks the assertion in the method. ;;;","30/Jun/11 02:40;brandon.williams;{quote}
This does happen already (i've seen it while testing initial patches that were no good).
Problem is that the TT is blacklisted, reducing hadoop's throughput for all jobs running.
{quote}

If the cassandra node where the TT resides isn't working, then throughput is reduced regardless.


bq. I bet too that a fallback to a replica is faster than a fallback to another TT.

I doubt that for any significant job.  Locality is important.  Move the job to the data, not the data to the job.

{quote}
There is no guarantee that any given TT will have its split accessible via a local c* node - this is only a preference in CFRR. A failed task may just as likely go to a random c* node. At least now we can actually properly limit to the one DC and sort by proximity.
{quote}

This sounds like the thing we need to fix, then.  Ensuring that the TT assigned to the map has a local replica.;;;","30/Jun/11 03:25;jbellis;bq. If the cassandra node where the TT resides isn't working, then throughput is reduced regardless.

Right: we _want_ it to be blacklisted in that scenario.;;;","30/Jun/11 03:45;jbellis;bq. This sounds like the thing we need to fix, then. Ensuring that the TT assigned to the map has a local replica.

reverted 1139358, 1139483 to make a fresh start for this.

how do we ""ensure"" this?  isn't that the JT's job, to send jobs to the splits we gave it from CFIF?  (which does make sure that only nodes with the data, are included in the split source list.);;;","30/Jun/11 03:49;mck;{quote}If the cassandra node where the TT resides isn't working, then throughput is reduced regardless.
bq. Right: we want it to be blacklisted in that scenario.{quote}
This is making the presumption that the hadoop cluster is only used with CFIF.
The TT could still be useful for other jobs submitted.
Furthermore a blacklisted TT does't automatically come back - it needs to be manually restarted. Isn't this creating more headache for operations?;;;","30/Jun/11 03:59;tjake;I dont think we should require the TT to be running locally. The whole idea is to support access to Cassandra data from hadoop even if it's just an import. 

This patch does spend a lot of time dealing with non local data for that reason. ;;;","30/Jun/11 04:47;brandon.williams;{quote}
This is making the presumption that the hadoop cluster is only used with CFIF.
The TT could still be useful for other jobs submitted.
{quote}

I'm fine with that assumption.  If you want to run other jobs, use a different cluster.  Cassandra's JVM is eating wasteful memory at that point.

{quote}
Furthermore a blacklisted TT does't automatically come back - it needs to be manually restarted. Isn't this creating more headache for operations?
{quote}

I don't think this is actually the case, see HADOOP-4305


{quote}
I dont think we should require the TT to be running locally. The whole idea is to support access to Cassandra data from hadoop even if it's just an import.

This patch does spend a lot of time dealing with non local data for that reason.
{quote}

I'm fine with dropping support for non-colocated TTs, or at least saying there's no DC-specific support.  Because frankly, that is a very suboptimal thing to do, transfer the data across the network all the time, and flies in the face of Hadoop's core principles.;;;","30/Jun/11 05:00;jbellis;bq. a blacklisted TT does't automatically come back

tlipcon says it comes back after 24h, fwiw.  In any case it's still the case that we DO want to blacklist it while it's down.  (Brisk could perhaps add a ""clear my tasktracker on restart"" operation as a further enhancement.)

bq. I'm fine with dropping support for non-colocated TTs

+1, it was a bad idea and I'm sorry I wrote it. :);;;","30/Jun/11 05:16;mck;bq. tlipcon says it comes back after 24h
just to be clear about my concerns. 
this means a dead c* node will bring down a TT. In a hadoop cluster with 3 nodes this means for 24hrs you're lost 33% throughput. (If less than 10% of hadoop jobs used CFIF i could well imagine some pissed users). (What if you have a temporarily problem with flapping c* nodes and you end up with a handful of blacklisted TTs? etc etc etc).

All this when using a replica, any replica, could have kept things going smoothly, the only slowdown being some of the data into CFIF had to go over the network instead...
;;;","30/Jun/11 08:32;jbellis;bq. this means a dead c* node will bring down a TT

Again: _this is what you want to happen_.  As long as the C* process on the same node is down, you want the TT to be blacklisted and the jobs to go elsewhere.

bq. In a hadoop cluster with 3 nodes this means for 24hrs you're lost 33% throughput

Right, but the real cause is because the C* process is dead, not b/c the TT is blacklisted.  Making the TT read from other nodes will only hurt your network, not fix the throughput problem, b/c i/o is the bottleneck.;;;","30/Jun/11 15:50;mck;Then i would hope for two separate InputFormats. One optimised for local node connection, where cassandra is deemed the more important system over hadoop, and another where data can be read in from anywhere. I think the latter should be supported in some manner  since users may not always have the possibility to install hadoop and cassandra on the same servers, or they might not think it to be so critical part (eg if CFIF is reading using a IndexClause the input data set might be quite small and the remaining code in the m/r be the bulk of the processing...);;;","30/Jun/11 21:17;jbellis;bq. another where data can be read in from anywhere

This is totally antithetical to how hadoop is designed to work.  I don't think it's worth supporting in-tree.;;;","01/Jul/11 05:43;mck;Is CASSANDRA-2388-local-nodes-only-rough-sketch the direction we want then?

This is very initial code, i can't get {{new JobClient(JobTracker.getAddress(conf), conf).getClusterStatus().getActiveTrackerNames()}} to work, need a little help here.
(Also CFRR.getLocations() can be drastically reduced).;;;","02/Jul/11 04:05;jbellis;+1 to CFRR changes

wasn't immediately clear to me what CFIF changes are doing, can you elaborate?;;;","03/Jul/11 06:07;mck;The idea is to setup splits to have only endpoints that are valid trackers. But now i see this is just a brainfart :-) Ofc the jobTracker will apply this match for us. And that CFIF was always 'restricted' to running on endpoints. Although the documentation on inputSplit.getLocations() is a little thin as to whether this restricts which trackers it should run on or whether is just a preference... I guess it doesn't matter, as you point out Jonathan all that's required here is the one line changed in CFRR.

;;;","03/Jul/11 06:16;mck;the new ""one-liner"" CASSANDRA-2388 attached. i'll ""submit patch"" once i've tested it some...;;;","03/Jul/11 06:53;jbellis;Sounds good, thanks!;;;","04/Jul/11 14:39;mck;{quote}2) If we ARE in that situation, the ""right"" solution would be to send the job to a TT whose local replica IS live, not to read the data from a nonlocal replica. How can we signal that?{quote}To /really/ solve this issue can we do the following? 
In CFIF.getRangeMap() take out of each range any endpoints that are not alive. A client connection already exists in this method. This filtering out of dead endpoints wouldn't be difficult, and would move tasks *to* the data making use of replica. This approach does need a new method in cassandra.thrift, eg {{list<string> describe_alive_nodes()}};;;","16/Aug/11 02:47;jbellis;Does that really fix things though?  Because you could have a data node be reachable from the coordinator answering describe_alive_nodes, but unreachable from the client.  So the client still needs to be able to skip unreachable endpoints itself, so describe_alive seems like gratuitous complexity.;;;","20/Aug/11 03:39;patrik.modesto;I'd like to point out the situation in which no node for a given range of keys is available. It can happen for example with keyspace set to RF=1 and a node goes down. I created a patch that gives a user a chance to ignore missing range/node and continue runnig the MapReduce job. The patch is here: http://pastebin.com/hhrr8m9P

Jonathan already replied to the ML with ""ignoring unavailable ranges is a misfeature, imo"".

In our case it's very usefull, although there may be another/smarter solution. We have a keyspace with RF=1 and the nature of our data allows us to ignore temporarily missing node. The current ColumnFamilyInputFormat fails with RuntimeException and AFAIK there is no way around.;;;","20/Aug/11 04:59;brandon.williams;bq. Does that really fix things though? Because you could have a data node be reachable from the coordinator answering describe_alive_nodes, but unreachable from the client. So the client still needs to be able to skip unreachable endpoints itself, so describe_alive seems like gratuitous complexity.

I agree, since the view is from the coordinator, describe_alive_nodes isn't very helpful, and also has to wait on the failure detector to mark the node down anyway.;;;","20/Aug/11 06:44;jbellis;bq. I agree, since the view is from the coordinator, describe_alive_nodes isn't very helpful

Committed Mck's most recent patch.

bq. We have a keyspace with RF=1 and the nature of our data allows us to ignore temporarily missing node

The ""right"" fix is to increase RF.  Ignoring missing data is not a scenario we want to support.;;;","22/Aug/11 16:27;hudson;Integrated in Cassandra-0.7 #539 (See [https://builds.apache.org/job/Cassandra-0.7/539/])
    fail jobs when Cassandra node has failed but TaskTracker has not
patch by Mck SembWever; reviewed by jbellis and brandonwilliams for CASSANDRA-2388

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1159807
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/hadoop/ColumnFamilyRecordReader.java
;;;","31/Aug/11 04:55;mck;This approach isn't really working for me and was committed too quickly i believe.

bq. Although the documentation on inputSplit.getLocations() is a little thin as to whether this restricts which trackers it should run on or whether is just a preference

Tasks are still being evenly distributed around the ring regardless of what the ColumnFamilySplit.locations is.

The chance of a task actually working is RF/N. Therefore the chances of a blacklisted node are high. Worse is that the whole ring can quickly become blacklisted.

http://abel-perez.com/hadoop-task-assignment has an interesting section in it explaining how the task assignment is supposed to work (and that data locality is preferred but not a requirement). Could ColumnFamilySplit.locations be in the wrong format? (eg they should ip not hostname?).;;;","01/Sep/11 00:23;mck;see last comment. (say if this should be a separate bug...)

Maybe hadoop's task allocation isn't working properly because i've an unbalanced ring (i'm working in parallel to fix that).
If this is the case i think it's an unfortunate limitation (the ring must be balanced to get any decent hadoop performance).
It's also probably likely when using {{ConfigHelper.setInputRange(..)}} that the number of nodes involved is small (approaching RF).
With the default hadoop scheduler your hadoop cluster is occupied while just a few taskTrackers are busy. Of course switching to FairScheduler will help some here.

I'll take a look into hadoop's task allocation code as well...;;;","08/Sep/11 14:01;mck;In the meantime could we make this behavior configurable.
eg replace CFRR:176 with something like
{noformat}
    if(ConfigHelper.isDataLocalityDisabled())
    {
        return split.getLocations()[0];
    }
    else
    {
        throw new UnsupportedOperationException(""no local connection available"");
    }{noformat}
;;;","08/Sep/11 20:55;jbellis;Should we just revert the change for now?;;;","09/Sep/11 05:36;mck;Well that would work for me, was only thinking you want to push a ""default behavior"" (especially for those using a RP). 
But I think a better understanding (at least from me) of hadoop's task scheduling is required before enforcing data locality, as as-is it certainly doesn't work for all.;;;","14/Sep/11 10:25;tjake;I just want to confirm what this ticket is about.

The JT has a list of endpoints for a given split.
When a task runs it may or may not be on one of those nodes 
If other tasks are running on all those replicas the JT may put them on a remote node.

So we need to decide which endpoint to connect to given the chance that nodes are down.

1. Check if the node running CFRR is one of the replicas (we have this) this means JT has assigned a data-local task (good)
2. If none of these nodes are local then pick another.
3. If connection fails try the one other nodes.
4. Try to avoid endpoints in a different DC.

The biggest problem is 4.  Maybe the way todo this is change getSplits logic to never return replicas in another DC.  I think this would require adding DC info to the describe_ring call.  Then we only need to worry about 1-3.






;;;","14/Sep/11 22:49;hudson;Integrated in Cassandra-0.7 #552 (See [https://builds.apache.org/job/Cassandra-0.7/552/])
    revert CASSANDRA-2388 (again)

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1170333
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/hadoop/ColumnFamilyRecordReader.java
;;;","09/Mar/12 06:38;jbellis;Marking as minor since the job should get re-submitted, and it's very difficult to reproduce when the tasktrackers are colocated with cassandra nodes (the recommended configuration).;;;","01/Nov/12 00:33;lannyripple;Would very much like a fix to this.  We have a 40 node ring running 2x hadoop clusters on 20 nodes each.  One cluster is on systems that are more flaky than the other (bad batch of memory).  When building a split on the first cluster if a ring node is down in the area of the second cluster we get timeouts with no way to blacklist the offending node even though we have replicas local to the first cluster.

The ring is partitioned into DC1:2, DC2:2 with a hadoop cluster over each DC.;;;","21/Nov/12 19:06;jbellis;Jake's plan above seems like a reasonable approach, but let me back up a step.  I'm just not convinced that the problem we're trying to solve is a real one.  Why do we want to suck a split's worth of data off-node?  If it's because you don't have TackTrackers running on your Cassandra nodes, well, go fix that.

If it's because Hadoop has created too many tasks and all the local replicas have their task queue full, won't assigning it to a non-local TT just cause more contention, than waiting for a local slot to free up?;;;","21/Nov/12 22:42;scottfines;I have two distinct use-cases where running TaskTrackers alongside Cassandra nodes does not accomplish our goals:

1. Joining data. We have a large data set in cassandra, true, but we have a *much* larger data set held in Hadoop itself (around 4 orders of magnitude larger in hadoop than in cassandra). We need to join the two datasets together, and use the output from that join to feed multiple systems, none of which are cassandra. Since the data in Hadoop is so much larger than that in Cassandra, we have to bring the Cassandra data to hadoop, not the other way around. Because of security concerns, we can't spread our hadoop data onto our cassandra nodes (even if that didn't screw with our capacity planning), so we have no other choice but to move the Cassandra data (in small chunks) onto Hadoop. Why not use HBase, you say? We needed Cassandra for its write performance for other problems than this one. 

1. Offline, incremental backups. We have a large volume of time-series data held in Cassandra, and taking nightly snapshots and moving them to our archival center is prohibitively slow--it turns out that moving RF copies of our entire dataset over a leased line every night is a pretty bad idea. Instead, I use MapReduce to take an incremental backup of a much smaller subset of the data, then move that. That way, we not only are not moving the entire data set, but we are also using Cassandra's consistency mechanisms to resolve all the replicas. The only efficient way I've found to do this is via MapReduce (we use the Random Partitioner), and since it's an offline backup, we need to move it over the network anyway--may as well use the optimized network connecting Hadoop and Cassandra instead of the tiny pipe connecting cassandra to our archival center. 

Both of these reasons dictate that we *not* run a TT alongside our Cassandra nodes, no matter what the *recommended* approach is. In this case, we need a strong, fault-tolerant CFIF to serve our purposes.

;;;","19/May/13 05:50;mck;Jonathan,
 I can't say i'm in favour of enforcing data locality.
Because  data locality in hadoop doesn't work this way… when a tasktracker through the next heartbeat announces that it has a task slot free the jobtracker will do its best to assign a task with data locality to it but failing this will assign it a random task. the number of these random tasks can be quite high, just like i mentioned above
{quote} Tasks are still being evenly distributed around the ring regardless of what the ColumnFamilySplit.locations is. {quote}

This can be almost solved by upgrading to hadoop-0.21+, using the fair scheduler and setting the property {code}<property>
        <name>mapred.fairscheduler.locality.delay</name>
        <value>360000000</value>
<property>{code}.

At the end of the day while hadoop encourages data locality it does not enforce it.
The ideal approach would be to sort all locations by proximity.
The feasible approach hopefully is still [~tjake]'s above. In addition i'd be in favour of a setting in the job's configuration as to whether a location from another datacenter can be used.

references:
 - http://www.infoq.com/articles/HadoopInputFormat
 - http://www.mentby.com/matei-zaharia/running-only-node-local-jobs.html
 - https://groups.google.com/a/cloudera.org/forum/?fromgroups#!topic/cdh-user/3ggnE5hV0PY
 - http://www.cs.berkeley.edu/~matei/papers/2010/eurosys_delay_scheduling.pdf;;;","28/May/13 00:02;jbellis;bq. The feasible approach hopefully is still T Jake Luciani's above

Okay.  Referring back to Jake's comments,

bq. The biggest problem is [avoiding endpoints in a different DC]. Maybe the way todo this is change getSplits logic to never return replicas in another DC. I think this would require adding DC info to the describe_ring call

I note that we expose node snitch location in system.peers.  So at worst we could ""join"" against that manually.;;;","28/May/13 02:41;mck;{quote}The biggest problem is [avoiding endpoints in a different DC]. Maybe the way todo this is change getSplits logic to never return replicas in another DC. I think this would require adding DC info to the describe_ring call{quote}

Tasktrackers may have access to a set of datacenters, so this DC info needs contain a list of DCs.

For example, our setup separates datacenters by physical datacenter and hadoop-usage, like:{noformat}DC1 ""Production + Hadoop""
  c*01 c*03
DC2 ""Production + Hadoop""
  c*02 c*04
DC3 ""Production""
  c*05
DC4 ""Production""
  c*06{noformat}

So here we'd pass to getSplits() a DC info like ""DC1,DC2"".
But the problem remain, given a task executing on c*01 that fails to connect to localhost, although we can now prevent a connection to DC3 or DC4, we can't favour a connection to any other split in DC1 over anything in DC2. Is this solvable? ;;;","09/Jun/14 22:19;pauloricardomg;Attached [2.0-CASSANDRA-2388.patch|https://issues.apache.org/jira/secure/attachment/12649379/2.0-CASSANDRA-2388.patch] (against 2.0), where CASSANDRA-6302 is ported to ColumnFamilyRecordReader, so the next replicas are tried when the first one fails.

Reading from a non-local-DC replica is not a problem anymore due to the introduction of describe_local_ring (CASSANDRA-6268), that limits the input splits to the local DC.

I would like to acknowledge my colleague Danilo Penna Queiroz who paired with me on this patch. ;;;","09/Jun/14 22:41;jbellis;[~pkolaczk] to review;;;","23/Jun/14 05:05;pauloricardomg;[~pkolaczk] any update on this? cheers! :);;;","23/Jun/14 15:44;pkolaczk;[~pauloricardomg] I can't promise, but I try to do that at the end of this week. ;;;","26/Jun/14 03:22;pauloricardomg;Attaching patch based on 1.2.16 and fixed patch (v2) for 2.0.

Maybe the 1.2 patch can still make it to 1.2.17...;;;","08/Aug/14 03:19;pauloricardomg;[~pkolaczk] any update on this? this review has been roaming for quite some time now...
sorry for bothering but would be nice to see this integrated. cheers!;;;","07/Oct/14 04:27;pauloricardomg;http://mail-archives.apache.org/mod_mbox/cassandra-dev/201410.mbox/%3CCALdd-zjmvp7JOtguZ_k951RQHDtFt1cthX=RnHQ332C=gAZbjw@mail.gmail.com%3E;;;","10/Oct/14 19:16;mck; [~pkolaczk] + [~pauloricardomg],  AFAIK everything thrift related is frozen, so i presume the patch isn't going to be applied to master.
Otherwise it's +1 on the patch from me.;;;","01/Dec/14 22:02;pkolaczk;+1;;;","25/Aug/15 00:18;jbellis;Paulo, can you rebase to 2.1?;;;","25/Aug/15 21:30;pauloricardomg;Rebased 2.1 patch available [here|https://github.com/pauloricardomg/cassandra/tree/2388-2.1].

2.1 tests:
* [testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2388-2.1-testall/lastCompletedBuild/testReport/]
* [dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2388-2.1-dtest/lastCompletedBuild/testReport/]

2.2 tests (don't know if {{ColumnFamilyInputFormat}} and {{ColumnFamilyRecordReader}} should be deprecated by then, but the classes are still present there):
* [testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2388-2.2-testall/lastCompletedBuild/testReport/]
* [dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2388-2.2-dtest/lastCompletedBuild/testReport/];;;","20/Nov/15 21:54;slebresne;Committed, thanks.;;;"
"Exception ""java.io.UTFDataFormatException: malformed input around byte 55"" retrieving  columns or slices",CASSANDRA-954,12461150,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,asl2,asl2,06/Apr/10 00:13,16/Apr/19 17:33,22/Mar/23 14:57,23/May/10 01:13,,,,,0,,,,,,"Using cassandra-cli, I get an exception on ""get PostingData.fields['146558416']['json']"", but ""get PostingData.fields['146558416']"" works fine and returns the expected data (a single column named 'json').  The exception happens on both nodes of a two-node cluster with RF set to 2.

This cluster was originally set up as a test and the problem data written with 0.5b2, so if there was a known bug in that release with these symptoms, please point me to it and close this.  I didn't notice it until after upgrading to 0.6rc1, but that could be coincidence.

grepping for 146558416 fields*-Index.db yields three posssible files, and only one seems to be late enough for the column timestamp: fields-3355-*.db.  That was created by a compaction (under 0.5b2), according to the log: INFO [COMPACTION-POOL:1] 2010-03-29 04:08:19,652 ColumnFamilyStore.java (line 944) Compacted to /mnt2/cassandra/data/PostingData/fields-3355-Data.db.  17698781375/14295733792 bytes for 4048148 keys.  Time: 943763ms.

I've changed my code to retrieve slices instead of specifying the column, which is an adequate workaround.

Here's the full traceback:
ERROR [pool-1-thread-29] 2010-04-05 15:43:17,450 Cassandra.java (line 1197) Internal error processing get
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.io.UTFDataFormatException: malformed input around byte 55
        at org.apache.cassandra.service.StorageProxy.weakReadLocal(StorageProxy.java:542)
        at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:406)
        at org.apache.cassandra.thrift.CassandraServer.readColumnFamily(CassandraServer.java:101)
        at org.apache.cassandra.thrift.CassandraServer.multigetInternal(CassandraServer.java:309)
        at org.apache.cassandra.thrift.CassandraServer.get(CassandraServer.java:274)
        at org.apache.cassandra.thrift.Cassandra$Processor$get.process(Cassandra.java:1187)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:1125)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.util.concurrent.ExecutionException: java.io.UTFDataFormatException: malformed input around byte 55
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
        at java.util.concurrent.FutureTask.get(FutureTask.java:111)
        at org.apache.cassandra.service.StorageProxy.weakReadLocal(StorageProxy.java:538)
        ... 10 more
Caused by: java.io.UTFDataFormatException: malformed input around byte 55
        at java.io.DataInputStream.readUTF(DataInputStream.java:656)
        at org.apache.cassandra.io.util.MappedFileDataInput.readUTF(MappedFileDataInput.java:431)
        at org.apache.cassandra.db.filter.SSTableNamesIterator.<init>(SSTableNamesIterator.java:55)
        at org.apache.cassandra.db.filter.NamesQueryFilter.getSSTableColumnIterator(NamesQueryFilter.java:69)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:830)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:750)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:719)
        at org.apache.cassandra.db.Table.getRow(Table.java:381)
        at org.apache.cassandra.db.SliceByNamesReadCommand.getRow(SliceByNamesReadCommand.java:56)
        at org.apache.cassandra.service.StorageProxy$weakReadLocalCallable.call(StorageProxy.java:763)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        ... 3 more

I'm often around on #cassandra as asl2, if anyone wants more info.",Ubuntu Jaunty on EC2 m1.large instance,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/10 05:43;asl2;sstable2json;https://issues.apache.org/jira/secure/attachment/12440952/sstable2json",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19936,,,Sat May 22 17:13:56 UTC 2010,,,,,,,,,,"0|i0g23r:",91771,,,,,Normal,,,,,,,,,,,,,,,,,"06/Apr/10 00:36;asl2;Actually I've realized that the grep hit in fields-3355-Index.db isn't on the key, so I'm not sure how to figure out which file has the problematic data.;;;","06/Apr/10 06:23;asl2;I've restarted cassandra on both nodes and can't reproduce the problem any more.;;;","06/Apr/10 06:30;jbellis;Well, that's definitely not supposed to happen.  Are you sure both were already running 0.6 before the restart?;;;","06/Apr/10 06:50;asl2;As mentioned on IRC, I'm fairly sure both were running 0.6, since the upgrade also included a change of the directory to which logs were written.  I'll reopen if it happens again.;;;","07/Apr/10 04:46;asl2;This seems to be happening again, but this time it's also affecting retrieval of single columns.  I've restarted one node in the cluster, and it worked again for ~5-10 minutes, then went back to giving the same error.;;;","07/Apr/10 04:50;jbellis;have you tried sstablexport?;;;","07/Apr/10 05:43;asl2;I just ran (for i in fields-*-Data.db; do echo $i; /mnt/cassandra/apache-cassandra-0.6.0-rc1/bin/sstable2json $i  -k 132565089 -x """"; done) and attached the result.

132565089 was the key for which we've most recently encountered the error.  The timestamp for the data in fields-3097-Data.db is both the largest and matches the timestamp for the CLI, so I've saved those files.;;;","07/Apr/10 05:52;jbellis;so sstable2json works fine, and includes the key and data that you can't slice?

does the error happen 100% of the time once it starts?

does it only happen with mmap'd i/o?  (force diskaccessmode to standard in your config to turn it off);;;","07/Apr/10 06:08;asl2;Yes, sstable2json works fine for the problematic key.   

So far, it seems to work for a while after it starts, and then switch into a mode where the error happens consistently, and not recover without a restart.

I've switched to <DiskAccessMode>standard</DiskAccessMode, and will comment if it happens again.;;;","23/May/10 01:13;jbellis;I believe this is the same as CASSANDRA-866 (fixed in 0.6.1);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Load spikes due to MessagingService-generated garbage collection,CASSANDRA-2058,12496753,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,ketralnis,ketralnis,26/Jan/11 09:20,16/Apr/19 17:33,22/Mar/23 14:57,16/Feb/11 03:48,0.6.11,0.7.1,,,0,,,,,,"(Filing as a placeholder bug as I gather information.)

At ~10p 24 Jan, I upgraded our 20-node cluster from 0.6.8->0.6.10, turned on the DES, and moved some CFs from one KS into another (drain whole cluster, take it down, move files, change schema, put it back up). Since then, I've had four storms whereby a node's load will shoot to 700+ (400% CPU on a 4-cpu machine) and become totally unresponsive. After a moment or two like that, its neighbour dies too, and the failure cascades around the ring. Unfortunately because of the high load I'm not able to get into the machine to pull a thread dump to see wtf it's doing as it happens.

I've also had an issue where a single node spikes up to high load, but recovers. This may or may not be the same issue from which the nodes don't recover as above, but both are new behaviour","OpenJDK 64-Bit Server VM (build 1.6.0_0-b12, mixed mode)
Ubuntu 8.10
Linux pmc01 2.6.27-22-xen #1 SMP Fri Feb 20 23:58:13 UTC 2009 x86_64 GNU/Linux",bcoverston,brandon.williams,cburroughs,dkuebric,jhermes,mmalone,stuhood,,,,,,,,,,,,,,,1440,1440,,0%,1440,1440,,,,,,,,,,,,,,,,,,,,"28/Jan/11 02:30;brandon.williams;2058-0.7-v2.txt;https://issues.apache.org/jira/secure/attachment/12469574/2058-0.7-v2.txt","28/Jan/11 02:40;jbellis;2058-0.7-v3.txt;https://issues.apache.org/jira/secure/attachment/12469575/2058-0.7-v3.txt","27/Jan/11 12:06;jbellis;2058-0.7.txt;https://issues.apache.org/jira/secure/attachment/12469512/2058-0.7.txt","27/Jan/11 05:49;jbellis;2058.txt;https://issues.apache.org/jira/secure/attachment/12469482/2058.txt","26/Jan/11 09:29;ketralnis;cassandra.pmc01.log.bz2;https://issues.apache.org/jira/secure/attachment/12469368/cassandra.pmc01.log.bz2","26/Jan/11 11:38;ketralnis;cassandra.pmc14.log.bz2;https://issues.apache.org/jira/secure/attachment/12469380/cassandra.pmc14.log.bz2","26/Jan/11 11:30;ketralnis;graph a.png;https://issues.apache.org/jira/secure/attachment/12469378/graph+a.png","26/Jan/11 11:30;ketralnis;graph b.png;https://issues.apache.org/jira/secure/attachment/12469379/graph+b.png",,,,,,,8.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20419,,,Tue Feb 15 19:48:56 UTC 2011,,,,,,,,,,"0|i0g92f:",92899,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"26/Jan/11 09:29;ketralnis;This is one of the affected nodes' logs from 7a..6p (uncompresses to ~33mb). Note that around 4p I added a job to pull a jstack every 120s. On this node around 5:46p I saw the version of the load spike where the node recovers (at around 17:53).;;;","26/Jan/11 09:30;ketralnis;It occurs to me that my timestamps may be in a different time zone than the logs themselves;;;","26/Jan/11 09:35;ketralnis;Also since then I've had notably worse performance, reading is maybe 30% slower than before.

My next step will be to hope that the jstacks in that log are the same as the ones causing the largest outages and to disable to dynamic snitch (as much as i'd like to get 100% reproduction, I'd also rather not take my site down) to see if that resolves the problem. If it doesn't, then I'll turn it back on and revert to 0.6.8 to see if that does it;;;","26/Jan/11 09:48;jbellis;I believe this is the same as CASSANDRA-2054 but will leave both open for now.;;;","26/Jan/11 11:30;ketralnis;Just had this happen again, attaching load/CPU graphs. Will have logs shortly.

I was in the middle of pushing out the change to turn off the DES. This is pmc14. As of when this happened, the nodes {pmc01 pmc04 pmc07 pmc10 pmc13 pmc16} had it turned off but the others have not been restarted;;;","26/Jan/11 13:10;jbellis;bq. If it doesn't, then I'll turn it back on and revert to 0.6.8 to see if that does it

You were running 0.6.8 + DS before?  Or is ""it"" not DynamicSnitch?;;;","26/Jan/11 14:48;ketralnis;bq. You were running 0.6.8 + DS before? Or is ""it"" not DynamicSnitch?

I was running 0.6.8 with no DES. Then I upgraded to 0.6.10 and turned it on. I had the aforementioned problems.

Now I'm running 0.6.10 with the DES turned off. (As of this writing, I'm still seeing the momentary spikes but thus far no sustained ones.)

If I continue to have the momentary or sustained spikes (I'll probably know by the morning), then I'll revert to 0.6.8, and turn *on* the DES.

If after that I continue to have problems I'll revert back to 0.6.8 with no DES, which is at least a configuration in which I didn't have any of these problems;;;","26/Jan/11 22:23;jbellis;DES in 0.6.8 is a no-op unless you're doing quorum reads.;;;","27/Jan/11 03:26;ketralnis;I am in fact still having both the momentary and the sustained failures and am rolling back to 0.6.8 with no DES (since you describe it as a no-op anyway);;;","27/Jan/11 05:22;ketralnis;I've rolled back to 0.6.8 with the DES disabled and not only has the load problem stopped, performance has also gone back up to previous levels;;;","27/Jan/11 05:49;jbellis;Brandon's testing has narrowed the culprit down to CASSANDRA-1959.  As discussed on CASSANDRA-2054, the main problem there is with the NonBlockingHashMap introduced to track timed out latencies.

This patch reverts that and takes a different approach, of tracking the latency in the callback map.  This means that we need a unique messageId for each target we send a message to.  The Right Way to do this would be to have Message objects only contain the data to send, not the From address and not the messageId.  Refactoring Message is outside our scope here though, so instead we create a new Message for each target.

This does let us clean up the callback map in ResponseVerbHandler instead of in each Callback.  (That is what is going on in the changes to QRH, WRH, and AR.);;;","27/Jan/11 11:49;tjake;This looks good overall, nothing major I can see.

The only niggles are:
 
1. ExpiringMap; we could do the same with MapMaker and may be more bulletproof. see EvictionListener http://guava-libraries.googlecode.com/svn/trunk/javadoc/com/google/common/collect/MapMaker.html

2. I also wonder what impact (if any) there will be for generating a message per endpoint rather than re-using the same one as was perviously done.

But as-is it's still +1;;;","27/Jan/11 11:57;jbellis;Thanks, Jake.

1. agreed, I'd like to upgrade at some point, but changing stuff i don't have to scares me at this point in 0.6.

2. we definitely saw a small speedup when i made that optimization the first time, but I'd rather have a working dynamic snitch.  (we can optimize later in 0.7 -- see The Right Way above.)  combined w/ the improved tcp performance in 0.6.10 we should still be ahead of 0.6.8 aka the last version that didn't have MessagingService bugs.;;;","27/Jan/11 12:06;jbellis;port to 0.7 attached.;;;","28/Jan/11 00:51;brandon.williams;0.6 version looks good, RR, HH, and DES work, no more CPU spikes under heavy load.;;;","28/Jan/11 00:56;jbellis;committed 0.6 version;;;","28/Jan/11 01:13;hudson;Integrated in Cassandra-0.6 #52 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/52/])
    reduce garbage generated by MessagingServiceto prevent loadspikes
patch by jbellis; reviewed by brandonwilliams and tjake for CASSANDRA-2058
;;;","28/Jan/11 02:30;brandon.williams;0.7 v2 fixes the DES by incorporating the approach from CASSANDRA-2004 and having it register with MS directly and removing ILP.  However, it does not receive timings for the local node.;;;","28/Jan/11 02:40;jbellis;v3 adds latency tracking to LocalReadRunnable;;;","28/Jan/11 06:25;brandon.williams;+1;;;","28/Jan/11 06:44;jbellis;committed to 0.7 and trunk;;;","28/Jan/11 07:48;mmalone;Jake/Jonathan,

FWIW, I re-implemented ExpiringMap with MapMaker using an eviction listener (but mostly maintaining the ExpiringMap API) a little while back while investigating some messaging service issues we were seeing. The patch is against 0.6.8, but here's the code if you wanna try it out: https://gist.github.com/a2f645c69ca8f44ccff3

It could definitely be simplified more by someone willing to make more widespread code changes. Actually, I think using MapMaker directly and getting rid of ExpiringMap would probably be best. *shrug*;;;","28/Jan/11 08:01;jbellis;bq. I think using MapMaker directly and getting rid of ExpiringMap would probably be best

Agreed, opened CASSANDRA-2070 for that;;;","01/Feb/11 05:14;ketralnis;I have upgraded to 0.6.11 and am definitely still seeing this problem (although I'm no longer seeing the 30% performance hit while the nodes are up);;;","01/Feb/11 05:51;jbellis;Please tell me you're at least seeing this less often than with .10 :);;;","01/Feb/11 08:07;ketralnis;It's hard to say. I lost 5 nodes in about an hour, but I don't know how many I lost last time;;;","01/Feb/11 23:58;pquerna;FYI, since upgrading to .10 we are also seeing this problem :(  Tried getting a jstack, but didn't work, tpstats etc all timed out.;;;","02/Feb/11 01:51;brandon.williams;Paul, I would expect to see it on .10 (I can repro there) but that is what this ticket was supposed to address.  Can you repro with .11?;;;","02/Feb/11 03:22;urandom;They're seeing it on r1064246 (one rev newer than 0.6.11).;;;","04/Feb/11 17:27;tbritz;I'm also seeing something similar on yesterday's svn version (the one with the Consistency level fix).

It only occurs if I enable JNA.

Nodes will experience enormous high kernel load (htop, red bar). Ssh sessions on these servers will lag extermely. Nodes won't take 100% cpu though, but the cluster is unusable.

(Just to note: it's a completely different pattern to the 100% cpu spike which occured before, and I can't reproduce it wihout JNA enabled)
;;;","05/Feb/11 02:02;ketralnis;I don't have JNA on these hosts, so at least in my case it's not JNA-related.;;;","05/Feb/11 04:20;brandon.williams;Could those who are seeing this issue please post the JVM flags they're using?;;;","05/Feb/11 05:08;ketralnis;JVM_OPTS="" \
        -ea \
        -Xms6656m \
        -Xmx6656m \
        -XX:+UseParNewGC \
        -XX:+UseConcMarkSweepGC \
        -XX:+CMSParallelRemarkEnabled \
        -XX:SurvivorRatio=8 \
        -XX:MaxTenuringThreshold=1 \
        -XX:CMSInitiatingOccupancyFraction=75 \
        -XX:+UseCMSInitiatingOccupancyOnly \
        -XX:+HeapDumpOnOutOfMemoryError \
        -XX:+UseThreadPriorities \
        -XX:ThreadPriorityPolicy=42 \
        -Dcassandra.compaction.priority=1 \
        -Dcom.sun.management.jmxremote.port=8080 \
        -Dcom.sun.management.jmxremote.ssl=false \
        -Dcom.sun.management.jmxremote.authenticate=false""


/usr/bin/java -ea -Xms6656m -Xmx6656m -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:SurvivorRatio=8 -XX:MaxTenuringThreshold=1 -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+UseThreadPriorities -XX:ThreadPriorityPolicy=42 -Dcassandra.compaction.priority=1 -Dcom.sun.management.jmxremote.port=8080 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Dstorage-config=bin/../conf -Dcassandra-foreground=yes -cp bin/../conf:bin/../build/classes:bin/../lib/antlr-3.1.3.jar:bin/../lib/clhm-production.jar:bin/../lib/commons-cli-1.1.jar:bin/../lib/commons-codec-1.2.jar:bin/../lib/commons-collections-3.2.1.jar:bin/../lib/commons-lang-2.4.jar:bin/../lib/google-collections-1.0.jar:bin/../lib/hadoop-core-0.20.1.jar:bin/../lib/high-scale-lib.jar:bin/../lib/jackson-core-asl-1.4.0.jar:bin/../lib/jackson-mapper-asl-1.4.0.jar:bin/../lib/jline-0.9.94.jar:bin/../lib/json-simple-1.1.jar:bin/../lib/libthrift-r917130.jar:bin/../lib/log4j-1.2.14.jar:bin/../lib/slf4j-api-1.5.8.jar:bin/../lib/slf4j-log4j12-1.5.8.jar org.apache.cassandra.thrift.CassandraDaemon


java version ""1.6.0_0""
IcedTea6 1.3.1 (6b12-0ubuntu6.7) Runtime Environment (build 1.6.0_0-b12)
OpenJDK 64-Bit Server VM (build 1.6.0_0-b12, mixed mode)


Linux pmc01 2.6.27-22-xen #1 SMP Fri Feb 20 23:58:13 UTC 2009 x86_64 GNU/Linux;;;","16/Feb/11 03:48;jbellis;closing this so it's clear that the excessive object creation problem introduced in CASSANDRA-1905 is fixed in 0.6.11 / 0.7.1.

opened CASSANDRA-2170 for other load spikes.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE During Repair In StreamReplyVerbHandler,CASSANDRA-2377,12502261,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,bcoverston,bcoverston,24/Mar/11 14:57,16/Apr/19 17:33,22/Mar/23 14:57,25/Mar/11 02:59,0.7.5,,,,0,,,,,,"ERROR [MiscStage:4] 2011-03-24 02:45:05,172 DebuggableThreadPoolExecutor.java (line 103) Error in ThreadPoolExecutorjava.lang.NullPointerException
        at org.apache.cassandra.streaming.StreamReplyVerbHandler.doVerb(StreamReplyVerbHandler.java:62)        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
ERROR [MiscStage:4] 2011-03-24 02:45:05,172 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[MiscStage:4,5,main]java.lang.NullPointerException
        at org.apache.cassandra.streaming.StreamReplyVerbHandler.doVerb(StreamReplyVerbHandler.java:62)        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)",CentOS,mishravivek,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Mar/11 02:30;brandon.williams;2377.txt;https://issues.apache.org/jira/secure/attachment/12474537/2377.txt",,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20590,,,Fri Mar 25 17:49:14 UTC 2011,,,,,,,,,,"0|i0gb13:",93217,,,,,Normal,,,,,,,,,,,,,,,,,"24/Mar/11 17:22;mishravivek;As per my analysis,
I can see that it is happening because StreamOutSession is not instantiated.
Which is done via ""create"" call in StreamOutSession. 

StreamReplyVerbHandler should create a new session, in case StreamOutSession instance is null?

Will this work as a solution?;;;","24/Mar/11 17:27;mishravivek;Can you please provide more log or details on it?;;;","25/Mar/11 02:30;brandon.williams;Patch to just log a warning when a stream command is received for an unknown session.;;;","25/Mar/11 02:36;jbellis;What could cause unexpected stream commands?  Is there a deeper bug we should be looking for?;;;","25/Mar/11 02:42;bcoverston;In the case I was looking at there were also some failures in compaction.

This was seen after running a full repair on the node.

I'm not sure that they are related:

ERROR [CompactionExecutor:1] 2011-03-24 01:56:48,515 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.IndexOutOfBoundsException
        at java.nio.Buffer.checkIndex(Unknown Source)
        at java.nio.HeapByteBuffer.getInt(Unknown Source)
        at org.apache.cassandra.db.DeletedColumn.getLocalDeletionTime(DeletedColumn.java:57)
        at org.apache.cassandra.db.ColumnFamilyStore.removeDeletedStandard(ColumnFamilyStore.java:879)
        at org.apache.cassandra.db.ColumnFamilyStore.removeDeletedColumnsOnly(ColumnFamilyStore.java:866)
        at org.apache.cassandra.db.ColumnFamilyStore.removeDeleted(ColumnFamilyStore.java:857)
        at org.apache.cassandra.io.PrecompactedRow.<init>(PrecompactedRow.java:94)
        at org.apache.cassandra.io.CompactionIterator.getCompactedRow(CompactionIterator.java:147)
        at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:108)
        at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:43)
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:73)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
        at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
        at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
        at org.apache.cassandra.db.CompactionManager.doValidationCompaction(CompactionManager.java:822)
        at org.apache.cassandra.db.CompactionManager.access$800(CompactionManager.java:56)
        at org.apache.cassandra.db.CompactionManager$6.call(CompactionManager.java:358)
        at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
        at java.util.concurrent.FutureTask.run(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
;;;","25/Mar/11 02:42;brandon.williams;In this specific case, the machine was performing a repair and then restarted.  The other nodes then sent it session close commands, so really this is just a cosmetic problem.;;;","25/Mar/11 02:46;jbellis;I think I'd rather see it at debug level if it's expected.  +1 otherwise;;;","25/Mar/11 02:59;brandon.williams;Committed with the message moved to debug.;;;","25/Mar/11 03:15;hudson;Integrated in Cassandra-0.7 #406 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/406/])
    Log a message when a streaming action for an unkown session is received
instead of NPE.
Patch by brandonwilliams, reviewed by jbellis for CASSANDRA-2377
;;;","25/Mar/11 17:55;mishravivek;I was going through the changes made. I wonder what should be the value of reply.getSessionID for:


 StreamReply reply = StreamReply.serializer.deserialize(new DataInputStream(bufIn), message.getVersion());

Should we check for this, instead of 

if (session == null)
            {
                logger.warn(""Received stream action "" + reply.action + "" for an unknown session from "" + message.getFrom());
                return;
         }
 
As per log it happens for :

 case SESSION_FINISHED:


So changes should be something like this:

if (SESSION_FINISHED.equals(reply.action.))
            {
                logger.warn(""Received stream action "" + reply.action + "" for an unknown session from "" + message.getFrom());
                return;
         }


Idea is to save any additional static call on StreamOutSession.get(message.getFrom(), reply.sessionId), which results in object instantiation for new Pair<InetAddress, Long>(host, sessionId).


;;;","26/Mar/11 01:49;brandon.williams;An unknown stream action could be received for any of them.  It's not hard to imagine a FILE_RETRY happening here, and all of them require a non-null session.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Corrupt sstables cause compaction to fail again, and again and again, ...",CASSANDRA-2084,12497252,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,dhendry,dhendry,01/Feb/11 04:08,16/Apr/19 17:33,22/Mar/23 14:57,08/Mar/11 00:56,,,,,1,,,,,,"I have been having some serious data corruption issues in my cluster. I suspect some deeper more serious Cassandra bug but I dont know what or where it is and I have not found a way to reproduce the issues I have been having. 

This ticket is for a behaviour I have observed where cassandra starts compacting a set of sstables, fails, does not clean up the tmp files, then start compacting the exact same set of sstables again. (See logs below). After awhile, the node runs out of disk space and crashes. At the very least, cassandra should clean up temp files after a failed compaction. Better yet, it should stop trying to compact that file and log what file the error occurred for. The list of corrupt sstables does not even have to be persistent, just an in memory list which gets wiped out on a restart.

Here is a sample log, the same 4 sstables are being compacted then failing then being compacted again. 

 INFO [CompactionExecutor:1] 2011-01-31 13:08:26,434 CompactionManager.java (line 272) Compacting [org.apache.cassandra.io.sstable.SSTableReader(path='/var/lib/cassandra/data/kikmetrics/DeviceEventsByDevice-e-562-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='/var/lib/cassandra/data/kikmetrics/DeviceEventsByDevice-e-692-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='/var/lib/cassandra/data/kikmetrics/DeviceEventsByDevice-e-773-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='/var/lib/cassandra/data/kikmetrics/DeviceEventsByDevice-e-940-Data.db')]
 INFO [HintedHandoff:1] 2011-01-31 13:08:28,878 HintedHandOffManager.java (line 226) Could not complete hinted handoff to /192.168.4.16
 INFO [HintedHandoff:1] 2011-01-31 13:08:28,879 ColumnFamilyStore.java (line 648) switching in a fresh Memtable for HintsColumnFamily at CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1296500864696.log', position=104140211)
 INFO [HintedHandoff:1] 2011-01-31 13:08:28,879 ColumnFamilyStore.java (line 952) Enqueuing flush of Memtable-HintsColumnFamily@1652350488(1155546 bytes, 20839 operations)
 INFO [FlushWriter:1] 2011-01-31 13:08:28,879 Memtable.java (line 155) Writing Memtable-HintsColumnFamily@1652350488(1155546 bytes, 20839 operations)
 INFO [FlushWriter:1] 2011-01-31 13:08:29,199 Memtable.java (line 162) Completed flushing /var/lib/cassandra/data/system/HintsColumnFamily-e-9-Data.db (1075487 bytes)
 INFO [GossipStage:1] 2011-01-31 13:08:45,508 Gossiper.java (line 569) InetAddress /192.168.4.16 is now UP
 INFO [COMMIT-LOG-WRITER] 2011-01-31 13:08:59,736 CommitLogSegment.java (line 50) Creating new commitlog segment /var/lib/cassandra/commitlog/CommitLog-1296500939735.log
 INFO [MutationStage:8] 2011-01-31 13:09:15,868 ColumnFamilyStore.java (line 648) switching in a fresh Memtable for UserSearch at CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1296500939735.log', position=56028937)
 INFO [MutationStage:8] 2011-01-31 13:09:15,868 ColumnFamilyStore.java (line 952) Enqueuing flush of Memtable-UserSearch@1186863256(174163962 bytes, 2097155 operations)
 INFO [FlushWriter:1] 2011-01-31 13:09:15,868 Memtable.java (line 155) Writing Memtable-UserSearch@1186863256(174163962 bytes, 2097155 operations)
ERROR [CompactionExecutor:1] 2011-01-31 13:09:22,462 AbstractCassandraDaemon.java (line 91) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.io.IOError: java.io.EOFException: attempted to skip 776104308 bytes but only skipped 8469212
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:78)
        at org.apache.cassandra.io.sstable.SSTableScanner$KeyScanningIterator.next(SSTableScanner.java:178)
        at org.apache.cassandra.io.sstable.SSTableScanner$KeyScanningIterator.next(SSTableScanner.java:143)
        at org.apache.cassandra.io.sstable.SSTableScanner.next(SSTableScanner.java:135)
        at org.apache.cassandra.io.sstable.SSTableScanner.next(SSTableScanner.java:38)
        at org.apache.commons.collections.iterators.CollatingIterator.set(CollatingIterator.java:284)
        at org.apache.commons.collections.iterators.CollatingIterator.least(CollatingIterator.java:326)
        at org.apache.commons.collections.iterators.CollatingIterator.next(CollatingIterator.java:230)
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:68)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
        at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
        at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
        at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:323)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:122)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:92)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.EOFException: attempted to skip 776104308 bytes but only skipped 8469212
        at org.apache.cassandra.io.sstable.IndexHelper.skipBloomFilter(IndexHelper.java:52)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:69)
        ... 20 more
 INFO [CompactionExecutor:1] 2011-01-31 13:09:22,463 CompactionManager.java (line 272) Compacting [org.apache.cassandra.io.sstable.SSTableReader(path='/var/lib/cassandra/data/kikmetrics/DeviceEventsByDevice-e-562-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='/var/lib/cassandra/data/kikmetrics/DeviceEventsByDevice-e-692-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='/var/lib/cassandra/data/kikmetrics/DeviceEventsByDevice-e-773-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='/var/lib/cassandra/data/kikmetrics/DeviceEventsByDevice-e-940-Data.db')]

 INFO [FlushWriter:1] 2011-01-31 13:09:29,010 Memtable.java (line 162) Completed flushing /var/lib/cassandra/data/kikmetrics/UserSearch-e-1264-Data.db (184687455 bytes)
 INFO [COMMIT-LOG-WRITER] 2011-01-31 13:09:38,221 CommitLogSegment.java (line 50) Creating new commitlog segment /var/lib/cassandra/commitlog/CommitLog-1296500978221.log
 INFO [COMMIT-LOG-WRITER] 2011-01-31 13:10:15,781 CommitLogSegment.java (line 50) Creating new commitlog segment /var/lib/cassandra/commitlog/CommitLog-1296501015781.log
ERROR [CompactionExecutor:1] 2011-01-31 13:10:29,139 AbstractCassandraDaemon.java (line 91) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.io.IOError: java.io.EOFException: attempted to skip 776104308 bytes but only skipped 8469212
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:78)
        at org.apache.cassandra.io.sstable.SSTableScanner$KeyScanningIterator.next(SSTableScanner.java:178)
        at org.apache.cassandra.io.sstable.SSTableScanner$KeyScanningIterator.next(SSTableScanner.java:143)
        at org.apache.cassandra.io.sstable.SSTableScanner.next(SSTableScanner.java:135)
        at org.apache.cassandra.io.sstable.SSTableScanner.next(SSTableScanner.java:38)
        at org.apache.commons.collections.iterators.CollatingIterator.set(CollatingIterator.java:284)
        at org.apache.commons.collections.iterators.CollatingIterator.least(CollatingIterator.java:326)
        at org.apache.commons.collections.iterators.CollatingIterator.next(CollatingIterator.java:230)
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:68)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
        at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
        at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
        at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:323)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:122)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:92)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.EOFException: attempted to skip 776104308 bytes but only skipped 8469212
        at org.apache.cassandra.io.sstable.IndexHelper.skipBloomFilter(IndexHelper.java:52)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:69)
        ... 20 more
 INFO [CompactionExecutor:1] 2011-01-31 13:10:29,148 CompactionManager.java (line 272) Compacting [org.apache.cassandra.io.sstable.SSTableReader(path='/var/lib/cassandra/data/kikmetrics/DeviceEventsByDevice-e-562-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='/var/lib/cassandra/data/kikmetrics/DeviceEventsByDevice-e-692-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='/var/lib/cassandra/data/kikmetrics/DeviceEventsByDevice-e-773-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='/var/lib/cassandra/data/kikmetrics/DeviceEventsByDevice-e-940-Data.db')]
","Ubuntu 10.10
Cassandra 0.7.0 (4 Nodes)

Java:
- java version ""1.6.0_22""
- Java(TM) SE Runtime Environment (build 1.6.0_22-b04)
- Java HotSpot(TM) 64-Bit Server VM (build 17.1-b03, mixed mode)
",cburroughs,christianmovi,dln,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20434,,,Mon Mar 07 16:56:56 UTC 2011,,,,,,,,,,"0|i0g97r:",92923,,,,,Normal,,,,,,,,,,,,,,,,,"01/Feb/11 13:53;stuhood;EDIT: Just double checked, apparently version 'f' was in the 0.7 branch, but did not make it into 0.7.0: apologies. I'll take a close look at this tomorrow.

-It looks like those SSTables were created with a pre-release version of Cassandra 0.7 (version 'e', vs the release version 'f'). Mind you, that is a usecase that we would like to support, but it's important information to include in a bug report.-

-This error occurs suspiciously close to the bloom filter reading code, which changed between e and f. I'll CC kingryan to have him take a look tomorrow.-

Keeping a list of uncompactable SSTables is an excellent idea: opened CASSANDRA-2087. Also opened CASSANDRA-2088 for the compaction cleanup problem. Thanks for the report!;;;","08/Mar/11 00:56;jbellis;closing as duplicate in favor of the more specific tickets opened by Stu;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Full disk can result in being marked down,CASSANDRA-809,12456744,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Duplicate,,kingryan,kingryan,19/Feb/10 11:19,16/Apr/19 17:33,22/Mar/23 14:57,29/May/12 04:22,,,,,0,,,,,,"We had a node file up the disk under one of two data directories. The result was that the node stopped making progress. The problem appears to be this (I'll update with more details as we find them):

When new tasks are put onto most queues in Cassandra, if there isn't a thread in the pool to handle the task immediately, the task in run in the caller's thread
(org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor:69 sets the caller-runs policy).  The queue in question here is the queue that manages flushes, which is enqueued to from various places in our code (and therefore likely from multiple threads). Assuming that the full disk meant that no threads doing flushing could make progress (it appears that way) eventually any thread that calls the flush code would become stalled.

Assuming our analysis is right (and we're still looking into it) we need to make a change. Here's a proposal so far:

SHORT TERM:
* change the  TheadPoolExecutor policy to not be caller runs. This will let other threads make progress in the event that one pool is stalled

LONG TERM
* It appears that there are n threads for n data directories that we flush to, but they're not dedicated to a data directory. We should have a thread per data directory and have that thread dedicated to that directory
* Perhaps we could use the failure detector on disks?
",,cagatayk,jeromatron,johanoskarsson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19874,,,Mon May 28 20:22:50 UTC 2012,,,,,,,,,,"0|i0g17j:",91626,,,,,Low,,,,,,,,,,,,,,,,,"19/Feb/10 12:00;jbellis;> change the TheadPoolExecutor policy to not be caller runs. This will let other threads make progress in the event that one pool is stalled 

disagree.  you can only do this by uncapping the collection, which is a cure worse than the disease.  (you go back to being able to make a node GC storm to death really really easily when you give it more data than it can flush)

> It appears that there are n threads for n data directories that we flush to, but they're not dedicated to a data directory. We should have a thread per data directory and have that thread dedicated to that directory 

yes, this would be my preferred design.  should be straightforward code to write, just hasn't been done yet.

> Perhaps we could use the failure detector on disks? 

Not sure what this looks like but I agree our story here needs a lot of improvement.

Short term my recommendation is to run w/ data files on a single raid0 unless you're sure you'll never get near the filling up point.;;;","20/Feb/10 06:29;kingryan;> > change the TheadPoolExecutor policy to not be caller runs. This will let other threads make progress in the event that one pool is stalled 
> 
> disagree. you can only do this by uncapping the collection, which is a cure worse than the disease. (you go back to being able to make a node GC storm to death really really easily when you give it more data than it can flush) 

I'll have to think more about this. I agree that we need to not let the queues grow in an unbounded way, but our current setup (basically all threads can be consumed by one queue and some of them will wait indefinitely for conditions).

We need to decide which kind of failure we want here. Our node that hit this condition is essentially dead (its not gossiping or accepting any writes or reads, but is still alive).

> > It appears that there are n threads for n data directories that we flush to, but they're not dedicated to a data directory. We should have a thread per data directory and have that thread dedicated to that directory 
> 
> yes, this would be my preferred design. should be straightforward code to write, just hasn't been done yet. 

I agree. We'll take a look at this early next week.

> > Perhaps we could use the failure detector on disks? 
> 
> Not sure what this looks like but I agree our story here needs a lot of improvement. 

I'm not entirely sure what this looks like either, but here are the properties I'd like cassandra to have:

* if a disk fills up, we stop trying to write to it
* if we're about to write more data to a disk than space available, we don't try and write to that disk
* we balance data relatively evenly between disks
* if a disk is misbehaving for a period of time, we stop using it and assume that data is lost (potentially notify an operator as well)

> Short term my recommendation is to run w/ data files on a single raid0 unless you're sure you'll never get near the filling up point.

This is probably the best advice for new clusters. Unfortunately we can't easily implement this right now.;;;","27/Dec/10 23:57;jbellis;Updating for the past 10 months' worth of changes:

bq. Our node that hit this condition is essentially dead (its not gossiping or accepting any writes or reads, but is still alive).

This is basically fixed now that flow control is implemented (CASSANDRA-685) and refined (CASSANDRA-1358).

bq. It appears that there are n threads for n data directories that we flush to, but they're not dedicated to a data directory. We should have a thread per data directory and have that thread dedicated to that directory 

At least until we cap sstable size (CASSANDRA-1608?), one data volume is going to be the recommended configuration, so this is low priority.

bq. if a disk fills up, we stop trying to write to it
bq. if we're about to write more data to a disk than space available, we don't try and write to that disk

these two Cassandra has always done on compaction.  less sure about flush.

the nice thing about writes is that erroring out is almost identical to being completely down for ConsistencyLevel purposes.

bq. we balance data relatively evenly between disks

also low priority given the above.

bq. if a disk is misbehaving for a period of time, we stop using it and assume that data is lost (potentially notify an operator as well)

this is the biggest problem right now: if a disk/volume goes down, the rest of the node (in particular gossip) will keep functioning, so other nodes will continue trying to read from it.

short term the best fix for this is to provide timeout information to the dynamic snitch (CASSANDRA-1905) so it can route around such nodes.;;;","29/May/12 04:22;jbellis;Created https://issues.apache.org/jira/browse/CASSANDRA-4292 to pursue the thread-per-disk idea.  CASSANDRA-2116 and CASSANDRA-2118 address the issue of what to do when disks error out.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Corrupted Commit logs,CASSANDRA-2128,12497895,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,pquerna,pquerna,08/Feb/11 03:41,16/Apr/19 17:33,22/Mar/23 14:57,10/Feb/11 01:07,0.6.12,0.7.2,,,0,,,,,,"Two of our nodes had a hard failure.

They both came up with a corrupted commit log.

On startup we get this:
{quote}
011-02-07_19:34:03.95124 INFO - Finished reading /var/lib/cassandra/commitlog/CommitLog-1297099954252.log
2011-02-07_19:34:03.95400 ERROR - Exception encountered during startup.
2011-02-07_19:34:03.95403 java.io.EOFException
2011-02-07_19:34:03.95403 	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:323)
2011-02-07_19:34:03.95404 	at java.io.DataInputStream.readUTF(DataInputStream.java:572)
2011-02-07_19:34:03.95405 	at java.io.DataInputStream.readUTF(DataInputStream.java:547)
2011-02-07_19:34:03.95406 	at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:363)
2011-02-07_19:34:03.95407 	at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:318)
2011-02-07_19:34:03.95408 	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:240)
2011-02-07_19:34:03.95409 	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:172)
2011-02-07_19:34:03.95409 	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:115)
2011-02-07_19:34:03.95410 	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
2011-02-07_19:34:03.95422 Exception encountered during startup.
2011-02-07_19:34:03.95436 java.io.EOFException
2011-02-07_19:34:03.95447 	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:323)
2011-02-07_19:34:03.95458 	at java.io.DataInputStream.readUTF(DataInputStream.java:572)
2011-02-07_19:34:03.95468 	at java.io.DataInputStream.readUTF(DataInputStream.java:547)
2011-02-07_19:34:03.95478 	at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:363)
2011-02-07_19:34:03.95489 	at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:318)
2011-02-07_19:34:03.95499 	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:240)
2011-02-07_19:34:03.95510 	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:172)
2011-02-07_19:34:03.95521 	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:115)
2011-02-07_19:34:03.95531 	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
{quote}

On node A, the commit log in question is 100mb.

On node B, the commit log in question is 60mb.

An ideal resolution would be if EOF is hit early, log something, but don't stop the startup.  Instead process everything that we have done so far, and keep going.
","cassandra-0.6 @ r1064246 (0.6.11)
Ubuntu 9.10
Rackspace Cloud
",cburroughs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Feb/11 00:36;jbellis;2128.txt;https://issues.apache.org/jira/secure/attachment/12470686/2128.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20456,,,Wed Feb 09 18:54:54 UTC 2011,,,,,,,,,,"0|i0g9hb:",92966,,gdusbabek,,gdusbabek,Normal,,,,,,,,,,,,,,,,,"08/Feb/11 03:43;gdusbabek;0.7 handles this gracefully by using checksums when the RowMutations are read from the CL. 0.6 could handle this better with a try{}catch{} where the RowMutation is deserialized.;;;","08/Feb/11 03:49;gdusbabek;Actually, this is kind of bizarre. 0.6 does use a checksum.  The failure is happening after the RM has been read (in full) while we are attempting to deserialize it.;;;","08/Feb/11 03:58;jbellis;0.6 uses checksums too, for this area.  (The one place it doesn't is the Header, which I posted a patch for in CASSANDRA-1815, but that doesn't seem to actually matter in practice.)

Here's the code in question:

{code}
                    long claimedCRC32;
                    byte[] bytes;
                    try
                    {
                        bytes = new byte[(int) reader.readLong()]; // readlong can throw EOFException too
                        reader.readFully(bytes);
                        claimedCRC32 = reader.readLong();
                    }
                    catch (EOFException e)
                    {
                        // last CL entry didn't get completely written.  that's ok.
                        break;
                    }

                    ByteArrayInputStream bufIn = new ByteArrayInputStream(bytes);
                    Checksum checksum = new CRC32();
                    checksum.update(bytes, 0, bytes.length);
                    if (claimedCRC32 != checksum.getValue())
                    {
                        // this part of the log must not have been fsynced.  probably the rest is bad too,
                        // but just in case there is no harm in trying them.
                        continue;
                    }

                    /* deserialize the commit log entry */
                    final RowMutation rm = RowMutation.serializer().deserialize(new DataInputStream(bufIn));
{code}

In other words, first we read the mutation into a buffer and checksum it.  If it passes, we try to deserialize that into a mutation.

It's expected that read-into-buffer can throw an EOF (which we expect and catch) but once it passes checksum it's not expected that mutation deserialize should fail.

(Yes, checksums aren't perfect, especially not 32bit ones, but for the checksum to generate a false positive on the last entry in the commitlog, on two different machines, is not a coincidence I'm buying.)

At first glance, the most likely explanation is a bug in RowMutation serializer.  But it's also possible that I'm wrong and it really is erroring out due to some unflushed data somehow.  If you turn on debug logging, it will include the offset in the commitlog of the mutation being replayed -- if it's very very close to the end of the file, then that would increase that likelihood.

bq. An ideal resolution would be if EOF is hit early, log something, but don't stop the startup. Instead process everything that we have done so far, and keep going.

Disagreed.  This is a serious enough bug that we should require human intervention before ignoring it and starting up anyway.;;;","09/Feb/11 23:07;jbellis;CASSANDRA-2055 reports the exact same stacktrace: RM.deserialize EOFing on the very first field it tries to read.  Suspicious!;;;","10/Feb/11 00:20;jbellis;Paul sent me one of the CommitLog files exhibiting this problem.  It is 99614720 bytes long.

I added one more log entry when replaying it:

{code}
                    logger.debug(""checksum of "" + bytes.length + "" successful: "" + claimedCRC32);
                    /* deserialize the commit log entry */
{code}

The last entries in the log before dying are

{noformat}
DEBUG [main] 2011-02-09 09:39:15,577 CommitLog.java (line 213) Reading mutation at 97750254
DEBUG [main] 2011-02-09 09:39:15,577 CommitLog.java (line 240) checksum of 1209 successful: 2281213824
DEBUG [main] 2011-02-09 09:39:15,578 CommitLog.java (line 244) replaying mutation for MonitorApp...
DEBUG [main] 2011-02-09 09:39:15,578 CommitLog.java (line 213) Reading mutation at 97751479
DEBUG [main] 2011-02-09 09:39:15,580 CommitLog.java (line 240) checksum of 0 successful: 0
 INFO [main] 2011-02-09 09:39:15,604 CommitLog.java (line 253) Finished reading /var/lib/cassandra/commitlog/CommitLog-1297099954252.log
ERROR [main] 2011-02-09 09:39:15,652 CassandraDaemon.java (line 242) Exception encountered during startup.
java.io.EOFException
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:323)
{noformat}

This is a good 2MB before the end of the file.  So I looked in a hex editor to see what was there, starting at offset 97751479.  There's about 8KB of pure 00 bytes, followed by nonsense that occasionally looks like a RowMutation (some appearances of column names cnt, avg, dev that are present in the early, intact part of the commitlog) but mostly does not: there are no more appearances of the keyspace name, MonitorApp, which is the first part of any real RowMutation in the 0.6 serialization format, and in one place there is even the string ""java.util.BitSet"" visible, which should only appear in the commitlog segment header at the start of the file (that we rewrite at each flush).

To me it looks like ""EC2 can write a bunch of nonsense in your commitlog during a hard failure,"" and the fix is to make the checksum process more robust against nonsense that happens to conform to the first couple bytes we read for a RowMutation.;;;","10/Feb/11 00:36;jbellis;Patch adds a check to the length of the row mutation read to make sure it is sane.  (Sane lengths will make sure the CRC can do its job.);;;","10/Feb/11 00:39;jbellis;0.7 already has a weaker fix for this (checking that serializedSize > 0, which isn't quite as strong as >= 10, but probably adequate in practice, and adding a separate checksum for the size itself.);;;","10/Feb/11 01:00;gdusbabek;+1, except don't modify the log4j config!;;;","10/Feb/11 01:07;jbellis;committed, without the log4j changes;;;","10/Feb/11 02:54;hudson;Integrated in Cassandra-0.6 #58 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/58/])
    update commitlog replay to catch bogus RowMutation lengths
patch by jbellis; reviewed by gdusbabek for CASSANDRA-2128
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition with ConcurrentLinkedHashMap,CASSANDRA-405,12434478,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,lenn0x,lenn0x,01/Sep/09 12:14,16/Apr/19 17:33,22/Mar/23 14:57,02/Sep/09 23:04,0.4,,,,0,,,,,,We are seeing a race condition with ConcurrentLinkedHashMap using appendToTail. We could remove the ConcurrentLinkedHashMap for now until that's resolved.,,ben.manes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Sep/09 22:44;jbellis;405.patch;https://issues.apache.org/jira/secure/attachment/12418250/405.patch","01/Sep/09 12:15;lenn0x;stack.log.gz;https://issues.apache.org/jira/secure/attachment/12418216/stack.log.gz",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19673,,,Tue Sep 15 20:11:54 UTC 2009,,,,,,,,,,"0|i0fyqf:",91225,,,,,Normal,,,,,,,,,,,,,,,,,"01/Sep/09 12:15;lenn0x;Our stack trace from the running node in question. ;;;","01/Sep/09 12:53;jbellis;have you checked the other nodes for stuck threads like this?;;;","01/Sep/09 12:57;sammy.yu;We did multiple stack dumps they are all the same.  There is no progression. 

In this stack trace there are two sets of stack traces
pool-1-thread-{63,62,61,59,58,54,53,51,49,47} and ROW-READ-STAGE{8,7,5,4,3,2,1}:
""ROW-READ-STAGE:8"" prio=10 tid=0x00007f1b78b52000 nid=0x1945 runnable [0x0000000046532000]
   java.lang.Thread.State: RUNNABLE
	at com.reardencommerce.kernel.collections.shared.evictable.ConcurrentLinkedHashMap$Node.appendToTail(ConcurrentLinkedHashMap.java:536)
	at com.reardencommerce.kernel.collections.shared.evictable.ConcurrentLinkedHashMap.putIfAbsent(ConcurrentLinkedHashMap.java:281)
	at com.reardencommerce.kernel.collections.shared.evictable.ConcurrentLinkedHashMap.put(ConcurrentLinkedHashMap.java:256)
	at org.apache.cassandra.io.SSTableReader.getPosition(SSTableReader.java:241)
	at org.apache.cassandra.db.filter.SSTableNamesIterator. (SSTableNamesIterator.java:46)
	at org.apache.cassandra.db.filter.NamesQueryFilter.getSSTableColumnIterator(NamesQueryFilter.java:69)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1445)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1379)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1398)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1379)
	at org.apache.cassandra.db.Table.getRow(Table.java:589)
	at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:65)
	at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:78)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:44)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)

ROW-READ-STAGE6 is a little different:
""ROW-READ-STAGE:6"" prio=10 tid=0x00007f1b78b4e000 nid=0x1943 runnable [0x0000000046330000]
   java.lang.Thread.State: RUNNABLE
	at com.reardencommerce.kernel.collections.shared.evictable.ConcurrentLinkedHashMap$Node.appendToTail(ConcurrentLinkedHashMap.java:540)
	at com.reardencommerce.kernel.collections.shared.evictable.ConcurrentLinkedHashMap.putIfAbsent(ConcurrentLinkedHashMap.java:281)
	at com.reardencommerce.kernel.collections.shared.evictable.ConcurrentLinkedHashMap.put(ConcurrentLinkedHashMap.java:256)
	at org.apache.cassandra.io.SSTableReader.getPosition(SSTableReader.java:241)
	at org.apache.cassandra.db.filter.SSTableNamesIterator. (SSTableNamesIterator.java:46)
	at org.apache.cassandra.db.filter.NamesQueryFilter.getSSTableColumnIterator(NamesQueryFilter.java:69)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1445)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1379)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1398)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1379)
	at org.apache.cassandra.db.Table.getRow(Table.java:589)
	at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:65)
	at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:78)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:44)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)

All the other pool-1-threads are in the readLock waiting state due to the writeLock
""pool-1-thread-12425"" prio=10 tid=0x00007f1b7857e000 nid=0x3fe6 waiting on condition [0x00007f1892528000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  0x00007f1b8534e848> (a java.util.concurrent.locks.ReentrantReadWriteLock$FairSync)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:747)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireShared(AbstractQueuedSynchronizer.java:877)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireShared(AbstractQueuedSynchronizer.java:1197)
	at java.util.concurrent.locks.ReentrantReadWriteLock$ReadLock.lock(ReentrantReadWriteLock.java:594)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1412)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1379)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1398)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1379)
	at org.apache.cassandra.db.Table.getRow(Table.java:589)
	at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:65)
	at org.apache.cassandra.service.StorageProxy.weakReadLocal(StorageProxy.java:609)
	at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:320)
	at org.apache.cassandra.service.CassandraServer.readColumnFamily(CassandraServer.java:92)
	at org.apache.cassandra.service.CassandraServer.getSlice(CassandraServer.java:173)
	at org.apache.cassandra.service.CassandraServer.get_slice(CassandraServer.java:213)
	at org.apache.cassandra.service.Cassandra$Processor$get_slice.process(Cassandra.java:551)
	at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:539)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:252)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619);;;","01/Sep/09 22:44;jbellis;remove LRU key cache from 0.4; the ConcurrentLinkedHashMap library is buggy (see http://code.google.com/p/concurrentlinkedhashmap/issues/detail?id=9)

I will revisit some kind of key cache early in 0.5.;;;","02/Sep/09 10:45;lenn0x;+1 This patch looks good. Let's remove from 0.4, and work on a better implementation on 0.5;;;","02/Sep/09 11:08;sammy.yu;Yes we have this running fine in production now.  Mentioned to jbellis other Concurrent LRU cache implementation:
http://svn.apache.org/viewvc/lucene/solr/trunk/src/common/org/apache/solr/common/util/ConcurrentLRUCache.java?view=log
http://svn.apache.org/viewvc/lucene/solr/trunk/src/java/org/apache/solr/search/FastLRUCache.java?view=log
that we could use in 0.5;;;","02/Sep/09 11:41;ben.manes;When performing a postmortem on this issue, please review how the ConcurrentLinkedHashMap was added.  The project page stated:

> Note: The algorithm needs further testing and is not deemed production ready. It is functional under concurrent tests, but needs additional load testing to assert correctness.

That load testing, provided in the standard unit test runs, uncovered the issue and thus it was not promoted to a release status.  I haven't had time in the last few months to work on this project, but even the last check-in notes that its leaving debug code to help resolve it later.  The project states on the front page and FAQ that the goal is more educational than formal usage, hence I avoided known algorithms (which would be the correct approach if it was work-related).

The ConcurrentLRUCache uses a watermark approach which is valid, but suffers from stampeding and is an offline algorithm.  Its still an excellent approach and one of many possibilities described in the FAQ.  I am personally a fan of soft-reference based caching for global data, which is evicted in LRU order, because it allows the GC to manage what it does best (memory!) and promotes not overburdening the application server.

Please treat this as an issue where the blame is both 3p as I did not stress heavily enough not to use this in production and internal for not evaluating a 3p project enough to recognize that it warned about its production status.  I will update the project page to better communicate and provide a performant modification that is thread-safe for those that need a solution.  Please re-evaluate your own internal processes to determine why the bad call was made.

I am not trying to shift blame, but my pet peeve is when firefighting production and no one learns because then it just happens again.  Its very frustrating, even more so if I actually work there! ;-)

Cheers!
Ben;;;","02/Sep/09 22:57;jbellis;Actually, I did read your svn logs enough to note that the bug referenced on the front page found by the ehcache people was fixed.  Too bad I missed the other, but it's not the end of the world.

In general, I suggest not getting all pissy with the people actually performing the ""additional load testing"" you claim is needed.  Early adopters are an important asset. :P

(Note that this bug is against a beta version of Cassandra, so at the lowest level the process worked: the bug was uncovered before the final release.  Although if the front page of CLRU weren't so out of date, or there had actually been an issue in the tracker for a bug known for months, the pain could have been avoided.);;;","02/Sep/09 23:04;jbellis;committed 405.patch to 0.4 and trunk;;;","03/Sep/09 01:00;ben.manes;No ""pissy""-ness, and I agree with you.  I've just been in too many firefighting sessions where the result is moving on and hitting the same problem 3 months later, and then 3 months later again.  People are usually so charged up fixing the issue that I find, perhaps incorrectly, that you have to be blunt to get their attention.  So no worries on my side. :-)

But yep, definately my fault for a good chunk of this.  Sorry for the inconvenience.

Cheers,
Ben;;;","03/Sep/09 20:52;hudson;Integrated in Cassandra #186 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/186/])
    remove buggy concurrentlinkedhashmap library and lru cache.  too late in 0.4 to try to debug the library -- will revisit for 0.5.  patch by jbellis; reviewed by Chris Goffinet for 
;;;","16/Sep/09 04:11;jbellis;CASSANDRA-423 is the cache-for-0.5 ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
make cache sizes in CfDef Strings again so we can use %s of rows in the CF as in 0.6,CASSANDRA-1313,12470053,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jhermes,jbellis,jbellis,24/Jul/10 06:21,16/Apr/19 17:33,22/Mar/23 14:57,13/Aug/10 23:56,0.7 beta 2,,,,0,,,,,,"(note, this was reverted for 0.7 in CASSANDRA-1394)",,py4fun,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Aug/10 04:56;jhermes;1313-SY.txt;https://issues.apache.org/jira/secure/attachment/12451370/1313-SY.txt","06/Aug/10 04:58;jhermes;1313-YB.txt;https://issues.apache.org/jira/secure/attachment/12451373/1313-YB.txt","06/Aug/10 04:56;jhermes;snakeyaml-percentToFloat.txt;https://issues.apache.org/jira/secure/attachment/12451371/snakeyaml-percentToFloat.txt","06/Aug/10 04:56;jhermes;snakeyaml-r1131-dev.jar;https://issues.apache.org/jira/secure/attachment/12451372/snakeyaml-r1131-dev.jar","06/Aug/10 04:47;jhermes;yamlbeans-1.04.jar;https://issues.apache.org/jira/secure/attachment/12451368/yamlbeans-1.04.jar",,,,,,,,,,5.0,jhermes,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20075,,,Sat Aug 14 12:48:35 UTC 2010,,,,,,,,,,"0|i0g4an:",92126,,,,,Low,,,,,,,,,,,,,,,,,"29/Jul/10 05:44;jhermes;Simply making \{rows,keys\}_cached strings instead of doubles is problematic. The current code looks for 'double \{rows,keys\}_cached', and changing every lookup is a non-trivial chunk of code. The lookup would also take extra cycles (or just two extra cycles if the first lookups cache the String->Double operation).

Loading a String into a Double from the YAML using snakeyaml directly has proven difficult:
* Getters and setters on the Config/RawKS/RawCF beans are never called, so there is no immediate hook.
* Telling the constructor in DD that ""50%"" should be a double does not help the constructor parse the string into a double -- snakeyaml explodes.
* Pushing the values into a String on the bean, then populating the double from the string leaves extra variables around. Also, these extra variables have to be named something different than \{rows,keys\}_cached, which means extra work in our 0.6->0.7 Converter.
* Pushing the values into a String on a FooConfig, then generating Config from FooConfig works, but means two passes on the load and hard to maintain later. 
* *Finally,* change the snakeyaml code directly. Whenever it sees a number followed by a percent, it should correctly construct a FloatType (be it Float, Double, or BigDecimal according to destination).

I decided the best fix is to effect a change in snakeyaml as outlined above (and tracked at: http://code.google.com/p/snakeyaml/issues/detail?id=75 ).

Attached are 3 things:
- The patch to snakeyaml trunk, named snakeyaml-percentToFloat.txt,
- The lib built from trunk after patch applied, named snakeyaml-r1131-dev1.jar,
- and a patch for C trunk (changes conf/cassandra.yaml and test/conf/cassandra.yaml), named 1313.txt.

With this, the values legal for \{rows,keys\}_cached is a literal (1000, 0.5, 0) or a string (""1000"", ""50%"", ""0%""). As before, a value between [0,1] is a fraction of the total, and all larger values are absolute. Because this is a change to the parser, you can also give percents anywhere else a double is expected (read_repair_chance: '72.25%' if you want).

Should this change not be accepted, one of the above methods can be employed in our code, or I can override-in all the functionality of the patch (though the code will look like a trainwreck).;;;","29/Jul/10 05:46;jhermes;Attached debugging patch to print the values of \{rows,keys\}_cached to STDOUT when the config is first loaded.
Named snakeyaml-output.patch.;;;","29/Jul/10 07:18;mdennis;+1 on the approach.  I'd like to see snakeyaml adopt this patch.;;;","29/Jul/10 10:53;jbellis;I like this, too.  Nice work.;;;","30/Jul/10 23:14;py4fun;You do not need to patch SnakeYAML to achieve the goal. SnakeYAML has the following feature: any scalar which matches a given regular expression can be constructed with some custom code.
Examples can be found in the tests.

We shall think twice before we patch SnakeYAML. Appending '%' to a number is not a common pattern. It is not part of the YAML specification. Other users may be confused when they get Float when they expect String

-
Andrey 
;;;","06/Aug/10 04:47;jhermes;Replacing snakeyaml with yamlbeans.

Status:
M       conf/cassandra.yaml
D       src/java/org/apache/cassandra/utils/SkipNullRepresenter.java
M       src/java/org/apache/cassandra/tools/NodeProbe.java
M       src/java/org/apache/cassandra/tools/SchemaTool.java
M       src/java/org/apache/cassandra/service/StorageService.java
M       src/java/org/apache/cassandra/service/StorageServiceMBean.java
M       src/java/org/apache/cassandra/config/DatabaseDescriptor.java
M       src/java/org/apache/cassandra/config/Config.java
M       src/java/org/apache/cassandra/config/Converter.java
M       src/java/org/apache/cassandra/config/RawColumnFamily.java
M       NOTICE.txt
D       lib/snakeyaml-1.6.jar
A       lib/yamlbeans-1.04.jar

Reading the config and loading keyspaces works properly.
Exporting keyspaces (using schematool) works properly given a target output file.
Converting from 0.6 to 0.7 has no diff from using snakeyaml.;;;","06/Aug/10 04:56;jhermes;Re-attaching snakeyaml approach.;;;","06/Aug/10 04:58;jhermes;And lastly fixing 1313-YB.txt to remove debug statements.;;;","06/Aug/10 21:46;py4fun;Jon, may I kindly ask you to clarify your decision ?

1)  you requested to change SnakeYAML without clear message what other users (not only Cassandra) will gain from such a change
2) I have provided you with a solution (http://code.google.com/p/snakeyaml/source/browse/src/test/java/examples/CustomImplicitResolverTest.java) which you can copy-and-paste to your code with minimum effort. You did not give any feedback back to the SnakeYAML community (http://code.google.com/p/snakeyaml/issues/detail?id=75). Why the solution did not work ?
3) using YamlBeans does not actually solve your problem. As far as I can see you introduced ""These getters/setters allow us to read X% in as a double"". The same can be done with SnakeYAML
4) Be aware that SnakeYAML implements complete 1.1 YAML specification and it has many other unique features not available in YamlBeans.


I strongly believe that communication is the best way to quickly solve the issues. SnakeYAML is open for suggestions and we are glad to help others to use the library. But we also expect that the community explains the decision and help us to improve SnakeYAML where it needs to be improved.;;;","07/Aug/10 00:24;jhermes;1) I've made it painfully clear why I offered the patch to the community.
2) I have many good solutions now. This has been a solved problem for a while. There is a reason why snakeyaml-75 is marked as an Enhancement, not a Bug.
3,4) Switching to YamlBeans was not because I couldn't solve the problem without it, but because the code is much easier to read, smaller, and removed many lines of our code.

If you still desire clarification, feel free to e-mail me. Unless there are complications on this defect, I'm considering it finished and moving on to other things. As for snakeyaml-75, I'm leaving it open to be closed as Invalid at your discretion.;;;","13/Aug/10 23:56;jbellis;committed, w/ removal of commented-out snakeyaml code and printStackTrace calls;;;","14/Aug/10 20:48;hudson;Integrated in Cassandra #514 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/514/])
    add back percentage option for cache size configuration, and replace SnakeYAML with YamlBeans.  patch by Jon Hermes; reviewed by jbellis for CASSANDRA-1313
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra Throws Exception During Batch Insert - SVN ,CASSANDRA-1303,12469729,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,jigneshdhruv,jigneshdhruv,21/Jul/10 00:44,16/Apr/19 17:33,22/Mar/23 14:57,22/Jul/10 00:04,,,,,0,,,,,,"Hello,

The latest source code in trunk throws exceptions on batch_mutate after a connection is open and few thousands records are inserted.
I am working with svn revision 965880 (latest source code as of 07/20/2010).

I am getting following exceptions randomly during batch inserts 
-----------------------------------------------------------------------------------------
ERROR 09:40:49,306 Thrift error occurred during processing of message.
org.apache.thrift.TException: Message length exceeded: 8
        at org.apache.thrift.protocol.TBinaryProtocol.checkReadLength(TBinaryProtocol.java:384)
        at org.apache.thrift.protocol.TBinaryProtocol.readBinary(TBinaryProtocol.java:361)
        at org.apache.cassandra.thrift.Column.read(Column.java:491)
        at org.apache.cassandra.thrift.SuperColumn.read(SuperColumn.java:390)
        at org.apache.cassandra.thrift.ColumnOrSuperColumn.read(ColumnOrSuperColumn.java:359)
        at org.apache.cassandra.thrift.Mutation.read(Mutation.java:346)
        at org.apache.cassandra.thrift.Cassandra$batch_mutate_args.read(Cassandra.java:16780)
        at org.apache.cassandra.thrift.Cassandra$Processor$batch_mutate.process(Cassandra.java:3041)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2531)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
-----------------------------------------------------------------------------------

Let me know if you need more information.

Thanks,
Jignesh
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jul/10 00:46;jigneshdhruv;TestSuperColumnTTL.java;https://issues.apache.org/jira/secure/attachment/12449939/TestSuperColumnTTL.java",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20070,,,Wed Jul 21 16:13:18 UTC 2010,,,,,,,,,,"0|i0g48n:",92117,,,,,Normal,,,,,,,,,,,,,,,,,"21/Jul/10 00:46;jigneshdhruv;Attached is a test java file which will demonstrate the problem. Right after inserting 127566 records an exception is being thrown on batch inserts.;;;","21/Jul/10 01:03;jbellis;This is a known problem introduced in CASSANDRA-475.  We're working on it.;;;","21/Jul/10 23:30;jigneshdhruv;Hello,

This bug was marked duplicate of CASSANDRA-475. But that bug has now being marked resolved and I am still having trouble with batch inserts.

I am using the latest source code from trunk. SVN 961952

After few hundred thousand inserts cassandra crashes and is throwing 2 different types of exceptions:
The first one being:
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
	at org.apache.thrift.transport.TFramedTransport.readFrame(TFramedTransport.java:129)
	at org.apache.thrift.transport.TFramedTransport.read(TFramedTransport.java:101)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:369)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:295)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:202)
	at org.apache.cassandra.thrift.Cassandra$Client.recv_batch_mutate(Cassandra.java:960)
	at org.apache.cassandra.thrift.Cassandra$Client.batch_mutate(Cassandra.java:944)
	at com.cbsi.pi.rtss.data.cassandra.CassandraDataManager.insert(CassandraDataManager.java:107)
	at com.cbsi.pi.rtss.service.bulk.BulkThread.run(BulkThread.java:59)
	at java.lang.Thread.run(Unknown Source)

and the second one being:
org.apache.thrift.transport.TTransportException: java.net.SocketException: Software caused connection abort: socket write error
	at org.apache.thrift.transport.TIOStreamTransport.write(TIOStreamTransport.java:147)
	at org.apache.thrift.transport.TFramedTransport.flush(TFramedTransport.java:156)
	at org.apache.cassandra.thrift.Cassandra$Client.send_set_keyspace(Cassandra.java:441)
	at org.apache.cassandra.thrift.Cassandra$Client.set_keyspace(Cassandra.java:430)
	at com.cbsi.pi.rtss.data.cassandra.CassandraDataManager.insert(CassandraDataManager.java:106)
	at com.cbsi.pi.rtss.service.bulk.BulkThread.run(BulkThread.java:59)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.net.SocketException: Software caused connection abort: socket write error
	at java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.net.SocketOutputStream.socketWrite(Unknown Source)
	at java.net.SocketOutputStream.write(Unknown Source)
	at java.io.BufferedOutputStream.flushBuffer(Unknown Source)
	at java.io.BufferedOutputStream.write(Unknown Source)
	at org.apache.thrift.transport.TIOStreamTransport.write(TIOStreamTransport.java:145)
	... 6 more

I was not getting this error yesterday but this morning when I updated my svn trunk I got update for following 2 files: 
U    src/java/org/apache/cassandra/thrift/CustomTThreadPoolServer.java
U    src/java/org/apache/cassandra/scheduler/RoundRobinScheduler.java

and that is causing the problem.;;;","21/Jul/10 23:42;jigneshdhruv;One more thing.

Before I did a SVN update this morning, I checkout thrift and applied the patch suggested by Nate in bug THRIFT-820 https://issues.apache.org/jira/browse/THRIFT-820. When I rebuild thrift jar file and used it, I had no crashes or exceptions yesterday.

But today with/without thrift patch, I am getting exceptions mentioned above.

Jignesh;;;","22/Jul/10 00:04;jbellis;Please keep comments in the relevant issue (CASSANDRA-475) instead of spreading them across multiple tickets.;;;","22/Jul/10 00:13;jigneshdhruv;OK. I will reopen ticket CASSANDRA-475 with the exception that I am getting.

Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Bootstrap breaks data stored (missing rows, extra rows, column values modified)",CASSANDRA-1992,12495685,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,matkor,matkor,16/Jan/11 05:47,16/Apr/19 17:33,22/Mar/23 14:57,20/Jan/11 03:32,0.7.1,,,,0,,,,,,"Scenario:
Two fresh (empty /data /commitog /saved_caches dirs) cassandra installs.
Start first one.
Run data inserting program [1],  run again in verify mode - all data intact.
Bootstrap 2nd node.
Run verification again, now it fails.

Issue is very strange to me as cassandra works perfectly for me when cluster nodes stay the same for days now but any bootstrap ( 1 -> 2 nodes, 2 -> 3 nodes, 2->3 nodes RF=2) breaks data.

I am running cassandra with 1GB heap size, 32bit userland on 64bit kernels, not sure what else could matter there.
Any hints ?
Thanks in advance, regards.

[1] simple program generating data and later verifying data.
http://beauty.ant.gliwice.pl/bugs/cassandra-bootstrap/test.py

[2] Logs from 1st node:
http://beauty.ant.gliwice.pl/bugs/cassandra-bootstrap/system-3.4.log

[3] Logs from 2nd (bootstraping node)
http://beauty.ant.gliwice.pl/bugs/cassandra-bootstrap/system-3.8.log

","Linux 2.6.36-1 #1 SMP Tue Nov 9 09:56:02 CET 2010 x86_64 Intel(R)_Core(TM)2_Quad_CPU____Q8300__@_2.50GHz PLD Linux
glibc-2.12-4.i686
java-sun-1.6.0.22-1.i686
",ivol,jdamick,johanoskarsson,marrs,matkor,,,,,,,,,,,,,,,,";19/Jan/11 16:34;brandon.williams;3600",28800,0,3600,12%,28800,0,3600,,,,,,,,,,,,,,,,,,,"19/Jan/11 16:21;brandon.williams;1992.txt;https://issues.apache.org/jira/secure/attachment/12468740/1992.txt",,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20391,,,Tue Feb 01 13:41:15 UTC 2011,,,,,,,,,,"0|i0g8nj:",92832,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"16/Jan/11 06:27;matkor;Not sure if it is important but:

After loading data ring looks like:
Address         Status State   Load            Owns    Token
192.168.3.4     Up     Normal  59.02 KB        100.00% 0

After bootstraping:
192.168.3.4     Up     Normal  199.28 KB       56.45%  0
192.168.3.8     Up     Normal  115.03 KB       43.55%  74091174110465149971373554442555361956

Load gets tripled on 1st node.;;;","16/Jan/11 11:21;stuhood;Are both nodes reporting the same ring (via nodetool ring) after the bootstrap? The last entry in 3.8.log indicates that it thinks 3.4 is dead, but this might just be because you stopped the nodes before collecting the logs.

Also, what exactly is the error you get from your script?;;;","16/Jan/11 18:51;matkor;Stu,
Yes both nodes show same via nodetool ring: 
192.168.3.4 Up Normal 199.28 KB 56.45% 0
192.168.3.8 Up Normal 115.03 KB 43.55% 74091174110465149971373554442555361956

You are right, I stopped cluster by stopping 3.4 first. (Anyway it would be good to store information of gracefull shutdown of node in system.log, IMHO) 

Error is:
[matkor@laptop-hp ~/src/caswife]$ python test.py
column_family: 'CF0'
row_key: 'row=0 '
row_key: 'row=1 \x00'
row_key: 'row=2 \x00\x01'
row_key: 'row=3 \x00\x01\x02'
row_key: 'row=4 \x00\x01\x02\x03'
row_key: 'row=5 \x00\x01\x02\x03\x04'
row_key: 'row=6 \x00\x01\x02\x03\x04\x05'
Traceback (most recent call last):
  File ""test.py"", line 36, in <module>
    loaded_cols_dict = current_cf.get(row_key)
  File ""/usr/share/python2.7/site-packages/pycassa/columnfamily.py"", line 362, in new_f
  File ""/usr/share/python2.7/site-packages/pycassa/columnfamily.py"", line 429, in get
pycassa.cassandra.ttypes.NotFoundException: NotFoundException()

So program found 7 rows but failed to find 8th as from my understanding of pycassa.;;;","16/Jan/11 19:31;matkor;One more thing:
I repeated setup today, also reaching missing row(s) but 2nd node got different token 61078635599166706937511052402724559481
Following
http://wiki.apache.org/cassandra/Operations#Load_balancing ,
having 1st node token set to 0 and using RandomPartitioner, I would expect  2nd node toke to be set always in middle of token space to
850705917302346158658436518579420528
?

;;;","16/Jan/11 20:23;matkor;Another thing: 
By luck, I discovered that restarting one or both nodes makes data to be served again intact.
To avoid picking token 2nd node  randomness I set it explicitly to 85070591730234615865843651857942052864
.
So scenario is now:
Clean pycassa installs with tokens set to 0 and 85070591730234615865843651857942052864.

Starting first node [1], uploading data, verifing ok, ring:
192.168.3.4     Up     Normal  59.02 KB        100.00% 0

Bootstraping 2nd node, waiting to finish streaming, data verification bad:
column_family: 'CF0'
row_key: 'row=0 '
Traceback (most recent call last):
  File ""test.py"", line 36, in <module>
    loaded_cols_dict = current_cf.get(row_key)
  File ""/usr/share/python2.7/site-packages/pycassa/columnfamily.py"", line 362, in new_f
  File ""/usr/share/python2.7/site-packages/pycassa/columnfamily.py"", line 429, in get
pycassa.cassandra.ttypes.NotFoundException: NotFoundException()
Final ring (same on both nodes):
192.168.3.4     Up     Normal  199.28 KB       50.00%  0
192.168.3.8     Up     Normal  135.7 KB        50.00%  85070591730234615865843651857942052864

Restaring 192.168.3.4, data _same_ _error_ as above, ring changes to:
192.168.3.4     Up     Normal  201.51 KB       50.00%  0
192.168.3.8     Up     Normal  135.7 KB        50.00%  85070591730234615865843651857942052864

Restarting 192.168.3.8, data _verified_ _ok_ , ring changes on (same on both nodes) to:
192.168.3.4     Up     Normal  201.51 KB       50.00%  0
192.168.3.8     Up     Normal  145.8 KB        50.00%  85070591730234615865843651857942052864

[1] Logs from 1st 192.168.3.4 node:
http://beauty.ant.gliwice.pl/bugs/cassandra-bootstrap/logs_with_restart/system-3.4.log

[2] Logs from 2nd 192.168.3.8 node:
http://beauty.ant.gliwice.pl/bugs/cassandra-bootstrap/logs_with_restart/system-3.8.log

HIH, regards;;;","17/Jan/11 02:25;brandon.williams;Since the issue appears to be a missing row, can you reproduce with contrib/py_stress?;;;","17/Jan/11 04:44;matkor;Brandon, yes and no ;).
Unable with original contrib/py_stress as it uses only one CF to do all tests. Most of my issues looks like missing row in 2nd CF or broken data in 2nd CF (like contents of 1st CF injected into 2nd CF).
I slightly modified contrib/py_stress so it created 3 standard CFs and 3 super CFs [1] and allows to select one wants to operate via --column_family_idx= switch and I can reproduce:

Starting 1st node.

$ python stress.py --nodes 192.168.3.8 --operation insert  --num-keys 100 --progress-interval 5  --keep-going --column_family_idx=1       
Created keyspaces.  Sleeping 1s for propagation.
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
100,20,20,0.00823852300644,0
$ python stress.py --nodes 192.168.3.8 --operation insert  --num-keys 100 --progress-interval 5  --keep-going --column_family_idx=2 
Keyspace already exists.
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
100,20,20,0.00132475852966,0
$ python stress.py --nodes 192.168.3.8 --operation insert  --num-keys 100 --progress-interval 5  --keep-going --column_family_idx=3
Keyspace already exists.
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
100,20,20,0.00138550519943,0

Verification of data in each CF:
$ python stress.py --nodes 192.168.3.8 --operation read  --num-keys 100 --progress-interval 5  --keep-going --column_family_idx=3
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
100,20,20,0.00282711744308,0
$ python stress.py --nodes 192.168.3.8 --operation read  --num-keys 100 --progress-interval 5  --keep-going --column_family_idx=2
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
100,20,20,0.00149053096771,0
$ python stress.py --nodes 192.168.3.8 --operation read  --num-keys 100 --progress-interval 5  --keep-going --column_family_idx=1
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
100,20,20,0.00125009775162,0

Bootstrap 2nd node and now failure:
$ python stress.py --nodes 192.168.3.8 --operation read  --num-keys 100 --progress-interval 5  --keep-going --column_family_idx=1
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
100,20,20,0.00376108169556,0
$ python stress.py --nodes 192.168.3.8 --operation read  --num-keys 100 --progress-interval 5  --keep-going --column_family_idx=2
Key 074 not found
Key 061 not found
Key 047 not found
( cut 40 more Key 0xx not found)
Key 047 not found
Key 042 not found
Key 058 not found
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
Key 033 not found
100,20,20,0.00241538286209,0

Similar failure for 3rd CF.

[1]: Modified stress.py from 0.7.0 with --column_family_idx= added.
http://beauty.ant.gliwice.pl/bugs/cassandra-bootstrap/stress.py
;;;","17/Jan/11 04:51;matkor;And again, restarting first node, cuts number of missing row by more or less half, restarting 2nd node cures all missing rows.;;;","17/Jan/11 17:33;ivol;I have the exact same problem with an existing installation and was preparing to create an issue for it, but found this issue just before creating it. I'll describe the issue I have, maybe that provides some relevant information.

I ran into this issue with Cassandra 0.7 trying to add just one node to an existing one-node cluster. The existing node contains already some data when the second node is added to the cluster. This is what I did:

Setup
I have two nodes both running on Linux; a server called 'veers' on 172.16.2.203 and a 'r2d2' on 172.16.2.206. I use Cassandra 0.7 and only change the following settings in the cassandra.yaml and log4j-server.properties (I use the default values for all other entries):

In cassandra.yaml:

initial_token: 0
data_file_directories: /vol/users/ivol/cassandra_work/data
commitlog_directory: /vol/users/ivol/cassandra_work/commitlog
saved_caches_directory: /vol/users/ivol/cassandra_work/saved_caches
seeds: 172.16.2.203
listen_address: 172.16.2.203
rpc_address: 172.16.2.203

In log4j-server.properties:

log4j.appender.R.File=/vol/users/ivol/cassandra_work/system.log


Now I start the first node and connect it using cassandra-cli. I add the following keyspace, column families and rows:

create keyspace Default;
use Default;

create column family Role;
set Role['user_1']['name'] = 'User 1';
set Role['user_2']['name'] = 'User 2';
set Role['user_3']['name'] = 'User 3';

create column family Gadget;
set Gadget['gadget_1']['name'] = 'Gadget 1';
set Gadget['gadget_2']['name'] = 'Gadget 2';
set Gadget['gadget_3']['name'] = 'Gadget 3';

After this 'list Role' and 'list Gadget' return the proper rows.

Now I append a second node to the cluster, with this configuration:

In cassandra.yaml:

initial_token:
auto_bootstrap: true
data_file_directories: /vol/users/ivol/cassandra_work/data
commitlog_directory: /vol/users/ivol/cassandra_work/commitlog
saved_caches_directory: /vol/users/ivol/cassandra_work/saved_caches
seeds: 172.16.2.203
listen_address: 172.16.2.206
rpc_address: 172.16.2.206

In log4j-server.properties:

log4j.appender.R.File=/vol/users/ivol/cassandra_work/system.log


Now I start the second node. Bootstrapping takes some time, about 2 minutes in total but finishes without any warnings or errors:

...
INFO [main] 2011-01-17 09:58:09,170 StorageService.java (line 399) Joining: getting load information
INFO [main] 2011-01-17 09:58:09,171 StorageLoadBalancer.java (line 366) Sleeping 90000 ms to wait for load information...
INFO [GossipStage:1] 2011-01-17 09:58:10,447 Gossiper.java (line 577) Node /172.16.2.203 is now part of the cluster
INFO [HintedHandoff:1] 2011-01-17 09:58:11,451 HintedHandOffManager.java (line 192) Started hinted handoff for endpoint /172.16.2.203
INFO [GossipStage:1] 2011-01-17 09:58:11,451 Gossiper.java (line 569) InetAddress /172.16.2.203 is now UP
INFO [HintedHandoff:1] 2011-01-17 09:58:11,453 HintedHandOffManager.java (line 248) Finished hinted handoff of 0 rows to endpoint /172.16.2.203
INFO [main] 2011-01-17 09:59:39,189 StorageService.java (line 399) Joining: getting bootstrap token
INFO [main] 2011-01-17 09:59:39,203 BootStrapper.java (line 148) New token will be 110533280274756817580689726417060138498 to assume load from /172.16.2.203
INFO [main] 2011-01-17 09:59:39,265 StorageService.java (line 399) Joining: sleeping 30000 ms for pending range setup
INFO [main] 2011-01-17 10:00:09,272 StorageService.java (line 399) Bootstrapping
INFO [main] 2011-01-17 10:00:09,663 CassandraDaemon.java (line 77) Binding thrift service to /172.16.2.206:9160
INFO [main] 2011-01-17 10:00:09,666 CassandraDaemon.java (line 91) Using TFramedTransport with a max frame size of 15728640 bytes.
INFO [main] 2011-01-17 10:00:09,671 CassandraDaemon.java (line 119) Listening for thrift clients...

Although everything seemed to worked just fine, when node 2 is completely finished bootstrapping the rows in the 'Role' and 'Gadget' Column Families are messed up;

list Role;

-------------------
RowKey: user_3
=> (column=6e616d65, value=557365722033, timestamp=1295254678545000)

1 Row Returned.


list Gadget;

-------------------
RowKey: user_2
=> (column=6e616d65, value=557365722032, timestamp=1295254678514000)
-------------------
RowKey: gadget_2
=> (column=6e616d65, value=4761646765742032, timestamp=1295254678805000)
-------------------
RowKey: gadget_3
=> (column=6e616d65, value=4761646765742033, timestamp=1295254679429000)
-------------------
RowKey: gadget_1
=> (column=6e616d65, value=4761646765742031, timestamp=1295254678771000)
-------------------
RowKey: user_1
=> (column=6e616d65, value=557365722031, timestamp=1295254678449000)

5 Rows Returned.

So 2 rows have been moved from CF 'Role' to 'Gadget', just by adding a node to the cluster. The actual result differs each time I try, but always some rows have been moved to some other CF. The problem seems the same as the one described by Mateusz.

I also found out that restarting the nodes seems to 'fix' the issue. Also changing the replication factor from 1 to 2 most of the times 'resolves' the issue.;;;","17/Jan/11 23:57;jbellis;if restarting nodes fixes it, it sounds like the streamed data is not getting wired in correctly to the sstabletracker;;;","19/Jan/11 16:21;brandon.williams;There were two bugs here in StreamInSession.  First, it was adding all streamed sstables to the last CFS it saw.  Secondly, secondary index generation was being performed against all sstables seen.  This patch switches from a scalar CFS and a separate list of all sstables to a hash of lists where the CFS is the key and the value is the sstables that belong to it.;;;","19/Jan/11 22:29;jbellis;I think we need to get rid of StreamInSession.table and StreamHeader.table too, then.  (But leave it on the wire protocol as an empty string, for compatibility w/ 0.7.0);;;","20/Jan/11 03:24;nickmbailey;This would be a good test to have in the distributed test set up.;;;","20/Jan/11 03:32;jbellis;Brandon said on IRC: ""maybe the table is there for some kind of reason, and the sessions are separated by keyspace.""

This is correct, so I'm going to leave the table field even though it's currently not referenced.  So it's not dead weight. I added an assert that each sstable we're adding at the end of the session belongs to the table that the sender said it was for.

Also added an assert to CFS.addSSTable to verify that the SSTR being added does belong to the CF it is being added to.

Committed w/ those two additions.;;;","20/Jan/11 03:52;hudson;Integrated in Cassandra-0.7 #177 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/177/])
    fix streaming of multiple CFs during bootstrap
patch by brandonwilliams; reviewed by jbellis for CASSANDRA-1992
;;;","01/Feb/11 21:41;jdamick;is there is any way to repair the problem without deleting all of my data? (shutting down and bringing back up did not solve the problem);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.lang.RuntimeException: java.lang.NegativeArraySizeException,CASSANDRA-2195,12499050,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Duplicate,,thegroove,thegroove,18/Feb/11 22:55,16/Apr/19 17:33,22/Mar/23 14:57,23/Feb/11 02:29,,,,,0,,,,,,"When putting my 0.7.2 node under load, I get a large amount of these: 

ERROR 15:33:25,075 Fatal exception in thread Thread[MutationStage:290,5,main]
java.lang.RuntimeException: java.lang.NegativeArraySizeException
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NegativeArraySizeException
        at org.apache.cassandra.utils.BloomFilterSerializer.deserialize(BloomFilterSerializer.java:49)
        at org.apache.cassandra.utils.BloomFilterSerializer.deserialize(BloomFilterSerializer.java:30)
        at org.apache.cassandra.io.sstable.IndexHelper.defreezeBloomFilter(IndexHelper.java:108)
        at org.apache.cassandra.db.columniterator.SSTableNamesIterator.read(SSTableNamesIterator.java:106)
        at org.apache.cassandra.db.columniterator.SSTableNamesIterator.<init>(SSTableNamesIterator.java:71)
        at org.apache.cassandra.db.filter.NamesQueryFilter.getSSTableColumnIterator(NamesQueryFilter.java:59)
        at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:80)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1275)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1167)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1095)
        at org.apache.cassandra.db.Table.readCurrentIndexedColumns(Table.java:510)
        at org.apache.cassandra.db.Table.apply(Table.java:445)
        at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:190)
        at org.apache.cassandra.service.StorageProxy$2.runMayThrow(StorageProxy.java:283)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more

On recommendation of driftx I forced a compaction, which finished. After a restart, the -Compacted files where removed and the node seemed to start up, querying some random rows seemed to go alright but after a few minutes I started getting the above messages again. I'm grabbing single rows, not slices.","Debian Lenny, Pelops-based servlet doing lots of List<Column> columns = selector.getColumnsFromRow(columnFamily, key, false, ConsistencyLevel.ONE); and mutator.writeColumns(columnFamily, key, mutator.newColumnList(...); mutator.execute(ConsistencyLevel.ANY); operations.",brandon.williams,cburroughs,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20495,,,Tue Feb 22 18:29:04 UTC 2011,,,,,,,,,,"0|i0g9wf:",93034,,,,,Critical,,,,,,,,,,,,,,,,,"19/Feb/11 00:43;jbellis;I take it ""after a few minutes"" includes additional writes?

How big is your cluster?  Can you reproduce on a single-node cluster?

How can we narrow this down?  Can you reproduce with appropriate settings of contrib/stress, for instance?  If not can you give us a stripped-down sample of your application that can reproduce the problem?;;;","19/Feb/11 00:55;jbellis;also, to verify that there are no problems with the row data itself, can you run sstable2json on your sstable files?  (i suspect there is not since compaction worked, but just to rule it out...)  sstable2json will throw very noisy errors and abort if it runs into a problem, so if it completes with the most recent output being json-looking, then it's clean.;;;","19/Feb/11 01:26;thegroove;Thanks for your reply, Jonathan. This is a single node cluster (recently upgraded from 0.6). I have a dev version of our web app, when I test locally with 
a small number of requests, everything is ok, but if I move to our live app, it starts throwing exceptions fairly quickly. Also, no preceeding errors, just that exception for at least a large number of mutations). What our app does, is try to get a row, if it doesn't exist it will write a row after that (we're using C for simple caching), about 30 of these per page load and probably a few dozen loads per second, so a few hundred reads and writes under load. I will try your suggestions on monday, also I'll test with just reads and just writes to see if I can narrow it down to one of those as opposed to a combination. I'll post my findings on monday, as well as relevant parts from the app (the environment field of this post has most of it, though, it's really quite simple: simple insert, key / value and simple read, using the Pelops library), since our shop is closed in the weekend.;;;","19/Feb/11 01:55;jbellis;Another thing to try: turn off mmap'd I/O (set disk_access_mode: standard in cassandra.yaml) to see if it's another bug in the ByteBuffer layer like CASSANDRA-2165.;;;","22/Feb/11 18:10;thegroove;Ok here's an update: I tried to sstable2json a few of my .db files, and this is what happens: sstable2json creates file handles until it eventually exits with 
Exception in thread ""main"" java.io.IOError: java.io.FileNotFoundException: /var/lib/cassandra/data/<ks>/Search-1265-Index.db (Too many open files)
        at org.apache.cassandra.io.util.BufferedSegmentedFile.getSegment(BufferedSegmentedFile.java:68)
        at org.apache.cassandra.io.util.SegmentedFile$SegmentIterator.next(SegmentedFile.java:130)
        at org.apache.cassandra.io.util.SegmentedFile$SegmentIterator.next(SegmentedFile.java:109)
        at org.apache.cassandra.io.sstable.SSTableReader.getPosition(SSTableReader.java:472)
        at org.apache.cassandra.io.sstable.SSTableReader.getFileDataInput(SSTableReader.java:563)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:49)
        at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:68)
        at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:80)
        at org.apache.cassandra.tools.SSTableExport.serializeRow(SSTableExport.java:175)
        at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:353)
        at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:375)
        at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:388)
        at org.apache.cassandra.tools.SSTableExport.main(SSTableExport.java:446)
Caused by: java.io.FileNotFoundException: /var/lib/cassandra/data/<ks>/Search-1265-Index.db (Too many open files)
        at java.io.RandomAccessFile.open(Native Method)
        at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:116)
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:111)
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:96)
        at org.apache.cassandra.io.util.BufferedSegmentedFile.getSegment(BufferedSegmentedFile.java:62)
        ... 12 more
Checked with lsof -n, doing a line count shows it gets near 65535 handles before it gives up, which is the limit shown by ulimit -H -a for open files). Sometimes it manages to write some actual json (in some cases 40-50MB), which at first glance looks ok (though I did notice the keys are encoded in hex, whereas they used to be plain text -- dehexing them does show expected values, though). I should also note that I'm successfully running sstable2json on an older version of the dataset, taken when it was still running on 0.6.

I think I failed to mention this dataset used to be owned by a 0.6.x instance, I moved over the files to a differen server, converted the original config file and made 0.7 load it. I also updated the CF by adding an UTF8Type extra column with index_type: KEYS. Also, in addition to Search-xxx-* I now have a number of Search-f-xxxx-* and Search.64617465-f-xx.* which I didn't use to have, is this ok?

Something definitely seems to be up with my sstables. Since this is a test node, I can afford to lose this dataset, but of course I'd like to find out what went wrong so it doesn't happen again (to me or others), so I hope there's anything you can extract from this information that is helpful.
;;;","22/Feb/11 20:01;thegroove;I should note that disk access mode was set to standard already.;;;","22/Feb/11 20:30;slebresne;HB, since it is a test node, do you mind trying the patch attached to CASSANDRA-2216, force a compaction again and check if you can reproduce.

As for your json2sstable problem, this is just due to too many open files. Not sure it is justified that it opens so many files, many json2sstable leaks file descriptor, but ni any case this is not related to a potential corruption problem (and if needed you can probably have it works by increasing the allowing number of open file using ulimit). But right now, my money is on CASSANDRA-2216.;;;","23/Feb/11 00:16;thegroove;I'm sorry to have to say that I had to wipe the node and recommission it, as my backup node got stuck in a major compaction with no space left to finish it. This means the dataset that was causing the pain is gone. On the bright side, I've been running it under load for a while now and it seems to be doing alright, which at least means my code is working reasonably (though with the small amount of keys involved since it's an empty node, it's obviously not a very reliable test).;;;","23/Feb/11 02:29;jbellis;Looks like 2216 is the culprit all right.  The fix there is committed and CASSANDRA-2217 is open to provide a rebuild-sstables-with-current-version-bloom-filters tool.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"changing row cache save interval is reflected in 'describe keyspace' on node it was submitted to, but not nodes it was propagated to",CASSANDRA-1853,12493092,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,gdusbabek,scode,scode,14/Dec/10 01:19,16/Apr/19 17:33,22/Mar/23 14:57,22/Dec/10 06:31,0.7.0 rc 3,,,,0,,,,,,"This is minor unless it indicates a bigger issue. On our test cluster (running cassandra 0.7 branch from today) we noticed that on submission of a new row cache save period, such as:

   update column family KeyValue with row_cache_save_period=3600;

The change would be reflected in describe_keyspace() (describe keyspace ... in cassandra-cli) on the node to which the schema migration was submitted, but not on other nodes in the cluster.

This in spite of the schema migration having propagated, judging by Schema['Last Migration'] being identical on all nodes. It is not n and of itself is not a big problem, but it does give the impression that the migrations have trouble propagating throughout the cluster even though they do.

(I had a quick (only) look in the code paths of migration application and did not find any obvious special casing of the node that happens to be local. Filing bug instead, hoping someone knows off hand what the reason is.)


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Dec/10 01:47;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-apply-CF-metadata-updates-on-replicas.txt;https://issues.apache.org/jira/secure/attachment/12466475/ASF.LICENSE.NOT.GRANTED--v1-0001-apply-CF-metadata-updates-on-replicas.txt","20/Dec/10 22:26;gdusbabek;ASF.LICENSE.NOT.GRANTED--v2-0001-apply-CF-metadata-updates-only-at-apply-time.txt;https://issues.apache.org/jira/secure/attachment/12466635/ASF.LICENSE.NOT.GRANTED--v2-0001-apply-CF-metadata-updates-only-at-apply-time.txt","21/Dec/10 20:15;gdusbabek;ASF.LICENSE.NOT.GRANTED--v3-0001-apply-CF-metadata-updates-only-at-apply-time.txt;https://issues.apache.org/jira/secure/attachment/12466704/ASF.LICENSE.NOT.GRANTED--v3-0001-apply-CF-metadata-updates-only-at-apply-time.txt","22/Dec/10 01:37;gdusbabek;ASF.LICENSE.NOT.GRANTED--v4-0001-apply-CF-metadata-updates-only-at-apply-time.txt;https://issues.apache.org/jira/secure/attachment/12466742/ASF.LICENSE.NOT.GRANTED--v4-0001-apply-CF-metadata-updates-only-at-apply-time.txt",,,,,,,,,,,4.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,682,,,Tue Dec 21 23:14:44 UTC 2010,,,,,,,,,,"0|i0g7tb:",92696,,,,,Low,,,,,,,,,,,,,,,,,"14/Dec/10 01:30;scode;Forgot to say that restarting the nodes in question (that do not show the updated value) causes it to catch up in describe keyspace output.;;;","18/Dec/10 03:49;jbellis;shouldn't applyModels be the One True Place to call CFMetaData.apply?  I'd like to r/m the apply calls in the constructor and in CassandraServer to avoid confusion.;;;","21/Dec/10 04:58;jbellis;- still have the apply in CassandraServer, is that redundant?
- iirc (and maybe i do not) the original explanation for this patch was that the other nodes don't go through the constructor that calls apply.  how else are they getting a UCF object?  intellij says the no-op constructor is unused.  i feel like i'm missing something important here.
- brace placement
;;;","21/Dec/10 20:15;gdusbabek;bq. still have the apply in CassandraServer
That apply() is on the Mutation, not a CFMetaData.

bq. how else are they getting a UCF object?
They deserialize the one they are sent by inflate()ing it.  It uses the no-arg constructor.

fixed the braces in v3.;;;","21/Dec/10 22:34;jbellis;bq. That apply() is on the Mutation, not a CFMetaData.

I'm talking about this one:
{code}
        CFMetaData oldCfm = DatabaseDescriptor.getCFMetaData(...);
...
            oldCfm.apply(cf_def);
{code}

bq. They deserialize the one they are sent by inflate()ing it. It uses the no-arg constructor.

inflate calls the many-arg constructor.  I removed the no-arg constructor to experiment and ""ant clean test"" passes.;;;","21/Dec/10 22:36;jbellis;bq. I'm talking about this one

... that's in the _avro_ CassandraServer, btw.;;;","22/Dec/10 01:37;gdusbabek;fixed the avro.CassandraServer problem.

bq. inflate calls the many-arg constructor. 
I think we're talking about different things. I referred to UCF.inflate().  Either way, CFM.apply() is only called on the active metadata in one place.

v4 attached.;;;","22/Dec/10 02:32;jbellis;I still don't see anything calling a no-arg UCF constructor, including UCF.subinflate (there is no UCF.inflate).

The reason I am stuck on this is, if everyone was calling the arg-full UCF constructor, then apply must have been getting called, which means our bug diagnosis is wrong.;;;","22/Dec/10 06:12;jbellis;{code}
 * Each class that extends Migration is required to implement a no arg constructor, which will be used to inflate the
 * object from it's serialized form.
{code}

reflection ftw.

+1 after fixing formatting of
{code}
        } catch (ConfigurationException ex) 
{code}

:);;;","22/Dec/10 06:31;gdusbabek;committed.;;;","22/Dec/10 06:43;hudson;Integrated in Cassandra-0.7 #106 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/106/])
    apply CF metadata updates only at apply time. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1853
;;;","22/Dec/10 07:14;hudson;Integrated in Cassandra #637 (See [https://hudson.apache.org/hudson/job/Cassandra/637/])
    apply CF metadata updates only at apply time. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1853
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row iteration can stomp start-of-row mark,CASSANDRA-1130,12465398,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,jigneshdhruv,jigneshdhruv,26/May/10 02:13,16/Apr/19 17:33,22/Mar/23 14:57,15/Jun/10 02:06,0.7 beta 1,,,,0,,,,,,"Hello,

I am trying to use TTL (timeToLive) feature in SuperColumns.
My usecase is:
- I have a SuperColumn and 3 subcolumns.
- I try to expire data after 60 seconds.

While Cassandra is up and running, I am successfully able to push and read data without any problems. Data compaction and all occurs fine. After inserting say about 100000 records, I stop Cassandra while data is still coming.

On startup Cassandra throws an exception and won't start up. (This happens 1 in every 3 times). Exception varies like:
- EOFException while reading data
- negative value encountered exception
- Heap Space Exception

Cassandra simply won't start up.

Again I get this problem only when I use TTL with SuperColumns. There are no issues with using TTL with regular Columns.

I tried to diagnose the problem and it seems to happen on startup when it sees a Column that is marked Deleted and its trying to read data. Its off by some bytes and hence all these exceptions.

Caused by: java.io.IOException: Corrupt (negative) value length encountered
        at org.apache.cassandra.utils.FBUtilities.readByteArray(FBUtilities.java:317)
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:84)
        at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:336)
        at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:285)
        at org.apache.cassandra.db.filter.SSTableSliceIterator$ColumnGroupReader.getNextBlock(SSTableSliceIterator.java:235)
        at org.apache.cassandra.db.filter.SSTableSliceIterator$ColumnGroupReader.pollColumn(SSTableSliceIterator.java:195)
        ... 18 more


Let me know if you need more information.

Thanks,
Jignesh",,billa,joosto,mdennis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jun/10 00:53;slebresne;0001-Allow-for-multiple-mark-on-a-file.patch;https://issues.apache.org/jira/secure/attachment/12447039/0001-Allow-for-multiple-mark-on-a-file.patch","15/Jun/10 00:53;slebresne;0002-Unit-test-for-row-iteration.patch;https://issues.apache.org/jira/secure/attachment/12447040/0002-Unit-test-for-row-iteration.patch","28/May/10 05:21;jigneshdhruv;TestSuperColumnTTL.java;https://issues.apache.org/jira/secure/attachment/12445706/TestSuperColumnTTL.java","28/May/10 04:22;slebresne;TestSuperColumnTTL.java;https://issues.apache.org/jira/secure/attachment/12445696/TestSuperColumnTTL.java","29/May/10 01:00;slebresne;cassandra_0.6-Allow_multiple_mark_on_file.diff;https://issues.apache.org/jira/secure/attachment/12445795/cassandra_0.6-Allow_multiple_mark_on_file.diff",,,,,,,,,,5.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20002,,,Tue Jun 22 14:08:08 UTC 2010,,,,,,,,,,"0|i0g36f:",91945,,,,,Normal,,,,,,,,,,,,,,,,,"26/May/10 02:51;slebresne;If you have some script that allows to reproduce, that would be awesome. Alternatively, if you're able to locate the culprit data file and if the infos are not sensible, providing the file could help.

Are you using latest trunk ?;;;","26/May/10 03:06;jigneshdhruv;Yes I am using the latest source code from trunk.

I have a small java application that deals with creating schema and populating data.

This is what I was able to debug till now:
The error occurs during deserialization in ColumnSerializer.

There is an extra int byte that needs to be read before ColumnSerializer.java:84. Value of this extra int byte is ""4"". Not sure what it stands for.

I am not sure from where that byte is set. After reading that byte, I get the localDeletionTime value.

Also this happens when the DELETION_MASK is set on a record. It works fine for records with EXPIRATION_MASK. I am thinking that converting a record from EXPIRY to DELETED is causing this error at startup or something like that.

Let me know if you need more information.

But you should be able to reproduce this if you have a SuperColumn and their subColumns with TTL. Stop and start cassandra after loading some data. It consistently fails to startup 1 out of every 3 times.

Jignesh
;;;","26/May/10 04:37;jigneshdhruv;OK. I think I narrowed it down further..

The bug may be in ""org/apache/cassandra/db/filter/SSTableSliceIterator.java:getNextBlock() method.

See the while loop on line 233.
Out here, its reading 1 column at a time.

As I said before, the problem is when an Column of Type ExpiringColumn becomes DeletedColumn when time has expired.

In that case, once the Supercolumn whose subcolumns are of type ""DELETED"" are read in this while loop, there are some extra bytes that needs to be skipped but instead it goes in the second iteration in the while loop and tries to read the next column and thats where all the problem starts.

Shouldn't the while loop just read one Column at a time and then exit. That is what it does when it reads all the bytes. If I put a ""break statement"" in the end of while loop after reading a column all works fine as the extra bytes are skipped during the next read of a Column.

I am not sure what is the purpose of this while loop? but if we break after reading  1 column at a time, all works fine and cassandra starts up smoothly.

This looks similar to issue
https://issues.apache.org/jira/browse/CASSANDRA-1073

Jignesh;;;","26/May/10 05:19;jigneshdhruv;I did some more testing with the patch that I suggested i.e. skipping extra bytes after reading the column and exiting the while loop worked fine. I am now able to load and start cassandra with SuperColumn data with TTL without any problems.;;;","26/May/10 05:30;jbellis;this sounds like something you could build a unit test for?;;;","26/May/10 05:40;slebresne;But they shouldn't be extra bytes to skip. An expired column becomes a deletedColumn only 
after everything is deserialized. It is expected that we read all and everything that is written.

The while loop you're referring to is here to read all the column (or super columns) that falls 
into one given index range (the index is sparse). If you exit the while loop, you will just potentially 
skip some columns (super columns in your example). That it makes cassandra start may just be 
that it skips the problematic parts. 

I'll try to reproduce this tomorrow (but if you want and can share your test code to make that 
easier, feel free to :)). ;;;","26/May/10 17:22;slebresne;Sorry but I don't seem able to reproduce this.
I've tried a simple test, inserting supercolumns with 100 colums in them, 
each having a TTL (that I varied from 10 seconds to like 3 minutes).
I typically let it insert over 10000 super columns  (so around 1 millions ttled 
columns) and kill it. I run cassandra again, let it compact, kill it again, run again, 
start insertion again, etc... I tried like 20 times, no crashes whatsoever.

I've tried with a trunk of a week or so ago and then with trunk from 1 hour ago. 
Sounds like you have no problem reproducing on your side so .. I don't know.

If you could somehow come up with a unit test that make it crashes or a small 
script test that triggers it, that would be amazing.;;;","27/May/10 01:29;jigneshdhruv;I checked out the latest source code this morning and I am still able to reproduce it.

My usecase is:
- Start Cassandra
- Keep adding SuperColumns with 3 subcolumns within each SuperColumn. Each subcolumn expires in 35 seconds.
- Let cassandra run until you see statements  like ""Deleted  files""
- Stop cassandra and try to start and it will give you all the exceptions that I am talking about.

Also I believe ExpiringColumn contains some more data compared to DeletedColumn. Correct? In my testing I found that length of each DeletedColumn was similar to ExpiringColumn and once a complete DeletedColumn record was read there were some more extra bytes at the end of the record which is causing all this issue?

When you convert a ExpiringColumn to DeletedColumn, is it in place replacement or the old record is marked for deletion by just changing the EXPIRING_MASK to DELETED_MASK.

I will try to produce a junit test case. But one needs to still stop cassandra when one sees some files being deleted. At that point you will see the error that I am talking about.

Jignesh;;;","28/May/10 04:22;slebresne;Sorry but I'm still unable to reproduce.

I'm attaching a small insert script (in java) that, as far as I can see,
seems to do what you say triggers the bug. Could you look if this fails 
on your side. If I haven't understand the steps correctly, would you mind 
updating this test so that it reproduce the bug you see ?
(the script uses raw thrift and requires a very recent trunk)

{quote}
Also I believe ExpiringColumn contains some more data compared to DeletedColumn. Correct? In my testing I found that length of each
DeletedColumn was similar to ExpiringColumn and once a complete DeletedColumn record was read there were some more extra bytes at the 
end of the record which is causing all this issue?
When you convert a ExpiringColumn to DeletedColumn, is it in place replacement or the old record is marked for deletion by just 
changing the EXPIRING_MASK to DELETED_MASK.
{quote}

File on disk are not updated in place. So we never change on disk an
ExpiringColumn to a DeletedColumn. There only is a small optimisation in the
deserialization code that, after having fully deserialize an ExpiringColumn,
will return an equivalent DeletedColumn if the column is expired. But we always
read exactly what we have written (or it's a bug).;;;","28/May/10 05:21;jigneshdhruv;Hello,

I am still able to reproduce the problem consistently. I have attached my JUNIT Test case which reproduces it 1 out of 3 times.

Streps to Reproduce:
- Run it until it inserts atleaset 200000 records or until you see a ""Deleted files"" message on your console.
- Stop cassandra while the data is still coming in.
- Start Cassandra and you should get exceptions.

If you do not get exceptions, without deleting the index repeat the above steps.

Let me know if you are able to reproduce the problem.

Thanks,
Jignesh;;;","28/May/10 06:08;slebresne;Ok, I'm able to reproduce with your test.
I'll have a look at it, thanks;;;","29/May/10 00:01;slebresne;Attached fiie should fix the problem. 

The problem is unrelated to TTL per se but a problem in row iterations so the 
ticket title can be a bit misleading.
Citing irc: 
  ""a columnGroupReader mark the file when it is created. Then it reset() when getting a next block.
   but with the way the row iteration works, a new columnGroupReader is created (and mark the file) before 
   the previous one has retrieved it's block
  (it's because computeNext() create the next SSTableSliceIterator before getReduced() had retrieved the actual 
   column of the previous one)""
The patch allow for each columnGroupReader to have it's own mark on the file

Btw, I was unable to reproduce previously because it's the cache preloading that 
trigged the error and I did not use one in my first tests. 

Thanks Jignesh for helping find this one.;;;","29/May/10 01:00;slebresne;Attached version of the patch rebased against 0.6;;;","29/May/10 01:32;slebresne;Attaching a unit test for trunk that fails without the patch and passes with it.

It doesn't fail in 0.6 though. I suspect this is because getRangeSlice doesn't 
work in the same way in 0.6. So I'm not sure how to reproduce in 0.6. But I'll 
have a better at how it works in 0.6 and see if I can have a unit test for it too.;;;","29/May/10 01:54;jigneshdhruv;Excellent. I guess the changes are in trunk. I will check it out.

Thanks,
Jignesh;;;","30/May/10 09:41;jbellis;This bug is only present in 0.7.;;;","15/Jun/10 00:08;jbellis;Sorry for the delay applying.  Can you rebase to trunk please?;;;","15/Jun/10 00:53;slebresne;No problem, attaching rebased patches;;;","15/Jun/10 02:06;jbellis;committed, thanks!;;;","22/Jun/10 12:06;jbellis;weird, I totally remember committing this (and editing SSTNI to make it apply) but it didn't make it to svn.  Probably I forgot git-svn dcommit somehow.

really committed this time.;;;","22/Jun/10 22:08;hudson;Integrated in Cassandra #473 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/473/])
    fix race condition in SSTable*Iterator.
patch by Sylvain Lebresne; reviewed by jbellis for CASSANDRA-1130
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Range scan over two nodes returns wrong data,CASSANDRA-348,12432373,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,markr,markr,06/Aug/09 18:35,16/Apr/19 17:33,22/Mar/23 14:57,13/Aug/09 05:27,0.4,,,,0,,,,,,"I've got two nodes with tokens 00000000 and 88888888. I add 16 rows in which are spread over them, then do a key range scan.

I can scan part of the range successfully, but if I try to scan the entire range of keys (0-f) then I get unexpected results

./Cassandra-remote -h localhost:9160 get_key_range Keyspace1 Standard1 0 31 1000
['00', '01', '10', '11', '20', '21', '30', '31']

./Cassandra-remote -h localhost:9160 get_key_range Keyspace1 Standard1 3 81 1000
['30', '31', '40', '41', '50', '51', '60', '61', '70', '71', '80', '81']

 ./Cassandra-remote -h localhost:9160 get_key_range Keyspace1 Standard1 7 b1 1000
['70', '71', '80', '81', '90', '91', 'a0', 'a1', 'b0', 'b1']

./Cassandra-remote -h localhost:9160 get_key_range Keyspace1 Standard1 a g 1000
['a0', 'a1', 'b0', 'b1', 'c0', 'c1', 'd0', 'd1', 'e0', 'e1', 'f0', 'f1']

All of which returned as I expected.

But when I range scan the whole lot (0-g) then I get:

./Cassandra-remote -h localhost:9160 get_key_range Keyspace1 Standard1 0 g 1000
[ '00',
  '90',
  '91',
  'a0',
  'a1',
  'b0',
  'b1',
  'c0',
  'c1',
  'd0',
  'd1',
  'e0',
  'e1',
  'f0',
  'f1']

Where have 01-81 gone?

I'll attach the data loading script.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Aug/09 04:28;markr;348-2-fixup-2.patch;https://issues.apache.org/jira/secure/attachment/12415886/348-2-fixup-2.patch","08/Aug/09 04:16;markr;348-2-fixup.patch;https://issues.apache.org/jira/secure/attachment/12415885/348-2-fixup.patch","08/Aug/09 00:52;jbellis;348-2.patch;https://issues.apache.org/jira/secure/attachment/12415866/348-2.patch","13/Aug/09 04:59;jbellis;348-3-v2.patch;https://issues.apache.org/jira/secure/attachment/12416357/348-3-v2.patch","13/Aug/09 01:51;jbellis;348-3.patch;https://issues.apache.org/jira/secure/attachment/12416343/348-3.patch","06/Aug/09 21:26;jbellis;348.diff;https://issues.apache.org/jira/secure/attachment/12415744/348.diff","07/Aug/09 17:03;markr;LoadAndScan.py;https://issues.apache.org/jira/secure/attachment/12415832/LoadAndScan.py","06/Aug/09 18:37;markr;setup.cas;https://issues.apache.org/jira/secure/attachment/12415723/setup.cas",,,,,,,8.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19645,,,Thu Aug 13 13:13:49 UTC 2009,,,,,,,,,,"0|i0fydz:",91169,,,,,Normal,,,,,,,,,,,,,,,,,"06/Aug/09 18:37;markr;This is a cassandra-cli script used to load the test data which gets the results above.

;;;","06/Aug/09 18:39;markr;Relevant config:

node1:

    <Partitioner>org.apache.cassandra.dht.OrderPreservingPartitioner</Partitioner>
    <InitialToken>00000000</InitialToken>

node2:

    <Partitioner>org.apache.cassandra.dht.OrderPreservingPartitioner</Partitioner>
    <InitialToken>88888888</InitialToken>

Most of the rest is as shipped.;;;","06/Aug/09 21:26;jbellis;this patch fixes a minor bug (probably not the cause of your problems) and adds debug logging.  can you try with this patch and post the debug statements involving RangeCommand and RangeReply?;;;","06/Aug/09 21:45;markr;I've applied the patch and the bug is still there, here is the debug output:

NODE 1 debug output:

DEBUG - get_key_range
DEBUG - reading RangeCommand(table='Keyspace1', columnFamily=Standard1, startWith='0', stopAt='g', maxResults=100) from 58@127.0.0.1:7000
DEBUG - Sending RangeReply(keys=[00, 90, 91, a0, a1, b0, b1, c0, c1, d0, d1, e0, e1, f0, f1], completed=false) to 58@127.0.0.1:7000
DEBUG - Processing response on an async result from 58@127.0.0.1:7000
DEBUG - reading RangeCommand(table='Keyspace1', columnFamily=Standard1, startWith='f1', stopAt='g', maxResults=85) from 59@127.0.0.2:7000
DEBUG - Processing response on an async result from 59@127.0.0.2:7000

NODE 2 debug output:

DEBUG - Sending RangeReply(keys=[], completed=false) to 59@127.0.0.1:7000

bin/nodeprobe -host localhost ring
DEBUG - Loading settings from bin/../conf/storage-conf.xml
Token(00000000)                                 1 127.0.0.2      |<--|
Token(88888888)                                 1 127.0.0.1      |-->|
;;;","06/Aug/09 22:09;jbellis;remember these are string keys, not really numeric.  '0' is not part of the ['00000000' , '88888888' ) range.  (neither is the key '00' of course.)

i bet you get all the keys if you query for '', 'g' instead of '0', 'g'.;;;","06/Aug/09 23:00;markr;I am aware that the keys are strings :)

Keys should presumably not HAVE to be within the range of two tokens in the ring - keys outside the range will be stored anyway?

I tried the above, same result:

cassandra/Cassandra-remote -h localhost:9160 get_key_range Keyspace1 Standard1 '' g 100
[ '00',
  '90',
  '91',
  'a0',
  'a1',
  'b0',
  'b1',
  'c0',
  'c1',
  'd0',
  'd1',
  'e0',
  'e1',
  'f0',
  'f1']
;;;","06/Aug/09 23:07;markr;Even more weird:

 cassandra/Cassandra-remote -h localhost:9160 get_key_range Keyspace1 Standard1 00000000 g 100
[ '90',
  '91',
  'a0',
  'a1',
  'b0',
  'b1',
  'c0',
  'c1',
  'd0',
  'd1',
  'e0',
  'e1',
  'f0',
  'f1']

Now it misses everything from 0 to 81;;;","06/Aug/09 23:10;markr;cassandra/Cassandra-remote -h localhost:9160 get_key_range Keyspace1 Standard1 '' g 100
[ '00',
  '90',
..
  'f1']

Debug logs:

NODE 1:

DEBUG - get_key_range
DEBUG - reading RangeCommand(table='Keyspace1', columnFamily=Standard1, startWith='', stopAt='g', maxResults=100) from 2208@127.0.0.1:7000
DEBUG - Sending RangeReply(keys=[00, 90, 91, a0, a1, b0, b1, c0, c1, d0, d1, e0, e1, f0, f1], completed=false) to 2208@127.0.0.1:7000
DEBUG - Processing response on an async result from 2208@127.0.0.1:7000
DEBUG - reading RangeCommand(table='Keyspace1', columnFamily=Standard1, startWith='f1', stopAt='g', maxResults=85) from 2209@127.0.0.2:7000
DEBUG - Processing response on an async result from 2209@127.0.0.2:7000

NODE 2:
DEBUG - Sending RangeReply(keys=[], completed=false) to 2209@127.0.0.1:7000
;;;","06/Aug/09 23:34;jbellis;looks like a bug in the node selection code.

i'll commit the bugfix+logging patch for now.;;;","06/Aug/09 23:41;junrao;The node selection code seems to be designed only for RackUnaware.;;;","07/Aug/09 00:34;jbellis;Node selection code is working as designed but it is not quite what getKeyRange expects.

The node selection is ""pick the node whose token is nearest to the decorated key, _always rounding up_.""  so what you end up with here is 3 range sections:

["""", 000000000] node A (the one with token 00000000)
(00000000, 88888888] node B (the one with token 88888888)
(88888888, infinity) node A again

so, key 00 goes on node A, but 01-88 go on node B.  then 09-ff go on node A again.

we could hack around this in getKeyRange but it seems like the Right Fix is to make it so A has ['', 88888888) and B has [88888888, inf), no?

what do you think, Jun?  is there any inherent advantage to ""round up"" instead of ""round down"" that I have forgotten?

[yeah, we're ignoring RackAware for now];;;","07/Aug/09 01:24;junrao;Interesting. It seems the problem is that you started with a key that's in the middle btw 2 adjacent tokens and you need to go back to the very first node to complete the full scan. The current code seems to stop as soon as you hit the first node again. It seems that this will happen whether you roundup or rounddown. So, maybe we should let the first node be scanned twice, one at the beginning and another at the end.
;;;","07/Aug/09 03:27;jbellis;You are right, we're going to have this problem either way we round the keys to tokens.  Take this example, I was wrong about how the tokens would work, it would be

[00000000, 88888888) A
[88888888, inf) and ['', 00000000) B

so either way starting from '' you're going to have to re-scan part of the same range when you wrap.;;;","07/Aug/09 17:03;markr;I have attached a python script LoadAndScan.py which uses the thrift interface to load a bunch of test data then do lots of range scans to check the results are right.

This can be made into an automated system test, you are free to use it.;;;","07/Aug/09 17:04;markr;The LoadAndScan.py script succeeds when there is a single node, and various cases fail when there are more nodes with tokens which overlap the range 0000-ffff 

I have tried it with 1,2 and 4 nodes, the more nodes the more failure cases.;;;","07/Aug/09 20:49;hudson;Integrated in Cassandra #160 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/160/])
    fix range query buglet; add debug logging
patch by jbellis; tested by Mark Robson for 
;;;","08/Aug/09 00:52;jbellis;This patch fixes the main problem.  There are two things going on in this patch:

 - we switch from trying to get the next endpoint by increasing offset to asking tokenMetadata for ""the next one.""  this will always be correct where the offset approach will not (usually you want offset to just be 1, but sometimes you have to keep increasing it if no results are found but the range is still not finished)
 - we merge results differently when the endpoint responsible for where the ring wraps is involved, since that endpoint can hold keys from both the beginning and end of the range.;;;","08/Aug/09 03:37;markr;With 348-2.patch I get

DEBUG - get_key_range
DEBUG - reading RangeCommand(table='Keyspace1', columnFamily=Standard1, startWith='0', stopAt='1', maxResults=1000) from 593@127.0.0.1:7000
DEBUG - Sending RangeReply(keys=[0000], completed=false) to 593@127.0.0.1:7000
DEBUG - Processing response on an async result from 593@127.0.0.1:7000
DEBUG - reading RangeCommand(table='Keyspace1', columnFamily=Standard1, startWith='0', stopAt='1', maxResults=999) from 594@127.0.0.2:7000
DEBUG - Processing response on an async result from 594@127.0.0.2:7000
ERROR - Internal error processing get_key_range
java.lang.UnsupportedOperationException
        at java.util.Collections$UnmodifiableCollection.addAll(Collections.java:1044)
        at org.apache.cassandra.service.StorageProxy.getKeyRange(StorageProxy.java:673)
        at org.apache.cassandra.service.CassandraServer.get_key_range(CassandraServer.java:557)
        at org.apache.cassandra.service.Cassandra$Processor$get_key_range.process(Cassandra.java:1095)
        at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:758)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:252)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)

When attempting to do a range scan which crosses over nodes.

Also get the warning:

    [javac] Note: /home/mark/cassandra/cassandra-trunk/src/java/org/apache/cassandra/tools/KeyChecker.java uses or overrides a deprecated API.

At compile-time.
;;;","08/Aug/09 04:16;markr;This patch goes on top of 348-2.patch and fixes the exception I described.;;;","08/Aug/09 04:28;markr;This patch 348-2-fixup-2.patch supersedes the previous one and fixes another case where it was trying to modify a readonly list.;;;","08/Aug/09 04:31;markr;With the combined efforts of your patch and my patch, range scans are a lot *closer* to working correctly. My system test program runs successfully with two nodes 0 and 8, but still fails a few test cases when there are four nodes 0,4,8,c

However, the remaining test cases which are failing are ones where a range covers at least three nodes.These are unlikely to happen to anyone in production unless their nodes are very close together or their keys very sparse and they're doing massive range scans.

But it would be nice if we covered all cases.;;;","08/Aug/09 04:48;jbellis;Committed -2 with a simpler fix for the readonly list.

If you can find a way to reproduce the remaining bug in a 2-node setup that will make it easier to debug.;;;","08/Aug/09 05:28;markr;Technically this is fixed as I can't reproduce it in a two-node setup any more. On the other hand, some range scans still return missing results in a three-node setup.

So either, close this and open a new one for the three-node case, or continue to work on a solution.

I fired up three nodes with tokens 0,4,8 then used the attached LoadAndScan.py.

This gives 3 errors out of 120 get_key_range commands. When run on two nodes (0,8) it passes, as on a single node.;;;","08/Aug/09 05:47;jbellis;can you post the debug logs from a 3-node failure as before?;;;","08/Aug/09 20:35;hudson;Integrated in Cassandra #161 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/161/])
    - switch from trying to get the next endpoint by increasing offset to asking tokenMetadata for ""the next
one."" this will always be correct where the offset approach will not (usually you want offset to just be 1,
but sometimes you have to keep increasing it if no results are found but the range is still not finished)
 - merge results differently when the endpoint responsible for where the ring wraps is involved, since
that endpoint can hold keys from both the beginning and end of the range.

patch by jbellis; tested by Mark Robson for 
;;;","13/Aug/09 01:51;jbellis;with the -3 patch, LoadAndScan.py passes all tests on 3 nodes for me.;;;","13/Aug/09 04:54;markr;With the -3 patch, LoadAndScan.py now passes with 3 and 4 nodes if ReplicationFactor=1.

Unfortunately, setting ReplicationFactor=2 now breaks it.
;;;","13/Aug/09 04:59;jbellis;fixes replication > 1 bugs;;;","13/Aug/09 05:27;jbellis;IRC: > Looks better

committed.;;;","13/Aug/09 14:12;markr;Jonathan, 

The latest patch passes every range scan I have thrown at it, including with replication > 1

So it all looks good to me.

I will hopefully incorporate my tests into the suite soon.

Mark;;;","13/Aug/09 21:13;hudson;Integrated in Cassandra #166 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/166/])
    give up on trying to optimize startWith -- it's basically impossible when replication factor > 1 b/c of the range wrap point.
patch by jbellis; tested by Mark Robson for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EOFException during name query,CASSANDRA-2165,12498670,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,slebresne,slebresne,16/Feb/11 02:11,16/Apr/19 17:33,22/Mar/23 14:57,16/Feb/11 04:42,0.7.2,,,,0,EOF,,,,,"As reported by Jonas Borgstrom on the mailing list:

{quote}
While testing the new 0.7.1 release I got the following exception:

ERROR [ReadStage:11] 2011-02-15 16:39:18,105
DebuggableThreadPoolExecutor.java (line 103) Error in ThreadPoolExecutor
java.io.IOError: java.io.EOFException
       at
org.apache.cassandra.db.columniterator.SSTableNamesIterator.<init>(SSTableNamesIterator.java:75)
       at
org.apache.cassandra.db.filter.NamesQueryFilter.getSSTableColumnIterator(NamesQueryFilter.java:59)
       at
org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:80)
       at
org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1274)
       at
org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1166)
       at
org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1095)
       at org.apache.cassandra.db.Table.getRow(Table.java:384)
       at
org.apache.cassandra.db.SliceByNamesReadCommand.getRow(SliceByNamesReadCommand.java:60)
       at
org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:473)
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
       at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
       at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
       at java.lang.Thread.run(Thread.java:636)
Caused by: java.io.EOFException
       at java.io.DataInputStream.readInt(DataInputStream.java:392)
       at
org.apache.cassandra.utils.BloomFilterSerializer.deserialize(BloomFilterSerializer.java:48)
       at
org.apache.cassandra.utils.BloomFilterSerializer.deserialize(BloomFilterSerializer.java:30)
       at
org.apache.cassandra.io.sstable.IndexHelper.defreezeBloomFilter(IndexHelper.java:108)
       at
org.apache.cassandra.db.columniterator.SSTableNamesIterator.read(SSTableNamesIterator.java:106)
       at
org.apache.cassandra.db.columniterator.SSTableNamesIterator.<init>(SSTableNamesIterator.java:71)
       ... 12 more

{quote}",,cburroughs,lenn0x,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,CASSANDRA-2234,,,,,,"16/Feb/11 02:13;slebresne;0001-Fix-bad-signed-conversion-from-byte-to-int.patch;https://issues.apache.org/jira/secure/attachment/12471093/0001-Fix-bad-signed-conversion-from-byte-to-int.patch","16/Feb/11 03:35;jbellis;2165-1.txt;https://issues.apache.org/jira/secure/attachment/12471101/2165-1.txt","16/Feb/11 03:38;jbellis;2165-2.txt;https://issues.apache.org/jira/secure/attachment/12471102/2165-2.txt",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20476,,,Fri Feb 18 20:03:01 UTC 2011,,,,,,,,,,"0|i0g9pr:",93004,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"16/Feb/11 02:13;slebresne;The problem is when creating an inputStream from a ByteBuffer. We directly return the result of BB.get(), but this can be negative which breaks the method contract.

The fix is the return of the get(), the addition of available() is more of an improvement.;;;","16/Feb/11 03:35;jbellis;patch 1 provides a long-test that exposes the bug.  this requires turning off row caching in the test keyspaces to keep them from covering up the error.  for good measure, key caching is also tured off except in KeyCacheSpace / KeyCacheTest.;;;","16/Feb/11 03:38;jbellis;patch 2 fixes the bug;;;","16/Feb/11 04:42;jbellis;committed;;;","16/Feb/11 05:43;hudson;Integrated in Cassandra-0.7 #280 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/280/])
    fix column bloom filter deserialization
patch by jbellis and slebresne for CASSANDRA-2165
;;;","19/Feb/11 04:03;jbellis;For those not following the mailing list: this is a read-time error, upgrading to 0.7.2 fixes the problem with no data loss.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clean up after failed (repair) streaming operation,CASSANDRA-2088,12497286,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,stuhood,stuhood,01/Feb/11 13:49,16/Apr/19 17:33,22/Mar/23 14:57,14/Apr/11 05:06,0.7.5,,,,1,,,,,,,,christianmovi,dln,skamio,slebresne,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Apr/11 01:45;slebresne;0001-Better-detect-failures-from-the-other-side-in-Incomi.patch;https://issues.apache.org/jira/secure/attachment/12476140/0001-Better-detect-failures-from-the-other-side-in-Incomi.patch","11/Apr/11 13:50;amorton;0001-detect-streaming-failures-and-cleanup-temp-files.patch;https://issues.apache.org/jira/secure/attachment/12475965/0001-detect-streaming-failures-and-cleanup-temp-files.patch","11/Apr/11 13:50;amorton;0002-delete-partial-sstable-if-compaction-error.patch;https://issues.apache.org/jira/secure/attachment/12475966/0002-delete-partial-sstable-if-compaction-error.patch",,,,,,,,,,,,3.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20437,,,Wed Apr 13 21:06:38 UTC 2011,,,,,,,,,,"0|i0g98n:",92927,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"01/Feb/11 13:53;stuhood;Regarding repair: http://www.mail-archive.com/user@cassandra.apache.org/msg09259.html
And compaction: CASSANDRA-2084;;;","07/Mar/11 13:23;amorton;I'm keen to try this ticket (to learn more about compaction and repair) if it's not already been worked on. Also if it's ok for me to take a couple of days while I dig into this.

For compaction I'm looking in
- CompactionManager.doCompaction where it creates a new SSTableWriter via cfs.createCompactionWriter() 
- CompactionManager.doCleanupCompaction() also uses an SSTableWriter

Are the sorts of failures we're considering for compaction ones that come from the CompactionIterator or SSTableScanner ?

For repair I'm looking in:
- IncomingStreamReader appears to clean up the temporary pending file in some error situations. Do we have any more info on the sorts of failures here? e.g. If there is an IOException sending the re-stream message, or a non checked exception it will fail to cleaup the file. 
- I'm looking into what happens in StreamInSession.finished() closeIfFinished()
- Are we considering failures during the streaming or when processing the data after the stream has finished?

Any guidance welcome. ;;;","08/Mar/11 00:59;jbellis;bq. Are the sorts of failures we're considering for compaction ones that come from the CompactionIterator or SSTableScanner ?

Both. Also I suppose it's possible for the writer to error out from lack of disk space since it only checks at the beginning for space and doesn't ""reserve"" it vs flushes.

bq. Are we considering failures during the streaming or when processing the data after the stream has finished?

The former is much more common (I've never seen the latter reported), so I'd start with that.;;;","11/Apr/11 13:50;amorton;patch 0001 tracks failures during AES streaming, files for failed Stream sessions are cleaned up and repair is allowed to continue. Failed files are logged at the StreamSession, TreeRequest, and RepairSession level. 

patch 0002 handle exceptions when doing a (normal) compaction and deletes the temp SSTable. The SSTableWriter components are closed before deletion so that windows will delete correctly. ;;;","13/Apr/11 01:45;slebresne;I think there is a few different things here and I think we should separate them somehow.

Fixing the fact that streaming leave tmp files around when it fails is a 2 lines fix and I think this is simple enough that it could go to 0.7. I'm attaching a patch against 0.7. It's extracted from Aaron first patch, although rebased on 0.7 (and fix a bug).

Making repair aware that there has been some failures is actually more complicated so that should go in 0.8.1 or something (and should go to CASSANDRA-2433 or another ticket that describe the problem better). ;;;","13/Apr/11 01:51;jbellis;bq. I'm attaching a patch against 0.7

Is that 0001-Better-detect-failures-from-the-other-side-in-Incomi.patch?  I don't see the connection to .tmp files.  (Also: have you verified that the channel will actually infinite-loop returning 0?  Kind of odd behavior, although I guess it's technically within-spec.);;;","13/Apr/11 02:21;slebresne;bq. Is that 0001-Better-detect-failures-from-the-other-side-in-Incomi.patch? I don't see the connection to .tmp files. (Also: have you verified that the channel will actually infinite-loop returning 0? Kind of odd behavior, although I guess it's technically within-spec.)

Yes. IncomingStreamReader does clean the tmp file when there is an expection (there's an enclosing 'try catch'). The problem is that no exception is raised if the other side of the connection dies. What will happen then is the read will infinitely read 0 bytes. So this actually avoid the infinite loop returning 0 (and so I think answered your second question, so it wasn't very clear).

Note that without this patch, there is an infinite loop that will hold a socket open forever (and consume cpu, though very few probably in that case). So this is not just merely a fix of deleting the tmp files. But it does as a consequence of correctly raising an exception when should be.;;;","13/Apr/11 02:25;jbellis;+1, and can you move some of that explanation inline as a comment?;;;","13/Apr/11 03:39;slebresne;Committed that first part. I think we should keep that open to fix the tmp files for failed compaction and move the rest to another ticket (like CASSANDRA-2433 for instance).

About the attached patch on cleaning up failed compaction:
  * We should also handle cleanup and scrub
  * We should handle SSTableWriter.Builder as it is yet another place where we could miss to cleanup a tmp file on error.
  * In theory a failed flush could leave a tmp file behind. If that happens having a tmp file would be the least of your problem but for completeness sake we could handle it.
  * The logging when failing to close iwriter and dataFile in SSTableWriter could probably go at error (we should not be failing there, if we do something is wrong)
  * That's nitpick but I'm not a huge fan of catching RuntimeException in this case as this pollute the code for something that would be a programming error (that's probably debatable though). Maybe another solution would be to have this in the final block. It means making sure closeAndDelete() is ok with the file being already closed and/or deleted and having this final block *after* the closeAndOpenReader call.
;;;","13/Apr/11 11:08;amorton;Thanks will take another look at the cleanup for compaction. 
;;;","14/Apr/11 05:06;jbellis;Created CASSANDRA-2468 for compaction cleanup. Will close this one for streaming.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IndexOutOfBoundsException during lazy row compaction of supercolumns,CASSANDRA-2104,12497548,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,dln,dln,03/Feb/11 18:08,16/Apr/19 17:33,22/Mar/23 14:57,25/Feb/11 03:09,0.7.3,,,,0,,,,,,"I ran into an exception when lazily compacting wide rows of TimeUUID columns.
It seems to trigger when a row is larger than {{in_memory_compaction_limit_in_mb}}.

Traceback:
{noformat}
 INFO [CompactionExecutor:1] 2011-02-03 10:59:59,262 CompactionIterator.java (line 135) Compacting large row XXXXXXXXXXXXX (76999384 bytes) incrementally
 ERROR [CompactionExecutor:1] 2011-02-03 10:59:59,266 AbstractCassandraDaemon.java (line 114) Fatal exception in thread T
 hread[CompactionExecutor:1,1,main]
 java.lang.IndexOutOfBoundsException
         at java.nio.Buffer.checkIndex(Buffer.java:514)
         at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:121)
         at org.apache.cassandra.db.marshal.TimeUUIDType.compareTimestampBytes(TimeUUIDType.java:56)
         at org.apache.cassandra.db.marshal.TimeUUIDType.compare(TimeUUIDType.java:45)
         at org.apache.cassandra.db.marshal.TimeUUIDType.compare(TimeUUIDType.java:29)
         at java.util.concurrent.ConcurrentSkipListMap$ComparableUsingComparator.compareTo(ConcurrentSkipListMap.java:606
 )
         at java.util.concurrent.ConcurrentSkipListMap.findPredecessor(ConcurrentSkipListMap.java:685)
         at java.util.concurrent.ConcurrentSkipListMap.doPut(ConcurrentSkipListMap.java:864)
         at java.util.concurrent.ConcurrentSkipListMap.putIfAbsent(ConcurrentSkipListMap.java:1893)
         at org.apache.cassandra.db.SuperColumn.addColumn(SuperColumn.java:170)
         at org.apache.cassandra.db.SuperColumn.putColumn(SuperColumn.java:195)
         at org.apache.cassandra.db.ColumnFamily.addColumn(ColumnFamily.java:221)
         at org.apache.cassandra.io.LazilyCompactedRow$LazyColumnIterator.reduce(LazilyCompactedRow.java:204)
         at org.apache.cassandra.io.LazilyCompactedRow$LazyColumnIterator.reduce(LazilyCompactedRow.java:185)
         at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:62)
         at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
         at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
         at com.google.common.collect.Iterators$7.computeNext(Iterators.java:604)
         at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
         at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
         at org.apache.cassandra.db.ColumnIndexer.serializeInternal(ColumnIndexer.java:76)
         at org.apache.cassandra.db.ColumnIndexer.serialize(ColumnIndexer.java:50)
         at org.apache.cassandra.io.LazilyCompactedRow.<init>(LazilyCompactedRow.java:88)
         at org.apache.cassandra.io.CompactionIterator.getCompactedRow(CompactionIterator.java:137)
         at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:108)
         at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:43)
         at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:73)
         at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
         at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
         at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
         at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
         at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:426)
         at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:122)
         at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:92)
         at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
         at java.util.concurrent.FutureTask.run(FutureTask.java:138)
         at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
         at java.lang.Thread.run(Thread.java:662)

{noformat}",,jborgstrom,,,,,,,,,,,,,,,,,,,,,28800,28800,,0%,28800,28800,,,,,,,,,,,,,,,,,,,,"25/Feb/11 00:32;slebresne;0001-Use-the-right-comparator-when-deserializing-superCol.patch;https://issues.apache.org/jira/secure/attachment/12471850/0001-Use-the-right-comparator-when-deserializing-superCol.patch","25/Feb/11 02:16;slebresne;Unit-test.patch;https://issues.apache.org/jira/secure/attachment/12471857/Unit-test.patch",,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20445,,,Thu Mar 10 10:04:36 UTC 2011,,,,,,,,,,"0|i0g9c7:",92943,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"04/Feb/11 00:41;jbellis;I believe you mentioned on the ML that you have data that compacts fine if you increase the in_memory limit, but not with lazy mode?

If so, can you get me a copy of that sstable to test with?  I can set up a place for you to upload it privately if you can't make it public.;;;","04/Feb/11 19:22;dln;Unfortunately, I'm not at liberty to share that data, sorry. But I'm working on seeing if I can reproduce it with non-sensitive data...

These are timeuuid cols, wide rows (100k or so cols, with a few multi-mb columns but mostly <1k sizes). No validators.

The instance in question had small memtable thresholds (32MB), a pretty low heap (6GB) but nothing out of the ordinary.;;;","16/Feb/11 10:51;jbellis;Any luck reproducing?;;;","23/Feb/11 22:30;dln;Nope, never managed to trigger it again.
Closing this issue for now.;;;","23/Feb/11 22:56;jborgstrom;Hi Jonathan,

I now have a python script that seems to be able to reproduce this after a few runs (it does not seem to always happen)

http://jonas.borgstrom.se/cassandra/CASSANDRA-2104.txt

I've tested this on an empty single node 0.7.2 cluster and default cassandra.yaml, except for in_memory_compaction_limit_in_mb=32 hoping that would make it easier to reproduce.

A complete copy of the data directory (and config) after running the script a couple of times can be downloaded here:
http://jonas.borgstrom.se/cassandra/CASSANDRA-2104.tar.gz

And a copy of system.log from booting cassandra using this data directory:
http://jonas.borgstrom.se/cassandra/CASSANDRA-2104-system.log

Let me know if you need any more details.;;;","23/Feb/11 23:14;jbellis;Jonas, are you also seeing the error only on rows larger than in_memory_compaction_limit then?;;;","24/Feb/11 00:14;jborgstrom;Jonathan, yes that seems to be the case. For me key 3139 (19) seems to be the one triggering this. So increasing in_memory_compaction_limit enough to cover that key seems to do the trick. Even if some other keys are compacted incrementally.;;;","24/Feb/11 19:42;jborgstrom;I've done some more testing now and I'm ONLY able to reproduce this when using both super columns and the TimeUUIDType column comparator.

After looking at the code TimeUUIDType seems to be the only marshaller that would actually notice (throw an exception) if its compare() method was called with a partial value (1-15 bytes in the case of UUIDs).

So to me it looks like the incremental compactor sometimes sends corrupted/partial data to the marshaller, at least for super column families. This corrupted/partial data is silently ignored unless the TimeUUIDType marshaller is used.;;;","24/Feb/11 21:46;slebresne;I have a hard time reproducing (using your script). Do you use TimeUUIDType for the column or the super column names ? (I've tried both though).

I'll continue trying. However, if you happen to have some SSTables that directly triggers it and that is smaller that the ones you added above, that would be awesome (I'm struggling making enough room on my laptop for the ones you attached above :)).

Anyway, thanks for taking time on this, I'll continue to try.;;;","24/Feb/11 22:47;jborgstrom;Hi Sylvain,

The last time I reproduce it I used ""create column family bar with column_type=Super and comparator=TimeUUIDType and subcomparator=UTF8Type;"" and had to run the test case twice.

Anyway here's a new data directory with only a subset of the sstables from the last dump. I get an exception in the log within seconds after issuing a ""compact"" command.

http://jonas.borgstrom.se/cassandra/CASSANDRA-2104-v2.tar.gz (153kB compressed and 127MB uncompressed);;;","25/Feb/11 00:36;slebresne;Thanks a lot Jonas, those new sstables were most useful.

Turns out the problem was that we were using the wrong comparator when deserializing, so we were comparing the column using TimeUUIDType (instead of UTF8Type).

Sadly, it could be that we were generating wrongly sorted on-disk superColumns. However any compaction with the attached patch should fix that anyway.;;;","25/Feb/11 02:16;slebresne;Attaching a unit test that exposes the bug (and confirm the fix);;;","25/Feb/11 03:09;jbellis;committed;;;","25/Feb/11 23:15;hudson;Integrated in Cassandra-0.7 #321 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/321/])
    ;;;","10/Mar/11 08:47;akaris;Hi,

I'm also getting an IndexOutOfBounds exception when compacting.

Here's the detailed error I get on screen when running ""nodetool -h 10.3.133.10 compact"":

Error occured while compacting keyspace test
java.util.concurrent.ExecutionException: java.lang.IndexOutOfBoundsException
    at java.util.concurrent.FutureTask$Sync.innerGet(Unknown Source)
    at java.util.concurrent.FutureTask.get(Unknown Source)
    at org.apache.cassandra.db.CompactionManager.performMajor(CompactionManager.java:186)
    at org.apache.cassandra.db.ColumnFamilyStore.forceMajorCompaction(ColumnFamilyStore.java:1678)
    at org.apache.cassandra.service.StorageService.forceTableCompaction(StorageService.java:1248)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
    at java.lang.reflect.Method.invoke(Unknown Source)
    at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(Unknown Source)
    at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(Unknown Source)
    at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(Unknown Source)
    at com.sun.jmx.mbeanserver.PerInterface.invoke(Unknown Source)
    at com.sun.jmx.mbeanserver.MBeanSupport.invoke(Unknown Source)
    at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(Unknown Source)
    at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(Unknown Source)
    at javax.management.remote.rmi.RMIConnectionImpl.doOperation(Unknown Source)
    at javax.management.remote.rmi.RMIConnectionImpl.access$200(Unknown Source)
    at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(Unknown Source)
    at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(Unknown Source)
    at javax.management.remote.rmi.RMIConnectionImpl.invoke(Unknown Source)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
    at java.lang.reflect.Method.invoke(Unknown Source)
    at sun.rmi.server.UnicastServerRef.dispatch(Unknown Source)
    at sun.rmi.transport.Transport$1.run(Unknown Source)
    at java.security.AccessController.doPrivileged(Native Method)
    at sun.rmi.transport.Transport.serviceCall(Unknown Source)
    at sun.rmi.transport.tcp.TCPTransport.handleMessages(Unknown Source)
    at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(Unknown Source)
    at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.lang.Thread.run(Unknown Source)
Caused by: java.lang.IndexOutOfBoundsException
    at java.nio.Buffer.checkIndex(Unknown Source)
    at java.nio.HeapByteBuffer.getInt(Unknown Source)
    at org.apache.cassandra.db.DeletedColumn.getLocalDeletionTime(DeletedColumn.java:57)
    at org.apache.cassandra.db.ColumnFamilyStore.removeDeletedStandard(ColumnFamilyStore.java:822)
    at org.apache.cassandra.db.ColumnFamilyStore.removeDeletedColumnsOnly(ColumnFamilyStore.java:809)
    at org.apache.cassandra.db.ColumnFamilyStore.removeDeleted(ColumnFamilyStore.java:800)
    at org.apache.cassandra.io.PrecompactedRow.<init>(PrecompactedRow.java:94)
    at org.apache.cassandra.io.CompactionIterator.getCompactedRow(CompactionIterator.java:139)
    at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:108)
    at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:43)
    at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:73)
    at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
    at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
    at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
    at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
    at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:427)
    at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:217)
    at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
    at java.util.concurrent.FutureTask.run(Unknown Source)
    ... 3 more

And here's the error I'm getting in my log file:

ERROR [CompactionExecutor:1] 2011-03-09 19:16:52,299 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.IndexOutOfBoundsException
    at java.nio.Buffer.checkIndex(Unknown Source)
    at java.nio.HeapByteBuffer.getInt(Unknown Source)
    at org.apache.cassandra.db.DeletedColumn.getLocalDeletionTime(DeletedColumn.java:57)
    at org.apache.cassandra.db.ColumnFamilyStore.removeDeletedStandard(ColumnFamilyStore.java:822)
    at org.apache.cassandra.db.ColumnFamilyStore.removeDeletedColumnsOnly(ColumnFamilyStore.java:809)
    at org.apache.cassandra.db.ColumnFamilyStore.removeDeleted(ColumnFamilyStore.java:800)
    at org.apache.cassandra.io.PrecompactedRow.<init>(PrecompactedRow.java:94)
    at org.apache.cassandra.io.CompactionIterator.getCompactedRow(CompactionIterator.java:139)
    at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:108)
    at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:43)
    at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:73)
    at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
    at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
    at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
    at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
    at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:427)
    at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:217)
    at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
    at java.util.concurrent.FutureTask.run(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.lang.Thread.run(Unknown Source)

I run Cassandra 0.7.2. We have 8 machines in the cluster, the error happens only on one machine. I'm not sure it's the same issue than this ticket but it's the only reference I found about compacting and IndexOutOfBounds. We're not inserting any SuperColumn in that database.

Thanks for the help.;;;","10/Mar/11 09:09;akaris;We upgraded to 0.7.3 and we still have the same error, so I guess it's a different problem :(;;;","10/Mar/11 18:04;slebresne;Can you open a new ticket with the description of the problem then. If you're at liberty to attach an bad sstable to reproduce the problem, that would clearly help.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
range movements can violate consistency,CASSANDRA-2434,12503655,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,tjake,scode,scode,08/Apr/11 01:23,16/Apr/19 17:33,22/Mar/23 14:57,01/May/14 21:52,2.1 beta2,,Legacy/Streaming and Messaging,,0,,,,,,"My reading (a while ago) of the code indicates that there is no logic involved during bootstrapping that avoids consistency level violations. If I recall correctly it just grabs neighbors that are currently up.

There are at least two issues I have with this behavior:

* If I have a cluster where I have applications relying on QUORUM with RF=3, and bootstrapping complete based on only one node, I have just violated the supposedly guaranteed consistency semantics of the cluster.

* Nodes can flap up and down at any time, so even if a human takes care to look at which nodes are up and things about it carefully before bootstrapping, there's no guarantee.

A complication is that not only does it depend on use-case where this is an issue (if all you ever do you do at CL.ONE, it's fine); even in a cluster which is otherwise used for QUORUM operations you may wish to accept less-than-quorum nodes during bootstrap in various emergency situations.

A potential easy fix is to have bootstrap take an argument which is the number of hosts to bootstrap from, or to assume QUORUM if none is given.

(A related concern is bootstrapping across data centers. You may *want* to bootstrap to a local node and then do a repair to avoid sending loads of data across DC:s while still achieving consistency. Or even if you don't care about the consistency issues, I don't think there is currently a way to bootstrap from local nodes only.)

Thoughts?

",,alienth,azotcsit,bradfordcp,cburroughs,christianmovi,elubow,jay.zhuang,jeromatron,jjordan,kohlisankalp,liqusha,rcoli,rfwagner@gmail.com,scode,thobbs,tjake,vijay2win@yahoo.com,wadey,weideng,Yasuharu,,,,,,,,,,,,CASSANDRA-3516,,,,,,,,,,,,,,,"09/Sep/11 05:57;thepaul;2434-3.patch.txt;https://issues.apache.org/jira/secure/attachment/12493677/2434-3.patch.txt","08/Sep/11 04:11;thepaul;2434-testery.patch.txt;https://issues.apache.org/jira/secure/attachment/12493374/2434-testery.patch.txt",,,,,,,,,,,,,2.0,tjake,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20623,,,Mon Jul 21 21:44:49 UTC 2014,,,,,,,,,,"0|i07z8f:",44475,,thobbs,,thobbs,Normal,,,,,,,,,,,,,,,,,"09/Apr/11 05:00;jbellis;ISTM the easiest fix is to always stream from the node that will be removed from the replicas for each range, unless given permission from the operator to choose a replica that is closer / less dead.;;;","03/May/11 21:41;jbellis;Related: CASSANDRA-833;;;","26/Aug/11 07:30;thepaul;So, it looks like it will be possible for the node-that-will-be-removed to change between starting a bootstrap and finishing it (other nodes being bootstrapped/moved/decom'd during that time period); in some cases, that could still lead to a consistency violation.  Is that unlikely enough that we don't care, here?  At least the situation would be better with the proposed fix than it is now.

Second question: what might the ""permission from the operator to choose a replica that is closer/less dead"" look like?  Maybe just a boolean flag saying ""it's ok to stream from any node for any range you need to stream""?  Or would we want to allow specifying precise source nodes for any/all affected address ranges?;;;","27/Aug/11 00:20;jbellis;bq. it looks like it will be possible for the node-that-will-be-removed to change between starting a bootstrap and finishing it

It's always been unsupported to bootstrap a second node into the same ""token arc"" while a previous one is ongoing.  Does that cover what you're thinking of or are we still on the hook?

bq. what might the ""permission from the operator to choose a replica that is closer/less dead"" look like?

It seems to me that the two valid choices are

- Stream from ""correct"" replica
- Stream from closest replica

I can't think of a reason to stream from an arbitrary replica other than those options.;;;","27/Aug/11 01:14;thepaul;bq. It's always been unsupported to bootstrap a second node into the same ""token arc"" while a previous one is ongoing. Does that cover what you're thinking of or are we still on the hook?

Is it also unsupported to decom within X's token arc, or move into/out of that arc, while X is bootstrapping? I think we're safe if so.;;;","27/Aug/11 01:19;jbellis;Yes.;;;","01/Sep/11 03:03;thepaul;This still needs some testing, but I'm putting it up now in case anyone has some time to take a look and make sure my approach is sane.

Adds an optional ""-n"" argument to ""nodetool join"" to allow bootstrapping from closest live node (n == non-strict). Also recognizes an optional property ""cassandra.join_strict"" which can be set to false when a bootstrap is triggered by cassandra.join_ring.;;;","01/Sep/11 03:30;nickmbailey;Seems to me like the option to stream from the closest replica might just add more confusion without really gaining anything. The node that is leaving the replica set will never be in another datacenter. It could be on a different rack, but if you are following best practices and alternating racks then it is likely either on the same rack or there is only one copy on that rack and the best case possible is streaming from another rack anyway.;;;","01/Sep/11 03:56;thepaul;Judging by the irc channel and user list, assuming people will follow best practices seems a bit of a dead end. Plus, what about the case where the node leaving the replica set is dead? You still want the option to allow choosing another to stream from. And we probably shouldn't default to choosing another without explicit permission, because of the consistency violation stuff.;;;","01/Sep/11 03:56;jbellis;bq. The node that is leaving the replica set will never be in another datacenter

I think that's only strictly true for NTS, but I'm fine leaving it out.  Not worth adding complexity for ONTS at this point.;;;","01/Sep/11 03:57;jbellis;bq. what about the case where the node leaving the replica set is dead

Good point.  We do need something to make that possible.;;;","01/Sep/11 04:02;nickmbailey;Yeah I was assuming ONTS is basically deprecated at this point. Didn't think about the dead case though. I suppose just a 'force' type of option and a warning indicating the possible consistency issues works.;;;","01/Sep/11 04:03;jbellis;As long as we need to handle the dead case I don't see any harm in having a slightly more generally-useful ""use closest"" option instead of ""force to pick random live replica"" option.;;;","01/Sep/11 04:16;nickmbailey;Well, I imagine the 'force' option would pick the nearest live node. By 'force' I mean the option should be posed to the user as ""We can't guarantee consistency in your cluster after this bootstrap since a node is down, if you would like to do this anyway, specify option X"". Just saying you can either bootstrap or bootstrap from the closest node doesn't convey the implications as well I don't think.

Maybe we are on the same page and arguing over wording though.;;;","03/Sep/11 01:10;nickmbailey;Just as initial feedback, I'm not sure we need a new getRangesWithSources method, especially with so much duplication between them. Seems like strict could be passed to the current method. Also, what about leaving getRangesWithSources how it is and passing strict to getWorkMap? That method can do the endpoint set math if it needs to and throw a more informative exception in the case that strict is set and the endpoint we want to fetch from is dead.;;;","03/Sep/11 04:03;thepaul;I did that (passing strict to getWorkMap) at first, but it wasn't too clean since it required adding a 'table' argument as well as 'strict', and it ended up replacing too much of the getRangesWithSources functionality. So then I added 'strict' as a parameter to getRangesWithSources (actually, it looks like I neglected to update its javadoc comment), but the differences between getRangesWithSources and getRangesWithStrictSource are such that a combined method feels a lot more special-casey and clunky. I like this way best in the end.;;;","03/Sep/11 04:21;nickmbailey;Well I guess it kind of depends on which approach we take as well. Is the option A) bootstrap from the right token or bootstrap from the closest token, or B) bootstrap from the right token, but if that one isn't up, bootstrap from any other token preferring the closer ones.

Like I said, I'd say B, but if you and Jonathan both disagree.;;;","03/Sep/11 05:50;thepaul;Yeah. B is probably easier on everyone, but I would say we simply can't do anything that might violate the consistency guarantee without explicit permission from the user.;;;","03/Sep/11 05:55;nickmbailey;Ok, so if we always prefer to bootstrap from the correct token, then I still think we should combine getRangesWithStrictSource and getRangesWithSources. Basically the logic should be, find the 'best' node to stream from. If the user requested it, also find a list of other candidates and order them by proximity. Right?;;;","04/Sep/11 11:38;jbellis;I'm okay with either A or B.

bq. I would say we simply can't do anything that might violate the consistency guarantee without explicit permission from the user

I'm not sure I understand, are you saying that B would violate this, or just that the status quo does?;;;","05/Sep/11 10:05;hanzhu;Is it possible to make the node does not reply to any request before bootstrap and anti-entrophy repair is finished?

This could fix the consistency problem brought by bootstrap.;;;","05/Sep/11 11:09;jbellis;Repair is a much, much more heavyweight solution to the problem than just ""stream from the node that is 'displaced.'"";;;","05/Sep/11 18:16;hanzhu;Sometimes, the node is replaced because the hardware is crashed. If so, ""streaming from the node being replaced"" is not available.

How about force the repair happens if the user specifies he needs the consistency of quorum while the original node has gone.
;;;","05/Sep/11 23:57;thepaul;bq. Ok, so if we always prefer to bootstrap from the correct token, then I still think we should combine getRangesWithStrictSource and getRangesWithSources. Basically the logic should be, find the 'best' node to stream from. If the user requested it, also find a list of other candidates and order them by proximity. Right?

I don't think so. I would still want to leave the option to stream from the closest even if the strict best node is available.;;;","06/Sep/11 00:00;thepaul;bq. I'm not sure I understand, are you saying that B would violate this, or just that the status quo does?

I'm saying B would violate this, yes. B was ""bootstrap from the right token, but if that one isn't up, bootstrap from any other token preferring the closer ones"", right? I'm saying we can't just automatically choose another token if the user didn't specifically say it's ok.;;;","06/Sep/11 00:38;nickmbailey;Paul,

The suggestion was that if the 'correct' node is down, you can force the bootstrap to complete anyway (probably from the closest node, but that is transparent to the user), but only if the 'correct' node is down. It sounds like you agree with Jonathan on the more general approach though.

Zhu,

Repair doesn't help in the case when you lost data due to a node going down. Also if only one node is down you should still be able to read/write at quorum and achieve consistency (assuming your replication factor is greater than 2).;;;","06/Sep/11 03:25;jbellis;bq. I'm saying we can't just automatically choose another token if the user didn't specifically say it's ok.

Oh, ok.  Right.  (I thought we were just bikeshedding over whether to call the ""manual override"" option ""use closest"" or ""force bootstrap."")

bq. Repair doesn't help in the case when you lost data due to a node going down

Additionally, I don't like the idea of automatically doing expensive things like repair; it feels cleaner to not do it automatically, and allow using the existing tool to perform one if desired, than to do it by default and have to add an option to skip it for when that's not desirable.;;;","06/Sep/11 07:19;hanzhu;bq. Also if only one node is down you should still be able to read/write at quorum and achieve consistency

I suppose quorum read plus quorum write should provide monotonic read consistency. [1] Supposing  quorum write on key1 hits node A and node B, not on node C due to temporal network partition. After that node B is replaced by node D since it is down, and node D streams data from node C. If the following quorum read on key1 hits only node C and node D, the monotonic consistency is violated. This is rare but not unrealistic, especially when hint handoff is disabled. 

Maybe it is more resonable to give the admin an option, to specify that the bootstrapped node should not accept any read request until the admin turn it on manually. So the admin can start a manual repair if he wants to assure everything goes fine.

[1]http://www.allthingsdistributed.com/2007/12/eventually_consistent.html;;;","06/Sep/11 09:41;thepaul;bq. The suggestion was that if the 'correct' node is down, you can force the bootstrap to complete anyway (probably from the closest node, but that is transparent to the user), but only if the 'correct' node is down.

Oh, ok. I misunderstood. This seems reasonable. I'd lean for the more general solution, yeah, but I don't feel very strongly about it.;;;","06/Sep/11 10:00;hanzhu;As peter suggested before, another approach to fix the consistency problem is streaming sstables from all alive peers if the ""correct"" node is down. And then leave them to normal compaction.  

This could be much lightweight than anti-entrophy repair, except the network IO pressure on the bootstrapping node.;;;","08/Sep/11 04:08;thepaul;updated patch fixes the docstring for getRangesWithStrictSource().;;;","08/Sep/11 04:11;thepaul;Patch 2434-testery.patch.txt adds a bit to unit tests to exercise o.a.c.dht.BootStrapper.getRangesWithStrictSource().;;;","09/Sep/11 05:57;thepaul;2434-3.patch.txt removes the bits that add the ""-n"" option to nodetool join. Apparently no ""nodetool join"" should ever result in a bootstrap, so it doesn't matter whether the caller wants ""strict"" or not.;;;","14/Sep/11 23:29;jbellis;Do we need to do anything special for move/decommission as well?;;;","15/Sep/11 00:21;nickmbailey;I had a note to remember to create a ticket for that, but if we want to do it here that works as well.

In any case, yes the same concerns exist when giving away ranges as when gaining ranges.;;;","15/Sep/11 01:23;thepaul;bq. Do we need to do anything special for move/decommission as well?

Yes, it looks like we do need to add similar logic for move. Expand the scope of this ticket accordingly?

I don't see any way decommission could be affected by this sort of problem.;;;","15/Sep/11 01:27;thepaul;bq. In any case, yes the same concerns exist when giving away ranges as when gaining ranges.

Oh? I must be missing something. What would a consistency violation failure scenario look like for giving away ranges?;;;","15/Sep/11 01:30;jbellis;bq. Expand the scope of this ticket accordingly?

Yes, let's solve them both here.;;;","15/Sep/11 01:32;nickmbailey;My comment wasn't very clear. Both decom and move currently, attempt to do the right thing. When a node is leaving, there should be one new replica for all the ranges it is responsible for. If it can't stream data to that replica there is a consistency problem.

Both operations currently try to do stream to that replica, but we should use the 'strict' logic in those cases as well and fail if we can't guarantee consistency and the user hasn't disabled strict.;;;","15/Sep/11 01:41;thepaul;If decom can't stream data to the appropriate replica, then it should just fail, right? Do we support decom in cases where a consistency violation would result? Seems like it has to be the user's responsibility to bring up or decom the other node first.

move could introduce a violation when it gains a new range, though, in the same cases as the bootstrap issue explained above.;;;","15/Sep/11 01:47;nickmbailey;If we support it for bootstrapping I don't see why we shouldn't support it for decom. Right, move has the problem in both cases (giving away ranges, gaining ranges).;;;","15/Sep/11 01:53;thepaul;I think we're talking about different things. Requiring the user to have the right nodes available for operation X is not the same as ""cassandra can 'lose' writes when it happens to stream from the wrong node, even if the user did everything right"".

This ticket is about the latter, I think.;;;","15/Sep/11 02:32;thepaul;Conversation on #cassandra-dev resulted in the conclusion that we'll fix this bug for range acquisition (bootstrap and move) now, and plan to allow the same looseness (non-strict mode, or whatever) for range egress (move and decom) in the future.

I think.;;;","20/Sep/11 19:58;jbellis;bq. It's always been unsupported to bootstrap a second node into the same ""token arc"" while a previous one is ongoing.

I'm pretty sure now that this is incorrect; we fixed it back in CASSANDRA-603.  I'm updating the comments in TokenMetadata as follows:

{noformat}
    // Prior to CASSANDRA-603, we just had <tt>Map<Range, InetAddress> pendingRanges<tt>,
    // which was added to when a node began bootstrap and removed from when it finished.
    //
    // This is inadequate when multiple changes are allowed simultaneously.  For example,
    // suppose that there is a ring of nodes A, C and E, with replication factor 3.
    // Node D bootstraps between C and E, so its pending ranges will be E-A, A-C and C-D.
    // Now suppose node B bootstraps between A and C at the same time. Its pending ranges
    // would be C-E, E-A and A-B. Now both nodes need to be assigned pending range E-A,
    // which we would be unable to represent with the old Map.  The same thing happens
    // even more obviously for any nodes that boot simultaneously between same two nodes.
    //
    // So, we made two changes:
    //
    // First, we changed pendingRanges to a <tt>Multimap<Range, InetAddress></tt> (now
    // <tt>Map<String, Multimap<Range, InetAddress>></tt>, because replication strategy
    // and options are per-KeySpace).
    //
    // Second, we added the bootstrapTokens and leavingEndpoints collections, so we can
    // rebuild pendingRanges from the complete information of what is going on, when
    // additional changes are made mid-operation.
    //
    // Finally, note that recording the tokens of joining nodes in bootstrapTokens also
    // means we can detect and reject the addition of multiple nodes at the same token
    // before one becomes part of the ring.
    private BiMap<Token, InetAddress> bootstrapTokens = HashBiMap.create();
    // (don't need to record Token here since it's still part of tokenToEndpointMap until it's done leaving)
    private Set<InetAddress> leavingEndpoints = new HashSet<InetAddress>();
    // this is a cache of the calculation from {tokenToEndpointMap, bootstrapTokens, leavingEndpoints}
    private ConcurrentMap<String, Multimap<Range, InetAddress>> pendingRanges = new ConcurrentHashMap<String, Multimap<Range, InetAddress>>();
{noformat}
;;;","20/Sep/11 23:55;thepaul;bq. I'm pretty sure now that this is incorrect;

Well, doh. That puts us back at my first question:

bq. So, it looks like it will be possible for the node-that-will-be-removed to change between starting a bootstrap and finishing it (other nodes being bootstrapped/moved/decom'd during that time period); in some cases, that could still lead to a consistency violation. Is that unlikely enough that we don't care, here? At least the situation would be better with the proposed fix than it is now.;;;","21/Sep/11 00:01;jbellis;Can you give an example for illustration?;;;","21/Sep/11 10:48;nickmbailey;Ok, so I think there are really two consistency issues here. Firstly, picking the 'right' node to stream data to/from when making changes to the ring. Secondly, disallowing concurrent changes that have overlapping ranges.

Currently we only disallow nodes from moving/decommissioning when they may potentially have data being streamed to them. There are a few examples of things we currently allow which I think are generally a bad idea.

1) Say you have nodes A and D, if you bootstrap nodes B and C at the same time in between A and D, it may turn out that the correct node to stream from for both nodes is D. Now say node C finishes bootstrapping before node B. At that point, the correct node for B to bootstrap from is technically C, although D still has the data. However, since D is no longer technically responsible for the data, the user could run cleanup on D and delete the data that B is attempting to stream.

2) The above case is also a problem when you bootstrap a node and the node it decides it needs to stream from is moving. Once that node finishes moving you could run cleanup on that node and delete data that the bootstrapping node needs. In this case, all documentation indicates you should do a cleanup after a move in order to remove old data, so it seems possibly more likely.

3) A variation of the above case is when you bootstrap a node and the node it streams from is leaving. In that case the decom may finish and the user could terminate the cassandra process and/or node breaking any streams. Not to mention the idea of a node in a decommissioned state continuing to stream seems like a bad idea. I believe it would work currently, but I'm not sure and it seems likely to break.

I can't really think of any other examples but I think thats enough to illustrate that overlapping concurrent ring changes are a bad idea and we should just attempt to prevent them in all cases. An argument could be made that this would prevent you from doubling your cluster (the best way to grow) all at once, but I don't think that's really a huge deal. At most you would need RF steps to double your cluster.;;;","22/Sep/11 02:21;thepaul;I think we can still allow overlapping concurrent ring changes, with the right set of invariants and/or operational rules. I've been trying to work on defining those. It's pretty tricky but I think I have the right way of approaching the model now. Will update later today with more.

Nick is right though, c* probably shouldn't support overlapping changes as is. Think bootstrapping >N nodes between the same two old nodes, decom'ing one node and bootstrapping another in such a way that the bootstrap source stream node becomes the wrong source, etc.;;;","22/Sep/11 02:30;nickmbailey;So the fact that the 'correct' bootstrap source switches mid stream isn't really the problem I don't think. We set up pending ranges so that when a node gets ready to bootstrap, writes start getting duplicated to both the currently correct replica, and the bootstrapping node. Since all new writes are duplicated we can stream from that node and as long as we get the entire dataset, consistency should be fine. The problem is there is nothing in place preventing someone from running cleanup or killing a decommed node or something.

I'm doubtful that the complexity of a correct set of rules for allowing overlapping ring changes is really worth the time/effort/fragility. It doesn't seem like that much of a loss to me to disallow them. Perhaps your set of rules will be super simple though :).;;;","22/Sep/11 02:40;thepaul;Right, I know how the pending-ranges writes work. But there are still possible openings for consistency violations for previously-written data, similar to the one outlined in the original ticket description. If the bootstrap stream source has an outdated value and it gets duplicated to a bootstrapping node, but then the bootstrap stream source doesn't leave the replication set (because something else changed in the interim), the outdated value is now on more nodes than it used to be- possibly now QUORUM, when previously it was safely below.;;;","23/Sep/11 05:01;thepaul;
Ok, prospective approach to totally safe range movements:

Operational rules:
* Cassandra will not allow two range motion operations (move, bootstrap, decom) at the same time on the same node.
* When a range motion operation is already pending, User should refrain from starting another range motion operation (if either motion operation overlaps the arc-of-effect of the other) until the gossip info about the first change has propagated to all affected nodes. (This is more simply approximated by the ""two minute rule"".)
* Every point in the tokenspace has the same number of natural endpoints, and they're ordered the same from the perspective of all nodes (is this an ok assumption?).
* It is User's responsibility to make sure that the right streaming source nodes are available. If they're not, the range motion operation may fail.

Procedure:
* For any motion involving range _R_, there will be a stream from endpoint _EP_source_ to endpoint _EP_dest_. Given the same information about what range motion operations are pending (_TokenMetadata_) and the range _R_, there is a bijection from _EP_source_ to _EP_dest_, shared by all nodes in the ring.
* Procedure to determine _EP_source_ from _EP_dest_:
** Let _REP_current_ be the existing (ordered) list of natural endpoints for _R_.
** Let _TM_future_ be a clone of the current _TokenMetadata_, but with all ongoing bootstraps, moves, and decoms resolved and completed.
** Let _REP_future_ be the list of (ordered) natural endpoints for _R_ according to _TM_future_.
** Let _EPL_entering_ be the list of endpoints in _REP_future_ which are not in _REP_current_ (preserving their order in _REP_future_).
** Let _EPL_leaving_ be the list of endpoints in _REP_current_ which are not in _REP_future_ (preserving their order in _REP_current_).
** _EPL_entering_ and _EPL_leaving_ are of the same length.
** Let _Pos_ be the position/index of _EP_dest_ in _EPL_entering_.
** Let _EP_source_ be the endpoint at position _Pos_ in _EPL_leaving_.
* Intuitively, this is the same as the rule expressed earlier in this ticket (stream from the node you'll replace), but also handles other ongoing range movements in the same token arc.
* These rules can be pretty trivially inverted to determine _EP_dest_ from _EP_source_.
* When any node gets gossip about a range motion occurring with its token arc-of-effect, it calculates (or recalculates) the streams in which it should be involved. Any ongoing streams which are no longer necessary are canceled, and any newly necessary streams are instigated.

I tried to construct a ruleset without that last rearrange-ongoing-streams rule, but it ended up with a pretty complicated set of extra restrictions, and a more complicated set of procedures than this.

This set of rules might look complicated, but I think it should be fairly straightforward to implement, and may even end up simpler overall than our current code.

Note that this procedure even maintains the consistency guarantee in cases like:

* In an RF=3 cluster with nodes A, E, and F, bootstrap B, C, and D in quick succession (E streams to B, F streams to C, A streams to D)
* In an RF=3 cluster with nodes A, C, and E, bootstrap B, D, and F, and decommission A, C, and E, all in quick succession (A streams to B, C streams to D, E streams to F)
* In an RF=3 cluster with nodes A, B, C, D, and E, decommission B and C in quick succession (B streams to D, C streams to E);;;","23/Sep/11 06:13;thepaul;Clarification: the cleanup operation would need to consider pending ranges in addition to the current natural range-endpoint mapping.

It would be possible with this proposal for a node _M_ to be streaming range _S_ to a bootstrapping node _Y_, and midway, for _M_ to stop being part of the replication set for _S_ (perhaps some other bootstraps nearby completed first). This should be ok for consistency, but a cleanup operation on _M_ while the stream is ongoing could potentially remove all the data in _S_ unless this change is made.

Further clarification: instead of the W+1 special case we now have for writing to a range _T_ with a pending motion, we would need to write to W+C replicas instead, where C is the number of pending motions within the replication set of _T_.;;;","17/Nov/11 06:12;jbellis;Coming back to this after the 1.0 scramble.

It sounds like you have a reasonable solution here, is there any reason not to implement it for 1.1?;;;","17/Nov/11 07:21;thepaul;bq. It sounds like you have a reasonable solution here, is there any reason not to implement it for 1.1?

Just that it's quite a bit more complex than simply disallowing overlapping ring movements, and the extra problems that come with higher complexity. I think this feature is worth it, on its own, but when i think of how much pain Brandon seems to be going through dealing with streaming code, maybe it's not.;;;","17/Nov/11 07:41;jbellis;No longer very optimistic on the ""may even end up simpler overall than our current code"" front?

TBH this area of the code is fragile and hairy and maybe starting from a clean slate with a real plan instead of trying to patch things in haphazardly would be a good thing.

But, I'd be okay with re-imposing the ""no overlapping moves"" rule and fixing the stream source problem if that's going to be substantially simpler.;;;","18/Nov/11 00:26;thepaul;No, I do think that if we tore out the existing code and replaced it, it would be simpler overall, but (a) that would probably also be true if we rewrote the existing code without implementing this; (b) it will be rather a lot of work; and (c) it may engender a whole new generation of subtle corner-case bugs (or maybe it will eliminate a lot of such bugs that already exist).;;;","18/Nov/11 00:52;jbellis;How much work would it be to add ""just one more bandaid"" for the stream source thing, in comparison?;;;","18/Nov/11 01:06;thepaul;Do you mean to implement the new ""safe range movements"" procedure outlined above, without rewriting the rest of the range movement code?

If so, I submit a SWAG of ""1/3-1/2 the cost of the rewrite option"". The bandaid would still touch a lot of different moving parts.;;;","29/Nov/11 04:01;jbellis;So either way, a substantial amount of work, but the bandaid still leaves us with rules that the operator must enforce or hit subtle problems.

My hang-up with the bandaid is the part where C* can't effectively enforce the guidelines to be safe.  Even ""wait X seconds between bootstrap operations"" is not a prereq I am comfortable with.

Unless the above is incorrect, I think we should bite the bullet and fix it ""right."";;;","08/Feb/12 12:16;thepaul;So, I believe that the rules outlined above can still work without the ""wait X seconds between bootstrap operations"" prereq, if a pretty simple extra step is added:

If any node learns about conflicting move operations, then some rules are applied to choose which will be honored and which will return an error to its caller (if still possible).

Those rules are:
* A decom for node X beats a move or bootstrap for node X
* Two decoms for node X from coordinator nodes Y and Z: the coordinator with the higher token wins
* Any other conflicts between move/bootstrap operations for the same node (which can arise in certain partition situations) are easily resolved by latest VersionedValue.

This should guarantee convergence of TokenMetadata across any affected parts of a cluster.;;;","10/Feb/12 05:44;jbellis;Sounds reasonable.;;;","03/May/12 01:32;jbellis;As a half measure, we can stream from the ""right"" node very easily if we continue to make the simplifying assumption that no other node movement happens in overlapping ranges during the operation.;;;","20/Feb/14 01:31;tjake;I've taken a crack at this, initially for 1.2 since it solves my pain. Appreciate a review.

As [~jbellis] mentions above it requires only one node to be added at a time. Also bootstrapping node must add -Dconsistent.bootstrap=true

#code
https://github.com/tjake/cassandra/tree/2434

#dtest showing it works (use ENABLE_VNODES=yes)
https://github.com/tjake/cassandra-dtest/tree/2434

;;;","20/Feb/14 02:33;jbellis;([~thobbs] to review);;;","20/Feb/14 05:05;thobbs;Thanks, Jake.

I strongly prefer to default to the strict/safe behavior and make the user supply a ""force"" option for non-strict behavior, like Nick and Paul agreed on above.  If the bootstrapping node cannot stream from the correct replica and the ""force"" option isn't set, it should abort the bootstrap with an error that describes the implications and mentions how to use the ""force"" option.

Additionally, I think your logic for picking the preferred replica could be greatly simplified.  Paul's 2434-3.patch.txt has a really simple version of this and also has the strict-by-default behavior.  It might be worthwhile to look at rebasing that patch as a start.

Paul mentioned this:

bq. Conversation on #cassandra-dev resulted in the conclusion that we'll fix this bug for range acquisition (bootstrap and move) now, and plan to allow the same looseness (non-strict mode, or whatever) for range egress (move and decom) in the future.

Looking at the irc logs, there wasn't a strong reason for this.  There's a lot of code overlap there, so it would be ideal to fix both types of operations at once.  Do you think you could take a stab at that?;;;","20/Feb/14 06:29;tjake;bq. Additionally, I think your logic for picking the preferred replica could be greatly simplified. Paul's 2434-3.patch.txt has a really simple version of this and also has the strict-by-default behavior. It might be worthwhile to look at rebasing that patch as a start.

I did look at the patch and I'll see how I can simplify my version.  Most of the complexity comes from multiple ranges living on the same address (vnodes). Which the old version didn't have to worry about.

I do think we can fix the other operations but those are less of a priority IMO and should be part of a follow up.  (does anyone use move with vnodes?)  

;;;","20/Feb/14 06:39;brandon.williams;bq. does anyone use move with vnodes?

No, but now they can use relocate (taketoken in nodetool);;;","21/Feb/14 02:57;tjake;Pushed an update to https://github.com/tjake/cassandra/tree/2434 that addresses the comments.  I'm going to work on support for move/relocate.;;;","21/Feb/14 04:41;tjake;Do we care about decommissions?  It seems when we ""push"" data to other nodes there isn't anything todo.  Only when we pick the replica to stream from does this ticket apply;;;","21/Feb/14 05:27;thobbs;bq. Do we care about decommissions? It seems when we ""push"" data to other nodes there isn't anything todo. Only when we pick the replica to stream from does this ticket apply

I checked that {{StorageService.getChangedRangesForLeaving()}} pushes to the correct nodes (those that are gaining a range), so you're right, we don't need to do anything new for decom.;;;","21/Feb/14 06:22;tjake;Updated branches with move/relocate support and added a dtest for move (in dtest branch linked above);;;","21/Feb/14 06:33;brandon.williams;We should probably add a test for relocate too since it's fundamentally different from move.;;;","22/Feb/14 04:12;thobbs;I tested bootstrapping a node while the preferred replica was down.  It turns out that CASSANDRA-6385 makes the bootstrapping node consider the replica up for long enough to pass the checks.  It looks like we need to special case the 6385 behavior for bootstraps if we want this patch to work.;;;","22/Feb/14 04:24;brandon.williams;You can test this now by setting cassandra.fd_initial_value_ms.;;;","22/Feb/14 08:42;thobbs;Okay, with the workaround on the FD, bootstrap seems to work.  Do we want to split that fix into a separate ticket?

However, relocate seems to be seriously broken.  With a three node cluster and one of the nodes down, I can make relocate fail in a couple of ways:
* {{oldEndpoints}} == {{newEndpoints}}, so the assertion that the difference between them has length 1 fails
* There are no ranges that contain the ""desiredRange"", resulting in the IllegalStateException being thrown (""No sources found for "" + toFetch);

With that said, nothing (including the tools) uses relocate.  (EDIT: shuffle uses it, but nobody uses shuffle in practice due to other problems.) The JMX version doesn't work with jconsole, so I had to add a method to test this.  I'm not even sure that relocate worked before this patch for vnodes, because there's only minimal test coverage for relocate.  IMO, we shouldn't even try to modify this without good test coverage.  But if nothing even uses relocate... I'm not sure what to do.  Thoughts?;;;","27/Feb/14 02:09;tjake;I will try working on a relocate test and look at addressing this;;;","14/Mar/14 21:22;tjake;What version should this go into?  I personally need this for 1.2 but I would put it in with the default of non-strict.;;;","19/Mar/14 03:45;thobbs;[~tjake] I think we want 2.1 with a default of strict.;;;","30/Apr/14 03:37;tjake;Rebased to 2.1 branch and pushed to https://github.com/tjake/cassandra/tree/2434-2

Also added a relocation dtest https://github.com/tjake/cassandra-dtest/tree/2434 ;;;","01/May/14 05:57;thobbs;+1 overall, with a few minor nitpicks on the dtest:

You can replace that string-building loop with:
{noformat}
tl = "" "".join(str(t) for t in tokens[0][:8])
{noformat}

There's also a leftover line: {{#assert 1 == 0}}

Don't forget to mark the new tests with a {{@since('2.1')}} decorator.;;;","01/May/14 21:52;tjake;Committed c* code , I'll push to dtests now;;;","22/Jul/14 05:18;kohlisankalp;This will be very nice to have in 2.0
cc [~brandon.williams];;;","22/Jul/14 05:44;brandon.williams;Does seem like mostly new code we could just add with the flag defaulting to off to give 2.0 the option.;;;",,,,
"getColumnFamily() return null, which is not checked in ColumnFamilyStore.java scan() method, causing Timeout Exception in query",CASSANDRA-2401,12502672,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,karshiang,karshiang,29/Mar/11 15:15,16/Apr/19 17:33,22/Mar/23 14:57,14/May/11 23:03,0.7.6,,,,0,,,,,,"ColumnFamilyStore.java, line near 1680, ""ColumnFamily data = getColumnFamily(new QueryFilter(dk, path, firstFilter))"", the data is returned null, causing NULL exception in ""satisfies(data, clause, primary)"" which is not captured. The callback got timeout and return a Timeout exception to Hector.

The data is empty, as I traced, I have the the columns Count as 0 in removeDeletedCF(), which return the null there. (I am new and trying to understand the logics around still). Instead of crash to NULL, could we bypass the data?

About my test:
A stress-test program to add, modify and delete data to keyspace. I have 30 threads simulate concurrent users to perform the actions above, and do a query to all rows periodically. I have Column Family with rows (as File) and columns as index (e.g. userID, fileType).

No issue on the first day of test, and stopped for 3 days. I restart the test on 4th day, 1 of the users failed to query the files (timeout exception received). Most of the users are still okay with the query.
","Hector 0.7.0-28, Cassandra 0.7.4, Windows 7, Eclipse",cburroughs,cdaw,cywjackson,karshiang,rjtg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/May/11 01:12;jbellis;2401-v2.txt;https://issues.apache.org/jira/secure/attachment/12478298/2401-v2.txt","06/May/11 01:31;jbellis;2401-v3.txt;https://issues.apache.org/jira/secure/attachment/12478303/2401-v3.txt","05/May/11 01:47;jbellis;2401.txt;https://issues.apache.org/jira/secure/attachment/12478188/2401.txt",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20603,,,Sat May 14 15:03:01 UTC 2011,,,,,,,,,,"0|i0gb5r:",93238,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"29/Mar/11 21:35;jbellis;Are you querying for zero columns?;;;","30/Mar/11 00:19;karshiang;Hi, nope. 

It is a query for 4 columns. 

I cheked that only 1 row has this problem (no column found), out of the 948 records returned; I skipped the row with zero columns. 

In my stress-test, all rows have 4 columns; i.e. row is the file, the 4 columns (index) are like its version, modified time, type, etc. I added all the columns when added each file. The addition should be working since there is no such exception on day 1, and I start and stop the stress tests until each users have around 1500 files. Row with 0 column only found on the 4th day after I continue to run it.

I will keep picking up cassandra logics, as I have little understanding about how data loaded, stored and deleted. Any suggestion / guide on how I should go on with my study is greatly appreciated. Thank you!

Btw, for this test, I have not yet going to 2 nodes / 3 nodes. It is only a single-node cassandra runnning on my localhost.
;;;","30/Mar/11 00:32;jbellis;Is there any data from earlier than 0.7.4?;;;","30/Mar/11 09:15;karshiang;Hi, 
This is a clean 0.7.4 setup, with zero data to start with. Dynamically, the keyspace schema is creted on the run, when required keyspace does not exist.;;;","30/Mar/11 10:18;karshiang;Hi,

New finding here:
For the 0-column data, it is because it is never read from the file. As I step through the line, here it returns -1 position from org.apache.cassandra.io.sstable.SSTableReader.java::getPosition(DecoratedKey decoratedKey, Operator op), line 448 (bf.isPresent(decoratedKey.key) is returning false) - key is missing.

There seem to be a missing record which is indexed or indexed column itself not updated when the record is removed (?). 

As for the data returned with 0-column, simply because a container is always created (final ColumnFamily returnCF = ColumnFamily.create(metadata)) and returned from getTopLevelColumns even if there is no read taken.

As for this case, it causes Timeout exception to Hector when null exception thrown without captured.;;;","04/Apr/11 02:55;rjtg;can you provide some unit tests that reproduce your error? i'd like to look into it, but i am not sure whether i understand the issue correctly.;;;","05/Apr/11 17:42;karshiang;Hi Roland,

Sure, as we are trying to do that. In the mean time, I would like to update you more about our findings:
We built a test case on the PC with the existing DB and to produce same issue, without hector API. The test case works (able to create null exception) on the original PC. 

java.lang.NullPointerException
	at org.apache.cassandra.db.ColumnFamilyStore.satisfies(ColumnFamilyStore.java:1787)
	at org.apache.cassandra.db.ColumnFamilyStore.scan(ColumnFamilyStore.java:1727)
	at TestScan.main(TestScan.java:74)

line 1787: IColumn column = data.getColumn(expression.column_name); where data is NULL


Zipping the 0.7.4 cassandra data to another new PC gives the same issue, but the missing key order may slightly different, e.g. on original PC it is at 430th, on the new PC it is 431th. Both keys appears to be same though (content in ByteBuffer).
(Edited: the new PC also found the problem - which makes more sense)

We will continue to check if it is due to the ""if (column.isMarkedForDelete())"" is not working on the PC with have the null encountered. Since we checked that, both PCs have the same number of columns returned in ""scan"" method at line ""ColumnFamily indexRow = indexCFS.getColumnFamily(indexFilter);"", where ""indexRow.getColumnCount()"" both giving 1996, with some rows already deleted as tombstones. 
;;;","05/Apr/11 17:47;karshiang;Some information i missed in update:
In the PC with NULL exception, I do a continue when found ""data"" is null, and ignore that. I will get 1040 columns returned. On the 2nd (new) PC, without the NULL exception nor additional code to bypass null data, it is getting 1040 records as well. From here, we will study more our DB to find out where it went wrong/different.;;;","05/Apr/11 19:33;rjtg;Sounds As if the Index is still pointing to deleted entriss. ;;;","05/Apr/11 22:23;karshiang;hi, yes. it seems to me so. Here, we create a table ""FileMap"", in which we store columns e.g. ""content"", ""authorID"", ""Version"", ""Modified Time"", ""File Type"", etc. Among them, sorted indices are ""authorID"" (as UserIndex), ""File Type"", ""Modified Time"", and ""CassType""; where CassType means generally 'file type' here in our case. It is not used though.

{{03/30/2011  09:34 AM        11,366,878 FileMap-f-53-Data.db}}
{{03/30/2011  09:34 AM            78,496 FileMap-f-53-Filter.db}}
{{03/30/2011  09:34 AM           735,930 FileMap-f-53-Index.db}}
{{03/30/2011  09:34 AM             4,264 FileMap-f-53-Statistics.db}}
{{03/30/2011  05:37 PM             4,055 FileMap-f-54-Data.db}}
{{03/30/2011  05:37 PM                40 FileMap-f-54-Filter.db}}
{{03/30/2011  05:37 PM               270 FileMap-f-54-Index.db}}
{{03/30/2011  05:37 PM             4,264 FileMap-f-54-Statistics.db}}
{{04/04/2011  04:07 PM            24,068 FileMap-f-55-Data.db}}
{{04/04/2011  04:07 PM               200 FileMap-f-55-Filter.db}}
{{04/04/2011  04:07 PM             1,746 FileMap-f-55-Index.db}}
{{04/04/2011  04:07 PM             4,264 FileMap-f-55-Statistics.db}}
{{04/04/2011  04:07 PM           961,808 FileMap.CassTypeIndex-f-53-Data.db}}
{{04/04/2011  04:07 PM             1,936 FileMap.CassTypeIndex-f-53-Filter.db}}
{{04/04/2011  04:07 PM                11 FileMap.CassTypeIndex-f-53-Index.db}}
{{04/04/2011  04:07 PM             4,264 FileMap.CassTypeIndex-f-53-Statistics.db}}
{{03/29/2011  02:52 PM           961,386 FileMap.FileTypeIndex-f-50-Data.db}}
{{03/29/2011  02:52 PM             1,936 FileMap.FileTypeIndex-f-50-Filter.db}}
{{03/29/2011  02:52 PM                11 FileMap.FileTypeIndex-f-50-Index.db}}
{{03/29/2011  02:52 PM             4,264 FileMap.FileTypeIndex-f-50-Statistics.db}}
{{03/30/2011  05:37 PM               404 FileMap.FileTypeIndex-f-51-Data.db}}
{{03/30/2011  05:37 PM                16 FileMap.FileTypeIndex-f-51-Filter.db}}
{{03/30/2011  05:37 PM                11 FileMap.FileTypeIndex-f-51-Index.db}}
{{03/30/2011  05:37 PM             4,264 FileMap.FileTypeIndex-f-51-Statistics.db}}
{{04/04/2011  04:07 PM             2,358 FileMap.FileTypeIndex-f-52-Data.db}}
{{04/04/2011  04:07 PM                16 FileMap.FileTypeIndex-f-52-Filter.db}}
{{04/04/2011  04:07 PM                11 FileMap.FileTypeIndex-f-52-Index.db}}
{{04/04/2011  04:07 PM             4,264 FileMap.FileTypeIndex-f-52-Statistics.db}}
{{03/29/2011  02:52 PM         3,298,947 FileMap.ModifiedIndex-f-50-Data.db}}
{{03/29/2011  02:52 PM            78,016 FileMap.ModifiedIndex-f-50-Filter.db}}
{{03/29/2011  02:52 PM           731,106 FileMap.ModifiedIndex-f-50-Index.db}}
{{03/29/2011  02:52 PM             4,264 FileMap.ModifiedIndex-f-50-Statistics.db}}
{{03/30/2011  05:37 PM             2,065 FileMap.ModifiedIndex-f-51-Data.db}}
{{03/30/2011  05:37 PM                64 FileMap.ModifiedIndex-f-51-Filter.db}}
{{03/30/2011  05:37 PM               450 FileMap.ModifiedIndex-f-51-Index.db}}
{{03/30/2011  05:37 PM             4,264 FileMap.ModifiedIndex-f-51-Statistics.db}}
{{04/04/2011  04:07 PM            13,835 FileMap.ModifiedIndex-f-52-Data.db}}
{{04/04/2011  04:07 PM               328 FileMap.ModifiedIndex-f-52-Filter.db}}
{{04/04/2011  04:07 PM             3,006 FileMap.ModifiedIndex-f-52-Index.db}}
{{04/04/2011  04:07 PM             4,264 FileMap.ModifiedIndex-f-52-Statistics.db}}
{{04/04/2011  04:07 PM           962,874 FileMap.UserIndex-f-53-Data.db}}
{{04/04/2011  04:07 PM             1,936 FileMap.UserIndex-f-53-Filter.db}}
{{04/04/2011  04:07 PM               420 FileMap.UserIndex-f-53-Index.db}}
{{04/04/2011  04:07 PM             4,264 FileMap.UserIndex-f-53-Statistics.db}}

In the search, we are using IndexClause as:
		ByteBuffer field_author = ByteBuffer.wrap(new byte[]{'a'});
		ByteBuffer author_1 = IntegerSerializer.get().toByteBuffer(1);
		
		ByteBuffer file_type = ByteBuffer.wrap(new byte[]{'t'});
		ByteBuffer filetype_3 = ByteBuffer.wrap(new byte[]{3}); //file type 3
		
		IndexClause indexClause = new IndexClause();
		indexClause.setCount(3000);
		ArrayList<IndexExpression> expressions = new ArrayList();
		expressions.add(new IndexExpression(field_author, IndexOperator.EQ, author_1)); //user ID = 1
		expressions.add(new IndexExpression(file_type, IndexOperator.EQ, filetype_3)); //file type = 3
		
		indexClause.setExpressions(expressions);
		indexClause.setStart_key(new byte[]{});

}}
In the search, it scans all the indices from ""FileMap.UserIndex"", within which there seems having a key (index) which is not found in the table ""FileMap""; and I roughly get that it breaks at data retrieval with ""FileMap-f-53-Data"", when the position for the key is not found / available in ""FileMap-f-53-Data"".;;;","19/Apr/11 10:46;jbellis;So when you created the data, you did not use any expiring columns (TTL), correct?;;;","19/Apr/11 10:52;jbellis;The more I think about it, the more I think that there is a rare race condition here -- we do a kind of row lock during updates of indexed data, but we do not lock during reads. So it's possible for an index read to say ""row X has this value"" and then have that value deleted (by another client's request) before we can read row X.

BUT that does not look like what you are seeing because if I understand correctly you are seeing that the index has permanently missed a delete operation.;;;","19/Apr/11 11:03;karshiang;hi Jon,

sorry for less updates for past few days as we were busy on other tasks. We are thinking to stress-test with 0.7.5 when it is out.

In the test, we have all operations e.g. ""insert, replace, and delete"". If not wrong, we have simulated 20-users to run concurrently, however, they likely not able to delete key of different user. I think there is no such a case when 1 user is modifying his record, when another user deleting the record.

There is no expiring columns (TTL) in this test.

Same data on another PC will able to give the same exception, though we found the index position (n variable) can be shifted by 1 or 2.

Thanks, Jon and your team for the gd work!;;;","19/Apr/11 11:11;karshiang;Hi Jon,

Allow me to add more information:

Each simulated user thread will do the following in repeatitive manner:

loop = 0;
while( running )
{
    if( loop % 5 ==0 ) { list all files in folder; }

    create around 4~10 files but cap the total files around 2000 files only.
    modified around 20 files;
    delete 1~4 files;

    loop ++;
}

The ""list all files in folder"" is the scan action, where it will later for 1 or 2 users giving us ""no file"" in return after the next few days when restarted the same test, without resetting data. Found out it is due to the issue above. ;;;","19/Apr/11 21:36;rjtg;i just looked a little closer at your index expressions again.
If i understand them correctly they are subject to https://issues.apache.org/jira/browse/CASSANDRA-2347
Although i don't really think it is the issue you are describing it would be nice if you could apply the patch and see if the error still occurs.

You are creating the bytebuffers for author_id and file_type in a different way. Is this a mistake? ;;;","20/Apr/11 11:46;karshiang;Hi Roland,

The 'mistake' is intended by reusing some Hector API code.

Hector has a Integer Serializer, which will generate 4-byte[] from given integer. The file_type is a 1-byte array. It is to produce exact effected client call into a test case, solely running cassandra. ;;;","20/Apr/11 12:24;karshiang;for issue: https://issues.apache.org/jira/browse/CASSANDRA-2347, I suspect we encountered that in another case. It has a validation failure at times.

I applied the change. As expected, the error is still there, data is missing or indexed key extra then throw NULL exception out.;;;","05/May/11 01:47;jbellis;I found *a* bug that could cause this: Cassandra will re-create a deleted index entry if it gets a write with an obsolete timestamp, but the data row tombstone will correctly suppress an update there. (So when you do an index query for value=X, and the index says ""row K has that value,"" then you get an error trying to read row K that doesn't exist.)

I don't think this is the bug Tey Kar is hitting, though, because unless I'm mistaken you won't get this NPE until after the data row tombstone is removed by compaction after gc_grace_seconds.  4 days isn't enough to see that unless you've tweaked gc_g_s.

Still, it's worth fixing.  Patch attached.  (Also adds an assert w/ more information if/when another way of triggering this is found.);;;","05/May/11 06:07;cywjackson;I have an existing data that was resulting similar NPE  before the patch. After applying the patch, the following observed:

{noformat}
DEBUG [ReadStage:82] 2011-05-04 21:23:27,114 ColumnFamilyStore.java (line 1514) fetched data row ColumnFamily(inode -deleted at 1304363600008- [70617468:false:49@1304363600219,])
DEBUG [ReadStage:82] 2011-05-04 21:23:27,114 ColumnFamilyStore.java (line 1532) row ColumnFamily(inode -deleted at 1304363600008- [70617468:false:49@1304363600219,]) satisfies all clauses
DEBUG [ReadStage:82] 2011-05-04 21:23:27,115 ColumnFamilyStore.java (line 1514) fetched data row ColumnFamily(inode [70617468:false:10@1304353355296,])
ERROR [ReadStage:82] 2011-05-04 21:23:27,115 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[ReadStage:82,5,main]
java.lang.AssertionError: No data found for NamesQueryFilter(columns=java.nio.HeapByteBuffer[pos=12 lim=16 cap=17]) in DecoratedKey(29842926756667498147838693957802723793, 3134346637326336393966396130336561376538623330316566383561616131):QueryPath(columnFamilyName='inode', superColumnName='null', columnName='null') (original filter NamesQueryFilter(columns=java.nio.HeapByteBuffer[pos=12 lim=16 cap=17])) from expression 73656e74696e656cEQ78
    at org.apache.cassandra.db.ColumnFamilyStore.scan(ColumnFamilyStore.java:1512)
    at org.apache.cassandra.service.IndexScanVerbHandler.doVerb(IndexScanVerbHandler.java:42)
    at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
{noformat}

was the fix intend to avoid future problem, as such existing problem would need a workaround solution?;;;","05/May/11 06:48;jbellis;bq. was the fix intend to avoid future problem

yes.  as discussed above, once you corrupt your index this way the corruption is recorded permanently and you need to drop the index and recreate it.;;;","05/May/11 23:19;slebresne;Comments on the patch:
  * ignoreObsoleteMutation() now forgot to actually remove the obsolete mutation from cf.
  * not sure why mutatedIndexColumns need to be concurrent. There is no concurrency in ignoreObsoleteMutation, is there ?
  * really minor: change to debug log ""Scanning index row %s ..."" seems misleading since the first argument is not a row name.

Other than that, I do agree with you that there is quite probably a race between reads and concurrent writes. But also agree that it doesn't seem to be the problem here;;;","05/May/11 23:54;jbellis;bq. ignoreObsoleteMutation() now forgot to actually remove the obsolete mutation from cf

this isn't actually necessary, though, since if it's taken out of the list of mutated index columns the obsolete columns will only be applied to the ""main"" data row, and including obsolete columns there is harmless.

bq. not sure why mutatedIndexColumns need to be concurrent

because we might remove from the collection while iterating over it.  treeset will throw concurrentmodificationexception.  but maybe iterator.remove would work, now that you mention it?;;;","06/May/11 00:35;slebresne;bq. this isn't actually necessary, though, since if it's taken out of the list of mutated index columns the obsolete columns will only be applied to the ""main"" data row, and including obsolete columns there is harmless.

Very true.

bq. but maybe iterator.remove would work

I think it will;;;","06/May/11 01:12;jbellis;v2 attached w/ iterator/Set change.

bq. change to debug log ""Scanning index row %s ..."" seems misleading since the first argument is not a row name

it actually is the same CF+row as before, I just encapsulated it in getExpressionString so I can re-use the method in case of assertion failure later. Tweaked format a bit in v2, here's an example debug output:

{noformat}
Scanning index 'world2 EQ 15' starting with
{noformat};;;","06/May/11 01:20;slebresne;+1 v2;;;","06/May/11 01:35;jbellis;Oops, that's actually column + value, not CF.

For the record, v3 adds CF:
{noformat}
Scanning index 'CF1.world2 EQ 15' starting with
{noformat}

Will commit based on v2 +1.;;;","06/May/11 02:14;hudson;Integrated in Cassandra-0.7 #470 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/470/])
    improve ignoring of obsoletemutations in index maintenance
patch by jbellis; reviewed by slebresne for CASSANDRA-2401
;;;","08/May/11 07:40;cywjackson;Here is 1 way that i could 100% reproduce the issue with data being null:

Need 2 nodes, 1 is gonna to autobootstrap to the other. Also assuming completely clean start (blow up the /var/lib/cassandra/ or where ever data are stored

i am also using brisk beta to test

to start:
node-A:
1) get brisk
2) start brisk  with -t (jobtracker)
3) run a simple hive query : 
 3a) bin/brisk hive 
 3b) create table foo (bar INT);
 3c) select count(*) from foo;
 3d) exit;
4) every thing should be so far so good, let the brisk node continue to be up

node-B:
1) get brisk
2) modify the resources/cassandra/conf/cassandra.yaml:
 2a) to enable autobootstrap. 
 2b) point seeds to node-A

3) put a sleep or break point in o.a.c.service.StorageService.joinTokenRing method, right after ""Map<InetAddress, Double> loadinfo = StorageLoadBalancer.instance.getLoadInfo();"" (personal preference: log a sleep line, add a thread.sleep(a_long_time))
4) start brisk with -t on node-B 
5) wait till the log line ""Joining: getting bootstrap token"" , it should now reaches your break point (or zz)
6) crash the jvm (personal preference: kill -9 <pid>)

back to node-A
1) exit the jvm (BriskDaemon) ""normally"" (kill <pid>)
2) start the brisk node again (with -t):

log from node-A: 
{noformat}
 INFO 23:25:00,213 Logging initialized
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/riptano/work/brisk/resources/cassandra/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/riptano/work/brisk/resources/hadoop/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
 INFO 23:25:00,235 Heap size: 510263296/511311872
 INFO 23:25:00,237 JNA not found. Native methods will be disabled.
 INFO 23:25:00,263 Loading settings from file:/home/riptano/work/brisk/resources/cassandra/conf/cassandra.yaml
 INFO 23:25:00,470 DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
 INFO 23:25:00,496 Detected Hadoop trackers are enabled, setting my DC to Brisk
 INFO 23:25:00,696 Global memtable threshold is enabled at 162MB
 INFO 23:25:00,846 Opening /var/lib/cassandra/data/system/IndexInfo-f-1
 INFO 23:25:00,912 Opening /var/lib/cassandra/data/system/Schema-f-2
 INFO 23:25:00,926 Opening /var/lib/cassandra/data/system/Schema-f-1
 INFO 23:25:00,951 Opening /var/lib/cassandra/data/system/Migrations-f-2
 INFO 23:25:00,954 Opening /var/lib/cassandra/data/system/Migrations-f-1
 INFO 23:25:00,970 Opening /var/lib/cassandra/data/system/LocationInfo-f-2
 INFO 23:25:00,989 Opening /var/lib/cassandra/data/system/LocationInfo-f-1
 INFO 23:25:01,089 Loading schema version c4fd2440-7900-11e0-0000-ba846f9adcf7
 INFO 23:25:01,499 Creating new commitlog segment /var/lib/cassandra/commitlog/CommitLog-1304810701499.log
 INFO 23:25:01,530 Replaying /var/lib/cassandra/commitlog/CommitLog-1304810455288.log
 INFO 23:25:01,675 Finished reading /var/lib/cassandra/commitlog/CommitLog-1304810455288.log
 INFO 23:25:01,730 Enqueuing flush of Memtable-MetaStore@102170028(869/1086 serialized/live bytes, 3 ops)
 INFO 23:25:01,735 Writing Memtable-MetaStore@102170028(869/1086 serialized/live bytes, 3 ops)
 INFO 23:25:01,743 Enqueuing flush of Memtable-sblocks@1075051425(3044096/3805120 serialized/live bytes, 17 ops)
 INFO 23:25:01,747 Enqueuing flush of Memtable-inode.path@780298059(2848/3560 serialized/live bytes, 59 ops)
 INFO 23:25:01,748 Enqueuing flush of Memtable-inode.sentinel@1934329031(2848/3560 serialized/live bytes, 59 ops)
 INFO 23:25:01,748 Enqueuing flush of Memtable-inode@1660575731(6393/7991 serialized/live bytes, 134 ops)
 INFO 23:25:01,821 Completed flushing /var/lib/cassandra/data/HiveMetaStore/MetaStore-f-1-Data.db (989 bytes)
 INFO 23:25:01,832 Writing Memtable-sblocks@1075051425(3044096/3805120 serialized/live bytes, 17 ops)
 INFO 23:25:01,927 Completed flushing /var/lib/cassandra/data/cfs/sblocks-f-1-Data.db (3045448 bytes)
 INFO 23:25:01,928 Writing Memtable-inode.path@780298059(2848/3560 serialized/live bytes, 59 ops)
 INFO 23:25:01,968 Completed flushing /var/lib/cassandra/data/cfs/inode.path-f-1-Data.db (5346 bytes)
 INFO 23:25:01,969 Writing Memtable-inode.sentinel@1934329031(2848/3560 serialized/live bytes, 59 ops)
 INFO 23:25:02,035 Completed flushing /var/lib/cassandra/data/cfs/inode.sentinel-f-1-Data.db (1735 bytes)
 INFO 23:25:02,036 Writing Memtable-inode@1660575731(6393/7991 serialized/live bytes, 134 ops)
 INFO 23:25:02,085 Completed flushing /var/lib/cassandra/data/cfs/inode-f-1-Data.db (8582 bytes)
 INFO 23:25:02,087 Log replay complete
 INFO 23:25:02,092 Cassandra version: 0.8.0-beta2-SNAPSHOT
 INFO 23:25:02,092 Thrift API version: 19.10.0
 INFO 23:25:02,092 Loading persisted ring state
 INFO 23:25:02,092 load token size: 0
 INFO 23:25:02,093 Starting up server gossip
 INFO 23:25:02,104 Enqueuing flush of Memtable-LocationInfo@22262475(29/36 serialized/live bytes, 1 ops)
 INFO 23:25:02,105 Writing Memtable-LocationInfo@22262475(29/36 serialized/live bytes, 1 ops)
 INFO 23:25:02,127 Completed flushing /var/lib/cassandra/data/system/LocationInfo-f-3-Data.db (80 bytes)
 INFO 23:25:02,149 Starting Messaging Service on port 7000
 INFO 23:25:02,172 Using saved token 152036150612811635197207268153837644139
 INFO 23:25:02,173 Enqueuing flush of Memtable-LocationInfo@1977026981(53/66 serialized/live bytes, 2 ops)
 INFO 23:25:02,174 Writing Memtable-LocationInfo@1977026981(53/66 serialized/live bytes, 2 ops)
 INFO 23:25:02,190 Completed flushing /var/lib/cassandra/data/system/LocationInfo-f-4-Data.db (163 bytes)
 INFO 23:25:02,193 Compacting Major: [SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-f-2-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-f-1-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-f-3-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-f-4-Data.db')]
 INFO 23:25:02,196 Will not load MX4J, mx4j-tools.jar is not in the classpath
 INFO 23:25:02,196 Starting up Hadoop trackers
 INFO 23:25:02,197 Waiting for gossip to start
 INFO 23:25:02,225 Major@1830423861(system, LocationInfo, 438/741) now compacting at 16777 bytes/ms.
 INFO 23:25:02,257 Compacted to /var/lib/cassandra/data/system/LocationInfo-tmp-f-5-Data.db.  741 to 447 (~60% of original) bytes for 3 keys.  Time: 64ms.
 INFO 23:25:07,272 Chose seed 10.179.96.212 as jobtracker
 WARN 23:25:09,331 Metrics system not started: Cannot locate configuration: tried hadoop-metrics2-jobtracker.properties, hadoop-metrics2.properties
 INFO 23:25:09,994 Chose seed 10.179.96.212 as jobtracker
 INFO 23:25:10,139 Updating the current master key for generating delegation tokens
 INFO 23:25:10,143 Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
 INFO 23:25:10,143 Scheduler configured with (memSizeForMapSlotOnJT, memSizeForReduceSlotOnJT, limitMaxMemForMapTasks, limitMaxMemForReduceTasks) (-1, -1, -1, -1)
 INFO 23:25:10,144 Updating the current master key for generating delegation tokens
 INFO 23:25:10,145 Refreshing hosts (include/exclude) list
 INFO 23:25:10,223 Starting jobtracker with owner as riptano
 INFO 23:25:10,245 Starting SocketReader
 INFO 23:25:10,374 Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
 INFO 23:25:10,623 Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
 INFO 23:25:10,673 Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 50030
 INFO 23:25:10,673 listener.getLocalPort() returned 50030 webServer.getConnectors()[0].getLocalPort() returned 50030
 INFO 23:25:10,674 Jetty bound to port 50030
 INFO 23:25:10,674 jetty-6.1.21
 INFO 23:25:11,140 Started SelectChannelConnector@0.0.0.0:50030
 INFO 23:25:11,147 JobTracker up at: 8012
 INFO 23:25:11,147 JobTracker webserver: 50030
 WARN 23:25:11,276 Incorrect permissions on cassandra://localhost:9160/tmp/hadoop-riptano/mapred/system. Setting it to rwx------
ERROR 23:25:11,321 Fatal exception in thread Thread[ReadStage:4,5,main]
java.lang.AssertionError: No data found for NamesQueryFilter(columns=java.nio.HeapByteBuffer[pos=12 lim=16 cap=17]) in DecoratedKey(55249227080490826413412398468829851220, 3165333533353736613164333836353061346636333465656437326131353939):QueryPath(columnFamilyName='inode', superColumnName='null', columnName='null') (original filter NamesQueryFilter(columns=java.nio.HeapByteBuffer[pos=12 lim=16 cap=17])) from expression 'inode.73656e74696e656c EQ 78'
        at org.apache.cassandra.db.ColumnFamilyStore.scan(ColumnFamilyStore.java:1513)
        at org.apache.cassandra.service.IndexScanVerbHandler.doVerb(IndexScanVerbHandler.java:46)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
 INFO 23:25:20,059 Deleted /var/lib/cassandra/data/system/LocationInfo-f-3
 INFO 23:25:20,060 Deleted /var/lib/cassandra/data/system/LocationInfo-f-4
 INFO 23:25:20,576 Deleted /var/lib/cassandra/data/system/LocationInfo-f-1
 INFO 23:25:20,577 Deleted /var/lib/cassandra/data/system/LocationInfo-f-2
 INFO 23:25:21,297 problem cleaning system directory: cassandra://localhost:9160/tmp/hadoop-riptano/mapred/system
java.io.IOException: TimedOutException()
        at org.apache.cassandra.hadoop.fs.CassandraFileSystemThriftStore.listDeepSubPaths(CassandraFileSystemThriftStore.java:523)
        at org.apache.cassandra.hadoop.fs.CassandraFileSystemThriftStore.listSubPaths(CassandraFileSystemThriftStore.java:529)
        at org.apache.cassandra.hadoop.fs.CassandraFileSystem.listStatus(CassandraFileSystem.java:171)
        at org.apache.hadoop.mapred.JobTracker.<init>(JobTracker.java:2374)
        at org.apache.hadoop.mapred.JobTracker.<init>(JobTracker.java:2174)
        at org.apache.hadoop.mapred.JobTracker.startTracker(JobTracker.java:303)
        at org.apache.hadoop.mapred.JobTracker.startTracker(JobTracker.java:294)
        at org.apache.cassandra.hadoop.trackers.TrackerInitializer$1.run(TrackerInitializer.java:93)
        at java.lang.Thread.run(Thread.java:662)
Caused by: TimedOutException()
        at org.apache.cassandra.thrift.CassandraServer.get_indexed_slices(CassandraServer.java:673)
        at org.apache.cassandra.hadoop.fs.CassandraFileSystemThriftStore.listDeepSubPaths(CassandraFileSystemThriftStore.java:506)
        ... 8 more
 WARN 23:25:31,300 Incorrect permissions on cassandra://localhost:9160/tmp/hadoop-riptano/mapred/system. Setting it to rwx------
ERROR 23:25:31,315 Fatal exception in thread Thread[ReadStage:6,5,main]
java.lang.AssertionError: No data found for NamesQueryFilter(columns=java.nio.HeapByteBuffer[pos=12 lim=16 cap=17]) in DecoratedKey(55249227080490826413412398468829851220, 3165333533353736613164333836353061346636333465656437326131353939):QueryPath(columnFamilyName='inode', superColumnName='null', columnName='null') (original filter NamesQueryFilter(columns=java.nio.HeapByteBuffer[pos=12 lim=16 cap=17])) from expression 'inode.73656e74696e656c EQ 78'
        at org.apache.cassandra.db.ColumnFamilyStore.scan(ColumnFamilyStore.java:1513)
        at org.apache.cassandra.service.IndexScanVerbHandler.doVerb(IndexScanVerbHandler.java:46)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
 INFO 23:25:41,303 problem cleaning system directory: cassandra://localhost:9160/tmp/hadoop-riptano/mapred/system
java.io.IOException: TimedOutException()
        at org.apache.cassandra.hadoop.fs.CassandraFileSystemThriftStore.listDeepSubPaths(CassandraFileSystemThriftStore.java:523)
        at org.apache.cassandra.hadoop.fs.CassandraFileSystemThriftStore.listSubPaths(CassandraFileSystemThriftStore.java:529)
        at org.apache.cassandra.hadoop.fs.CassandraFileSystem.listStatus(CassandraFileSystem.java:171)
        at org.apache.hadoop.mapred.JobTracker.<init>(JobTracker.java:2374)
        at org.apache.hadoop.mapred.JobTracker.<init>(JobTracker.java:2174)
        at org.apache.hadoop.mapred.JobTracker.startTracker(JobTracker.java:303)
        at org.apache.hadoop.mapred.JobTracker.startTracker(JobTracker.java:294)
        at org.apache.cassandra.hadoop.trackers.TrackerInitializer$1.run(TrackerInitializer.java:93)
        at java.lang.Thread.run(Thread.java:662)
Caused by: TimedOutException()
        at org.apache.cassandra.thrift.CassandraServer.get_indexed_slices(CassandraServer.java:673)
        at org.apache.cassandra.hadoop.fs.CassandraFileSystemThriftStore.listDeepSubPaths(CassandraFileSystemThriftStore.java:506)
        ... 8 more
 WARN 23:25:51,308 Incorrect permissions on cassandra://localhost:9160/tmp/hadoop-riptano/mapred/system. Setting it to rwx------
ERROR 23:25:51,321 Fatal exception in thread Thread[ReadStage:8,5,main]
java.lang.AssertionError: No data found for NamesQueryFilter(columns=java.nio.HeapByteBuffer[pos=12 lim=16 cap=17]) in DecoratedKey(55249227080490826413412398468829851220, 3165333533353736613164333836353061346636333465656437326131353939):QueryPath(columnFamilyName='inode', superColumnName='null', columnName='null') (original filter NamesQueryFilter(columns=java.nio.HeapByteBuffer[pos=12 lim=16 cap=17])) from expression 'inode.73656e74696e656c EQ 78'
        at org.apache.cassandra.db.ColumnFamilyStore.scan(ColumnFamilyStore.java:1513)
        at org.apache.cassandra.service.IndexScanVerbHandler.doVerb(IndexScanVerbHandler.java:46)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

{noformat}

;;;","08/May/11 10:36;jbellis;kill -9 w/o the bootstrap is not sufficient to cause the problem?

If you allow the bootstrap to finish does it work correctly if you kill -9 node A?

Bootstrap shouldn't cause anything to be written to node A (except the presence of a new node, to system table) so I'm inclined to think the kill -9 of A is the important part.;;;","08/May/11 10:41;jbellis;bq. Bootstrap shouldn't cause anything to be written to node A

Hmm, but it does cause A to flush. I wonder if that's the connection.

Can you try with invoking nodetool flush against A, instead of doing a bootstrap?;;;","08/May/11 11:15;jbellis;Another thing to try: after kill -9 of A but before restarting it, remove the commitlog *header* files (just the header ones). This should force full CL replay on restart.;;;","13/May/11 01:54;slebresne;From irc:
{noformat}
pcmanus : jbellis: do you know what's up with #2401 ?
jbellis : jackson can't reproduce anymore either, but he wants to test more before calling it fixed
{noformat}
So I'm going to mark this resolved as this fixed a legit bug and I don't want to push it 0.7.7.
If there is still related problems, let's open another ticket.;;;","14/May/11 05:53;thobbs;With these changes, using a count of 0 in the SlicePredicate produces the following AssertionError (and a TimedOutExc for the client):

{noformat}
ERROR 16:13:38,864 Fatal exception in thread Thread[ReadStage:16,5,main]
java.lang.AssertionError: No data found for SliceQueryFilter(start=java.nio.HeapByteBuffer[pos=10 lim=10 cap=30], finish=java.nio.HeapByteBuffer[pos=17 lim=17 cap=30], reversed=false, count=0] in DecoratedKey(81509516161424251288255223397843705139, 6b657931):QueryPath(columnFamilyName='cf', superColumnName='null', columnName='null') (original filter SliceQueryFilter(start=java.nio.HeapByteBuffer[pos=10 lim=10 cap=30], finish=java.nio.HeapByteBuffer[pos=17 lim=17 cap=30], reversed=false, count=0]) from expression 'cf.626972746864617465 EQ 1'
	at org.apache.cassandra.db.ColumnFamilyStore.scan(ColumnFamilyStore.java:1517)
	at org.apache.cassandra.service.IndexScanVerbHandler.doVerb(IndexScanVerbHandler.java:42)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{noformat}

This was during a get_indexed_slices().;;;","14/May/11 23:03;jbellis;Created CASSANDRA-2653 to address this, since it will probably be in a different release than the original 2401 fix.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CompareWith=""LongType"" CF mis-applies tombstones",CASSANDRA-386,12433532,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,eweaver,eweaver,20/Aug/09 12:00,16/Apr/19 17:33,22/Mar/23 14:57,25/Aug/09 05:49,,,,,0,,,,,,"jbellis: what is ""mis applying?""
evn: later inserts have no effect
jbellis: so you do a remove with timestamp X, then timestamp X + 1 has no effect?
evn: yeah
jbellis: did you try a similar test w/ a ascii comparewith?
evn: well the identical test passes w/ TimeUUIDType

      <Keyspace Name=""MultiblogLong"">      
        <KeysCachedFraction>0.01</KeysCachedFraction>
        <ColumnFamily CompareWith=""LongType"" Name=""Blogs""/>
        <ColumnFamily CompareWith=""LongType"" Name=""Comments""/>
      </Keyspace>

$ ruby test/cassandra_test.rb -n test_get_first_long_column
insert at 1250740275826063
.
1 tests, 3 assertions, 0 failures, 0 errors

$ ruby test/cassandra_test.rb -n test_get_first_long_column
remove at 1250740278998607
insert at 1250740279011751
F
  1) Failure:
test_get_first_long_column(CassandraTest) [test/cassandra_test.rb:70]:
<{<Cassandra::Long#13703350 time: Tue Jul 14 00:20:16 -0400 1970, usecs: 0, jitter: 3626>=>
  ""I like this cat""}> expected but was
<{}>.
1 tests, 1 assertions, 1 failures, 0 errors",,eweaver,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Aug/09 22:07;jbellis;386-v2.patch;https://issues.apache.org/jira/secure/attachment/12417274/386-v2.patch","21/Aug/09 08:41;jbellis;386.patch;https://issues.apache.org/jira/secure/attachment/12417199/386.patch",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19665,,,Tue Aug 25 14:21:27 UTC 2009,,,,,,,,,,"0|i0fym7:",91206,,,,,Normal,,,,,,,,,,,,,,,,,"20/Aug/09 12:04;eweaver;exact sequence is as so:

1 read key range - no results
2 delete all keys in key range - which is nothing
3 insert key
4 check that key is there
5 read key range - one resutl
6 delete all keys in key range - which is key from step 3
7 insert key again, with new timestamp
8 check that key is there ***THIS FAILS***

each operation gets its own timestamp based on the (then) current time.;;;","21/Aug/09 00:29;jbellis;this test passes for me:

    def test_long_remove(self):
        path = ColumnPath('StandardLong1', column=_i64(1))
        client.insert('Keyspace1', 'key1', path, 'value1', 0, ConsistencyLevel.ONE)
        client.remove('Keyspace1', 'key1', path, 1, ConsistencyLevel.ONE)
        _expect_missing(lambda: client.get('Keyspace1', 'key1', path, ConsistencyLevel.ONE))
        L = client.get_key_range('Keyspace1', 'StandardLong1', '', '', 10)
        assert L == [], L
        # resurrect
        client.insert('Keyspace1', 'key1', path, 'value2', 2, ConsistencyLevel.ONE)
        c = client.get('Keyspace1', 'key1', path, ConsistencyLevel.ONE).column
        assert c.value == 'value2', c
        L = client.get_key_range('Keyspace1', 'StandardLong1', '', '', 10)
        assert L == ['key1'], L

;;;","21/Aug/09 00:31;jbellis;also if i change the remove selectivity to the CF:

        client.remove('Keyspace1', 'key1', ColumnPath('StandardLong1'), 1, ConsistencyLevel.ONE)
;;;","21/Aug/09 00:33;eweaver;will dig deeper;;;","21/Aug/09 06:29;eweaver;it only happens with a small count value (try 1)

 I think the tombstones are getting applied against the count;;;","21/Aug/09 06:29;eweaver;  def test_long_remove_bug
    @blogs_long.insert(:Blogs, key, {@longs[0] => 'I like this cat'})
    @blogs_long.remove(:Blogs, key)
    assert_equal({}, @blogs_long.get(:Blogs, key, :count => 1))

    @blogs_long.insert(:Blogs, key, {@longs[0] => 'I like this cat'})
    assert_equal({@longs[0] => 'I like this cat'}, @blogs_long.get(:Blogs, key, :count => 1)) # FAILS HERE
  end

;;;","21/Aug/09 06:32;eweaver;Note that I am using get_slice, not get (sorry, that was not clear at all before):

""send_get_slice(""
""MultiblogLong""
""test_long_remove_bug""
<CassandraThrift::ColumnParent column_family:""Blogs"">
<CassandraThrift::SlicePredicate slice_range:<CassandraThrift::SliceRange start:"""", finish:"""", reversed:false, count:1>>
1
"")""
;;;","21/Aug/09 06:53;jbellis;still confused. this works both with reversed slice and not:

    def test_long_remove(self):
        path = ColumnPath('StandardLong1', column=_i64(1))
        column_parent = ColumnParent('StandardLong1')
        sp = SlicePredicate(slice_range=SliceRange('', '', False, 1))

        client.insert('Keyspace1', 'key1', path, 'value1', 0, ConsistencyLevel.ONE)
        client.remove('Keyspace1', 'key1', ColumnPath('StandardLong1'), 1, ConsistencyLevel.ONE)
        slice = client.get_slice('Keyspace1', 'key1', column_parent, sp, ConsistencyLevel.ONE)
        assert slice == [], slice
        # resurrect
        client.insert('Keyspace1', 'key1', path, 'value2', 2, ConsistencyLevel.ONE)
        slice = [result.column
                 for result in client.get_slice('Keyspace1', 'key1', column_parent, sp, ConsistencyLevel.ONE)]
        assert slice == [Column(_i64(1), 'value2', 2)], slice
;;;","21/Aug/09 07:12;eweaver; it's intermittent for me... if you're resetting the data folder each time, try running the test a bunch of times without doing that.;;;","21/Aug/09 07:19;eweaver;full dump of client messages: http://pastie.org/pastes/590542;;;","21/Aug/09 07:34;jbellis;ok, i can reproduce when using (a) multiple iterations and (b) a different column name on each one;;;","21/Aug/09 08:41;jbellis;include column container's deletion status when determining whether to include a column in the live count;;;","21/Aug/09 11:18;junrao;Can't apply patch to trunk. Could you rebase?;;;","21/Aug/09 11:31;jbellis;patch applies fine for me on a different machine.  what error are you seeing?;;;","21/Aug/09 11:37;jbellis;(verified that patch applies to fresh checkout here);;;","21/Aug/09 14:25;euphoria;Patch *applies* but that lone ""if"" line at SliceQueryFilter.java:113 doesn't build or make sense over here.;;;","21/Aug/09 14:27;eweaver;Doesn't apply to git checkout:

100%[=============================================================================================================================>] 6,902       --.-K/s   in 0.05s   

2009-08-20 23:26:18 (128 KB/s) - `-' saved [6902/6902]

patching file src/java/org/apache/cassandra/db/Column.java
patching file src/java/org/apache/cassandra/db/IColumn.java
patching file src/java/org/apache/cassandra/db/IColumnContainer.java
patching file src/java/org/apache/cassandra/db/SuperColumn.java
patching file src/java/org/apache/cassandra/db/filter/SliceQueryFilter.java
patching file test/system/test_server.py
[trunk]: created 8319a20: ""Applied patch: ""http://issues.apache.org/jira/secure/attachment/12417199/386.patch""""
 6 files changed, 57 insertions(+), 5 deletions(-)
Building Cassandra
Buildfile: build.xml

build-subprojects:

init:
    [mkdir] Created dir: /Users/eweaver/cassandra/server/build/classes
    [mkdir] Created dir: /Users/eweaver/cassandra/server/build/test/classes
    [mkdir] Created dir: /Users/eweaver/cassandra/server/src/gen-java

check-gen-cli-grammar:

gen-cli-grammar:
     [echo] Building Grammar /Users/eweaver/cassandra/server/src/java/org/apache/cassandra/cli/Cli.g  ....

build-project:
     [echo] apache-cassandra-incubating: /Users/eweaver/cassandra/server/build.xml
    [javac] Compiling 257 source files to /Users/eweaver/cassandra/server/build/classes
    [javac] /Users/eweaver/cassandra/server/src/java/org/apache/cassandra/db/filter/SliceQueryFilter.java:113: '(' expected
    [javac]             if 
    [javac]               ^
    [javac] /Users/eweaver/cassandra/server/src/java/org/apache/cassandra/db/filter/SliceQueryFilter.java:114: ')' expected
    [javac]             logger.debug(""collecting "" + column.getString(comparator));
    [javac]                                                                       ^
    [javac] 2 errors

BUILD FAILED
/Users/eweaver/cassandra/server/build.xml:119: Compile failed; see the compiler error output for details.

;;;","21/Aug/09 21:52;jbellis;ah, thanks michael.  sometimes i need someone to point me to the obvious :);;;","21/Aug/09 22:07;jbellis;attached version that actually builds :);;;","22/Aug/09 04:42;eweaver;ship it!;;;","22/Aug/09 05:21;junrao;The last test in SliceQueryFilter.collectReducedColumns() doesn't seem quite right. It seems that the test should be the following:
            // but we need to add all non-gc-able columns to the result for read repair
            if ( ( !column.isMarkedForDelete()
                   && (!container.isMarkedForDelete() || column.mostRecentChangeAt() > container.getMarkedForDeleteAt())
                 )    // this is to get all included columns
                 || (column.isMarkedForDelete() && column.getLocalDeletionTime() > gcBefore) // this is to get all deleted, but not garbage-collected columns
              )

;;;","24/Aug/09 23:01;jbellis;that will return redundant columns if the container has been deleted more recently than anything relevant in the column.

added comments to the patch version:

            // but we need to add all non-gc-able columns to the result for read repair:
            // the column itself must be not gc-able, (1)
            // and if its container is deleted, the column must be changed more recently than the container tombstone (2)
            // (since otherwise, the only thing repair cares about is the container tombstone)
            if ((!column.isMarkedForDelete() || column.getLocalDeletionTime() > gcBefore) // (1)
                && (!container.isMarkedForDelete() || column.mostRecentChangeAt() > container.getMarkedForDeleteAt())) // (2)
;;;","25/Aug/09 05:49;jbellis;irc:
> junrao: jbellis: #386 looks fine to me

committed, w/ comments as given above.;;;","25/Aug/09 22:21;hudson;Integrated in Cassandra #177 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/177/])
    need to include column container's deletion status when determining whether to include a column in the live count.
patch by jbellis; reviewed by Jun Rao for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OOM on repair with many inconsistent ranges,CASSANDRA-2301,12500936,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,j.casares,j.casares,j.casares,10/Mar/11 03:46,16/Apr/19 17:33,22/Mar/23 14:57,11/Mar/11 10:57,0.7.4,,,,0,,,,,,"Repair can OOM when lots of ranges are inconsistent, causing many sstables to be streamed.

I replicated the error by making 1264 3MB sstables on one node, added a second node, changed the replication factor to 2, and ran a repair.

Looking at the heap dump of the original failure, there were 2.4GB of FutureTasks, each taking up 8MB of space. I tracked down the BufferedRandomAccessFile and made sure that it was cleared every time it was closed inside of src/java/org/apache/cassandra/io/sstable/SSTableWriter.java.

Attached is the patch I used which stopped the error when I was trying to replicate it.",,mdennis,,,,,,,,,,,,,,,,,,,,,14400,14400,,0%,14400,14400,,,,,,,,,,,,,,,,,,,,"10/Mar/11 04:09;jbellis;2301-v2.txt;https://issues.apache.org/jira/secure/attachment/12473193/2301-v2.txt","10/Mar/11 04:14;jbellis;2301-v3.txt;https://issues.apache.org/jira/secure/attachment/12473194/2301-v3.txt","10/Mar/11 03:47;j.casares;2301.diff;https://issues.apache.org/jira/secure/attachment/12473187/2301.diff",,,,,,,,,,,,3.0,j.casares,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20548,,,Fri Mar 11 03:47:16 UTC 2011,,,,,,,,,,"0|i0gakf:",93142,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"10/Mar/11 04:09;jbellis;I'm actually not sure how this could help -- dfile.close() already un-references the buffer, so there's no need to set dfile itself to be null.  Also, the file pointer changes as we read through the file, so changing bytescomplete to init-once is broken. (This is what nodetool compactionstats uses.)

My guess is that the second time around you just got lucky and index build was able to keep up w/ streaming enough to avoid OOMing.

But there is a bug here with the file/buffer handling -- we should lazy-init this once we're ready to build the index, rather than when we enqueue the task.  Patch attached w/ this approach.;;;","10/Mar/11 04:14;jbellis;v2 overcomplicated the problem.  v3 attached w/o extra Builder fields.;;;","11/Mar/11 05:23;mdennis;+1 on v3;;;","11/Mar/11 07:54;j.casares;+1 on v3.

Ran the patched code twice and my cluster of 2 didn't OOM.;;;","11/Mar/11 10:57;jbellis;committed;;;","11/Mar/11 11:47;hudson;Integrated in Cassandra-0.7 #373 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/373/])
    reduce memory use during streaming of multiple sstables
patch by jbellis; reviewed by mdennis and tested by Joaquin Casares for CASSANDRA-2301
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Not restarting due to Invalid saved cache,CASSANDRA-2076,12497183,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,mdennis,tbritz,tbritz,31/Jan/11 17:15,16/Apr/19 17:33,22/Mar/23 14:57,10/Feb/11 11:29,0.7.1,,,,0,,,,,,"This occured on two nodes on me (running 0.7.1 from svn)

One node was killed by the kernel due to a OOM and the other node was haning and I had to kill it manually with kill -9 (kill didn't work). (maybe these were faulty hardware nodes, I don't know)

The saved_cache was corrupt afterwards and I couldn't start the nodes. 

After deleting the saved_caches directory I could start the nodes again. 

Instead of not starting when an error occurs, cassandra could simply delete the errornous file and continue to start?




 INFO 22:31:11,570 reading saved cache
/hd1/cassandra_md5/saved_caches/table_attributes-table_attributes-KeyCache
ERROR 22:31:11,595 Exception encountered during startup.
java.lang.RuntimeException: The provided key was not UTF8 encoded.
       at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:159)
       at org.apache.cassandra.dht.OrderPreservingPartitioner.decorateKey(OrderPreservingPartitioner.java:44)
       at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:281)
       at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:218)
       at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:458)
       at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:440)
       at org.apache.cassandra.db.Table.initCf(Table.java:360)
       at org.apache.cassandra.db.Table.<init>(Table.java:290)
       at org.apache.cassandra.db.Table.open(Table.java:107)
       at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:167)
       at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:312)
       at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:81)
Caused by: java.nio.charset.MalformedInputException: Input length = 1
       at java.nio.charset.CoderResult.throwException(CoderResult.java:260)
       at java.nio.charset.CharsetDecoder.decode(CharsetDecoder.java:781)
       at org.apache.cassandra.utils.FBUtilities.decodeToUTF8(FBUtilities.java:403)
       at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:155)
       ... 11 more
Exception encountered during startup.
java.lang.RuntimeException: The provided key was not UTF8 encoded.
       at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:159)
       at org.apache.cassandra.dht.OrderPreservingPartitioner.decorateKey(OrderPreservingPartitioner.java:44)
       at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:281)
       at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:218)
       at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:458)
       at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:440)
       at org.apache.cassandra.db.Table.initCf(Table.java:360)
       at org.apache.cassandra.db.Table.<init>(Table.java:290)
       at org.apache.cassandra.db.Table.open(Table.java:107)
       at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:167)
       at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:312)
       at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:81)
Caused by: java.nio.charset.MalformedInputException: Input length = 1
       at java.nio.charset.CoderResult.throwException(CoderResult.java:260)
       at java.nio.charset.CharsetDecoder.decode(CharsetDecoder.java:781)
       at org.apache.cassandra.utils.FBUtilities.decodeToUTF8(FBUtilities.java:403)
       at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:155)
       ... 11 more",linux,cburroughs,tbritz,,,,,,,,,,,,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,,,,,"09/Feb/11 07:19;mdennis;2076-cassandra-0.7.txt;https://issues.apache.org/jira/secure/attachment/12470637/2076-cassandra-0.7.txt","05/Feb/11 21:56;jbellis;2076-v2.txt;https://issues.apache.org/jira/secure/attachment/12470361/2076-v2.txt",,,,,,,,,,,,,2.0,mdennis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20430,,,Thu Feb 10 04:34:54 UTC 2011,,,,,,,,,,"0|i0g96f:",92917,,jbellis,,jbellis,Critical,,,,,,,,,,,,,,,,,"31/Jan/11 23:01;tbritz;This might be related:

Two other nodes (still running) also show up the ""The provided key was not UTF8 encoded."" error in the log.

I have never seen this error in 0.7.0


ERROR [MutationStage:19] 2011-01-30 21:36:16,951 DebuggableThreadPoolExecutor.java (line 103) Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:159)
        at org.apache.cassandra.dht.OrderPreservingPartitioner.decorateKey(OrderPreservingPartitioner.java:44)
        at org.apache.cassandra.db.Table.apply(Table.java:406)
        at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:190)
        at org.apache.cassandra.service.StorageProxy$2.runMayThrow(StorageProxy.java:288)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
Caused by: java.nio.charset.MalformedInputException: Input length = 1
        at java.nio.charset.CoderResult.throwException(CoderResult.java:260)
        at java.nio.charset.CharsetDecoder.decode(CharsetDecoder.java:781)
        at org.apache.cassandra.utils.FBUtilities.decodeToUTF8(FBUtilities.java:403)
        at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:155)
        ... 8 more
ERROR [MutationStage:19] 2011-01-30 21:36:16,991 AbstractCassandraDaemon.java (line 119) Fatal exception in thread Thread[MutationStage:19,5,main]
java.lang.RuntimeException: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:159)
        at org.apache.cassandra.dht.OrderPreservingPartitioner.decorateKey(OrderPreservingPartitioner.java:44)
        at org.apache.cassandra.db.Table.apply(Table.java:406)
        at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:190)
        at org.apache.cassandra.service.StorageProxy$2.runMayThrow(StorageProxy.java:288)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
Caused by: java.nio.charset.MalformedInputException: Input length = 1
        at java.nio.charset.CoderResult.throwException(CoderResult.java:260)
        at java.nio.charset.CharsetDecoder.decode(CharsetDecoder.java:781)
        at org.apache.cassandra.utils.FBUtilities.decodeToUTF8(FBUtilities.java:403)
        at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:155)
        ... 8 more
 WARN [ScheduledTasks:1] 2011-01-30 21:36:21,450 MessagingService.java (line 506) Dropped 8 MUTATION messages in the last 5000ms



;;;","31/Jan/11 23:22;tbritz;I brought down and restarted the entire cluster. (100 nodes, 5x20 nodes)

Every single node complains of an invalid file in the saved_cache directory.;;;","01/Feb/11 19:20;tbritz;This just happens if you kill the node with -9;;;","04/Feb/11 00:46;jbellis;The problem is that we're not cloning the buffer backing the key read from thrift, so we are hitting a problem similar to CASSANDRA-1743.  Matt is working on a fix.;;;","04/Feb/11 07:35;mdennis;attached patch (depends on CASSANDRA-2102) allows C* to start when the saved caches are invalid/corrupt.

CASSANDRA-2102 prevents the saved caches from becoming corrupt in the first place.

;;;","04/Feb/11 17:22;tbritz;I was running yesterday's version with the Consitency fix and getting a similar error messages while reading the commitlog at start. I had to delete the commitlog (data loss) to restart cassandra.

Is this also related to CASSANDRA-2102 or shall I open a new bug report?


 INFO 23:50:00,922 Replaying /hd1/cassandra_md5/commitlog/CommitLog-1296731219671.log, /hd1/cassandra_md5/commitlog/CommitLog-1296759540149.log, /hd1/cassandra_md5/commitlog/CommitLog-1296760444366.log, /hd1/cassandra_md5/commitlog/CommitLog-1296761120546.log, /hd1/cassandra_md5/commitlog/CommitLog-1296762054192.log, /hd1/cassandra_md5/commitlog/CommitLog-1296773137101.log, /hd1/cassandra_md5/commitlog/CommitLog-1296773242671.log
 INFO 23:50:02,192 Finished reading /hd1/cassandra_md5/commitlog/CommitLog-1296731219671.log
ERROR 23:50:02,235 Fatal exception in thread Thread[MutationStage:7,5,main]
java.lang.RuntimeException: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:159)
        at org.apache.cassandra.dht.OrderPreservingPartitioner.decorateKey(OrderPreservingPartitioner.java:44)
        at org.apache.cassandra.db.Table.apply(Table.java:406)
        at org.apache.cassandra.db.commitlog.CommitLog$2.runMayThrow(CommitLog.java:294)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 6 more
Caused by: java.nio.charset.MalformedInputException: Input length = 1
        at java.nio.charset.CoderResult.throwException(CoderResult.java:260)
        at java.nio.charset.CharsetDecoder.decode(CharsetDecoder.java:781)
        at org.apache.cassandra.utils.FBUtilities.decodeToUTF8(FBUtilities.java:403)
        at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:155)
        ... 10 more
 INFO 23:50:02,245 Finished reading /hd1/cassandra_md5/commitlog/CommitLog-1296759540149.log
ERROR 23:50:02,245 Exception encountered during startup.
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:455)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:301)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:159)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:166)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:307)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:81)
Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:451)
        ... 5 more
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:159)
        at org.apache.cassandra.dht.OrderPreservingPartitioner.decorateKey(OrderPreservingPartitioner.java:44)
        at org.apache.cassandra.db.Table.apply(Table.java:406)
        at org.apache.cassandra.db.commitlog.CommitLog$2.runMayThrow(CommitLog.java:294)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 6 more
Caused by: java.nio.charset.MalformedInputException: Input length = 1
        at java.nio.charset.CoderResult.throwException(CoderResult.java:260)
        at java.nio.charset.CharsetDecoder.decode(CharsetDecoder.java:781)
        at org.apache.cassandra.utils.FBUtilities.decodeToUTF8(FBUtilities.java:403)
        at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:155)
        ... 10 more
Exception encountered during startup.
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:455)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:301)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:159)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:166)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:307)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:81)
Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:451)
        ... 5 more
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:159)
        at org.apache.cassandra.dht.OrderPreservingPartitioner.decorateKey(OrderPreservingPartitioner.java:44)
        at org.apache.cassandra.db.Table.apply(Table.java:406)
        at org.apache.cassandra.db.commitlog.CommitLog$2.runMayThrow(CommitLog.java:294)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 6 more
Caused by: java.nio.charset.MalformedInputException: Input length = 1
        at java.nio.charset.CoderResult.throwException(CoderResult.java:260)
        at java.nio.charset.CharsetDecoder.decode(CharsetDecoder.java:781)
        at org.apache.cassandra.utils.FBUtilities.decodeToUTF8(FBUtilities.java:403)
        at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:155)
        ... 10 more


;;;","04/Feb/11 23:23;jbellis;It's a different bug.

Did you have any nodes down for part of the time (any hinted handoff going on?);;;","04/Feb/11 23:32;jbellis;I reverted CASSANDRA-1743 which is what caused CASSANDRA-2102.  I suspect it's causing this other problem too for reasons not yet understood.;;;","05/Feb/11 01:41;tbritz;I can't say for sure. I guess so.

Can you trigger a hudson build with the most recent fixes as well. (Don't know for sure which revision to take) So I can let it run over the weekend and check if any of the exceptions and bugs (these ones, cpu spike, consistency...) occurs again or not.


;;;","05/Feb/11 03:41;jbellis;build scheduled;;;","05/Feb/11 11:03;mdennis;the commit logs have checksums in them to attempt to catch corrupted files.  C* is supposed to log that it is corrupted, skip over it and then continue.  The fact that *any* value inside a commit log can cause C* not to start is a bug of it's own.  I've opened CASSANDRA-2113 to track this.;;;","05/Feb/11 21:56;jbellis;v2 is a less-invasive fix to the original problem.;;;","09/Feb/11 07:19;mdennis;I like the first patch better (recently rebased) as it adds a version field at the top and operates on an actual count instead of using in.available() which is not actually reliable in all cases.

In addition the rebase fixes a recently introduced bug that prevented the caches from getting saved where BufferedRandomAccessFile doesn't accept ""w"" as a mode and adds some comments in CacheWriter about how the totalBytes reported via ICompactionInfo is just an approximation.

If we don't want these other changes, +1 on the v2 patch.;;;","10/Feb/11 11:29;jbellis;I'm not excited about the enumerated cache format change, because the benefit is nominal (if available() is wrong, then the cache is corrupt and we'll find out soon enough; knowing how many their SHOULD have been is of purely theoretical interest at that point) while the cost is making the cache useless for upgrades, which is the reason it was added in the first place.

Committed w/o the format change, but with the change to getTotalBytes to take the max of estimated/written.;;;","10/Feb/11 12:34;hudson;Integrated in Cassandra-0.7 #273 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/273/])
    continue starting when invalid savedcache entries are encountered
patch by mdennis and jbellis for CASSANDRA-2076
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
removetoken drops node from ring before re-replicating its data is finished,CASSANDRA-1216,12467577,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,nickmbailey,jbellis,jbellis,22/Jun/10 22:28,16/Apr/19 17:33,22/Mar/23 14:57,28/Sep/10 13:56,0.7 beta 2,,,,0,,,,,,this means that if something goes wrong during the re-replication (e.g. a source node is restarted) there is (a) no indication that anything has gone wrong and (b) no way to restart the process (other than the Big Hammer of running repair),,gdusbabek,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Sep/10 04:24;nickmbailey;0001-Modify-removeToken-to-be-similar-to-decommission.patch;https://issues.apache.org/jira/secure/attachment/12455298/0001-Modify-removeToken-to-be-similar-to-decommission.patch","23/Sep/10 04:24;nickmbailey;0002-Additional-tests-for-removeToken.patch;https://issues.apache.org/jira/secure/attachment/12455299/0002-Additional-tests-for-removeToken.patch","27/Sep/10 23:54;nickmbailey;0003-Fixes-from-review.patch;https://issues.apache.org/jira/secure/attachment/12455666/0003-Fixes-from-review.patch",,,,,,,,,,,,3.0,nickmbailey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20035,,,Tue Sep 28 13:31:34 UTC 2010,,,,,,,,,,"0|i0g3pj:",92031,,gdusbabek,,gdusbabek,Normal,,,,,,,,,,,,,,,,,"02/Jul/10 23:16;nickmbailey;A possible solution I see for this is to keep nodes in the justRemovedEndpoints map in Gossiper until we can verify that replication has completed.  I think we could accomplish verification through a callback on the replicate request.  I'm unsure about what data gets persisted so I don't know if a restart would wipe out the justRemovedEndpoints map;;;","03/Jul/10 00:12;nickmbailey;So clearly that solution would fail in the case of the node that is attempting to retrive the data failing.  Perhaps a better solution is simply not removing the node until replication and done.  Perhaps marking it with a new state?;;;","07/Jul/10 03:41;nickmbailey;It seems like this should follow a pattern similar to decommissioning a node.

* If nodeA has removeToken called on it, it becomes responsible for nodeB, the node to remove
* nodeA sets the MOVE_STATE of nodeB to STATE_REMOVING
* This is gossipped throughout the ring.
* Nodes see this change and fetch any ranges they are becoming responsible for
** After this is complete they will need to notify nodeA somehow that this is complete
* Once nodeA sees all replications have finished, change state of nodeB to STATE_REMOVED
* All nodes then remove nodeB from their ring.;;;","07/Jul/10 03:42;jbellis;agreed;;;","07/Jul/10 05:46;nickmbailey;A side effect of this approach may be that you would need to call removeToken on a node that had seen the token previously.;;;","07/Jul/10 07:19;jbellis;since all tokens will be propagated to all nodes (even ones brought up after the dead node went down), that's not a problem;;;","21/Jul/10 06:37;nickmbailey;* 0001 - changes to make removeToken behave similarly to decomission
* 0002 - fixes to existing tests since the state for STATE_LEFT changed

I am still working on some good unit tests for these changes but these are the changes so far.

The new process for removeToken is basically the one outlined above. One change is that instead of a STATE_REMOVED state it seemed like tokens that are removed should just go into STATE_LEFT similar to nodes that are decommissioned.

One thing I'm not sure of is the timeout values for waiting for replications to stream and for waiting for replication notifications. Currently they are just set arbitrarily in that patch. Need to determine good values for these.
;;;","28/Jul/10 04:53;nickmbailey;Some fixes and tests added.

There is one thing that still needs to be fixed.
 * Currently the call to removeToken blocks either:
 ** until all nodes confirm that they have replicated the data for the dead node.
 ** or a timeout is reached
 * I'm not sure what the timeout for this should be. Additionally when nodes throughout the ring attempt to replicate data there should be a similar timeout before they give up on a source and retry.
 * Also clients may timeout before the timeout is even reached or all the data is replicated. I'm not sure how the user will be able to determine if the remove finished correctly or repair should be run.  
;;;","28/Jul/10 05:39;nickmbailey;Updated 0001 patch. It was missing a class before. Oops.;;;","13/Aug/10 22:36;gdusbabek;Nick, can you rebase?;;;","14/Aug/10 04:11;nickmbailey;Rebased.;;;","14/Aug/10 06:18;nickmbailey;Re-rebased.;;;","18/Aug/10 02:08;gdusbabek;RemoveTest needs some cleanup.
* ReplicationSink doesn't need callCount
* NotificationSink doesn't need hitList
* testRemoveToken and testStartRemoving abuse Gossiper.start().  Consider adding a method to Gossiper that initializes the epstate for a given node.  E.g.: initializeNodeUnsafe(InetAddr addr, int generation).
* (minor nit) I wish there were a way to assert that tmd.getLeavingNodes() actually has nodes in it.
* all the methods throw UnknownHostException, but don't need to (IOException covers it)
* testStartRemoving should assert preconditions before calling ss.onChange (it also makes the same assertion twice).

StorageService:
* (minor nit) a comment describing the distinction between the leaving and removing constants.
* SS.removeToken() shouldn't throw a RuntimeException, as the client won't know what to make of it.  Declare an exception in the interface and throw it in the impl.  I imagine this will be a fairly common case (e.g.: when a node is down).
* SS.setReplicatingNodes and clearReplicatingNodes can be inlined into removeToken. It saves a few lines and obviates a local var.
* SS.replicateTables should probably be merged into SS.restoreReplicaCount.

Was the intent that SS.replicateTables block until the files are transferred?  Because it doesn't.  AFAICT it blocks until the first ack comes back from each source node, which is a good indication that streaming has started, but not that it is finished.

I couldn't verify that the callbacks are ever called.  That happens on the READ_RESPONSE stage and afaict, none of the streaming code path ever puts a task there.  That's a painful interface to follow though, so I might be wrong.;;;","19/Aug/10 03:51;nickmbailey;Yeah I wasn't really understanding that streaming/messaging code at all.

The current StreamOut implementation has a callback concept however.  I think this should be moved into the StreamContext object and then both StreamOut and StreamIn can perform callbacks on actual stream completion.  ;;;","19/Aug/10 04:15;gdusbabek;The StreamOut callback works differently than the MessagingService callback.  Your approach sounds workable.  I don't think it matters where you push the callback to, so long as you make sure it gets executed after the stream is finished.;;;","24/Aug/10 05:12;nickmbailey;bq. (minor nit) I wish there were a way to assert that tmd.getLeavingNodes() actually has nodes in it.

This is what tmd.isLeaving() does

bq. testStartRemoving should assert preconditions before calling ss.onChange (it also makes the same assertion twice).

I'm not sure what preconditions you mean. I added an assertion to make sure there are no endpoints already leaving.

bq. SS.removeToken() shouldn't throw a RuntimeException,

Do you think the UnsupportedOperationExceptions should be removed as well? These existed previously.

I modified the callback support for streaming so that the code should wait for all streams to finish before confirming. I also added a reply to the ReplicationFinishedHandler so the IAsyncResult will be updated.  

Thoughts?

The timeout values for waiting on the latches still need to be updated.
;;;","25/Aug/10 22:51;gdusbabek;> Do you think the UnsupportedOperationExceptions should be removed as well? These existed previously.
My bad; I didn't notice that.  RTE was probably ok.

> I modified the callback support for streaming so that the code should wait for all streams to finish before confirming. I also added a reply to the ReplicationFinishedHandler so the IAsyncResult will be updated
First glance tells me this will work.  I'll run some tests after I'm done reviewing.

> The timeout values for waiting on the latches still need to be updated.
Is this coming in another patch?;;;","26/Aug/10 00:30;gdusbabek;I see this in RemoveTest:


 [junit] Testsuite: org.apache.cassandra.service.RemoveTest
    [junit] Tests run: 4, Failures: 0, Errors: 0, Time elapsed: 1.97 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] ERROR 11:27:58,277 Did not find matching ranges on /127.0.0.6
    [junit] ERROR 11:27:58,279 Did not find matching ranges on /127.0.0.6
    [junit] ERROR 11:27:58,280 Did not find matching ranges on /127.0.0.5
    [junit] ERROR 11:27:58,280 Did not find matching ranges on /127.0.0.4
    [junit] ERROR 11:27:58,280 Did not find matching ranges on /127.0.0.2
    [junit] ERROR 11:27:59,264 Did not find matching ranges on /127.0.0.6
    [junit] ERROR 11:27:59,272 Did not find matching ranges on /127.0.0.6
    [junit] ERROR 11:27:59,276 Did not find matching ranges on /127.0.0.5
    [junit] ERROR 11:27:59,279 Did not find matching ranges on /127.0.0.4
    [junit] ERROR 11:27:59,283 Did not find matching ranges on /127.0.0.2
    [junit] ------------- ---------------- ---------------

Is that ok?;;;","26/Aug/10 00:38;nickmbailey;Re: timeouts

Yes I'm just not sure how to approach determining the right values for these.  Depends mostly on the amount of data and network bandwidth.

Re: RemoveTest

Yeah. The message sink in the test immediately responds to the stream request saying there are no files to stream.  This makes the StreamInManager think the data didn't exist remotely.  Doing it that way seems much easier than trying to make the test actually stream something.;;;","26/Aug/10 00:42;gdusbabek;Some questions about the coordinator...  I see that removeToken() is quasi-blocking now, like unbootstrap() (it was fire-and-forget before).  What are the consequences of the coordinator node going down?  Assuming a dead coordinator, would it be Bad for another node to remove-token on the same token while the transfers initiated by the original failed coordinator were in process?  Or assuming the transfers were finished, would a remove-token on a new coordinator generally do little other than get the state to LEFT?

I think I'm of the opinion that removeToken should either block until the transfer is complete (or failed), or should return instantly, and that we need to make sure that subsequent removeToken calls do not upset existing transfers.  Having it return error after a timeout (which is possible in the case of LOTS of data) makes me think we should be doing differently.

Or is the only recourse to repair?;;;","26/Aug/10 01:06;nickmbailey;I believe the only consequences of calling removeToken on another node when the coordinator goes down would be that the entire operation would be repeated. So any data that was transferred before would be transferred again.  I think this is the right behavior since there is no way of knowing what was transferred before the coordinator went down.  

It might be useful to add a 'force' option though.  If the coordinator goes down and the token gets stuck in a REMOVING state you may want to force removal rather than redoing the entire operation. 

It should be possible to remove the timeout so that removeToken blocks until the transfer is completely finished.  The code for streaming in the remote data blocks until all streams are complete and the code for sending a confirmation to the coordinator will keep retrying until it is received or the coordinator dies.  

I think this would work if a check was added so that you can only call removeToken a second time if the coordinator is down.  It wouldn't handle two calls that occurred before the state made its way through gossip though.  

;;;","28/Aug/10 03:06;nickmbailey;After some more thinking I think there are two problems here.

 * The timeout for waiting on a stream to complete - An arbitrary timeout here is not the right way to do this. What we really need is the concept of stream progress. We should be able to verify that a stream is progressing or not and based on that retry it.  CASSANDRA-1438 kind of relates to this problem and could be modified to implement this.  

 * The timeout waiting for nodes to confirm replication - Ideally there could be no timeout here. The problem though is if a node that should be grabbing data goes down permanently, removeToken will wait forever.  I think it's reasonable to have some sort of timeout in this case. A log message/error can indicate which machines were being waited on for replication. An administrator should know if that machine went down or is still streaming. That will determine if repair needs to be run.  The alternative to this I guess would be periodically waking up and checking that the nodes we are waiting on are still alive.  That wouldn't be particularly hard to implement

I don't think returning immediately from the call is the right approach.  That is part of the reason why this ticket is created. In the case that replication fails somewhere, there is no feedback to the user.  At least timing out eventually provides information about which machines we think failed to replicate data.  

As far as multiple remove calls and the coordinator going down.  I think there should be a 'force' option in the case the coordinator goes down and you believe the rest of the nodes completed the operation.  To prevent multiple calls to removeToken there should just be a check to make sure the coordinator is dead before another call can be performed.

So besides those few changes above, I think we should either implement this part way with a time out for stream replication or postpone completion here until we add the concept of stream progress.
;;;","22/Sep/10 03:43;nickmbailey;Patches:
 * 0001
 ** Modifies the removeToken operation to follow a pattern of NORMAL->REMOVING->LEFT, rather than the current pattern of a coordinator node setting its own status to a special cased version of NORMAL.
 ** Fixes a small bug in StreamHeader serialization
 ** Adds the ability to either get the status of a remove operation taking place or force a remove operation to finish immediately
 * 0002
 ** Tests for removing tokens
 ** Move shared code for creating a ring to Util class


Removal Process:
 * Normal Case
 *# Coordinator sets status of failed node to REMOVING
 *# Coordinator blocks on confirmation from other nodes
 *# Any newly responsible nodes stream data
 *# Newly responsible nodes send confirmation once all data has streamed
 *# Coordinator updates status of failed node to LEFT
 *# Done
 * Failure Cases
 ** Coordinator failure
 *** If the coordinator fails the remove operation will need to be retried
 *** This can be done on any node in the cluster.  
 **  Newly responsible node failure
 *** If a newly responsible node fails but comes back up, it should see the REMOVING status in gossip and restart the operation
 *** If a newly responsible node fails permanently or a streaming operation fails and the node stays up, the coordinator will block forever while waiting for confirmation.  The best solution is to force the remove operation to complete and then run repair on the failed node.;;;","23/Sep/10 00:01;nickmbailey;Bah.  Gossip marks the node alive when it receives an updated application state. Reverting it to modifying the coordinator nodes state.;;;","23/Sep/10 04:29;nickmbailey;Ok this should be ready for review now.  The process is:

# Coordinator node modifies its own status to NORMAL - REMOVING to indicate which node is being removed
# Coordinator blocks on removal confirmaton from other nodes
# Newly responsible nodes see this status and begin fetching new data
# Newly responsible nodes notify coordinator they have replicated all data
# Coordinator node updates its own status to NORMAL - REMOVED to indicate the removal is complete
# This causes all nodes to remove the node from gossip/tokenmetadata. 
# Done

Tested this with a 3 node cluster in the cloud, as well as testing the new getStatus and forceRemoval operations.;;;","27/Sep/10 15:58;gdusbabek;This looks good.

1.  There were a few unused local variables in SS.retoreReplicationCount().  Was this just leftovers from a rebase?
2.  SS.handleStateRemoving removes a null check that previously existed for epThatLeft (renamed removeEndpoint).  Was the original null-check pointless or was something missed in the change?
3.  You made a change to StreamHeader that made me think you were running into cases where SH.pendingFiles == null.  Is that true?  Tracing the codepaths makes me think this is not possible.

Don't bother with the cleanup in 1.  I'm more curious about 2 and 3.;;;","27/Sep/10 23:54;nickmbailey;1 and 2 are just errors on my part. I changed 3 because I was under the impression that a stream request to an endpoint that doesn't contain any of the ranges requested would create a header with null for pendingFiles.  I at first wrote one of the tests to behave like that, and got the NPE.  Looks like it changed or was never like that.

Fixed all that in a quick patch and attached it.;;;","28/Sep/10 13:56;gdusbabek;committed.;;;","28/Sep/10 21:31;hudson;Integrated in Cassandra #549 (See [https://hudson.apache.org/hudson/job/Cassandra/549/])
    changes update for CASSANDRA-1216
modify removetoken so that the coordinator relies on replicating nodes for updates. patch by Nick Bailey, reviewed by Gary Dusbabek. CASSANDRA-1216
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CompareSubcolumnsWith= has no effect,CASSANDRA-357,12432635,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,eweaver,eweaver,10/Aug/09 03:59,16/Apr/19 17:33,22/Mar/23 14:57,11/Aug/09 10:11,0.4,,,,0,,,,,,"CompareSubcolumnsWith= has no effect.

          <ColumnFamily CompareWith=""UTF8Type"" CompareSubcolumnsWith=""TimeUUIDType"" ColumnType=""Super"" Name=""StatusRelationships"" />  

I insert:

<[<Cassandra::UUID#13700550 time: Sun Jan 24 00:40:32 -0800 1971>,
 <Cassandra::UUID#13700530 time: Wed Feb 16 09:21:04 -0800 1972>,
 <Cassandra::UUID#13700540 time: Wed Apr 03 03:42:08 -0700 1974>,
 <Cassandra::UUID#13700520 time: Tue Jul 04 14:24:16 -0700 1978>,
 <Cassandra::UUID#13700560 time: Mon Jan 05 10:48:32 -0800 1987>]> 

But:

    keys = @twitter.get(:StatusRelationships, key, ""user_timelines"").keys

Responds with:

<[<Cassandra::UUID#13700560 time: Mon Jan 05 10:48:32 -0800 1987>,
 <Cassandra::UUID#13700550 time: Sun Jan 24 00:40:32 -0800 1971>,
 <Cassandra::UUID#13700540 time: Wed Apr 03 03:42:08 -0700 1974>,
 <Cassandra::UUID#13700530 time: Wed Feb 16 09:21:04 -0800 1972>,
 <Cassandra::UUID#13700520 time: Tue Jul 04 14:24:16 -0700 1978>]>.

PS. The debug log says:

weakreadlocal reading SliceFromReadCommand(table='Twitter', key='test_get_super_sub_keys_with_ranges', column_parent='QueryPath(columnFamilyName='StatusRelationships', superColumnName='[B@370410a7', columnName='null')', start='', finish='', isAscending=true, count=100)

",,nzkoz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-356,,,,,,,,,,,,,,,"11/Aug/09 00:24;jbellis;357-3.patch;https://issues.apache.org/jira/secure/attachment/12416086/357-3.patch","10/Aug/09 22:40;jbellis;357-v2.patch;https://issues.apache.org/jira/secure/attachment/12416074/357-v2.patch","10/Aug/09 12:21;jbellis;357.patch;https://issues.apache.org/jira/secure/attachment/12416016/357.patch",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19650,,,Tue Aug 11 12:57:30 UTC 2009,,,,,,,,,,"0|i0fyfz:",91178,,,,,Normal,,,,,,,,,,,,,,,,,"10/Aug/09 04:15;jbellis;it looks like your client is returning a result as a hash.  isn't your client's hashing going to screw with the ordering, even if cassandra's is correct?

I'd check the raw thrift results -- I don't see anything obviously wrong on the cassandra code and this is a path that the system tests check.;;;","10/Aug/09 04:44;eweaver; I have ordered hashes. Thrift returns:

[<CassandraThrift::ColumnOrSuperColumn column:<CassandraThrift::Column name:""\023\201@\0000\242\021\305\233\274a\252\a\300\325\355"", value:""v5"", timestamp:1249850151372872>>,
 <CassandraThrift::ColumnOrSuperColumn column:<CassandraThrift::Column name:""\023\201@\000N\377\021\263\204\020\370\224\245\341\202\034"", value:""v1"", timestamp:1249850151372872>>,
 <CassandraThrift::ColumnOrSuperColumn column:<CassandraThrift::Column name:""\023\201@\000\342\206\021\266\212\320\027J\207\000!\351"", value:""v3"", timestamp:1249850151372872>>,
 <CassandraThrift::ColumnOrSuperColumn column:<CassandraThrift::Column name:""\023\201@\000\200,\021\264\226\217\311~\024\333qD"", value:""v2"", timestamp:1249850151372872>>,
 <CassandraThrift::ColumnOrSuperColumn column:<CassandraThrift::Column name:""\023\201@\000\247:\021\273\2244\265\307*\375\200\237"", value:""v4"", timestamp:1249850151372872>>]

If I add in TimeUUIDType:

    public int compare(byte[] o1, byte[] o2)
    {
+        if (true) { throw new MarshalException(""Crap""); }
        long t1 = LexicalUUIDType.getUUID(o1).timestamp();
        long t2 = LexicalUUIDType.getUUID(o2).timestamp();
        return t1 < t2 ? -1 : (t1 > t2 ? 1 : 0);
    }

it's never thrown in any place that I can find, at least.
;;;","10/Aug/09 12:21;jbellis;here you go.  test is longer than the actual fix :);;;","10/Aug/09 12:55;eweaver;Fixes one bug, but causes another.

DEBUG - weakreadlocal reading SliceFromReadCommand(table='Twitter', key='test_get_super_sub_keys_with_count', column_parent='QueryPath(columnFamilyName='StatusRelationships', superColumnName='[B@2bb57fd1', columnName='null')', start='', finish='', isAscending=false, count=1)
ERROR - Internal error processing get_slice
java.nio.BufferUnderflowException
  at java.nio.Buffer.nextGetIndex(Buffer.java:480)
  at java.nio.HeapByteBuffer.getLong(HeapByteBuffer.java:387)
  at org.apache.cassandra.db.marshal.LexicalUUIDType.getUUID(LexicalUUIDType.java:11)
  at org.apache.cassandra.db.marshal.TimeUUIDType.compare(TimeUUIDType.java:10)
  at org.apache.cassandra.db.marshal.TimeUUIDType.compare(TimeUUIDType.java:5)
  at org.apache.cassandra.db.filter.SliceQueryFilter.collectReducedColumns(SliceQueryFilter.java:80)
  at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1391)
  at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1365)
  at org.apache.cassandra.db.Table.getRow(Table.java:589)
  at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:65)
  at org.apache.cassandra.service.StorageProxy.weakReadLocal(StorageProxy.java:609)
  at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:320)
  at org.apache.cassandra.service.CassandraServer.readColumnFamily(CassandraServer.java:94)
  at org.apache.cassandra.service.CassandraServer.getSlice(CassandraServer.java:175)
  at org.apache.cassandra.service.CassandraServer.get_slice(CassandraServer.java:220)
  at org.apache.cassandra.service.Cassandra$Processor$get_slice.process(Cassandra.java:587)
  at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:575)
  at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:252)
  at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
  at java.lang.Thread.run(Thread.java:637)

[9:47pm] jbellis: evn: the fix is to duplicate the length==0 special cases from LexicalUUIDType.compare in TimeUUIDType
[9:47pm] jbellis: but i need to figure out why my test case isn't tripping that bug
[9:47pm] jbellis: but my brain is done for the night.  bedtime.

;;;","10/Aug/09 13:17;eweaver;While you're at it, can you see if LongType has the same problems?;;;","10/Aug/09 22:40;jbellis;v2 fixes comparing with byte[0].

requires latest 356 patch for tests to pass.;;;","10/Aug/09 22:40;jbellis;LongType looks fine with 356 + 357 patches.;;;","10/Aug/09 23:23;eweaver;
I now get an order/limit/range error. Range nil..@uuids[2] and @uuids[2]..nil both return the first item in the subcolumn.

  1) Failure:
test_get_super_sub_keys_with_ranges(CassandraTest) [./test/cassandra_test.rb:109]:
<{<Cassandra::UUID#13817770 time: Wed Feb 16 09:21:04 -0800 1972, usecs: 0 jitter: 7897980574650157968>=>
  ""v2""}> expected but was
<{[<Cassandra::UUID#13748260 time: Sun Jan 24 00:40:32 -0800 1971, usecs: 0 jitter: 10815637326122044813>,
  ""v1""]=>nil}>.

DEBUG - weakreadlocal reading SliceFromReadCommand(table='Twitter', key='test_get_super_sub_keys_with_ranges', column_parent='QueryPath(columnFamilyName='StatusRelationships', superColumnName='[B@38990402', columnName='null')', start='?@?,????aB?', finish='', isAscending=true, count=1)

;;;","11/Aug/09 00:24;jbellis;respect ""start"" filter argument for subcolumns.  applies on top of 357-v2;;;","11/Aug/09 03:47;eweaver;Still broken.  Here is my test case (elements of the @uuids array are in increasing order by time):

  def test_get_super_sub_keys_with_ranges              
    @twitter.insert(:StatusRelationships, key, 
      {'user_timelines' => {
        @uuids[1] => 'v1', 
        @uuids[2] => 'v2', 
        @uuids[3] => 'v3',
        @uuids[4] => 'v4', 
        @uuids[5] => 'v5'}})

    keys = @twitter.get(:StatusRelationships, key, ""user_timelines"").keys
    assert_equal keys.sort, keys    
    assert_equal({@uuids[1] => 'v1'}, @twitter.get(:StatusRelationships, key, ""user_timelines"", :finish => @uuids[2], :count => 1))
 
    # FAILS ON NEXT LINE
    assert_equal({@uuids[2] => 'v2'}, @twitter.get(:StatusRelationships, key, ""user_timelines"", :start => @uuids[2], :count => 1))

    assert_equal 4, @twitter.get(:StatusRelationships, key, ""user_timelines"", :start => @uuids[2], :finish => @uuids[5]).size

Here is the result:

  1) Failure:
test_get_super_sub_keys_with_ranges(CassandraTest) [./test/cassandra_test.rb:110]:
<{<Cassandra::UUID#13756560 time: Wed Feb 16 09:21:04 -0800 1972, usecs: 0 jitter: 3921283106724136851>=>
  ""v2""}> expected but was
<{[<Cassandra::UUID#13696690 time: Sun Jan 24 00:40:32 -0800 1971, usecs: 0 jitter: 2275131151012163974>,
  ""v1""]=>nil}>.

The server said:

DEBUG - get_slice_from
DEBUG - weakreadlocal reading SliceFromReadCommand(table='Twitter', key='test_get_super_sub_keys_with_ranges', column_parent='QueryPath(columnFamilyName='StatusRelationships', superColumnName='[B@2eb89c06', columnName='null')', start='?@?,??X?Y?v', finish='', isAscending=true, count=1)

;;;","11/Aug/09 04:17;jbellis;I added this to the patched test_time_uuid in test_server.py:

        p = SlicePredicate(slice_range=SliceRange(L[2].bytes, '', True, 1))
        column_parent = ColumnParent('Super4', 'sc1')
        slice = [result.column
                 for result in client.get_slice('Keyspace2', 'key1', column_parent, p, ConsistencyLevel.ONE)]
        assert slice == [Column(L[2].bytes, 'value2', 2)], slice

this passes.  I'm not sure how this differs from yours.
;;;","11/Aug/09 08:10;eweaver;It works! My checkout was broken.

Ship it!;;;","11/Aug/09 10:11;jbellis;fixed;;;","11/Aug/09 20:57;hudson;Integrated in Cassandra #164 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/164/])
    respect ""start"" filter argument for subcolumns.
patch by jbellis; reviewed by Even Weaver for 
fix typo breaking CompareSubcolumnsWith.  fix timeuuid compare with byte[0].
patch by jbellis; reviewed by Evan Weaver for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
0.7 migrations/schema serializations are incompatible with trunk,CASSANDRA-2001,12495898,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,gdusbabek,gdusbabek,gdusbabek,18/Jan/11 22:03,16/Apr/19 17:33,22/Mar/23 14:57,19/Jan/11 02:25,0.8 beta 1,,,,0,,,,,,"Two problems:
1. inserting replicate_on_write into the middle of the CfDef members created a problem with serialization.  
2. merging the genavro files created a strange namespacing problem.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Jan/11 05:11;stuhood;0004-Set-a-default-for-rep-on-write-and-revert-0001.txt;https://issues.apache.org/jira/secure/attachment/12468682/0004-Set-a-default-for-rep-on-write-and-revert-0001.txt","19/Jan/11 01:19;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-use-the-writer-schema-and-not-the-current-schema-when-.txt;https://issues.apache.org/jira/secure/attachment/12468653/ASF.LICENSE.NOT.GRANTED--v1-0001-use-the-writer-schema-and-not-the-current-schema-when-.txt","19/Jan/11 01:19;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0002-get-rid-of-the-avro-db.migrations-classes-that-were-du.txt;https://issues.apache.org/jira/secure/attachment/12468654/ASF.LICENSE.NOT.GRANTED--v1-0002-get-rid-of-the-avro-db.migrations-classes-that-were-du.txt","19/Jan/11 01:19;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0003-fix-order-of-replicate_on_write-and-make-sure-it-s-not.txt;https://issues.apache.org/jira/secure/attachment/12468655/ASF.LICENSE.NOT.GRANTED--v1-0003-fix-order-of-replicate_on_write-and-make-sure-it-s-not.txt","19/Jan/11 04:09;gdusbabek;data.zip;https://issues.apache.org/jira/secure/attachment/12468676/data.zip",,,,,,,,,,5.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20397,,,Tue Jan 18 23:16:43 UTC 2011,,,,,,,,,,"0|i0g8pj:",92841,,eevans,,eevans,Low,,,,,,,,,,,,,,,,,"18/Jan/11 22:03;gdusbabek;fwiw CASSANDRA-1923 contains unit tests that will catch these kinds of problems in the future.;;;","19/Jan/11 01:20;gdusbabek;also fixes the incorrect schema that was used when deserializing an object.;;;","19/Jan/11 01:54;urandom;Just for posterity sake: CASSANDRA-926 moved  the remaining Avro records from o.a.c.avro elsewhere, to packages that made it obvious which components were still using them (but obviously, created a bug in the process).

+1 on this patchset though (with or without the o.a.c.db.migration.avro -> o.a.c.db.avro move).;;;","19/Jan/11 02:24;hudson;Integrated in Cassandra #676 (See [https://hudson.apache.org/hudson/job/Cassandra/676/])
    fix order of replicate_on_write and make sure it's not null. patch by gdusbabek, reviewed by eevans. CASSANDRA-2001
get rid of the avro db.migrations classes that were duplicated elsewhere. patch by gdusbabek, reviewed by eevans. CASSANDRA-2001
use the writer schema and not the current schema when deserializing. patch by gdusbabek, reviewed by eevans. CASSANDRA-2001
;;;","19/Jan/11 02:25;gdusbabek;committed;;;","19/Jan/11 04:09;gdusbabek;schema from a vanilla 0.7 node.;;;","19/Jan/11 04:27;gdusbabek;If you bypass the ClassCastException that 926 introduced, you end up with an avro error:
org.apache.avro.AvroTypeException: Found {""type"":""record"",""name"":""CfDef"",""namespace"":""org.apache.cassandra.avro"",""fields"":[{""name"":""keyspace"",""type"":""string""},{""name"":""name"",""type"":""string""},{""name"":""column_type"",""type"":[""string"",""null""]},{""name"":""comparator_type"",""type"":[""string"",""null""]},{""name"":""subcomparator_type"",""type"":[""string"",""null""]},{""name"":""comment"",""type"":[""string"",""null""]},{""name"":""row_cache_size"",""type"":[""double"",""null""]},{""name"":""key_cache_size"",""type"":[""double"",""null""]},{""name"":""read_repair_chance"",""type"":[""double"",""null""]},{""name"":""gc_grace_seconds"",""type"":[""int"",""null""]},{""name"":""default_validation_class"",""type"":[""null"",""string""],""default"":null},{""name"":""min_compaction_threshold"",""type"":[""null"",""int""],""default"":null},{""name"":""max_compaction_threshold"",""type"":[""null"",""int""],""default"":null},{""name"":""row_cache_save_period_in_seconds"",""type"":[""int"",""null""],""default"":0},{""name"":""key_cache_save_period_in_seconds"",""type"":[""int"",""null""],""default"":3600},{""name"":""memtable_flush_after_mins"",""type"":[""int"",""null""],""default"":60},{""name"":""memtable_throughput_in_mb"",""type"":[""null"",""int""],""default"":null},{""name"":""memtable_operations_in_millions"",""type"":[""null"",""double""],""default"":null},{""name"":""id"",""type"":[""int"",""null""]},{""name"":""column_metadata"",""type"":[{""type"":""array"",""items"":{""type"":""record"",""name"":""ColumnDef"",""fields"":[{""name"":""name"",""type"":""bytes""},{""name"":""validation_class"",""type"":""string""},{""name"":""index_type"",""type"":[{""type"":""enum"",""name"":""IndexType"",""symbols"":[""KEYS""],""aliases"":[""org.apache.cassandra.config.avro.IndexType""]},""null""]},{""name"":""index_name"",""type"":[""string"",""null""]}]}},""null""]}]}, expecting {""type"":""record"",""name"":""CfDef"",""namespace"":""org.apache.cassandra.avro"",""fields"":[{""name"":""keyspace"",""type"":""string""},{""name"":""name"",""type"":""string""},{""name"":""column_type"",""type"":[""string"",""null""]},{""name"":""comparator_type"",""type"":[""string"",""null""]},{""name"":""subcomparator_type"",""type"":[""string"",""null""]},{""name"":""comment"",""type"":[""string"",""null""]},{""name"":""row_cache_size"",""type"":[""double"",""null""]},{""name"":""key_cache_size"",""type"":[""double"",""null""]},{""name"":""read_repair_chance"",""type"":[""double"",""null""]},{""name"":""gc_grace_seconds"",""type"":[""int"",""null""]},{""name"":""default_validation_class"",""type"":[""null"",""string""],""default"":null},{""name"":""min_compaction_threshold"",""type"":[""null"",""int""],""default"":null},{""name"":""max_compaction_threshold"",""type"":[""null"",""int""],""default"":null},{""name"":""row_cache_save_period_in_seconds"",""type"":[""int"",""null""],""default"":0},{""name"":""key_cache_save_period_in_seconds"",""type"":[""int"",""null""],""default"":3600},{""name"":""memtable_flush_after_mins"",""type"":[""int"",""null""],""default"":60},{""name"":""memtable_throughput_in_mb"",""type"":[""null"",""int""],""default"":null},{""name"":""memtable_operations_in_millions"",""type"":[""null"",""double""],""default"":null},{""name"":""id"",""type"":[""int"",""null""]},{""name"":""column_metadata"",""type"":[{""type"":""array"",""items"":{""type"":""record"",""name"":""ColumnDef"",""fields"":[{""name"":""name"",""type"":""bytes""},{""name"":""validation_class"",""type"":""string""},{""name"":""index_type"",""type"":[{""type"":""enum"",""name"":""IndexType"",""symbols"":[""KEYS""],""aliases"":[""org.apache.cassandra.config.avro.IndexType""]},""null""]},{""name"":""index_name"",""type"":[""string"",""null""]}],""aliases"":[""org.apache.cassandra.config.avro.ColumnDef""]}},""null""]},{""name"":""replicate_on_write"",""type"":[""boolean"",""null""]}],""aliases"":[""org.apache.cassandra.config.avro.CfDef""]}

basically, avro was expecting to see replicate_on_write.;;;","19/Jan/11 05:11;stuhood;Attaching the patch discussed in IRC.

In summary: it is necessary to add a default with every new field, independent of whether you are using a union to make the field optional. Defaults are used in places where the writer's schema doesn't contain a field, but the reader's schema does. Unions are used to make fields optional: you can use a union together with a default to add a new optional field.;;;","19/Jan/11 05:28;gdusbabek;backed out of the original fix and committed the one-liner that sets a default on CfDef.replicate_on_write.;;;","19/Jan/11 07:16;hudson;Integrated in Cassandra #677 (See [https://hudson.apache.org/hudson/job/Cassandra/677/])
    set a default for replicate_on_write. patch by stuhood and gdusbabek. CASSANDRA-2001
back out of 2001. patch by gdusbabek. CASSANDRA-2001
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReadCallback AssertionError: resolver.getMessageCount() <= endpoints.size(),CASSANDRA-2282,12500665,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,thobbs,thobbs,08/Mar/11 03:34,16/Apr/19 17:33,22/Mar/23 14:57,09/Mar/11 22:58,0.7.4,,,,0,,,,,,"In a three node cluster with RF=2, when trying to page through all rows with get_range_slices() at CL.ONE, I get timeouts on the client side.  Looking at the Cassandra logs, all of the nodes show the following AssertionError repeatedly:

{noformat}
ERROR [RequestResponseStage:2] 2011-03-07 19:10:27,527 DebuggableThreadPoolExecutor.java (line 103) Error in ThreadPoolExecutor
java.lang.AssertionError
        at org.apache.cassandra.service.ReadCallback.response(ReadCallback.java:127)
        at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:49)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
ERROR [RequestResponseStage:2] 2011-03-07 19:10:27,529 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[RequestResponseStage:2,5,main]
java.lang.AssertionError
        at org.apache.cassandra.service.ReadCallback.response(ReadCallback.java:127)
        at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:49)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
{noformat}

The nodes are all running 0.7.3.  The cluster was at size 3 before any data was inserted, and everything else appears perfectly healthy.",,cburroughs,jeromatron,skamio,tapter,terjem,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Mar/11 02:02;jbellis;2282-v2.txt;https://issues.apache.org/jira/secure/attachment/12473017/2282-v2.txt","09/Mar/11 00:34;jbellis;2282.txt;https://issues.apache.org/jira/secure/attachment/12472984/2282.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20538,,,Fri Apr 22 05:59:19 UTC 2011,,,,,,,,,,"0|i0gag7:",93123,,thobbs,,thobbs,Normal,,,,,,,,,,,,,,,,,"08/Mar/11 12:26;skamio;In our case, the same AssertionError occurs on multi node cluster with replication factor = 3 (0.7.3 release version).
Feeding data into cassandra looks ok (consistency level = QUORUM). Though, UnavailableException was received via hector 0.7.0-28 several times. It warns about number of replica (see the following stack trace). It might relate to this problem. But not sure. 

When querying data, AssertionError occurs in cassandra and client gets timedout exception.
Our client queries in several query types in different column families. This timeout occurs quite often in secondary index query.
The error is only logged on the host which client connects via thrift (according to timestamp in log).

Another experience is when I tried to retrieve data via CLI.
Query like ""list Standard1 limit 10"" returns results normally. But the cassandra logs the AssertionError on the host. Other node doesn't have the log.
(When query returns ""null"" (I guess there is not enough replica), this exception is very likely to be logged.)

-------------------
* Unavailable exception stack trace received via hector 0.7.0-28 when feeding data into cassandra:

me.prettyprint.hector.api.exceptions.HUnavailableException: : May not be enough replicas present to handle consistency level.
        at me.prettyprint.cassandra.service.ExceptionsTranslatorImpl.translate(ExceptionsTranslatorImpl.java:52)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl$1.execute(KeyspaceServiceImpl.java:95)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl$1.execute(KeyspaceServiceImpl.java:88)
        at me.prettyprint.cassandra.service.Operation.executeAndSetResult(Operation.java:101)
        at me.prettyprint.cassandra.connection.HConnectionManager.operateWithFailover(HConnectionManager.java:221)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl.operateWithFailover(KeyspaceServiceImpl.java:129)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl.batchMutate(KeyspaceServiceImpl.java:100)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl.batchMutate(KeyspaceServiceImpl.java:106)
        at me.prettyprint.cassandra.model.MutatorImpl$2.doInKeyspace(MutatorImpl.java:203)
        at me.prettyprint.cassandra.model.MutatorImpl$2.doInKeyspace(MutatorImpl.java:200)
        at me.prettyprint.cassandra.model.KeyspaceOperationCallback.doInKeyspaceAndMeasure(KeyspaceOperationCallback.java:20)
        at me.prettyprint.cassandra.model.ExecutingKeyspace.doExecute(ExecutingKeyspace.java:85)
        at me.prettyprint.cassandra.model.MutatorImpl.execute(MutatorImpl.java:200)
        at jp.co.rakuten.gsp.cassandra_connector.feeder.CassandraFeeder.batchInsert(CassandraFeeder.java:506)
        at jp.co.rakuten.gsp.purchase_history.cassandra_connector.PHCassandraFeeder.consume(PHCassandraFeeder.java:240)
        at jp.co.rakuten.gsp.cassandra_connector.feeder.CassandraFeeder.process(CassandraFeeder.java:330)
        at jp.co.rakuten.gsp.cassandra_connector.feeder.Feeder.run(Feeder.java:164)
        at java.lang.Thread.run(Thread.java:662)
Caused by: UnavailableException()
        at org.apache.cassandra.thrift.Cassandra$batch_mutate_result.read(Cassandra.java:16485)
        at org.apache.cassandra.thrift.Cassandra$Client.recv_batch_mutate(Cassandra.java:916)
        at org.apache.cassandra.thrift.Cassandra$Client.batch_mutate(Cassandra.java:890)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl$1.execute(KeyspaceServiceImpl.java:93)
        ... 16 more

;;;","08/Mar/11 22:04;terjem;From ReadCallback.java


        this.endpoints = repair || resolver instanceof RowRepairResolver
                       ? endpoints
                       : endpoints.subList(0, Math.min(endpoints.size(), blockfor)); // min so as to not throw exception until assureSufficient is called

This will cut the list of endpoints to whatever is the consistency requirement (in the case where repair is false, which for instance happens all the time for a rangescan).

Later:
assert resolver.getMessageCount() <= endpoints.size()

which will cause an assert if all nodes answers on a range request (or if you have a random readrepair).

I am actually not 100% sure what the fix is right now as I have not had time to scan the rest of the code and need to leave office for today, but that is the problem anyway :)
To be honest, the logic here may look a bit broken.

The assert happening here may good either. The error condition is not returned to the client, so it will hang around waiting for a timeout to occur.
Maybe the code should throw some exception before the assert?

Something like
assert resolver.getMessageCount() <= endpoints.size() :  ""Got "" + resolver.getMessageCount() + "" replies but only know "" + endpoints.size() + "" endpoints"";
may also be good?;;;","08/Mar/11 22:06;terjem;Just to make this very clear.
The list of endpoints is cut to the consistency requirement so if:
- The request requires gossip with more than one node
- all nodes answers
- you do not use consistencylevel ALL,
then the assert will trigger.;;;","09/Mar/11 00:34;jbellis;The bug is that range queries were not restricting the set of endpoints it queries to handler.endpoints the way single-row queries do. Fix attached with a couple extra comments.;;;","09/Mar/11 01:57;skamio;Hi Jonathan,

I tried your patch. But still the assertion happens in our cluster. Query is IndexedSlicesQuery with QUORUM.


ERROR [RequestResponseStage:23] 2011-03-09 02:47:23,343 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[RequestResponseStage:23,5,main]
java.lang.AssertionError: Got 3 replies but requests were only sent to 2 endpoints
        at org.apache.cassandra.service.ReadCallback.response(ReadCallback.java:129)
        at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:49)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
ERROR [RequestResponseStage:10] 2011-03-09 02:48:48,463 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[RequestResponseStage:10,5,main]
java.lang.AssertionError: Got 3 replies but requests were only sent to 2 endpoints
        at org.apache.cassandra.service.ReadCallback.response(ReadCallback.java:129)
        at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:49)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
;;;","09/Mar/11 02:02;jbellis;v2 adds same fix for indexed queries;;;","09/Mar/11 05:40;jbellis;Workaround: remove this line from cassandra-env.sh

{noformat}
JVM_OPTS=""$JVM_OPTS -ea""
{noformat};;;","09/Mar/11 05:50;thobbs;v2 patch fixes the issue for me.;;;","09/Mar/11 11:18;skamio;v2 patch works for me too. Server side errors are gone.

My client still gets TimedOut exception. But maybe this is a different problem.;;;","09/Mar/11 22:55;jbellis;bq. My client still gets TimedOut exception. But maybe this is a different problem.

real timeouts can still happen. :)

i'd suggest 1) checking for dropped message warnings in the log; if there aren't any, 2) enabling debug logging to see where the commands are going and not coming back from in time.;;;","09/Mar/11 22:58;jbellis;committed;;;","09/Mar/11 23:08;skamio;I'm trying to find out when the timeout happens. So, I put logging to CassandraServer.java and  ReadCallback.java just before timeout exception.
I saw Timeout occurs even when 2 responses are received for query with quorum. 2 responses should be enough for quorum. Is it correct behavior?

------
 INFO [pool-1-thread-44] 2011-03-09 23:34:47,685 ReadCallback.java (line 125) Operation timed out - received only 2 responses from /xx.xx.xx.67, /xx.xx.xx.68, command class = org.apache.cassandra.db.SliceFromReadCommand
 INFO [pool-1-thread-44] 2011-03-09 23:34:47,686 CassandraServer.java (line 110) --- timed out: Operation timed out - received only 2 responses from /xx.xx.xx.67, /xx.xx.xx.68,  .
-----
;;;","09/Mar/11 23:16;hudson;Integrated in Cassandra-0.7 #366 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/366/])
    update CHANGES for CASSANDRA-2282 that got committed already (in r1079812 I think)
;;;","09/Mar/11 23:21;jbellis;And what does the Blockfor/repair is %s/%s; setting up requests to %s debug line show?;;;","09/Mar/11 23:39;skamio;Timeout setting in cassandra.yaml is 15sec. So, the following must be the line. IP address of cassandra node which has the log is xx.xx.xx.70.

----
DEBUG [pool-1-thread-121] 2011-03-09 23:34:32,726 ReadCallback.java (line 88) Blockfor/repair is 2/true; setting up requests to /xx.xx.xx.66,/xx.xx.xx.67,/xx.xx.xx.68
----
;;;","10/Mar/11 01:04;jbellis;Sounds like you're getting replies back from digest request but not data.  In StorageProxy:

{code}
            // The data-request message is sent to dataPoint, the node that will actually get
            // the data for us. The other replicas are only sent a digest query.
{code}

Note that debug logs show which node gets the data request.;;;","22/Apr/11 13:59;stuhood;> Sounds like you're getting replies back from digest request but not data.
Created CASSANDRA-2540;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ColumnFamilyOutputFormat performs blocking writes for large batches,CASSANDRA-1434,12472542,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,stuhood,stuhood,26/Aug/10 07:20,16/Apr/19 17:33,22/Mar/23 14:57,28/Sep/10 06:30,0.7 beta 2,,,,0,,,,,,"By default, ColumnFamilyOutputFormat batches {{mapreduce.output.columnfamilyoutputformat.batch.threshold}} or {{Long.MAX_VALUE}} mutations, and then performs a blocking write.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-1368,,,,,,,,,,,,,,,,,"08/Sep/10 08:16;stuhood;0001-Switch-away-from-Multimap-and-fix-regression-introdu.patch;https://issues.apache.org/jira/secure/attachment/12454059/0001-Switch-away-from-Multimap-and-fix-regression-introdu.patch","08/Sep/10 08:16;stuhood;0002-Improve-concurrency-and-add-basic-retries-by-attempt.patch;https://issues.apache.org/jira/secure/attachment/12454060/0002-Improve-concurrency-and-add-basic-retries-by-attempt.patch","19/Sep/10 08:42;stuhood;0003-Switch-RingCache-back-to-multimap.patch;https://issues.apache.org/jira/secure/attachment/12454961/0003-Switch-RingCache-back-to-multimap.patch","19/Sep/10 08:42;stuhood;0004-Replace-Executor-with-map-of-threads.patch;https://issues.apache.org/jira/secure/attachment/12454962/0004-Replace-Executor-with-map-of-threads.patch","19/Sep/10 23:44;jbellis;1434-v3.txt;https://issues.apache.org/jira/secure/attachment/12454975/1434-v3.txt","22/Sep/10 23:00;jbellis;1434-v4.txt;https://issues.apache.org/jira/secure/attachment/12455273/1434-v4.txt","23/Sep/10 04:51;jbellis;1434-v5.txt;https://issues.apache.org/jira/secure/attachment/12455309/1434-v5.txt","28/Sep/10 04:43;jbellis;1434-v6.txt;https://issues.apache.org/jira/secure/attachment/12455749/1434-v6.txt","28/Sep/10 05:18;jbellis;1434-v7.txt;https://issues.apache.org/jira/secure/attachment/12455757/1434-v7.txt",,,,,,9.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20136,,,Mon Sep 27 22:30:29 UTC 2010,,,,,,,,,,"0|i0g51b:",92246,,stuhood,,stuhood,Normal,,,,,,,,,,,,,,,,,"30/Aug/10 15:31;mrflip;The blocking behavior is causing 'broken pipe' errors (even with relatively small batch sizes) when cassandra latency is high. (This is afaict not network latency but response latency due to a compaction or flush, etc.)

It also makes the whole cluster resonate: one slow node blocks many writers, which then all unblock at the same time, write bursts of enough size to cause a compaction or GC, etc simultaneously on every node. This means adding more writers doesn't work around the blocking write;;;","03/Sep/10 14:19;stuhood;0001 and 0003 are minor fixes, but 0002:
* Avoids blocking processing for writes (but only 2 * batchSize mutations may be in memory at a time, so we may still block)
* Changes the default batchSize to 2^14
* Rotates through possible endpoints for a range per flush, which should more evenly distribute client connections when there are small numbers of keys in play

One issue we haven't tackled yet is how to handle failures: I've reopened CASSANDRA-1264 to handle that.;;;","03/Sep/10 14:24;stuhood;Doh... this applies atop CASSANDRA-1368.;;;","04/Sep/10 01:43;stuhood;0004 collects all replicas for each range in RingCache, which I broke in 1322 (previously, we were completely rebuilding the tokenmap using a replication strategy, which would have recreated the lost information).;;;","07/Sep/10 00:13;jbellis;why the double-flushing in close()?  can you add a comment for that?;;;","07/Sep/10 00:18;jbellis;committed 01.  declining to apply 03; in general refactoring out a method that is called in a single place obscures control flow rather than clarifies it.  04 looks ok but i'm not sure to what degree it depends on 02 (see above) so leaving alone for now.;;;","07/Sep/10 04:10;mrflip;Right now the code does { buffer n mutations, holding each  acc. to its endpoint. After n writes, check that all endpoint writes are finished, and dispatch to each endpoint its share of the n mutations }

This is non-blocking at the socket level but ends up being blocking at the app level, and the wide variance in size has bad effects on gc at the cassandra end.

I think the ColumnFamilyRecordWriter would see a speedup & improved stability with  { buffer mutations, holding each acc. to its endpoint. When an endpoint has seen n writes, check that any previous write has finished, and dispatch to this endpoint a full buffer of N mutations }.;;;","08/Sep/10 08:16;stuhood;0001 is changes to the RingCache that survived from v1: it fixes the bug in ringcache that was handled by pre-0004, and removes the multimap.

0002 is a completely revamped ColumnFamilyRecordWriter: nothing from the original patch survived.
* Launches a client thread per unique range, which is responsible for communicating with endpoint replicas for that range.
** The client threads receives mutations for the range from the parent thread on a bounded queue.
** Client threads will attempt to send a full batch of mutations to its replicas in order: this means that each batch gets up to RF retries before failing, but without any failures, connections will always be made to the first replica.
* The parent thread loops trying to offer to queues for client threads, and checks that they are still alive (and fails if they aren't).
* For a N node cluster, up to (2 * N * batchSize) mutations will be in memory at once, so the default batchSize was lowered to 4096.

Fairly well tested against a 12 node cluster: no obvious races or bottlenecks.;;;","08/Sep/10 23:16;jbellis;why is switching from Multimap<Range, InetAddress> to Map<Range, List<InetAddress>> an improvement?;;;","16/Sep/10 22:50;stuhood;Sent via e-mail while I was on vacation:
{quote}
I wanted to dodge object creation, but I guess I assumed that Multimap created Set and Collection facades for every call. Also, there didn't appear to be a way to iterate over unique keys without a facade.
{quote};;;","18/Sep/10 21:16;jbellis;Had a look at 02. Still don't understand your objection to using Multimap -- use ListMultimap if you want to preserve ordering.  It's noticeably cleaner than Map<X, List<Y>>, and the Guava guys are very careful about performance.

Also, 02 kind of abuses an executor when a map of threads would be clearer as to what is going on, while not requiring much more code.  ""Send this message"" is a good task to submit to an executor; ""run an infinite loop pulling messages off a public queue"" is not.;;;","19/Sep/10 08:42;stuhood;Adding 0003 and 0004 with the requested changes: exception handling was also improved a bit.;;;","19/Sep/10 23:44;jbellis;I squashed and added code to keep CFRW from slamming Cassandra with spikes of load: it keeps a pooled connection, and sends mutations one at a time over that.  This is only a trivial amount of overhead compared to using a large batch, since we're not reconnecting for each message.  (The main advantage of using a larger batch is that it gives you an idempotent group of work to replay if necessary, which doesn't matter here.  Under the hood it takes the same code path.)

Also attempted to distinguish between recoverable errors and non- in the exception handling.;;;","20/Sep/10 01:45;stuhood;* ArrayBlockingQueue.isEmpty will kill client threads if their queue is ever empty
* Interrupt handling doesn't seem like a clearer solution for killing client threads: what happens when an interrupt in received during a mutation?
* I don't like the idea of indefinite retries: pretending that the cluster is never unavailable sidesteps Hadoop's own retry system
* As mentioned in IRC, batchSize == 1 does not seem like a good value to hardcode. Any amount of overhead becomes measurable when you are sending small enough values: mutations containing a single integer might increase in size X fold for instance;;;","20/Sep/10 02:17;jbellis;bq. ArrayBlockingQueue.isEmpty will kill client threads if their queue is ever empty

it's while (run || isEmpty).  am i missing something?

bq. what happens when an interrupt in received during a mutation

nothing. InterruptedException is only thrown at well-defined points (one of the few times checked exceptions have done me a favor), and blocking socket send is not one of them.  the JDK uses this pattern to shut down threadpoolexecutors.

bq. I don't like the idea of indefinite retries

the idea is it tries each endpoint, then throws if they all fail. (if !iter.hasnext() then throw)

bq. batchSize == 1 does not seem like a good value to hardcode

as described above, batching > 1 is a misfeature that has been demonstrated to cause badness in practice.;;;","20/Sep/10 02:28;stuhood;> it's while (run || isEmpty). am i missing something?
Ah, sorry.

> the idea is it tries each endpoint, then throws if they all fail. (if !iter.hasnext() then throw)
Gotcha. I missed that part because there doesn't appear to be a way for the parent thread to figure out that a client died, so I assumed that the clients never died. Does it need an UncaughtExceptionHandler that alerts the parent thread? This was what was accomplished by using offer() rather than put() in the previous version.

> as described above, batching > 1 is a misfeature that has been demonstrated to cause badness in practice.
In Cassandra, or in general?;;;","20/Sep/10 02:33;jbellis;> Does it need an UncaughtExceptionHandler that alerts the parent thread?

Probably.  What should the parent thread do?

> In Cassandra, or in general?

In Cassandra.  Flip spent several days in the user IRC channel trying to deal with the load spikes.;;;","20/Sep/10 02:44;stuhood;> Probably. What should the parent thread do?
Probably what the previous version did.

> In Cassandra. Flip spent several days in the user IRC channel trying to deal with the load spikes.
Does changing this patch solve his problem, or are we assuming that?;;;","22/Sep/10 23:00;jbellis;v4 attached to throw IOException on put or stopNicely if the thread has errored out;;;","23/Sep/10 00:35;stuhood;* There is a race condition in put() between {{!run}} and the put itself
* Exceptions thrown by child threads will be logged, but not reported to the Hadoop frontend, since they aren't what kill the parent thread

I'm -0 on v3 and v4: but I'll add a 0005 to separate queue size from batch size, so that we can tune down the batch size for Flip.;;;","23/Sep/10 00:46;jbellis;i'm -1 on batching at all.;;;","23/Sep/10 04:51;jbellis;v5 uses a small batch size and eagerly sends out ""incomplete"" batches if the reducer falls behind;;;","23/Sep/10 09:22;stuhood;* ColumnFamilyOutputFormat.createAuthenticatedClient calls socket.open, so the second open in RangeClient is getting {{TTransportException: Socket already connected}}
* Logging a NPE for the first batch is pretty ugly
* The default batchSize was increased back up to Long.MAX_VALUE: it should probably be significantly lower (32~128) for the reasons you've mentioned;;;","28/Sep/10 04:43;jbellis;v6.

bq. There is a race condition in put() between !run and the put itself

not really.  the check in put is just an attempt to abort earlier if possible.

bq. Exceptions thrown by child threads will be logged, but not reported to the Hadoop frontend

saved actual exceptions.

bq. the second open in RangeClient is getting TTransportException: Socket already connected

fixed

bq. Logging a NPE for the first batch is pretty ugly

nothing is logged.  the alternatives strike me as uglier.

bq. The default batchSize was increased back up to Long.MAX_VALUE

fixed;;;","28/Sep/10 05:10;stuhood;This patch includes a change CompactionManager.java.

>> There is a race condition in put() between !run and the put itself
> not really. the check in put is just an attempt to abort earlier if possible.
put() is called from the parent thread: it isn't interrupted by the child thread, so it will block indefinitely if an exception occurs between lastException != null and the blocking put(). Unlikely, but...

----

Other than those two nitpicks, +1: tested against a 12 node cluster and saw smooth network utilization.;;;","28/Sep/10 05:18;jbellis;v7

bq. This patch includes a change CompactionManager.java

fixed

bq.  it will block indefinitely if an exception occurs between lastException != null and the blocking put()

you're right.  fixed;;;","28/Sep/10 06:00;stuhood;+1;;;","28/Sep/10 06:30;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra 0.5 version throttles and sometimes kills traffic to a node if you restart it.,CASSANDRA-651,12444117,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,gdusbabek,rrabah,rrabah,24/Dec/09 01:35,16/Apr/19 17:33,22/Mar/23 14:57,31/Dec/09 07:20,0.5,,,,1,,,,,,"From the cassandra user message board: 
""I just recently upgraded to latest in 0.5 branch, and I am running
into a serious issue. I have a cluster with 4 nodes, rackunaware
strategy, and using my own tokens distributed evenly over the hash
space. I am writing/reading equally to them at an equal rate of about
230 reads/writes per second(and cfstats shows that). The first 3 nodes
are seeds, the last one isn't. When I start all the nodes together at
the same time, they all receive equal amounts of reads/writes (about
230).
When I bring node 4 down and bring it back up again, node 4's load
fluctuates between the 230 it used to get to sometimes no traffic at
all. The other 3 still have the same amount of traffic. And no errors
what so ever seen in logs. "" ",latest in 0.5 branch,brandon.williams,steel_mental,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Dec/09 02:58;gdusbabek;651-v2.patch;https://issues.apache.org/jira/secure/attachment/12429078/651-v2.patch","30/Dec/09 07:09;gdusbabek;651-v3.patch;https://issues.apache.org/jira/secure/attachment/12429102/651-v3.patch","31/Dec/09 04:36;jbellis;651-v4.patch;https://issues.apache.org/jira/secure/attachment/12429168/651-v4.patch",,,,,,,,,,,,3.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19803,,,Thu Dec 31 12:38:20 UTC 2009,,,,,,,,,,"0|i0g08v:",91470,,,,,Normal,,,,,,,,,,,,,,,,,"24/Dec/09 01:39;rrabah;More info:
 I do see that Node X.X.X.X is dead, and
Node X.X.X.X has restarted.

This show up on all the 3 other servers:
 INFO [Timer-1] 2009-12-22 20:38:43,738 Gossiper.java (line 194)
InetAddress /10.6.168.20 is now dead.

Node /10.6.168.20 has restarted, now UP again
 INFO [GMFD:1] 2009-12-22 20:43:12,812 StorageService.java (line 475)
Node /10.6.168.20 state jump to normal

;;;","24/Dec/09 01:43;rrabah;This is definitely a regression in 0.5. I tested out 0.4.2 and it works perfectly fine, and load goes back up to 100% on the restarted node. ;;;","29/Dec/09 06:00;brandon.williams;I was able to reproduce this in a 4 node setup as well.  The recovered node does not appear to receive any writes after rejoining the cluster, and I receive TimedOutExceptions in the client.   I was able to write directly to the recovered node and things appeared to work, however after a while a different node OOM'd.  I examined the dump in MAT and it shows org.apache.cassandra.net.MessagingService occupies 72.53% of the available heap, followed by java.util.concurrent.LinkedBlockingQueue using 11.87%.;;;","29/Dec/09 11:34;steel_mental;Confirm this issue by 8 nodes tests,

After read system.log, I found after one node down and up again, some other nodes will not establish tcp connection to it(on tcp port 7000 ) forever! 
And read request sent to it (into Pending-Writes because socket channel is closed) will not sent to ethernet forever(from observing tcpdump).

maybe PendingWrites Queue consume lots memory and OOM (yes, it not the recovered node, but  the node who try to send request to recovered node!)

It's seems when recovered node going down, some other node's socket channel was reset , after it come back, these socket channel remain closed, forever
;;;","29/Dec/09 22:26;jbellis;Brandon Williams said: ""I see what's happening with 651 -- TcpConnectionManager keeps trying to reuse a closed connection and never opens new ones to the recovered node""

sounds like a regression from CASSANDRA-488 to me.;;;","30/Dec/09 02:45;gdusbabek;Patched into trunk.  Link the gossip and messaging service so that invalid connection pools can be shutdown when a node goes offline.;;;","30/Dec/09 02:46;gdusbabek;Patch is for trunk.;;;","30/Dec/09 02:51;jbellis;relying on FD to notice is not going to work, though, since FD is not instantaneous (and cannot be made so).  [edit: that is, a node could die and come back, or be partitioned and be available again, quickly enough that FD does not notice but old connections are still invalid.] 

can we have it attempt to reconnect when it encounters an error sending instead?;;;","30/Dec/09 02:58;gdusbabek;The last patch diffed in the wrong direction.  This one is correct.;;;","30/Dec/09 03:09;jbellis;that said, it could be good to have the FD *in addition* to the other, so that if a node goes down for a while that doesn't have much traffic, we don't lose the first attempted message once it's back up unnecessarily.;;;","30/Dec/09 03:54;brandon.williams;I can still reproduce the issue with this patch applied.  I'm receiving the following traceback:

ERROR - Fatal exception in thread Thread[TCP Selector Manager,5,main]
java.lang.AssertionError
        at org.apache.cassandra.net.TcpConnectionManager.destroy(TcpConnectionManager.java:85)
        at org.apache.cassandra.net.TcpConnection.errorClose(TcpConnection.java:319)
        at org.apache.cassandra.net.TcpConnection.connect(TcpConnection.java:364)
        at org.apache.cassandra.net.SelectorManager.doProcess(SelectorManager.java:143)
        at org.apache.cassandra.net.SelectorManager.run(SelectorManager.java:107)

Because ackCon is already null.  With the assert removed, I'm unable to reproduce.;;;","30/Dec/09 07:09;gdusbabek;Updated to include replacing calls to destroy() with shutdown().  Chances are if one TC is crappy, the other is not going to be useful either.;;;","30/Dec/09 09:02;brandon.williams;+1, no longer reproducible with this patch.  The recovered node begins receiving writes normally.;;;","30/Dec/09 21:40;gdusbabek;Jaako and I had a discussion in which we agreed that it would be better to have MessagingService implement IEndPointStateChangeSubscriber and subscribe to the gossiper rather than just implement IFailureDetector.  This would have provided a way to basically turn connection pools on and off and would allow writes to fail a bit faster.

I implemented that this morning.  It's a few more lines of code and all it really buys us is more descriptive error messages.  We'll just have to put up with errored writes until gossip takes the failed node out of the ring.;;;","31/Dec/09 02:51;rrabah;+1 from me too, this seems to have fixed the problem. ;;;","31/Dec/09 03:01;jbellis;I still think we should not rely on FD, or we will still hit this bug with short-lived partitions (which do occur in the wild).

something like brandon's throwing an exception if not connected or awaiting connection.

I'm still baffled that write() apparently doesn't throw when the connection dies...  Are we missing something there?;;;","31/Dec/09 03:27;jbellis;My mistake: write _was_ throwing, but clearing the old conn out was not working.  Still trying to understand why.;;;","31/Dec/09 03:30;gdusbabek;MessagingService.sendOneWay inconveniently swallows errors, so StorageProxy is never wise about them.;;;","31/Dec/09 04:28;jbellis;Got it: the problem is that sendOneWay calls shutdown on SocketException, not errorClose.  so the part that really matters in gary's fix is adding the nulling out to shutdown.;;;","31/Dec/09 04:35;jbellis;this version gets rid of the formely-problematic-and-now-redundant SocketException block, and renames TCM.shutdown to reset.;;;","31/Dec/09 06:06;gdusbabek;Reviewed.  +1 on the v4 patch.;;;","31/Dec/09 07:20;gdusbabek;Fixed in trunk and 0.5 branch.  Patch by Gary Dusbabek and Jonathan Ellis.  Reviewed by same.;;;","31/Dec/09 20:38;hudson;Integrated in Cassandra #309 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/309/])
      TcpConnectionManager was holding on to disconnected connections, giving the false indication they were being used.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError SSTableSliceIterator.java:126,CASSANDRA-866,12458569,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,btoddb,btoddb,10/Mar/10 02:24,16/Apr/19 17:33,22/Mar/23 14:57,13/Apr/10 10:22,0.6.1,,,,0,,,,,,"also seeing these, using cassandra-0.6.0-beta2/

2010-03-09 07:37:57,683 ERROR [ROW-READ-STAGE:77] [CassandraDaemon.java:78] Fatal exception in thread Thread[ROW-READ-STAGE:77,5,main]
java.lang.AssertionError
        at org.apache.cassandra.db.filter.SSTableSliceIterator$ColumnGroupReader.<init>(SSTableSliceIterator.java:126)
        at org.apache.cassandra.db.filter.SSTableSliceIterator.<init>(SSTableSliceIterator.java:59)
        at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:63)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:851)
        at org.apache.cassandra.db.ColumnFamilyStore.cacheRow(ColumnFamilyStore.java:748)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:773)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:740)
        at org.apache.cassandra.db.Table.getRow(Table.java:381)
        at org.apache.cassandra.db.SliceByNamesReadCommand.getRow(SliceByNamesReadCommand.java:56)
        at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:80)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:40)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
",,brandon.williams,johanoskarsson,thuske,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Apr/10 21:06;jbellis;ASF.LICENSE.NOT.GRANTED--866.txt;https://issues.apache.org/jira/secure/attachment/12441337/ASF.LICENSE.NOT.GRANTED--866.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19896,,,Tue Apr 13 02:22:40 UTC 2010,,,,,,,,,,"0|i0g1k7:",91683,,,,,Normal,,,,,,,,,,,,,,,,,"10/Mar/10 02:56;jbellis;are you missing the ""caused by"" part of the exception?  this looks like CASSANDRA-857 to me;;;","10/Mar/10 03:09;btoddb;unfortunately i no longer have the log.  i don't believe i missed a caused by, but i'll keep an eye out for this error to happen again;;;","10/Mar/10 04:28;btoddb;Actually i did have the logs and there is no caused by for these AssertionErrors;;;","17/Mar/10 00:28;btoddb;i still see these a lot ... here is a slightly different stack trace:

2010-03-16 06:57:20,191 ERROR [HINTED-HANDOFF-POOL:1] [DebuggableThreadPoolExecutor.java:94] Error in executor futuretask
java.util.concurrent.ExecutionException: java.lang.AssertionError
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:86)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.AssertionError
        at org.apache.cassandra.db.filter.SSTableSliceIterator$ColumnGroupReader.<init>(SSTableSliceIterator.java:126)
        at org.apache.cassandra.db.filter.SSTableSliceIterator.<init>(SSTableSliceIterator.java:59)
        at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:63)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:800)
        at org.apache.cassandra.db.ColumnFamilyStore.cacheRow(ColumnFamilyStore.java:697)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:722)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:689)
        at org.apache.cassandra.db.HintedHandOffManager.sendMessage(HintedHandOffManager.java:122)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:250)
        at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:80)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:280)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
;;;","20/Mar/10 05:35;jbellis;how big are the index files corresponding to your sstables?  in particular, do you have any > 2 GB?;;;","23/Mar/10 06:10;btoddb;yes ...

-rw-rw-r-- 1 bburruss bburruss 2506278504 Mar 22 14:09 /data/cassandra-data/data/uds/bucket-2858-Index.db
-rw-rw-r-- 1 bburruss bburruss  170769982 Mar 22 14:24 /data/cassandra-data/data/uds/bucket-3150-Index.db
-rw-rw-r-- 1 bburruss bburruss  170040554 Mar 22 14:39 /data/cassandra-data/data/uds/bucket-3155-Index.db
-rw-rw-r-- 1 bburruss bburruss   45092660 Mar 22 14:41 /data/cassandra-data/data/uds/bucket-3156-Index.db
-rw-rw-r-- 1 bburruss bburruss    5758530 Mar 22 14:47 /data/cassandra-data/data/uds/bucket-3161-Index.db
-rw-rw-r-- 1 bburruss bburruss    5775322 Mar 22 14:52 /data/cassandra-data/data/uds/bucket-3166-Index.db
-rw-rw-r-- 1 bburruss bburruss    1454824 Mar 22 14:53 /data/cassandra-data/data/uds/bucket-3167-Index.db
-rw-rw-r-- 1 bburruss bburruss    1439313 Mar 22 14:54 /data/cassandra-data/data/uds/bucket-3168-Index.db
-rw-rw-r-- 1 bburruss bburruss    1448648 Mar 22 14:54 /data/cassandra-data/data/uds/bucket-3169-Index.db
-rw-rw-r-- 1 bburruss bburruss    1447836 Mar 22 14:55 /data/cassandra-data/data/uds/bucket-3170-Index.db
-rw-rw-r-- 1 bburruss bburruss    5778333 Mar 22 14:56 /data/cassandra-data/data/uds/bucket-3171-Index.db
-rw-rw-r-- 1 bburruss bburruss    1473373 Mar 22 14:56 /data/cassandra-data/data/uds/bucket-3172-Index.db
-rw-rw-r-- 1 bburruss bburruss    1457463 Mar 22 14:57 /data/cassandra-data/data/uds/bucket-3173-Index.db
-rw-rw-r-- 1 bburruss bburruss    1462000 Mar 22 14:58 /data/cassandra-data/data/uds/bucket-3174-Index.db
-rw-rw-r-- 1 bburruss bburruss    1451259 Mar 22 14:59 /data/cassandra-data/data/uds/bucket-3175-Index.db
-rw-rw-r-- 1 bburruss bburruss    5829897 Mar 22 14:59 /data/cassandra-data/data/uds/bucket-3176-Index.db
-rw-rw-r-- 1 bburruss bburruss   22964898 Mar 22 15:00 /data/cassandra-data/data/uds/bucket-3177-Index.db
-rw-rw-r-- 1 bburruss bburruss    1475776 Mar 22 15:00 /data/cassandra-data/data/uds/bucket-3178-Index.db
-rw-rw-r-- 1 bburruss bburruss    1462095 Mar 22 15:01 /data/cassandra-data/data/uds/bucket-3179-Index.db
-rw-rw-r-- 1 bburruss bburruss    1453720 Mar 22 15:02 /data/cassandra-data/data/uds/bucket-3180-Index.db
-rw-rw-r-- 1 bburruss bburruss    1337336 Mar 22 15:03 /data/cassandra-data/data/uds/bucket-3181-Index.db
-rw-rw-r-- 1 bburruss bburruss    5718888 Mar 22 15:03 /data/cassandra-data/data/uds/bucket-3182-Index.db
-rw-rw-r-- 1 bburruss bburruss    1489499 Mar 22 15:04 /data/cassandra-data/data/uds/bucket-3183-Index.db
-rw-rw-r-- 1 bburruss bburruss    1520616 Mar 22 15:05 /data/cassandra-data/data/uds/bucket-3184-Index.db
;;;","23/Mar/10 06:24;jbellis;Can you test w/ latest 0.6 svn branch, to see if the fix in CASSANDRA-857 for large index files helps?  (this did not make it into beta3 unfortunately);;;","23/Mar/10 06:41;btoddb;trying it now;;;","23/Mar/10 07:43;btoddb;slightly different, but still getting error

2010-03-22 16:39:26,164 ERROR [ROW-READ-STAGE:22] [CassandraDaemon.java:78] Fatal exception in thread Thread[ROW-READ-STAGE:22,5,main]
java.lang.AssertionError: DecoratedKey(151727172804471108783669089800305019461, vmguest85__791890602)
        at org.apache.cassandra.db.filter.SSTableNamesIterator.<init>(SSTableNamesIterator.java:56)
        at org.apache.cassandra.db.filter.NamesQueryFilter.getSSTableColumnIterator(NamesQueryFilter.java:69)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:830)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:750)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:719)
        at org.apache.cassandra.db.Table.getRow(Table.java:381)
        at org.apache.cassandra.db.SliceByNamesReadCommand.getRow(SliceByNamesReadCommand.java:56)
        at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:80)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:40)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
;;;","27/Mar/10 03:59;jbellis;What is the difference between queries that work, and queries that do not?

(I'm assuming you do have some queries that work.)

Do the problematic ones start working when you force DiskAccessMode to standard?;;;","27/Mar/10 04:16;btoddb;all of my queries are the same - i'm doing straight get, put, deletes.  one column family, one column.  get a ""bucket"" of data, put a ""bucket"" of data.

i don't know which ones are working at which ones don't.  i haven't tried correlating the AssertionError back to a particular request.  i can say that not all gets fail.

last night i saw a bunch of AssertionErrors and then eventually the node OOM exception.  what assertion is failing?

here's my column family definition.

      <ColumnFamily CompareWith=""BytesType"" Name=""bucket""
                    RowsCached=""20%""
                    KeysCached=""0%""
                    />
;;;","27/Mar/10 04:30;jbellis;I've committed an improved assertion to the 0.6 branch that will give the key and filename involved so we can have a look and see what's going on there.;;;","27/Mar/10 04:31;jbellis;Please re-test with that, and if the given data file is small enough, zip it up w/ its index and filter and send it to my gmail address.  Otherwise we can work out another way to test.;;;","28/Mar/10 10:13;jbellis;Bumping to 0.6.1.

In the meantime, if you are affected by this switching DiskAccessMode to standard should work around the bug.;;;","28/Mar/10 10:18;btoddb;I'm out of the office.  Back on 4/5/2010.
;;;","02/Apr/10 03:20;brandon.williams;I tried reproducing this with the data set provided.  I couldn't reproduce at all on a 64bit jvm, and I tried on a 32bit, which also succeeded with the disk mode set to auto.  I had a theory that this was caused by forcing mmap mode on 32bits, but that actually raises a different exception:

Exception in thread ""main"" java.io.IOException: Map failed
        at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:803)
        at org.apache.cassandra.io.SSTableReader.mmap(SSTableReader.java:206)
        at org.apache.cassandra.io.SSTableReader.<init>(SSTableReader.java:152)
        at org.apache.cassandra.io.SSTableReader.<init>(SSTableReader.java:216)
        at org.apache.cassandra.io.SSTableReader.open(SSTableReader.java:121)
        at org.apache.cassandra.io.SSTableReader.open(SSTableReader.java:112)
        at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:304)
        at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:329)
        at org.apache.cassandra.tools.SSTableExport.main(SSTableExport.java:373)
Caused by: java.lang.OutOfMemoryError: Map failed
        at sun.nio.ch.FileChannelImpl.map0(Native Method)
        at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:800)
        ... 8 more


Todd, can you give us more details about your environment?;;;","02/Apr/10 03:28;jbellis;the most important question is, ""what key is failing.""  (this is part of the error message, now.);;;","02/Apr/10 03:28;jbellis;... and does it fail reproducibly, and if so, what query can we use to repro.;;;","07/Apr/10 00:21;jbellis;Todd: can you update?;;;","07/Apr/10 00:50;btoddb;64bit CentOS  2.6.18

single column family, only one column.  simple get, put, deletes.  would you like my storage-conf.xml?

i don't know what key, but it is reproducible.  i could make it happen simply by starting the node.  during startup i will see the exception.  it doesn't happen until some amount of data is in the database.  in other words, it runs fine until either the number of keys or the amount of data crosses some threshold.

i can't repo it at the moment as the cluster is shared and i needed to erase data and work on something else.  but i can try again probably later this week.;;;","07/Apr/10 05:57;jbellis;Great, I'm glad we downloaded a 72 GB test corpus that you blew away. :(

The key is logged with every exception message in latest 0.6 code.  From there I would hope that it would be easy for you to track down what command is causing it.  If not, turning on debug logging on the Cassandra would get that information for you.
;;;","08/Apr/10 07:20;thuske;I think we are seeing this error as well.

ERROR [ROW-READ-STAGE:17] 2010-04-07 21:04:36,861 CassandraDaemon.java (line 78) Fatal exception in thread Thread[ROW-READ-STAGE:17,5,main]
java.lang.AssertionError: DecoratedKey(112702293498592974518440554745446764341, 4452215176) != DecoratedKey(112702294131127473014102114707935791646, 11667886399) in /data2/data/Twitter/Statuses-4327-Data.db
	at org.apache.cassandra.db.filter.SSTableSliceIterator$ColumnGroupReader.<init>(SSTableSliceIterator.java:127)
	at org.apache.cassandra.db.filter.SSTableSliceIterator.<init>(SSTableSliceIterator.java:59)
	at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:63)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:830)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:750)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:719)
	at org.apache.cassandra.db.Table.getRow(Table.java:381)
	at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:59)
	at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:80)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:40)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)

We're using the RandomPartitioner, and a replication factor of 3, the relevant SSTable is 146GB, and the index file is 18GB.  This assertion only occurs on the primary node, whereas the data is returned correctly from the other 2 when asked directly.  We had been running with 0.6beta2 and a DiskAccessMode of ""auto"" which set the mode to mmap.  We've since upgraded to 0.6rc1, and changed the DAM to ""standard"", but the error still occurs.

From cassandra-cli, connected to the primary node:
cassandra> get Twitter.Statuses['11667886399']           
...RpcTimeout seconds later...
Exception Internal error processing get_slice

Connected to an unrelated node:
cassandra> get Twitter.Statuses['11667886399']
...RpcTimeout seconds later...
Exception null

We have a number of these errors, and for each one, the token for the key read off disk is numerically very close to that of the requested key:
DecoratedKey(79954008423729056632733094063639634941, 8307722089) != DecoratedKey(79954009259417349514057411741701713236, 6153489921)
DecoratedKey(80379343112914474054622124950679452815, 3768834481) != DecoratedKey(80379344845214890401447094271897994174, 11653098762)
DecoratedKey(80779546346447228478584703771606621740, 8569013773) != DecoratedKey(80779548447419746111153057677392388488, 11764962920)
DecoratedKey(82844158105946904890171938436663575930, 4464394167) != DecoratedKey(82844159691805202566257389052615565753, 6874217428)
DecoratedKey(84289221786768678402050522652258180028, 6057213014) != DecoratedKey(84289224459791176731103465787486907832, 9899742458)
DecoratedKey(86300142475818241616994325294057703135, 5832470418) != DecoratedKey(86300142677962509841823563793986169119, 11448189666)
DecoratedKey(92092914035356318872651100377285498797, 3473992372) != DecoratedKey(92092915436251199165174090571909314827, 8851204821)
DecoratedKey(94745421400532682848559427993237380460, 9893103006) != DecoratedKey(94745431874154322071681265706426033545, 10438638106);;;","08/Apr/10 09:51;jbellis;Tim, can you make that sstable (+ index + filter) available for us to debug locally?;;;","09/Apr/10 05:34;btoddb;awright, big daddy ... reproduced again, and data is saved ;)

2010-04-07 21:21:10,520 ERROR [ROW-READ-STAGE:4] [CassandraDaemon.java:78] Fatal exception in thread Thread[ROW-READ-STAGE:4,5,main]
java.lang.AssertionError: DecoratedKey(147587030576932389559405437065042613628, vmguest85__-1131008275) != DecoratedKey(147587045543996727516366006491335105792, vmguest85__-888697030) in /data/cassandra-data/data/uds/bucket-1439-Data.db
        at org.apache.cassandra.db.filter.SSTableSliceIterator$ColumnGroupReader.<init>(SSTableSliceIterator.java:127)
        at org.apache.cassandra.db.filter.SSTableSliceIterator.<init>(SSTableSliceIterator.java:59)
        at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:63)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:830)
        at org.apache.cassandra.db.ColumnFamilyStore.cacheRow(ColumnFamilyStore.java:727)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:752)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:719)
        at org.apache.cassandra.db.Table.getRow(Table.java:381)
        at org.apache.cassandra.db.SliceByNamesReadCommand.getRow(SliceByNamesReadCommand.java:56)
        at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:70)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:40)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)

emailed jonathan an http site to download data files that reproduce this problem.

AssertionError happened on node 105 (see ring below), and according to the keys shown in above exception, 105 is the primary for the key.  ReplicationFactor = 3 so 102 and 103 are replicas, right?  i can use the cassandra-cli to retrieve the value on the key's replica nodes, just not on the primary node.

Address       Status     Load          Range                                      Ring
                                       170141183460469231731687303715884105728
192.168.132.102Up         41.64 GB      42535295865117307932921825928971026431     |<--|
192.168.132.103Up         41.33 GB      85070591730234615865843651857942052863     |   |
192.168.132.104Up         41.52 GB      127605887595351923798765477786913079295    |   |
192.168.132.105Up         36.86 GB      170141183460469231731687303715884105728    |-->|
;;;","09/Apr/10 07:31;kingryan;Jonathan- 

We can't easily make that data available because it contains private data from some of our users. Sorry. :(;;;","10/Apr/10 21:06;jbellis;Patch attached, with unit test.

Thanks to Todd for getting us a reproducible test case!;;;","12/Apr/10 23:38;btoddb;i'll verify the fix with my dataset today by grabbing the tip of 0.6 branch and applying patch.  stay tuned;;;","13/Apr/10 00:25;gdusbabek;encodedUTF8Length doesn't account for 4-byte chars:

0x00-0x7f : 1 byte
0x80-0x7ff: 2 bytes
0x800-0xffff: 3 bytes
>=0x10000 (technically to 0x10ffff): 4 bytes.;;;","13/Apr/10 01:55;jbellis;DataInput.writeUTF, which is what keys are written with, only uses 1-3 bytes.  http://java.sun.com/javase/6/docs/api/java/io/DataInput.html;;;","13/Apr/10 03:00;gdusbabek;+1;;;","13/Apr/10 04:13;btoddb;I have been running with the patch for about 2 hrs now and haven't seen any AssertionErrors.  I would have previously seen them by now.  I'll keep watching things, but it looks good to me.;;;","13/Apr/10 07:18;jmhodges;We're pushing out a version of the current cassandra-0.6 branch with this patch applied. A canary machine is currently freaking out. We're going to move forward and hope it's just this particular machine, but is there any hope of getting this patch rebased to 0.6-rc1 (a.k.a. 0.6-final)?;;;","13/Apr/10 10:22;jbellis;committed.  it will be in 0.6.1.

no, not taking requests to rebase vs other revisions, but it should be fairly straightforward, code churn has been low in 0.6 post-rc1.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get_columns_in fails when when routed to a node that isn't the home for the key,CASSANDRA-21,12421486,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,sandeep_tata,sandeep_tata,sandeep_tata,29/Mar/09 05:28,16/Apr/19 17:33,22/Mar/23 14:57,30/Mar/09 23:26,,,,,0,,,,,,"get_columns_in fails when the request cannot be satisfied locally.

What steps will reproduce the problem?
1. Insert multiple columns in some row R in a Cassandra cluster that 
contains more than 1 node.
2. Submit a get_columns_in query to a bunch of nodes. Using the python thrift 
interface, this is something  like: 
./Cassandra-remote -h node0:9160 get_columns_in 'Mailbox' 'rowid123' 
'HeaderList' ""['col1','col2']""
./Cassandra-remote -h node1:9160 get_columns_in 'Mailbox' 'rowid123' 
'HeaderList' ""['col1','col2']""
 

I've traced the error to a bug in how ReadMessage.java gets de-serialized. See attached unit-test to reproduce this.
I'm also attaching a patch that fixes this.",all,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Mar/09 05:30;sandeep_tata;ReadMessageTest.java;https://issues.apache.org/jira/secure/attachment/12404065/ReadMessageTest.java","29/Mar/09 05:30;sandeep_tata;patch_readmessage.txt;https://issues.apache.org/jira/secure/attachment/12404066/patch_readmessage.txt",,,,,,,,,,,,,2.0,sandeep_tata,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19519,,,Mon Mar 30 15:26:07 UTC 2009,,,,,,,,,,"0|i0fwen:",90848,,,,,Normal,,,,,,,,,,,,,,,,,"29/Mar/09 05:30;sandeep_tata;ReadMessageTest simple constructs a readmessage, serializes it, deserializes it and checks if this was the same as the original.

My patch fixes it for the case of get_columns_in breaking. Haven't tested other cases.;;;","29/Mar/09 07:22;neophytos;Check get_slice_by_names ot get_slice_super_by_names. ;;;","30/Mar/09 22:32;neophytos;My previous comment was in relation to the actual name of the ""IN"" operator in Cassandra, i.e. get_slice_by_names instead of get_columns_in.  The bug is still present though. It's missing an else before ""if( sinceTimestamp > 0 )"" in ReadMessage.java. Otherwise, get_columns_in / get_slice_by_names would fail when the request is not satisfied locally.;;;","30/Mar/09 23:26;jbellis;Applied with minor modifications (toString method added to ReadMessage), thanks.

For future reference, it saves time if you can use the Cassandra code style conventions (particularly braces on new lines).  Also Cassandra is using testng, not junit.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra silently loses data when a single row gets large,CASSANDRA-7,12416995,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,sandeep_tata,sandeep_tata,sandeep_tata,17/Mar/09 03:40,16/Apr/19 17:33,22/Mar/23 14:57,21/Mar/09 05:29,,,,,0,,,,,,"When you insert a large number of columns in a single row, Cassandra silently loses some of these inserts.
This does not happen until the cumulative size of the columns in a single row exceeds several megabytes.

Say each value is 1MB large, 

insert(""row"", ""col0"", value, timestamp)
insert(""row"", ""col1"", value, timestamp)
insert(""row"", ""col2"", value, timestamp)
...
...
insert(""row"", ""col100"", value, timestamp)

Running: 
get_column(""row"", ""col0"")
get_column(""row"", ""col1"")
...
..
get_column(""row"", ""col100"")

The sequence of get_columns will fail at some point before 100. This was a problem with the old code in code.google also.
I will attach a small program that will help you reproduce this. 

1. This only happens when the cumulative size of the row exceeds several megabytes. 
2. In fact, the single row should be large enough to trigger an SSTable flush to trigger this error.
3. No OutOfMemory errors are thrown, there is nothing relevant in the logs.




","code in trunk, Red Hat 4.1.2-33,  Linux version 2.6.23.1-42.fc8, java version ""1.7.0-nio2""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Mar/09 03:44;sandeep_tata;BigReadWriteTest.java;https://issues.apache.org/jira/secure/attachment/12402305/BigReadWriteTest.java","17/Mar/09 04:06;sandeep_tata;dirty_bit_patch.txt;https://issues.apache.org/jira/secure/attachment/12402307/dirty_bit_patch.txt","17/Mar/09 04:22;sandeep_tata;dirty_bit_patch_v2.txt;https://issues.apache.org/jira/secure/attachment/12402308/dirty_bit_patch_v2.txt",,,,,,,,,,,,3.0,sandeep_tata,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19511,,,Fri Mar 20 21:29:33 UTC 2009,,,,,,,,,,"0|i0fwbr:",90835,,,,,Critical,,,,,,,,,,,,,,,,,"17/Mar/09 03:44;sandeep_tata;This program simply writes a bunch of data first and tries to read it all back.
If the write phase spans multiple SSTables, you will notice that the read phase fails with missing values.
The ""--numColumns"" needs to be large enough -- try something like 6000. It might take a couple of minutes to run the test.



;;;","17/Mar/09 04:00;sandeep_tata;Another way to check that this is really a bug is to list the columns in the serialized SSTable. You will notice a large contiguous range of missing columns. The trunk does not have a ""show SSTable"" utility -- mine depends on a bunch of other code, I'll try and put one up soon.

Opening up the SSTable in a binary file viewer might be enough -- you'll see a large swath of zeroes in the middle where real data should be.

;;;","17/Mar/09 04:06;sandeep_tata;This is a patch to the data-loss bug.

This seemingly simple fix caused a whole lot of headache :-)

The bug is in io.BufferedRandomAccess.writeAtMost

If it takes more than 2 invocations to write out the data, except for the first and last calls, the data is not flushed out to disk because the dirty bit is not set correctly.

BufferedRandomAccess.write(byte[] b, int off, int len) will work correctly if 1 or 2 iterations happen in the loop because the bit is set to true before first invocation and the bit is set to true in the end. The fix simply sets this bit to true after the call to  System.arraycopy.
;;;","17/Mar/09 04:22;sandeep_tata;Also patches the element comment to say :

/* Write at most ""len"" bytes *from* ""b"" starting at .... */

Thanks to jbellis for pointing this out!;;;","17/Mar/09 05:37;neophytos;The issue still persists. I remember sending a bug report about this (old repository). IIRC, the issue was that the ColumnIndexer would raise a java.util.ConcurrentModificationException when it tried to read the sortedSet_ from EfficientBidiMap while flushing. No exception is raised using the new repository  you still get zero-size Data files when you batch insert lots of simple and super columns in the same row without any throttle. 

The basic test I am using for this is as follows: I have a collection of 100000 content items. For each of the content items I batch insert (in the same row) 5000-10000 supercolumns each time for a total of about  a million different supercolumn names before I get the first zero-sized data file.  I have tried this with different MemtableSizeInMB, MemTableObjectCountInMillions (also tried with a smaller threshold), MemtableLifetimeInDays, and with your patch. Always, the same behavior (except when I use a throttle).

PS. The old repository had an issue with addColumn in ColumnFamily.java but that was not it, see: 
http://groups.google.com/group/cassandra-user/browse_thread/thread/329b7700ebda3072/2da827df755eb168;;;","17/Mar/09 05:48;sandeep_tata;Neo:

This issue only deals with the serialization bug while flushing the memtable to disk. It has nothing to do with batch_insert, or supercolumn sizes, or thread-unsafe access of the sortedSet in EfficientBidiMap. 

Can you open a new ticked and attach a driver program that might help us reproduce the problem you're describing?;;;","17/Mar/09 05:58;neophytos;(a) It happens when you insert a large number of columns in a single row
(b) Cassandra silently loses some of these inserts (batch inserts are also inserts). 
(c) This DOES happen when the threshold is violated (the cumulative size is only one of the reasons for the threshold to be violated)
(d) It is also while flushing the memtable to disk.

Yes, I can open a new ticket but it seemed relevant to this issue.;;;","21/Mar/09 05:29;sandeep_tata;Fixed by svn commit: r756155;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
race with insufficiently constructed Gossiper,CASSANDRA-1160,12466186,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,mdennis,jbellis,jbellis,04/Jun/10 21:59,16/Apr/19 17:33,22/Mar/23 14:57,11/Jun/10 01:06,0.6.3,0.7 beta 1,,,0,,,,,,"Gossiper.start needs to be integrated into the constructor.  Currently you can have threads using the gossiper instance before start finishes (or even starts?), resulting in tracebacks like this:

ERROR [GMFD:1] 2010-06-02 10:45:49,878 CassandraDaemon.java (line 78) Fatal exception in thread Thread[GMFD:1,5,main]
java.lang.AssertionError
	at org.apache.cassandra.net.Header.<init>(Header.java:56)
	at org.apache.cassandra.net.Header.<init>(Header.java:74)
	at org.apache.cassandra.net.Message.<init>(Message.java:58)
	at org.apache.cassandra.gms.Gossiper.makeGossipDigestAckMessage(Gossiper.java:294)
	at org.apache.cassandra.gms.Gossiper$GossipDigestSynVerbHandler.doVerb(Gossiper.java:935)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:40)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
ERROR [GMFD:2] 2010-06-02 10:45:49,880 CassandraDaemon.java (line 78) Fatal exception in thread Thread[GMFD:2,5,main]
java.lang.AssertionError
	at org.apache.cassandra.net.Header.<init>(Header.java:56)
	at org.apache.cassandra.net.Header.<init>(Header.java:74)
	at org.apache.cassandra.net.Message.<init>(Message.java:58)
	at org.apache.cassandra.gms.Gossiper.makeGossipDigestAckMessage(Gossiper.java:294)
	at org.apache.cassandra.gms.Gossiper$GossipDigestSynVerbHandler.doVerb(Gossiper.java:935)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:40)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-1010,,,,,,,,,,,,"08/Jun/10 01:50;mdennis;0001-cassandra-0.6-1160.patch;https://issues.apache.org/jira/secure/attachment/12446505/0001-cassandra-0.6-1160.patch","09/Jun/10 00:04;mdennis;0002-cassandra-0.6-1160.patch;https://issues.apache.org/jira/secure/attachment/12446601/0002-cassandra-0.6-1160.patch","09/Jun/10 05:06;mdennis;0003-cassandra-0.6-1160.patch;https://issues.apache.org/jira/secure/attachment/12446628/0003-cassandra-0.6-1160.patch","10/Jun/10 23:39;mdennis;0004-cassandra-0.6-1160.patch;https://issues.apache.org/jira/secure/attachment/12446768/0004-cassandra-0.6-1160.patch",,,,,,,,,,,4.0,mdennis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20012,,,Thu Jun 10 17:06:55 UTC 2010,,,,,,,,,,"0|i0g3d3:",91975,,,,,Low,,,,,,,,,,,,,,,,,"08/Jun/10 01:50;mdennis;moving start() to Gossiper.<init> causes failures in testClientOnlyMode and I didn't see a way to readily make client/server mode available to Gossiper.<init>

0001-cassandra-0.6-1160.patch delays creation of the Gossiper instance until start is called.  Any thread attempting to obtain a reference to Gossiper before it is started busy waits until it is fully initialized.

I'm not sure I actually like this solution, but thought I would submit for comments/feedback.

If we end up taking this solution, I'll submit a patch for trunk as well.;;;","08/Jun/10 06:26;gdusbabek;What were the failures in testClientOnlyMode?  I'm sure they can be worked around.

I'm not a fan of Gossiper.preRegistrations.  I think it is a code smell that we're not spinning up Gossiper the right way.  What if we were to make Gossiper.start() blocking until we were sure it was in a healthy state (keep in mind the single-node cluster) and then make sure it was called before MessageService.listen()?;;;","08/Jun/10 07:33;mdennis;I should have been more clear about that.  The failures in StorageServiceClientTest are because files from DatabaseDescriptor.getAllDataFileLocations() exist (the test asserts they do not).  This is because when I moved start into <init> I passed the same generation number that that initServer() uses, but initClient (used by the test) passes a generation number based on currentTimeMillis() which doesn't cause the file paths to exist.  It wasn't clear to me there was an easy way to determine what generation to use.

More importantly, this lead me to notice that both StorageService and StorageLoadBalancer are registered with Gossiper before start is called.  If start was moved into <init> I was afraid of introducing new race conditions (e.g. Gossiper starting up, doing things that would have resulted in publications and then later receiving the registration for such publications).

I agree that Gossiper is likely not being spun up correctly.  I think this applies to other things as well (in general things seem to have many interdependencies on boot.  I don't have anything specific to point at, but it feels like there are several race conditions that just happen to usually not be exhibited).

One solution I've used in the past that I like for this in general is that none of the singletons are inited in their own classes, but only by a BootInitializer of sorts that completely controls when the objects are inited/started/created/etc, in particular controlling the order things are done.  A single entry point into the system if you will.  This seems like some work from the point we're at now and probably a bit unnecessary.

Along those lines I had a similar patch that I didn't submit that took a third argument, List<IEndpointStateChangeSubsciber> initialSubscribers.  Each initialSubscriber was registered before the existing code in start was called.  initServer built a list of the StorageService and StorageLoadBalance instances and passed that to start to ensure none of the publications were missed by either service.  Like the 0001-cassandra-0.6-1160.patch references to .instance were replaced with getInstance() calls which busy waited (via yield) until start completed but this meant that StorageService was managing StorageLoadBalancer registrations and that didn't feel right, hence preRegistrations() (so each could manager their own registrations).

So, basically two issues/questions if Gossiper.start moves to Gossiper.<init>:

1) how does Gossiper.<init> get the correct generation number (server v client)?

2) what about missed publications because Gossiper was started before StorageService and/or StorageLoadBalancer got a chance to register?
;;;","08/Jun/10 21:18;gdusbabek;I'm taking a closer look. In the mean time can you rebase your patch?;;;","09/Jun/10 00:04;mdennis;0002-cassandra-0.6-1160.patch is against r952679;;;","09/Jun/10 02:36;mdennis;just FYI, if Gossiper.start() is moved before MessageService.listen() the unit tests show sporadic (~ 1 in 5) errors even though the tests still pass.



{code}
    [junit] ------------- Standard Error -----------------
    [junit] Exception in thread ""ACCEPT-/127.0.0.1"" java.lang.RuntimeException: java.nio.channels.ClosedChannelException
    [junit] 	at org.apache.cassandra.net.MessagingService$SocketThread.run(MessagingService.java:488)
    [junit] Caused by: java.nio.channels.ClosedChannelException
    [junit] 	at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:130)
    [junit] 	at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:84)
    [junit] 	at org.apache.cassandra.net.MessagingService$SocketThread.run(MessagingService.java:477)
{code}

and

{code}
    [junit] Testsuite: org.apache.cassandra.service.AntiEntropyServiceTest
    [junit] Tests run: 10, Failures: 0, Errors: 0, Time elapsed: 3.282 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] ERROR 11:18:22,111 Error in executor futuretask
    [junit] java.util.concurrent.ExecutionException: java.lang.NullPointerException
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
    [junit] 	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
    [junit] 	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:86)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:619)
    [junit] Caused by: java.lang.NullPointerException
    [junit] 	at java.util.AbstractCollection.addAll(AbstractCollection.java:303)
    [junit] 	at org.apache.cassandra.service.AntiEntropyService.getNeighbors(AntiEntropyService.java:149)
    [junit] 	at org.apache.cassandra.service.AntiEntropyService$Validator.call(AntiEntropyService.java:491)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	... 2 more
{code};;;","09/Jun/10 03:03;gdusbabek;Intermittent test failures indicate you have a timing problem.

The fact that the assertion (the original intermittent problem) fails because a static class member is null makes me think we're dealing with static initializers firing in the order we don't suspect.  

I'd start by getting the inner classes (the verb handlers) out of Gossiper and into their own classes.  Then there is the race between Gossiper and MessageService.  You could let the gossiper start before the MessageService starts listening, but have the GossipTimerTask check to make sure the MessageService is listening before it sends out any gossip requests.  This still isn't going to protect us in the case of a node getting restarted where other nodes already know about it and and diligently trying to contact what they thought was a dead node.
;;;","09/Jun/10 05:06;mdennis;0003-cassandra-0.6-1160.patch as requested moves inner classes and Gossiper to top level classes and has Gossiper TimerTask busy wait until MessagingService is up;;;","10/Jun/10 21:15;gdusbabek;Last patch is looking good, except you should consider replacing the busy-wait Thread.yield() stuff with a CountDownLatch and a simple MessageService.awaitListen() call or something like that.  My reasoning is that Thread.yeild() behaves differently across platforms and can possibly starve threads running at a lower priority.;;;","10/Jun/10 22:01;jbellis;also consider using SimpleCondition.;;;","10/Jun/10 22:31;gdusbabek;Wouldn't a Condition cause the waiter to block if somehow signalAll() (which in this case would only be called once) was called before await()?  I know it's not likely, but there isn't any such problem with a CountDownLatch.;;;","10/Jun/10 22:57;jbellis;// [SimpleCondition] fulfils the Condition interface without spurious wakeup problems
// (or lost notify problems either: that is, even if you call await()
// _after_ signal(), it will work as desired.)
;;;","10/Jun/10 23:14;gdusbabek;I see.  I figured SimpleCondition was something out of the JDK.

Spurious bikeshed comment:  SimpleCondition.set should be volatile, no?  otherwise set=true ins't guaranteed to be visible to any thread stuck in await().;;;","10/Jun/10 23:27;jbellis;I believe the synchronized keyword takes care of that, but wait/notify are admittedly subtle.  I could be wrong.;;;","10/Jun/10 23:39;mdennis;0004-cassandra-0.6-1160.patch uses a gate to wait until MessagingService is listening;;;","10/Jun/10 23:40;gdusbabek;You're right. synchronized establishes happens-before with respect to object state.;;;","11/Jun/10 01:06;gdusbabek;replaced the latch with a SimpleCondition and committed.  Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"After bootstrap, some nodes cannot find keys",CASSANDRA-682,12444943,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,brandon.williams,brandon.williams,08/Jan/10 01:49,16/Apr/19 17:33,22/Mar/23 14:57,15/Jan/10 01:24,0.5,,,,0,,,,,,"I started a 3 node cluster with RF=3 and loaded about 3M keys into it.  I proceeded to bootstrap a 4th node and it completed.  Upon performing a read test, I found that  two nodes could find all keys, and two nodes could find none, depending upon which machine I queried.  Here is the ring output:

Address       Status     Load          Range                                      Ring
                                       127605887595351923798765477786913079296
10.242.4.13   Up         1023.44 MB    0                                                                                       |<--|
10.242.4.10   Up         1.34 GB       42535295865117307932921825928971026432     |    |
10.242.4.11   Up         1.33 GB       85070591730234615865843651857942052864     |    |
10.242.4.12   Up         1.41 GB       127605887595351923798765477786913079296   |-->|

Token 0 was the newly bootstrapped node.  Tokens 127605887595351923798765477786913079296 and 85070591730234615865843651857942052864 were able to read all keys, the other two nodes were not.","debian lenny amd64 OpenJDK 64-Bit Server VM (build 1.6.0_0-b11, mixed mode)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-696,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19817,,,Thu Jan 14 17:24:01 UTC 2010,,,,,,,,,,"0|i0g0fj:",91500,,,,,Normal,,,,,,,,,,,,,,,,,"09/Jan/10 02:45;brandon.williams;I tested bootstrap on 0.5 and this did not occur, for what it's worth.;;;","09/Jan/10 02:46;jbellis;i take it you specified initialtoken manually, get the result of 0?;;;","09/Jan/10 02:52;brandon.williams;Yes, the tokens are spread evenly and I used 0 instead of 2**127, simply because it's easier to remember and type.;;;","13/Jan/10 19:09;steel_mental;maybe it's the same bug as 673?

does you use multi-keyspace, and one of them is empty??;;;","14/Jan/10 03:18;brandon.williams;No, this was with the default config, so only one keyspace.;;;","14/Jan/10 08:01;brandon.williams;Some more info after repeating...I received the follow error a couple of times on node 42535295865117307932921825928971026432 while node 0 was still in the process of bootstrapping:

INFO - Waiting for transfer to /10.242.4.13 to complete
WARN - Running on default stage - beware
WARN - Running on default stage - beware
WARN - Problem reading from socket connected to : java.nio.channels.SocketChannel[conne
cted local=/10.242.4.10:45868 remote=/10.242.4.13:7000]
java.io.IOException: Reached an EOL or something bizzare occured. Reading from: /10.242
.4.13 BufferSizeRemaining: 16
        at org.apache.cassandra.net.io.StartState.doRead(StartState.java:44)
        at org.apache.cassandra.net.io.ProtocolState.read(ProtocolState.java:39)
        at org.apache.cassandra.net.io.TcpReader.read(TcpReader.java:95)
        at org.apache.cassandra.net.TcpConnection$ReadWorkItem.run(TcpConnection.java:4
27)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:11
10)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:6
03)
        at java.lang.Thread.run(Thread.java:636)
INFO - Closing errored connection java.nio.channels.SocketChannel[connected local=/10.2
42.4.10:45868 remote=/10.242.4.13:7000]

No other nodes reported any errors.;;;","15/Jan/10 00:49;gdusbabek;These are likely the same issue.;;;","15/Jan/10 01:24;gdusbabek;Same root cause as CASSANDRA-696.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Quorum reads are not monotonically consistent,CASSANDRA-2494,12504486,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,sbridges,sbridges,17/Apr/11 22:29,05/Dec/19 03:38,22/Mar/23 14:57,12/Aug/11 03:29,1.0.0,,,,2,,,,,,"As discussed in this thread,

http://www.mail-archive.com/user@cassandra.apache.org/msg12421.html

Quorum reads should be consistent.  Assume we have a cluster of 3 nodes (X,Y,Z) and a replication factor of 3. If a write of N is committed to X, but not Y and Z, then a read from X should not return N unless the read is committed to at  least two nodes.  To ensure this, a read from X should wait for an ack of the read repair write from either Y or Z before returning.

Are there system tests for cassandra?  If so, there should be a test similar to the original post in the email thread.  One thread should write 1,2,3... at consistency level ONE.  Another thread should read at consistency level QUORUM from a random host, and verify that each read is >= the last read.",,aaaron,cherro,donnchadh,doubleday,hanzhu,jay.zhuang,jeromatron,jjordan,scode,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-15442,,,,,,,,"04/Aug/11 00:34;jbellis;2494-v2.txt;https://issues.apache.org/jira/secure/attachment/12489205/2494-v2.txt","27/Jul/11 09:46;jbellis;2494.txt;https://issues.apache.org/jira/secure/attachment/12487930/2494.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20654,,,Thu Sep 24 20:45:58 UTC 2015,,,,,,,,,,"0|i0gbp3:",93325,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"18/Apr/11 05:01;scode;As far as I can tell the consistency being asked for was never promised by Cassandra is in fact not expected.

The expected behavior of writes is that they propagate; the difference between ONE and QUORUM is just how many are required to receive a write prior to a return to the client with a successful error code. For reads, that means you may get lucky at ONE or you may get lucky at QUORUM; the positive guarantee is in the case of a *completing* QUORUM write followed by a QUORUM read.

So just to be clear, although I don't think this is what is being asked for: As far as I know, it has never been the case, nor the intent to promise, that a write which fails is guaranteed not to eventually complete. Simply ""fixing"" reads is not enough; by design the data will be replicated during read-repair and AES - this is how consistency is achieved in Cassandra.

However, it sounds like what is being asked for is not that they don't propagate in the event of a write ""failure"", but just that reads don't see the writes until they are sufficiently propagated to guarantee that any future QUORUM read will also see the data. I can understand that is desirable, in the sense of achieving monotonically forward-moving data as the benchmark/test from the e-mail thread does. Another way to look at is that maybe you never want to read data successfully prior to achieving a certain level of replication, in order to avoid a client ever seeing data that may suddenly go away due to e.g. a node failure in spite of said failure not exceeding the number of failures the cluster was designed to survive.

So the key point would be the bit about guaranteeing that any ""future QUORUM read will see the data or data subsequently overwritten"", and actively read-repairing and waiting for it to happen would take care of that. It would be important to ensure that the act of ensuring a quorum of nodes have seen the data is the important part; one should not await for a quorum to agree on the *current* version of the data as that would create potentially unbounded round-trips on hotly contended data.

Thing to consider: One might think about cases where read-repair is currently not done, like range slices, and how an implementation that requires read repair for consistency affects that.

;;;","18/Apr/11 12:20;sbridges;Peter Shuller wrote,

""However, it sounds like what is being asked for is not that they don't propagate in the event of a write ""failure"", but just that reads don't see the writes until they are sufficiently propagated to guarantee that any future QUORUM read will also see the data.""

Yes, that is the issue.  The comment in the bug about writing at ONE and reading at QUORUM is just a way of testing this new guarantee in a distributed test, if Cassandra has those.;;;","19/Apr/11 04:41;jjordan;I would think that reads at QUORUM should never go backwards.  Even if the Write was at ZERO.  If there were writes to the cluster of a=1 time=5, a=2 time=10, a=3 time=15, and I do a read at QUORUM which tells me a=3 time=15, I should not be able to do another read at QUORUM and get a=2 time=10.;;;","22/Apr/11 13:17;stuhood;W plus R must be _greater than_ N for consistency.

EDIT: And adding a blocking implicit write step to QUORUM reads by waiting for read repair is not reasonable.;;;","22/Apr/11 16:26;scode;I don't think anyone is claiming otherwise, unless I'm misunderstanding. The problem is that while the ""if sucessfully written to quorum, subsequent quorum reads will see it"" guarantee is indeed maintained, it is possible for quorum reads to see data go backwards (on a timeline) in the event of a *failed* attempted quorum write. This includes the possibility of reads seeing data that then permanently vanishes, even though you only lost say 1 node that you designed your cluster for surviving (RF >= 3, QUORUM). (""lost 1 node"" can be substituted with ""killed 1 node in periodic commit mode"")

I still don't think this is a violation of what was promised, but I can see how making the further guarantee would make for more useful consistency semantics in some cases.

With respect to implicit write: An alternative is to adjust reconciliation logic when applied as part of reads (as opposed to AES,  hinted hand-off, writes) to take consistency level into account and only consider columns whose timestamp is >= the greatest timestamp that has quorum (off the top of my head I think that should be correct in call cases, but I didn't think this through terribly).
;;;","22/Apr/11 16:34;scode;Ok, so my last suggestion is in fact broken. A counter example is:

 A: column @ t1
 B: column @ t2
 C: column @ t3

If A + B is participating, A's column @ t1 has timestamp quorum and would be selected. If B + C is participating, B's column is picked. Thus, a read where B + C participates will see data that will be reverted once A + B happens to be picked.

Note to self: Think before posting.
;;;","22/Apr/11 22:45;sbridges;I think the guarantee of quorum reads not seeing old writes once a quorum read sees a new write is  very useful.  I suspect most people already think that this guarantee occurs, including, it seems, Jonathan Ellis whose quote can be found in the email thread linked to in the bug,

""The important guarantee this gives you is that once one quorum read sees the new value, all others will too.   You can't see the newest version, then see an older version on a subsequent write [sic, I
assume he meant read], which is the characteristic of non-strong consistency""



;;;","22/Apr/11 23:01;slebresne;The problem is you are considering the consistency of reads but not write. The guarantee is: ""quorum reads will not see old quorum write once a quorum read sees a new quorum"". Period. I you don't consider the consistency of a write, consider the case of a CL.ANY write. In this case, the update may not be at all on any replica. How can we ensure the quorum read property that you want ? We query all nodes for quorum reads in case there is an hint somewhere ?

If you look at the Consistency part of http://wiki.apache.org/cassandra/ArchitectureOverview, it seems to me that it is pretty clear that the consistency of reads *and* writes is involved to achieve strong consistency. So I would hope 'most people' are aware of that.;;;","22/Apr/11 23:11;scode;The issue is that of *failed* QUORUM writes. I.e., you design your system to use QUORUM writes and QUORUM reads, and expect that once a QUORUM read sees a given piece of data a subsequent QUORUM read will also see it (or a later data). A *failed* QUORUM write that was replicated to less than a QUORUM would be visible as part of QUORUM reads that happen to touch one of those replicas, but there is no guarantee that subsequent reads see it.

I was under the impression this was never an intended guarantee. Apparently I may be wrong about that given the jbellis quote above. In either case, it is certainly not an *actual* guarantee given by the current implementation.

The guarantee that a *successful* QUORUM write is seen by a subsequent QUORUM read is, as far as I can tell, not in question here.

;;;","22/Apr/11 23:22;sbridges;To be clear, this is a new guarantee.  The current guarantee is R+W>N gives you consistency.  This bug is asking that a successful quorum read of A means that A has been committed to a quorum of nodes.

""How can we ensure the quorum read property that you want ?""

If when reading at quorum, and no quorum can be found which agrees on a particular value, then the coordinator ( ? ) will wait for acks of read repair writes (or perhaps just do normal writes) to be returned from a sufficient number of nodes to ensure that the value has been committed to a quorum of nodes.

Without this new guarantee it is hard for readers to function correctly.  The reader does not know that the quorum write failed, or is still in progress, so without reading at ALL, the R+W>N guarantee does not help the reader.



;;;","27/Jul/11 09:46;jbellis;I don't see any reason not to guarantee that the replicas we read from provide monotonic read consistency.

Patch attached to do this.
;;;","27/Jul/11 19:08;slebresne;Ok, I now see what you mean :)
Makes perfect sense.

Comments on the patch:
* There is a number of case where scheduleRepairs may not have been called (if the read for repair timeout and/or we had no or only 1 response or we have the situation were removeDeleted removes everything), so repairResults will be null in those cases. In SP, we should check for it.
* That new wait can extend the rpc timeout to almost twice what it should be. I agree that it is not a huge deal, but by exposing the 'startTime' stored in RepairCallback we can make it so we don't extend it that way.
* Shouldn't we give the same love to range requests, now that we do repairs there too ?;;;","04/Aug/11 00:34;jbellis;bq. There is a number of case where scheduleRepairs may not have been called

defaulted repairResults to emptyList.

bq. That new wait can extend the rpc timeout to almost twice what it should be

Well, sort of -- rpctimeout is working exactly as intended, i.e., to prevent waiting indefinitely for a node that died after we sent it a request.  Treating it as ""max time to respond to client"" has never really been correct.  (E.g., in the CL > ONE case we can already wait up to rpctimeout twice, one for the original digest read set, and again for the data read after mismatch.)  So I don't think we should try to be clever with that here.

bq. Shouldn't we give the same love to range requests, now that we do repairs there too

done.;;;","11/Aug/11 23:34;slebresne;bq. Well, sort of – rpctimeout is working exactly as intended, i.e., to prevent waiting indefinitely for a node that died after we sent it a request. Treating it as ""max time to respond to client"" has never really been correct. (E.g., in the CL > ONE case we can already wait up to rpctimeout twice, one for the original digest read set, and again for the data read after mismatch.) So I don't think we should try to be clever with that here.

Fair enough. It would probably be useful to make rpctimeout meaning closer to ""max time to respond to client"". Created CASSANDRA-3018 for that though.

+1 on v2.;;;","12/Aug/11 03:29;jbellis;committed;;;","12/Aug/11 04:14;hudson;Integrated in Cassandra #1017 (See [https://builds.apache.org/job/Cassandra/1017/])
    provide monotonic read consistency
patch by jbellis; reviewed by slebresne for CASSANDRA-2494

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1156758
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/service/StorageProxy.java
* /cassandra/trunk/src/java/org/apache/cassandra/service/RepairCallback.java
* /cassandra/trunk/src/java/org/apache/cassandra/service/RowRepairResolver.java
* /cassandra/trunk/src/java/org/apache/cassandra/utils/FBUtilities.java
* /cassandra/trunk/src/java/org/apache/cassandra/service/RangeSliceResponseResolver.java
;;;","25/Sep/15 04:45;aaaron;The relevant code in the patch has changed significantly. Is the monotonic read consistency guarantee still provided?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GossipTimerTask stops running if an Exception occurs,CASSANDRA-1289,12469488,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,wadey,wadey,17/Jul/10 04:29,16/Apr/19 17:33,22/Mar/23 14:57,18/Jul/10 10:10,0.6.4,,,,0,,,,,,"The GossipTimerTask run() method has a try/catch around its body, but it re-throws all Exceptions as RuntimeExceptions. This causes the GossipTimerTask to no longer run (due to the way the underlying Java Timer implementation works), stopping the periodic gossip status checks.

Combine this problem with a bug like CASSANDRA-757 (not yet fixed in 0.6.x) and you get into a state where the server keeps running, but gossip is no longer occurring, preventing node addition / removal from happening.

I see two potential choices:
1) Log the error but don't re-throw it so that the GossipTimerTask will continue to run on its next interval.
2) Shutdown the server, since continuing to run without gossip subtly breaks other functionality / knowledge of other nodes.",,brandon.williams,mojodna,wadey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jul/10 06:02;brandon.williams;1289.txt;https://issues.apache.org/jira/secure/attachment/12449769/1289.txt",,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20060,,,Sun Jul 18 02:09:51 UTC 2010,,,,,,,,,,"0|i0g45j:",92103,,,,,Normal,,,,,,,,,,,,,,,,,"18/Jul/10 05:34;brandon.williams;Patch to catch the exception and log it, as suggested in CASSANDRA-757;;;","18/Jul/10 10:09;jbellis;committed w/ changes since it was simple:

uses .error instead of .warn

uses .error(message, exception) so the entire stack trace will be logged;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MutationTest of the distributed-test suite fails,CASSANDRA-1964,12495250,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,stuhood,tawamudu,tawamudu,11/Jan/11 22:08,16/Apr/19 17:33,22/Mar/23 14:57,20/Jan/11 03:53,0.7.1,0.8 beta 1,,,0,,,,,,"MutationTest of the distributed-test test suite causes errors on trunk.

To reproduce, issue:

ant distributed-test -Dwhirr.config=<path_to_whirr_config_file>

from the project root.

relevant whirr configuration settings used:

whirr.service-name=cassandra
whirr.cluster-name=cassandra_test
whirr.instance-templates=4 cassandra
whirr.version=0.3.0-incubating-SNAPSHOT
whirr.location-id=us-west-1
whirr.image-id=us-west-1/ami-16f3a253
whirr.hardware-id=m1.large
whirr.blobstore.provider=s3
whirr.blobstore.container=tawamuducassandratests
whirr.provider=ec2
whirr.run-url-base=http://hoodidge.net/scripts/

Traceback:

distributed-test:   
     [echo] running distributed tests
    [junit] WARNING: multiple versions of ant detected in path for junit 
    [junit]          jar:file:/usr/share/ant/lib/ant.jar!/org/apache/tools/ant/Project.class
    [junit]      and jar:file:/Users/mallen/Desktop/cassandra-trunk/build/lib/jars/ant-1.6.5.jar!/org/apache/tools/ant/Project.class
    [junit] Testsuite: org.apache.cassandra.MovementTest
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 446.65 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] SLF4J: Class path contains multiple SLF4J bindings.
    [junit] SLF4J: Found binding in [jar:file:/Users/mallen/Desktop/cassandra-trunk/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
    [junit] SLF4J: Found binding in [jar:file:/Users/mallen/Desktop/cassandra-trunk/build/test/lib/jars/whirr-cli-0.3.0-incubating-SNAPSHOT.jar!/org/slf4j/impl/StaticLoggerBinder.class]
    [junit] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
    [junit]  WARN 12:12:46,654 over limit 471283/262144: wrote temp file
    [junit]  WARN 12:12:48,572 over limit 374979/262144: wrote temp file
    [junit]  WARN 12:12:50,701 over limit 892174/262144: wrote temp file
    [junit]  WARN 12:12:54,442 over limit 612358/262144: wrote temp file
    [junit] ------------- ---------------- ---------------
    [junit] Testsuite: org.apache.cassandra.MutationTest
    [junit] Tests run: 4, Failures: 0, Errors: 3, Time elapsed: 110.971 sec
    [junit] 
    [junit] Testcase: testInsert(org.apache.cassandra.MutationTest):    Caused an ERROR
    [junit] null
    [junit] NotFoundException()
    [junit]     at org.apache.cassandra.thrift.Cassandra$get_result.read(Cassandra.java:6900)
    [junit]     at org.apache.cassandra.thrift.Cassandra$Client.recv_get(Cassandra.java:568)
    [junit]     at org.apache.cassandra.thrift.Cassandra$Client.get(Cassandra.java:541)
    [junit]     at org.apache.cassandra.MutationTest.getColumn(MutationTest.java:210)
    [junit]     at org.apache.cassandra.MutationTest.testInsert(MutationTest.java:66)
    [junit] 
    [junit] 
    [junit] Testcase: testWriteAllReadOne(org.apache.cassandra.MutationTest):   Caused an ERROR
    [junit] null
    [junit] NotFoundException()
    [junit]     at org.apache.cassandra.thrift.Cassandra$get_result.read(Cassandra.java:6900)
    [junit]     at org.apache.cassandra.thrift.Cassandra$Client.recv_get(Cassandra.java:568)
    [junit]     at org.apache.cassandra.thrift.Cassandra$Client.get(Cassandra.java:541)
    [junit]     at org.apache.cassandra.MutationTest.getColumn(MutationTest.java:210)
    [junit]     at org.apache.cassandra.MutationTest.testWriteAllReadOne(MutationTest.java:87)
    [junit] 
    [junit] 
    [junit] Testcase: testWriteOneReadAll(org.apache.cassandra.MutationTest):   Caused an ERROR
    [junit] null
    [junit] TimedOutException()
    [junit]     at org.apache.cassandra.thrift.Cassandra$insert_result.read(Cassandra.java:15392)
    [junit]     at org.apache.cassandra.thrift.Cassandra$Client.recv_insert(Cassandra.java:907)
    [junit]     at org.apache.cassandra.thrift.Cassandra$Client.insert(Cassandra.java:879)
    [junit]     at org.apache.cassandra.MutationTest.insert(MutationTest.java:202)
    [junit]     at org.apache.cassandra.MutationTest.testWriteOneReadAll(MutationTest.java:185)
    [junit] 
    [junit] 
    [junit] TEST org.apache.cassandra.MutationTest FAILED
    [junit] Tests FAILED

BUILD FAILED
/Users/mallen/Desktop/cassandra-trunk/build.xml:557: The following error occurred while executing this line:
/Users/mallen/Desktop/cassandra-trunk/build.xml:540: Some distributed test(s) failed.

Total time: 10 minutes 15 seconds
","OS X Version 10.6.6
java version ""1.6.0_22""
Java(TM) SE Runtime Environment (build 1.6.0_22-b04-307-10M3261)
Java HotSpot(TM) 64-Bit Server VM (build 17.1-b03-307, mixed mode)
Apache Ant version 1.8.1 compiled on September 21 2010",johanoskarsson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-1986,CASSANDRA-1989,CASSANDRA-2005,,,,,,,CASSANDRA-1965,CASSANDRA-1988,CASSANDRA-1986,,,,,,"15/Jan/11 12:49;stuhood;0001-Add-a-builder-for-a-retrying-get-attempt-and-use-to-im.txt;https://issues.apache.org/jira/secure/attachment/12468443/0001-Add-a-builder-for-a-retrying-get-attempt-and-use-to-im.txt",,,,,,,,,,,,,,1.0,stuhood,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20385,,,Wed Jan 19 20:15:44 UTC 2011,,,,,,,,,,"0|i0g8hb:",92804,,xedin,,xedin,Low,,,,,,,,,,,,,,,,,"12/Jan/11 03:27;stuhood;I think we have a race condition on our hands. Trunk tests just passed for me.

I'll see if we can add more blocking.;;;","14/Jan/11 12:59;stuhood;Attaching a fix for the first failing testcase: when doing a write at ONE and a read at ONE, we retry to ensure that the read eventually succeeds.

The other two failures are more interesting, and are not bugs in the testcase. Kelvin is investigating, but it appears that reads and writes at ALL are not blocking appropriately on the server side: he'll open a separate ticket for this tomorrow.;;;","15/Jan/11 05:29;kelvin;bug found.;;;","15/Jan/11 05:29;kelvin;may also be related.;;;","15/Jan/11 12:49;stuhood;Ready for review: CASSANDRA-1986 is closed, and we pass all tests consistently.

I was a little bit uncomfortable having to set such high retry timeouts in cases where we need to wait for gossip (in particular when waiting for the ""fail fast"" behaviour from CASSANDRA-1803 to kick in), but it is a bit of a necessary evil with cloud providers. I opened CASSANDRA-1988 to discuss ways to improve this behaviour.;;;","15/Jan/11 13:45;stuhood;Ack... after a rebase, it looks like we're blocked again: this time by CASSANDRA-1989. These tests restart the nodes multiple times.;;;","16/Jan/11 23:10;jbellis;With 1989 committed, is this patch good to go?;;;","17/Jan/11 06:37;stuhood;+1
This is ready.;;;","17/Jan/11 07:13;jbellis;Should have asked earlier -- for 0.7.1 or just trunk?  (I ask b/c CASSANDRA-1989 just affected trunk.);;;","17/Jan/11 08:37;stuhood;Both of 0.7.1 and 0.8 should receive this fix.;;;","19/Jan/11 19:38;xedin;Tested on both trunk and cassandra-0.7 branches, patch fixes TimedOutException and NotFoundException in both cases, code looks good but maybe move RetryingAction to the BaseTest to make it reusable?;;;","20/Jan/11 03:15;stuhood;> maybe move RetryingAction to the BaseTest to make it reusable
Good idea... I've actually done this in a separate issue: CASSANDRA-2005;;;","20/Jan/11 03:45;xedin;Great, I have nothing to add then! :);;;","20/Jan/11 03:53;jbellis;committed;;;","20/Jan/11 04:15;hudson;Integrated in Cassandra-0.7 #178 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/178/])
    fix distributed-test MutationTest
patch by stuhood; reviewed by Pavel Yaskevich for CASSANDRA-1964
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
When multiple DataFileDirectory entries exist in config files for the same sstable can end up in different directories.,CASSANDRA-730,12446198,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,djnym,djnym,21/Jan/10 14:38,16/Apr/19 17:33,22/Mar/23 14:57,21/Jan/10 21:35,,,,,0,,,,,,"I have a cluster of 8 cassandra nodes, each node has 3 disks and I have the following in my storage-conf.xml
  <DataFileDirectories>
      <DataFileDirectory>/var/lib/cassandra/data1</DataFileDirectory>
      <DataFileDirectory>/var/lib/cassandra/data2</DataFileDirectory>
      <DataFileDirectory>/var/lib/cassandra/data3</DataFileDirectory>
  </DataFileDirectories>
Upon adding a new node, setting AutoBootstrap to true and starting it, it seemed to start bootstrapping, but then
failed with the following exception.

java.io.IOException: rename failed of /var/lib/cassandra/data3/retarget/user_to_acctids-1-Index.db
java.io.IOError: java.io.IOException: rename failed of /var/lib/cassandra/data3/retarget/user_to_acctids-1-Index.db
        at org.apache.cassandra.io.SSTableWriter.rename(SSTableWriter.java:154)
        at org.apache.cassandra.io.SSTableWriter.renameAndOpen(SSTableWriter.java:161) 
        at org.apache.cassandra.io.Streaming$StreamCompletionHandler.onStreamCompletion(Streaming.java:301)
        at org.apache.cassandra.net.io.ContentStreamState.handleStreamCompletion(ContentStreamState.java:108)
        at org.apache.cassandra.net.io.ContentStreamState.read(ContentStreamState.java:90)
        at org.apache.cassandra.net.io.TcpReader.read(TcpReader.java:95)
        at org.apache.cassandra.net.TcpConnection$ReadWorkItem.run(TcpConnection.java:445)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.IOException: rename failed of /var/lib/cassandra/data3/retarget/user_to_acctids-1-Index.db
        at org.apache.cassandra.utils.FBUtilities.renameWithConfirm(FBUtilities.java:306)
        at org.apache.cassandra.io.SSTableWriter.rename(SSTableWriter.java:150)
        ... 9 more

Upon inspecting the directories I noticed the following

find /var/lib/cassandra -follow
/var/lib/cassandra
/var/lib/cassandra/data2
/var/lib/cassandra/data2/system
/var/lib/cassandra/data2/retarget
/var/lib/cassandra/data2/retarget/user_to_acctids-tmp-1-Filter.db
/var/lib/cassandra/.cassandra-placeholder
/var/lib/cassandra/data3
/var/lib/cassandra/data3/system
/var/lib/cassandra/data3/retarget
/var/lib/cassandra/data3/retarget/user_to_acctids-tmp-1-Data.db
/var/lib/cassandra/logs
/var/lib/cassandra/logs/system.log
/var/lib/cassandra/data1
/var/lib/cassandra/data1/system
/var/lib/cassandra/data1/retarget
/var/lib/cassandra/data1/retarget/user_to_acctids-tmp-1-Index.db
/var/lib/cassandra/commitlog
/var/lib/cassandra/commitlog/CommitLog-1264050238700.log

showing the sstable files were in different directories.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19841,,,Thu Jan 21 18:58:32 UTC 2010,,,,,,,,,,"0|i0g0pz:",91547,,,,,Normal,,,,,,,,,,,,,,,,,"21/Jan/10 17:29;steel_mental;it's looks like same bug as CASSANDRA-716;;;","21/Jan/10 21:35;jbellis;Ah, Michael is right.  Closing this one so we track it in one place.  Sorry Anthony. :);;;","22/Jan/10 02:51;djnym;Not a problem, just glad it's on the radar, and hopefully it'll make it into 0.5.1.

By the way, a workaround when you get into this situation is to switch to a single directory in the config for the bootstrapping node and restart it.

Just stopping the bootstrapping node, actually puts the system into a broken state, where you constantly get these exceptions

ERROR [pool-1-thread-64] 2010-01-21 07:37:42,610 Cassandra.java (line 1064) Internal error processing insert
java.lang.AssertionError
        at org.apache.cassandra.locator.TokenMetadata.getToken(TokenMetadata.java:212)  
        at org.apache.cassandra.locator.AbstractReplicationStrategy.getHintedMapForEndpoints(AbstractReplicationStrategy.java:129)
        at org.apache.cassandra.locator.AbstractReplicationStrategy.getHintedEndpoints(AbstractReplicationStrategy.java:76)
        at org.apache.cassandra.service.StorageService.getHintedEndpointMap(StorageService.java:1186)
        at org.apache.cassandra.service.StorageProxy.insertBlocking(StorageProxy.java:169)
        at org.apache.cassandra.service.CassandraServer.doInsert(CassandraServer.java:466)
        at org.apache.cassandra.service.CassandraServer.insert(CassandraServer.java:417)
        at org.apache.cassandra.service.Cassandra$Processor$insert.process(Cassandra.java:1056)
        at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:817) 
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)

So I had to restart the bootstrapping node.  This seems like it could be an issue if for whatever reason you start bootstrapping a node, then have to undo that action.  Maybe there's the need for some sort of 'recover' command which helps you get out of a failed bootstrap?;;;","22/Jan/10 02:58;jbellis;Yes, CASSANDRA-722 has a patch to fix that assertionerror.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BootstrapTest occasionally fails,CASSANDRA-395,12434219,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,sandeep_tata,jbellis,jbellis,28/Aug/09 04:45,16/Apr/19 17:33,22/Mar/23 14:57,28/Aug/09 11:06,0.4,,,,0,,,,,,"sample failure:

    [junit] Testcase: testAntiCompaction1(org.apache.cassandra.db.BootstrapTest):       Caused an ERROR
    [junit] /home/jonathan/projects/cassandra/git-trunk/build/test/cassandra/data/Keyspace1/Standard1-2-Data.db (No such file or directory)                                                                                               
    [junit] java.io.FileNotFoundException: /home/jonathan/projects/cassandra/git-trunk/build/test/cassandra/data/Keyspace1/Standard1-2-Data.db (No such file or directory)                                                                
    [junit]     at java.io.RandomAccessFile.open(Native Method)                                                      
    [junit]     at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)                                        
    [junit]     at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)                                         
    [junit]     at org.apache.cassandra.io.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)        
    [junit]     at org.apache.cassandra.io.FileStruct.<init>(FileStruct.java:49)                                     
    [junit]     at org.apache.cassandra.io.SSTableReader.getFileStruct(SSTableReader.java:321)                       
    [junit]     at org.apache.cassandra.db.ColumnFamilyStore.initializePriorityQueue(ColumnFamilyStore.java:655)     
    [junit]     at org.apache.cassandra.db.ColumnFamilyStore.doFileAntiCompaction(ColumnFamilyStore.java:911)        
    [junit]     at org.apache.cassandra.db.ColumnFamilyStore.doAntiCompaction(ColumnFamilyStore.java:814)            
    [junit]     at org.apache.cassandra.db.BootstrapTest.testAntiCompaction(BootstrapTest.java:63)                   
    [junit]     at org.apache.cassandra.db.BootstrapTest.testAntiCompaction1(BootstrapTest.java:72)                  

I have seen someone else mention this on IRC too.  Most of the time, the test passes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Aug/09 09:25;sandeep_tata;395.patch;https://issues.apache.org/jira/secure/attachment/12417946/395.patch",,,,,,,,,,,,,,1.0,sandeep_tata,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19669,,,Fri Aug 28 15:07:40 UTC 2009,,,,,,,,,,"0|i0fyo7:",91215,,,,,Normal,,,,,,,,,,,,,,,,,"28/Aug/09 08:43;sandeep_tata;Ah this is a fun bug -- looks like this happens if the following sequence occurs:

1. Call to flush memtable
2. Compaction submitted (from storeLocation after flushing memtable) but not yet completed.
3. CFS.doAntiCompaction reads keys from ssTables_
4. Compaction completes
5. SSTableReader.getApproximateKeyCount(files) now points to files that don't exist!

We have to prevent compaction from messing up the list of files that anticompaction needs to work on to build the new file.
I didn't expect this would happen in a short Junit test. Need to see why compaction is getting triggered.
;;;","28/Aug/09 08:55;sandeep_tata;Quick Fix: Change MemtableObjectCountInMillions to 0.0002. This allows memtables to be bigger and avoids kicking off a compaction before the anticompaction can complete. BootstrapTest will no longer exhibit intermittent failures. (verified)

But that's just a band-aid -- the real solution is to synchronize compaction & anticompaction correctly.;;;","28/Aug/09 09:25;sandeep_tata;Patch to call doAntiCompaction in the tests the same way we do in the actual code -- by submitting it to the MinorCompactionManager so it serializes with other compaction tasks.

Ran the tests a 200 times -- I don't see failures anymore.;;;","28/Aug/09 11:06;jbellis;excellent.  committed;;;","28/Aug/09 23:07;hudson;Integrated in Cassandra #180 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/180/])
    call doAntiCompaction in the tests the same way we do in the actual code -- by submitting it to the MinorCompactionManager so it serializes with other compaction tasks.  patch by Sandeep Tata; reviewed by jbellis for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove assumption that Key to Token is one-to-one,CASSANDRA-1034,12463287,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,stuhood,stuhood,29/Apr/10 11:21,16/Apr/19 17:33,22/Mar/23 14:57,01/Dec/11 18:14,1.1.0,,,,5,,,,,,"get_range_slices assumes that Tokens do not collide and converts a KeyRange to an AbstractBounds. For RandomPartitioner, this assumption isn't safe, and would lead to a very weird heisenberg.

Converting AbstractBounds to use a DecoratedKey would solve this, because the byte[] key portion of the DecoratedKey can act as a tiebreaker. Alternatively, we could make DecoratedKey extend Token, and then use DecoratedKeys in places where collisions are unacceptable.",,cburroughs,donal,jeromatron,leonardo.stern,mck,stinkymatt,stuhood,tjake,xedin,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-1600,,,,,,,CASSANDRA-1978,,CASSANDRA-1205,,,,,,"24/Nov/11 00:07;slebresne;0001-Generify-AbstractBounds-v2.patch;https://issues.apache.org/jira/secure/attachment/12504886/0001-Generify-AbstractBounds-v2.patch","30/Nov/11 18:44;slebresne;0001-Generify-AbstractBounds-v3.patch;https://issues.apache.org/jira/secure/attachment/12505606/0001-Generify-AbstractBounds-v3.patch","18/Nov/11 22:20;slebresne;0001-Generify-AbstractBounds.patch;https://issues.apache.org/jira/secure/attachment/12504214/0001-Generify-AbstractBounds.patch","24/Nov/11 00:07;slebresne;0002-Remove-assumption-that-token-and-keys-are-one-to-one-v2.patch;https://issues.apache.org/jira/secure/attachment/12504887/0002-Remove-assumption-that-token-and-keys-are-one-to-one-v2.patch","30/Nov/11 18:44;slebresne;0002-Remove-assumption-that-token-and-keys-are-one-to-one-v3.patch;https://issues.apache.org/jira/secure/attachment/12505607/0002-Remove-assumption-that-token-and-keys-are-one-to-one-v3.patch","18/Nov/11 22:20;slebresne;0002-Remove-assumption-that-token-and-keys-are-one-to-one.patch;https://issues.apache.org/jira/secure/attachment/12504215/0002-Remove-assumption-that-token-and-keys-are-one-to-one.patch","24/Nov/11 00:07;slebresne;0003-unit-test-v2.patch;https://issues.apache.org/jira/secure/attachment/12504888/0003-unit-test-v2.patch","30/Nov/11 18:44;slebresne;0003-unit-test-v3.patch;https://issues.apache.org/jira/secure/attachment/12505608/0003-unit-test-v3.patch","18/Nov/11 22:20;slebresne;0003-unit-test.patch;https://issues.apache.org/jira/secure/attachment/12504216/0003-unit-test.patch","09/Nov/10 13:19;tjake;1034_v1.txt;https://issues.apache.org/jira/secure/attachment/12459138/1034_v1.txt","11/Oct/11 04:42;xedin;CASSANDRA-1034.patch;https://issues.apache.org/jira/secure/attachment/12498465/CASSANDRA-1034.patch",,,,11.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,14985,,,Thu Dec 01 10:14:43 UTC 2011,,,,,,,,,,"0|i0g2lj:",91851,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"29/Apr/10 12:12;jbellis;The problem is that we are using DK both for routing and for local key sorting, partly because it's very convenient to be able to use the ""natural"" compareTo to compare those two kinds of DK.

If the only place we have DK with null key is for the routing case, then the right thing is to convert those usages to raw Tokens and make key non-optional in DK.

i have a nagging feeling that there are more complications though.;;;","29/Apr/10 12:13;stuhood;MD5 collisions are rare enough, so somebody would probably have to write their own partitioner to trigger this.;;;","09/Nov/10 12:54;jbellis;bq. it's very convenient to be able to use the ""natural"" compareTo to compare those two kinds of DK

In particular, we generate (Token, null) DKs for range scans, at least in part because Hadoop thinks in terms of TokenRanges instead of DecoratedKeyRanges.  (Presumably it is still ok to assume that a partitioner generates many more tokens than there are nodes in the cluster; if not, this would need to change.)

We might be able to still do this, if we just say that DK(T, null) always sorts before DK(T, non-null-key) for any given Token T.

I still suspect we're using DK in some places where Token is all we really need.;;;","09/Nov/10 13:19;tjake;Discovered the same issue with a partitioner that shares tokens for many keys.  This patch fixes the issue. all tests pass.;;;","10/Nov/10 00:14;stuhood;Jake's patch is only a partial fix for this problem, so I've moved it to #1720. The core of this ticket is either: changes to the class hierarchy, or changes to ranges.;;;","11/Nov/10 03:07;jbellis;Can you elaborate as to what else needs to be fixed?

As I said above, ""Presumably it is still ok to assume that a partitioner generates many more tokens than there are nodes in the cluster"" so I don't think we need to make Range a DK pair for granularity's sake.;;;","11/Nov/10 03:52;stuhood;> Can you elaborate as to what else needs to be fixed?
I guess the larger problem here is that if a range query asks for 10 rows using a Token range, but there are 1000 rows sharing a particular token, which 10 rows do you return?;;;","11/Nov/10 05:13;jbellis;Okay, so that is a problem because we create DK(Token, null) internally for range queries currently even when using the key-based API.

Once that is fixed I'm fine with saying ""the first 10"" (or if more convenient, ""that's undefined""), since only Hadoop or similar iterate-over-everything approaches should be using Token-based range queries at the API level.  (Again, I'm assuming that there are enough tokens to achieve sufficient granularity, iow, that the number of rows sharing a token is less than the InputSplit size.);;;","04/Mar/11 03:12;slebresne;Attaching a patch that I believe solves this. It makes Range accept both Token and DecoratedKey and makes those two compare together correctly.

It introduces a new marker interface (RingPosition) instead of making DecoratedKey extends Token (for reason explained in the comments of RingPosition but to sum up: I think it's cleaner).

The second patch attached is just a stupid partitioner that use for token the length of the key. It's just for testing and not meant for inclusion. But this shows that with the first patch, you can do correct range query that go from 'the middle of a token' to the 'middle of another one'.

An important note is that this breaks the serialization unit tests, because now an AbstractBounds can use decoratedKeys, and thus serialized AbstractBounds are incompatible with previous version. Not sure how to deal with that though, I though we had a plan for dealing with that but I'll admit I don't remember the details.;;;","16/Mar/11 22:53;slebresne;Patch rebased;;;","24/Mar/11 17:47;slebresne;Rebased patch with code for binary backward compatibility. This still needs the first part of CASSANDRA-2361 to fully pass the serialization unit tests.;;;","26/Mar/11 01:43;jbellis;What is LengthPartitioner for?;;;","26/Mar/11 02:22;slebresne;Oh, it's just a stupid partitioner with tons of collision (and predictable ones) that I used for testing and attached so that other can test too. Not meant for inclusion.;;;","26/Mar/11 02:58;jbellis;Initial feedback:

- I'm a fan of the RingPosition approach
- Less of a fan of pretending that Tokens and DK are equal if the token component of DK is equal.  Shouldn't we force caller to ask Token.equals(DK.token) if that's what they mean? As you pointed out in RP docstring, there is not an is-a relationship there.
- Should we add RP.isToken to encapsulate RP.asDecoratedKey.key == null checks?
- DK docstring is obsolete now
;;;","26/Mar/11 03:24;slebresne;bq. Less of a fan of pretending that Tokens and DK are equal if the token component of DK is equal. Shouldn't we force caller to ask Token.equals(DK.token) if that's what they mean? As you pointed out in RP docstring, there is not an is-a relationship there.

The thing is, we need them to be equal for compareTo() (because we can't have token > keys nor token < keys, otherwise that would mess up our ranges). Then for the equals, the motivation is summed up by the Comparable documentation:
{noformat}
It is strongly recommended (though not required) that natural orderings be consistent with equals. This is so because sorted sets (and sorted maps) without explicit comparators behave ""strangely"" when they are used with elements (or keys) whose natural ordering is inconsistent with equals. In particular, such a sorted set (or sorted map) violates the general contract for set (or map), which is defined in terms of the equals method.
{noformat}
And I do fear that we would get something inconsistent at some point.
But I'm not a super fan either, just felt the less evil of the two choices.
I'm happy with suggestion though and I'll work out the other remarks.

;;;","26/Mar/11 03:39;jbellis;I understand the Comparable docs, but 
- that's primarily concerned w/ compareTo + equals b/t members of the same class
- it's valid to say ""these are tied for sorting purposes, and yet they are not equal""

In other words I'm more worried about subtle bugs if we allow the equals, than if we don't. :)

The Map example is a good one -- if I set

map[token(1)] = foo
map[dk(1, 1)] = bar

I would expect two map entries, not one.  (If you want one, you explicitly use asToken, then there is no ambiguity.)

How about if we add an assert to both equals to make sure we don't pass in the other kind of object?;;;","26/Mar/11 03:44;slebresne;You're right, I'm convinced, it's probably safer to have equals be a true equals.
I'll do the change.;;;","26/Mar/11 18:13;stuhood;I was reaaally hoping we could subclass here... adding RingPosition leads to explicit conversions scattered all over that end up obscuring  implicit conversions.

The hairiest part of subclassing would be renaming all of our Token subclasses with DecoratedKey subclasses, but it cleans up unnecessary references: for example, for a DecoratedKey for ByteOrderPartitioner or LocalPartitioner you have:
{code}
DecoratedKey
   BytesToken token;
      ByteBuffer token;
   ByteBuffer key;
{code}
... while with subclassing you could save two object references:
{code}
DecoratedKey
   ByteBuffer keyAndToken;
{code}

Also, the Comparable dilemma is relatively straightforward with subclassing: Token implements Comparable<Token>, the subclasses override, call super.compare, and if their superclass is equal, fall back to instanceof(myclass) to see whether they can compare the key data.;;;","30/Mar/11 23:21;slebresne;I realize that this is a little more subtle than I first though.

You just cannot compare a Token and a DecoratedKey simply, because a Token is actually a range of keys. Hence dealing with a Range that mixes Token and DecoratedKey correctly is doable, but a bit complicated (typically, it involves declaring multiple different comparison functions). To take quick example, consider that if you mix DK and Token, you must have the following that stands:
{noformat}
    (T(2), T(8)] should not contain DK(T(2), ""foo"") => DK(T(2), ""foo"") < T(2)
    [T(2), T(8)] should contain     DK(T(2), ""foo"") => DK(T(2), ""foo"") >= T(2)
{noformat}
So there is no way to write a compareTo() function dealing with both DK and token.

So I think that it will be simplest to not mix DK and Token in the same ranges. We'll have ranges of Token (for everything related to ring management) and ranges of DK (for rangeSlice and scan). This is what the patch does (and the patch 'generify' AbstractBounds, Range and Bounds (a fair part of the patch) to keep type information around and avoid unnecessary casts all over the place).

We still want to make a rangeSlice/scan over a range of token. To do that, we simply convert a range of Token to a range of DK. This involves declaring for a given token a smallest key and biggest key, and this in turn comes a slight complication related to the minimum token, but the detail are in the docstrings of the patch. I am reasonably confident on that new patch.

(Note that this patch is much bigger than the previous one, but this is mostly due to the generification of Range);;;","31/Mar/11 02:57;jbellis;Can you break the generification out into a separate patch?;;;","02/Apr/11 01:08;slebresne;Generification broken into a separate patch + some tiny code style update;;;","05/Apr/11 00:16;slebresne;Attaching v2 for my second patch. There was some failure in the unit tests for getRestrictedRanges. This fix and improves those test and fix a small bug related to the handling of the minimum value for DecoratedKey.;;;","05/Apr/11 00:18;jbellis;I think we are almost done.  A couple comments:

- DK.isEmpty seems like a bad method name for a Key object -- intuitively, keys are a specific point and should not contain other points except for the obvious identity case.  Would isMinimum be a better name? 
- I don't understand RP.toSplitValue or why DK would throw away information, when calling it.  More generally, I'm unclear why we would have null keys in DK -- shouldn't you use a Token, if you don't have key information?
- using MINIMUM_TOKEN for both sort-before-everything and sort-after-everything values has always been confusing.  Should we introduce a MAXIMUM_TOKEN value to clear that up?;;;","05/Apr/11 05:44;slebresne;Reattaching v2, previous had a stupid mistake, sorry about that.;;;","08/Apr/11 00:24;slebresne;bq. DK.isEmpty seems like a bad method name for a Key object – intuitively, keys are a specific point and should not contain other points except for the obvious identity case. Would isMinimum be a better name?

Actually I don't even like isEmpty for token, so in favor of isMinimum for both DK and token.

bq.  don't understand RP.toSplitValue or why DK would throw away information, when calling it. More generally, I'm unclear why we would have null keys in DK – shouldn't you use a Token, if you don't have key information?

Current patch don't allow to mix token and DK in a range/bounds (because that comes with its whole sets of complications). However getRestrictedRange must be able to break a range of DK based on a node token. So RP.toSplitValue() returns for a given token the value that splits the range: for a token range it's the token itself, but for a DK range, it's the largest DK having this token.
The null keys is related: even though we don't mix DK and token in range, we need to be able to have a range of DK that includes everything from x token to y token. Hence, for a given token t, we need two DK: the smallest DK having t and the biggest DK having t. In the patch, slightly but not totally randomly, I use DK(t, EMPTY_BB) for the smallest key and DK(t, null) for the biggest one, hence the ""need"" for null keys. 

bq. using MINIMUM_TOKEN for both sort-before-everything and sort-after-everything values has always been confusing. Should we introduce a MAXIMUM_TOKEN value to clear that up?

I think that would make wrapping stuffs more complicated. Because then what would be the difference between the following ranges: (MIN, MIN], (MAX, MAX], (MIN, MAX] and (MAX, MIN]. For DK, the code is already enforcing that the we only have one minimum key (that is DK(MIN, EMPTY_BB)) and never ever use DK(MIN, null) because that poses problems. I think a MAX token would make that worst. ;;;","09/Apr/11 04:55;jbellis;bq. RP.toSplitValue() returns for a given token the value that splits the range: for a token range it's the token itself, but for a DK range, it's the largest DK having this token. The null keys is related: even though we don't mix DK and token in range, we need to be able to have a range of DK that includes everything from x token to y token

This feels messy and error-prone to me. I wonder if we haven't found the right approach yet.;;;","10/Apr/11 04:07;stuhood;I agree that using null is a necessary solution here: you need a max value for keys, since they are essentially the ""child"" of a one-token-range. The key range is bounded (since it has parents), but the token range is not, so I agree with sylvain that a MAX_TOKEN is probably not necessary.

One way to remove toSplitValue would be to use DecoratedKey everywhere; DecoratedKey is a compound of the Token and the key blob. The equivalent of today's Token is a DecoratedKey for that token with a null key: it compares greater than all valid child keys, so it contains them.

I hope that it won't muddy the water, but the <empty> as min and <null> as max approach is the same one I took forthe first cut of the file-format, and it worked very well. You can use the min/max values to find the beginning or end of a child range. See [ColumnKey.java|https://github.com/stuhood/cassandra-old/blob/674/src/java/org/apache/cassandra/db/ColumnKey.java#L225]

EDIT: Actually... I'm not so sure about not having MAX_TOKEN... it might actually clean things up quite a bit. Any time a range ends with what use to be the min token, you can make a direct translation to MAX_TOKEN.;;;","09/Jun/11 19:01;slebresne;Patch rebased, this is against trunk.;;;","09/Jun/11 19:32;slebresne;bq. One way to remove toSplitValue would be to use DecoratedKey everywhere;

I'm not saying it's not possible, but I think this is overkill (in the changes it involves). Moreover, all the code that deals with topology really only care about token. That's the right abstraction for those part of the code. So I really (really) doubt using decorated key everywhere would be cleaner. Of course, anyone is free to actually do the experiment and prove me wrong. I also don't think it would remove the need for splitValue, it would just maybe call it differently.

bq. The equivalent of today's Token is a DecoratedKey for that token with a null key

This is only true today because we assume key and token are one-to-one. The goal is to change that. If multiple keys can have the same token (by definition the token is really the hash of a key), then the statement above is false. If a token correspond to an infinite set of key (with is the case with md5 btw, we just ignore it), then replacing a token by given key *cannot* work.

Overall, it could be that there is better way to do this, but having spend some time on this, I have a reasonable confidence on that it fixes the issue at hand without being too disruptive (which is not saying there isn't a few points here and there that couldn't be improved).;;;","21/Aug/11 06:08;mck;What's the status on this? This issue and its relations back to CASSANDRA-2878 are the only reason we're using OPP. I suspect other users setup with both cassandra and hadoop (or brisk) could be in the same boat. Not only does OPP leave an unbalanced ring (i've had a case where all data went to one node because the keys/tokens were longer than normal) it leaves poor performance to hadoop jobs as tasks requirement on data locality has become stricter (w/ CASSANDRA-2388). Apart from the plain preference to be using secondary indexes over OPP.;;;","21/Aug/11 12:20;jbellis;Status is, I'm hoping that someone comes up with a fix that doesn't look error prone.  Otherwise we'll probably end up with merging Sylvain's solution for 1.1.;;;","03/Oct/11 22:08;xedin;I'm thinking of making Token an interface and implementing two classes RoutingToken(token) and QueryToken(token, key) so all current token implementations LocalToken, StringToken, BytesToken, BigIntegerToken are going to extend QueryToken. RoutingToken is going to be used for operations where we don't need a key - bootstrap, midpoint calculation, TokenMetadata class; QueryToken is going to be a replacement for DK, that will allow us to remove DK completely and operate only on the token basis. Thoughts?;;;","03/Oct/11 22:23;slebresne;bq. Thoughts?

From your comment only I don't see right away what you are proposing besides a renaming of Token -> RoutingToken and DecoratedKey -> QueryToken.;;;","11/Oct/11 04:42;xedin;Patch removes DK and IPartitioner.decorateKey(ByteBuffer key), which is replaced by IPartitioner.getToken(ByteBuffer key), Token now takes second parameter - ByteBuffer key. Most of the patch are replacements for DK -> Token and decorateKey -> getToken. All tests (test, test-compression, long-test) pass.

Rebased with the latest trunk (last commit 7624536ae7fea52bcf761c7dea212fe12d2f4586);;;","11/Oct/11 04:57;tjake;At first glancei  like this because it makes the Token first class and the key not required. cleaning up the code below.

{code}
-        DecoratedKey startWith = new DecoratedKey(range.left, null);
-        DecoratedKey stopAt = new DecoratedKey(range.right, null);
+        Token startWith = range.left;
+        Token stopAt = range.right;
{code};;;","11/Oct/11 19:42;slebresne;I have 2 major problems with that patch:

The first one is I really dislike the idea of merging DK into Token (I disliked the idea of merging Token into DK and that roughly the same idea).  First, I fail to see how this is of any help in solving what this ticket is trying to solve. Second, I think it's a very bad use of types. A Token is not a Key. By merging those together we just weaken our type hierarchy, thus getting less insurance from types. Typically, with this patch, a function that really want a DK, could get a Token, i.e getKey() is not guaranteed to return a valid key. Now I know, we are already using 'false' DK by feeding null as a key sometimes. Well, that is ugly and error prone. I don't think generalizing this everywhere while introducing a 300K patch is the right way to go, quite the contrary. Besides, it's inefficient. All the places were we do use only a Token, we'll now have a bigger structure with a useless pointer to the EMPTY_BYTE_BUFFER (granted this has probably little impact, but it's another sign that it's doing it wrong).

The second problem is this doesn't work. This DK->Token really just muddy the water but it doesn't solve anything. What we want is to fix the fact that the code identifies token and keys as a one to one mapping. In particular, this is forced in DK.compareTo(), which only compare the tokens, ignoring the keys.  Fixing that place is easy, and the patch does it, but it's really just a few lines change.

The real problem is that the code make the assumption that key <-> token is one to one in other places. So making DK.compareTo takes key into account breaks other parts. For instance, in RowIteratorFactory, we have this:
{noformat}
return startWith.compareTo(row.getKey()) <= 0
       && (stopAt.isEmpty() || row.getKey().compareTo(stopAt) <= 0);
{noformat}
and say that startWith and stopAt are token only. The semantic is that this is supposed to be inclusive on both bound. With the last patch, this would include keys having the startWith token, but *not* the ones having stopAt as token, because in the patch, a token compares strictly before all of the key having this token (concretely, the attached patch skips keys during range queries).

And this is not the only places in the code where this problem manifest, because this is the symptom of a larger problem. If more than one key can have the same token, then tokens are a range of keys.
If you ask for the range of tokens [1, 4], then you expect that it will return all the keys having token 1, 2, 3 and 4. That excludes having a token comparing strictly before all the keys having this token (or having it compare strictly after all the keys having it as token for that matter). Merging Token and DK just doesn't work.

At the risk of sounding cocky, I really encourage people to have another look at my patch. I do believe that once you've realized what solving this problem entails, it's a solution that strike a reasonable balance in fixing the problem without a entire rewrite of Cassandra.
;;;","12/Oct/11 04:50;tjake;@Sylvain This is all really confusing and I agree the core of the ticket is to make key->token 1:1

The core of the problem initially was explained in CASSANDRA-1733

bq. A Range object (which Hadoop splits generate) is start-exclusive. A Bounds object (which normal user scan queries generate) is start-inclusive.

So by making Token the only way to deal with keys it feels like a more consistent api.  Since Key can be null it needs to be Token that becomes the primary internal class. 

In your impl we now have DK, Token, RingPosition which too me is more confusing than having one Token class.


;;;","12/Oct/11 18:43;slebresne;I'm not sure I'm am completely clear, so allow me to try to improve that. I think there is two issues with the last patch that are largely orthogonal:
  # the patch is broken (again, this is largely not related to the shoving of DK into Token)
  # I believe shoving DK into Token is a bad, error-prone idea that have no tangible advantages

But let's me first focus on the first issue, if only because it involves no opinion whatsoever: I'm either right or wrong that it's broken (but i'm pretty sure I'm right). So let's be concrete and take an example.

The patch ""merges"" Token and DecoratedKey together, so let me take the following notation:
  * t(1, a) for what is the DecoratedKey a of token 1 in current but is is just a Token instance in the patch
  * t(1) for the 'pure' token 1. In other word it's a shortcut for t(1, EMPTY_BYTE_BUFFER) in the attached patch and correspond to just a Token in the current code.
(as a side note, the fact that I have to speak of DecoratedKey and 'pure' token to explain is imo yet another sign than melting everything into Token is a bad idea but I'm diverging)

Since when Token are compared, the token is compared then the key is on token equality, we have t\(n) < t(n, k) whatever the token n and key k are (since t\(n) is t(n, EMPTY_BYTE_BUFFER) and EMPTY_BYTE_BUFFER < k for any valid key k) .

Let's now take an example of multiple keys having the same token and say that I have the following keys in my cluster:
{noformat}
tokens |   1   |     2     |   3
keys   | a | b | c | d | e | f | g
{noformat}
In other words, a and b have the same token 1; c, d and e have the same token 2; ...

The goal for this ticket is to support that situation correctly. Sor for instance, we should have that:
   range_slice(start=t(1), end=t(3)) returns c, d, e, f and g
(because range_slice with tokens is start exclusive).  However, with the attached patch:
   range_slice(start=t(1), end=t(3)) will return a, b, c, d and e

The reason is fairly simple: we have that t(1) < t(1, k) for any k and t(3) < t(3, k) for any k.

Another way to put it is that it breaks our token ranges: if you have a node that owns Range(t(1), t(3)), it's supposed to not contains any key with token 1 and all keys with token 3, but it fails at both.

So it's broken. Now there is something we could be tempted to do. We could make it so that t\(n) > t(n, k) for any token n and any key k. But in turn that would break Bounds (i.e, start inclusive) of 'pure' tokens. I.e, Bounds(t(1), t(2)) is supposed to include all keys with token 1, but if t(1) > t(1, k) for any key k, it won't include it.

One could argue however that this is still solution because I *think* that right now we never really use a Bounds of 'pure' tokens (more precisely, the current code does it, but only in place where we are actually doing a range slice between keys). And I *think* that functions that take a startKey, when fed a 'pure' token only do start exclusive. So I suppose we could assert that we never create Bounds of Token and put some assert here and there (in SSTableReader.getPosition() for instance) and go with that.

But imho this is a bad idea. Because it's fragile and because this is ignoring a problem that may screw us up later. Why not fix it the right way now? What if tomorrow we do want to be able to query all the keys having a given token ? That is, what if we want to query [t(1), t(1)] ? We would not be able to, because if t(1) > t(1, k) for any k, then [t(1), t(1)] don't include anything.

Again, all this is because a token actually correspond to a set of keys (once you acknowledge multiple keys can have the same token), and so if you want to do things correctly, you need for a given token n to have a representation for both:
  * the smallest key having token n
  * the greatest key having token n

With that, you can query all the keys having token n. Without, you can't. That is what my patch does and I believe fairly strongly is the right way to do it.


Alright, that the first thing that a patch to this ticket must deal with. Then there is another thing: the current code only allow for AbstractBounds of Token (by typing), but we want for this patch that once you do a range_slice query with at startKey and endKey, you get a range of keys in ColumnFamilyStore.getRangeSlice(), so that you can precisely answer those queries. That means we must be able to construct AbstractBounds with keys in them. Note that it's basically just a typing problem.

The answer to that of this patch is show DK into Token. So yeah, it fixes that problem, but what I'm arguing is that:
  * It's error-prone and make coding *more* complicated. We're merging object that are not the same thing. Today if a methods takes a Token, you know it won't do anything at the granularity of keys (well today Token and keys have the same granularity but this ticket is supposed to change that). You lose that information if you merge DK and Token. And if a method takes a DecoratedKey, you know that it doesn't expect a Token (more precisely, Typing ensures it). Sure, we do already use a trick in a few places where we create 'fake' DK(null, key). But at the very least, when we do that, we know we're doing something weird, and we are extra careful that methods we call on that fake DK handle that case correctly. If everything is Token, now the signature for a lot of method will suggest it is ok to give a 'pure' Token. So what, all method should defensively assert this is not the case ? This is what types are for.
  * It's a ~300K patch. Sure it's mostly simple changes, but it's still that many changes that could introduce a typo somewhere that causes a bug.
  * It's a tad less efficient because each time we really only care about 'pure' Token (and there is quite a bunch of code that does that), we would allocate a slightly larger structure for no reason. And I'm pretty sure god kills a kitten each time you do that.

The solution to that very same type problem I'm proposing (in my patch) is instead simply to generalize AbstractBound slightly so you can have both AbstractBound of Token and of DecoratedKey. That sound very reasonable to me.  After all we should be able to have AbstractBounds of anything that implements Comparable right ? Well, as it turns out our implementation of AbstractBound needs a little more than that (because our ranges wraps, we need Comparable but with a minimum value for instance) and that is what RingPosition is for.  But it's only a marker interface, and if you look at the code it's actually used in a small number of places, so I admit I fail to see how this make thing much more complicated.;;;","12/Oct/11 19:14;xedin;Can you please define what do you mean by ""pure token""? Aren't we supposed to generate token from key in all situations except initial token in config and middle point between tokens? So if you do a range slice using tokens instead of keys TokenFactory.fromString will force you to use correctly serialized token data which will also include key.

bq. It's error-prone and make coding more complicated. We're merging object that are not the same thing etc...

If token is generated from key than for me it's natural to have a key as member. The thing is that you are enable to create a ""pure"" token, Partitioner will always give you a Token with valid key except for midpoint method so if partitioner is used to generate tokens you are guaranteed to have a valid key in the resulting token instance.

bq. It's a ~300K patch. Sure it's mostly simple changes, but it's still that many changes that could introduce a typo somewhere that causes a bug.

The same thing I can say about your set of patches - it's 198 KB. Aren't we writing tests to catch such bugs?;;;","12/Oct/11 20:23;slebresne;
bq. Can you please define what do you mean by ""pure token""?

In you patch, it's a Token whose key is EMPTY_BYTE_BUFFER (which is *not* a valid row key, hence the 'pure' token name).

bq. Aren't we supposed to generate token from key in all situations except initial token in config and middle point between tokens?

And? Is that not enough? There is tons of place in the code where we manipulate those tokens 'not created from a key' (all the distribution code basically, which is a big part of Cassandra). Moreover, there is also range_slice that accept a range of token.

bq. So if you do a range slice using tokens instead of keys TokenFactory.fromString will force you to use correctly serialized token data which will also include key.

To what is this supposed to be an answer ?

bq. If token is generated from key than for me it's natural to have a key as member. The thing is that you are enable to create a ""pure"" token, Partitioner will always give you a Token with valid key except for midpoint method so if partitioner is used to generate tokens you are guaranteed to have a valid key in the resulting token instance.

But it's not always generated from a key! There is nothing natural to a key member in all the Token object manipulated by TokenMetadata and other, since there is not such key.

bq. The same thing I can say about your set of patches - it's 198 KB. Aren't we writing tests to catch such bugs?

Well, in my patch, 148K of those are a type generification only (that's why I've separated it). Because generics are erased at runtime, as long as it compile, there is *NO* chance this can introduce a bug. As for trusting tests to catch bugs, I think it's being overconfident in tests. But in the end, I'm happily taking back that objection as this is by far the less important.


Let me try to put things graphically, everyone loves a graph: if I draw the set of all keys as this:
{noformat}
[-----------------------------------------------------------------------------[
{noformat}
i.e, the ring but as a line because I'm ignoring wrapping for this.

Now, if I display row keys (decorated or not, that doesn't matter, both are keys), I would have for instance:
{noformat}
[---------------------------|-------|---------------|---------------|---------[
                            k1      k2              k3              k4
{noformat}
A key is a point on the ring.

Now if keys and tokens are a 1 to 1 mapping, then it could be ok to say that a token is a point on the ring, but once it's not the case, then it looks like that:
{noformat}
                                t                     t'              t''
[-------------------------[*|*******|*]----------[**|****]-------[**|******]--[
                            k1      k2              k3              k4
{noformat}
where t is the token for both k1 and k2 (and an infinite number of other keys (actually finite because we're working on a computer)), t' the token of k3 (and an 'infinite' number of other keys), etc...

A token is intrinsically a range, a segment on the ring. Shoving DK and Token into the same class everywhere in the code is saying that we'll use the same class for a point and an interval. How can that be a good idea? How can that not backfire on us and be hard to work with, making it easy to introduce errors?
;;;","12/Oct/11 20:33;tjake;bq. A token is intrinsically a range, a segment on the ring. 

But the whole point of the ticket is to remove this concept. Are you saying that can't be guaranteed?

This should be possible by making a equals consider the token AND key.  The problem with CASSANDRA-1733 is sometimes we don't specify a key since we have have Min token and an intrinsic Max token.  ;;;","12/Oct/11 21:34;slebresne;bq. But the whole point of the ticket is to remove this concept. Are you saying that can't be guaranteed?

There is a misunderstanding. The whole point of this ticket is to *enforce* this concept. A token is a range, a segment on the ring, there is nothing we can do about it. It's like saying the point of the ticket is to remove the concept that a segment is different from a point.

I'm happy to discuss that, and that is clearly where we should start, but I'm pretty sure that the *problem* we want to fix is that the current code is pretending than a segment is equal to a point. The current code is pretending that a token t is the same thing than a key having this token. This only work if there is only one key have a given token, otherwise it's buggy, you identify all keys having the same token as equal, that is the problem.

And saying that you'll change the comparison of DK to include the key and pretending that a token is the same thing that some fictive key that as far as key comparison is concerned would be before any key having the token (which is *exactly* what Pavel's patch is doing) doesn't work either. As I've said earlier with examples.

I'm saying that the right way to fix is to make the code treats Token as a segment (because you know, that's what it is) and a key as a point. Now that, imho, is not of a debatable nature: it's either true or false (and imho clearly true but maybe i'm completely stupid). But at the very least we should agree on that first, even before thinking about how we will code it.

Then, once we agree on the problem, there is the question of how we do it. And then, my second argument is that shoving a token (a segment) and a (decorated) key (a point) into the same class (that we would happen to call Token) is, why probably ""possible"", likely an error-prone, confusing and frankly ugly idea. You can create a class representing both a segment and point, having it work correctly underneath and write code using that, but it will unlikely be beautiful nor easy to use. But it's ""possible"". ;;;","12/Oct/11 21:46;jbellis;bq. The whole point of this ticket is to enforce this concept. A token is a range, a segment on the ring, there is nothing we can do about it.

Right.

Is this still a fair summary of why we want to fix this?

bq. the problem is that we are using DK both for routing and for local key sorting;;;","12/Oct/11 22:31;tjake;My view is a Key requires a Token in our system. I understand that you cant keep multiple keys from mapping to the same token, still I would have liked to see the code deal with Tokens with (optional) keys then a mix of keys and tokens.  I see now this idea is broken in the sense that sorting a list of tokens means different things depending on the context (partitioner bounds vs user defined range)
;;;","12/Oct/11 22:46;slebresne;{quote}
Is this still a fair summary of why we want to fix this?

the problem is that we are using DK both for routing and for local key sorting
{quote}
Hum, I would actually rephrase it with token instead of DK, in the sense that we don't really use DK for routing, DK is a key with it's token ""cached"" to speed up computing it, we're using only the token to route. The problem is we're also using only the token for local key sorting.

But while we could/should be using token to route and the DK for local key sorting, we still need to be able to handle local key *search* by token. And that is imho the difficulty of this ticket (if we always had an actual valid key to do local key search it would be much easier). And we need local search based on tokens because:
  * we allow range_slices on a range of tokens (so this translate ultimately to local search by token)
  * even for range_slices by keys, we still end up splitting the key range by a token in getRestrictedRanges, hence resulting ultimately to a local search by token.
Then the problem is that since a token is a segment and a key (what we're searching for) a point, we can't really compare those, in the sense that a key is not necessarily either stricly greater, equal, or stricly lesser than a token. So you do have to consider both the ""bounds"" of the token, which are now point and that you can compare to keys.;;;","18/Nov/11 22:20;slebresne;Attaching rebased patch (against trunk).

I've slightly refactored the patches too. The first contains only the generification of AbstractBounds. It's obviously a bit ""dumb"" taken alone since it generify but doesn't allow anything else than tokens. The only other noticeable thing is the removal of the Range.compare() method (in favor of the compareTo method of Token). I have no idea what that method was about in the first place. The second patch does the rest of the work and has got some minor cleanups. I've also tried to add some new comments to make it more digestible.  I also include a third patch with a small unit test.

Having spend quite some time thinking about this issue, I do think that this is a good way to fix it, the alternative of allowing to mixing Token and DecoratedKey directly in a Range being (to have pursued it a bit before giving up) much more messy and error prone imho. Now I can't force anyone to like this solution but I also won't rebase this forever.;;;","19/Nov/11 07:51;jbellis;{noformat}
+    public static final DecoratedKey minKey = new DecoratedKey(partitioner.getMinimumToken(), false);
{noformat}

I think I'd rather have these in the partitioner.  (I know partitioner is cluster-global right now but it still feels wrong to ""hoist"" something partitioner dependent out and make it static final.)

{noformat}
+        assert token != null && key != null;
{noformat}

This feels odd when we go ahead and construct DKs with null key anyway in the other constructor.

*Important*: I think my biggest problem with this patch is that a DK may or may not have a key that when given to the partitioner, results in the Token in the DK.  And there's nothing to show that is the case, except that key == null or Empty.  So we're still pretending a Token ""is"" a key, we've just made it more complicated.  Could we update the methods for whose benefits we're performing the Token -> DK conversion, to accept RingPosition instead?

{noformat}
+        return token.hashCode() + (key == null ? 0 : key.hashCode());
{noformat}

I don't see a good reason to not use a ""real"" hashcode implementation (Objects.hashCode is useful here).

{noformat}
+        // null is used as a 'end of range' marker, so DK(t, k) is always before DK(t, null) unless k == null
{noformat}

Still not a huge fan of using null to mean end of range, but I guess I don't have a better suggestion. There's clearly a lot of places in this patch where it's causing special case ugliness though, independent of its status as ""max.""

{noformat}
+        // minimunKey, see Token.upperBoundKey()
{noformat}

typo.  (both occurrences.)

{noformat}
-        T min = (T)current.partitioner.getMinimumToken();
+        T min = (T)current.left.minimumValue(current.partitioner);
{noformat}

I think the positives of making this Generic are outweighed by the negative of implying that minimum value for partitioner X depends on the RingPosition that is returning it.  I think I'd rather accept the casting ugliness of having a Partitioner method that does instanceof checks to return the appropriate type.  

*Serializer code*: How does DK, AB, etc. code deal w/ backwards compatibility issues?  Looks like some (AES) can get by with saying ""we don't support mixed-version streaming"" but others (IndexScanCommand) cannot.

{noformat}
+        assert left.compareTo(right) <= 0 || right.isMinimum(partitioner) : ""["" + left + "","" + right + ""]"";
{noformat}

What if we added a Partitioner reference so we could just ask isMinimum()?

;;;","19/Nov/11 08:03;jbellis;bq. I think my biggest problem with this patch is that a DK may or may not have a key that when given to the partitioner, results in the Token in the DK.

Put another way: in my ideal world, DK.token would be purely an optimization to avoid calling partitioner.getToken(key) over and over.;;;","22/Nov/11 00:54;slebresne;bq. Put another way: in my ideal world, DK.token would be purely an optimization to avoid calling partitioner.getToken(key) over and over.

I understand that, but I think there is two different things and I want to know exactly where the disagreement/problem is.

The first problem, which is imho the core of this ticket, is that the code needs to be able somehow to deal with things like (where I use k for keys and t for tokens, and the term range for either Range or Bounds):
  * Is key k in the range [k', t] (or (t', k''])? Because when you do a range_slice of [k', k''] and there is multiple nodes and [k', k''] spans multiple replica, we will end up requesting all keys in [k', t] (for some t) or (t', k''].
  * Is key k in range (t, t']? Because we're allowed to range query keys by a token range, but also a few other reason, like the fact that during validation compaction we hashes together keys within a token range.
Note that those are not trivial questions, because for instance [k', t], while we intuitively understand what it represents is a weird beast in that is a range a point and a segment?!

Or in other words, as much as I'd like the operations on Tokens and the ones on Keys to be two completely orthogonal sets of operation with no interaction whatsoever, it is not the case and we have to deal with it.

Dealing with the case where we need tokens and we have keys is trivial (we just call Key.getToken() and boom, we're back in the case with only tokens).

The problem is when we fundamentally work on keys, but have only token to start with. Today (i.e. before this ticket), we take a simplification by doing essentially the same thing that in the 'needs token but got keys' case by having a sort of Token.getKey() (it's more ugly in practice, we inline calls to new DecoratedKey(t, null), but that's the same thing). But doing that forces in itself the fact that key an token are in bijection and we want to lift that.

One solution could be to try to keep Token as long as we can, even in places where we really need a key and have the code deal with that. I can understand that on the surface that could look clean, but in practice the code to do that correctly would a pure nightmare. Just trying to implement a Range that would mix token and keys (like the [k', t] range above) is a complete mess.

So what this patch does is realizing that you could characterize the set of keys that a token t represents with only two keys: the smallest key having token t, and the biggest key having token t.

Now, supposing we agree on what is above, the rest is implementation details and that's probably a much simpler discussion. Note that above I'm not talking of DecoratedKey, only key. But the question is, how do you represent those two new keys (for each token). The patch uses special values of the key field of DK to deal with those. I can agree this is not the cleanest thing ever and I'm fine looking for a different encoding, but I just don't have a much better idea, and frankly I don't find that horrible either.

bq. I think I'd rather have these in the partitioner

Good idea.

bq. his feels odd when we go ahead and construct DKs with null key anyway in the other constructor.

The goal here is to avoid constructing one of the two 'fake' keys by accident For that the second constructor is dedicated to their construction and as the commnet says, you're not even supposed to use this second constructor, but use Token.{upper|lower}Bound instead. Actually, the assert should check for the EMPTY_BYTE_BUFFER.

bq. Could we update the methods for whose benefits we're performing the Token -> DK conversion, to accept RingPosition instead?

Frankly, and as argumented above, no, not without *huge* pain. We only do that conversion in places where we will have to do it at some point, and trying to push Tokens deeper would only serve in having operations that make no real sense for Tokens be able to actually deal with Token. As one example, we would have to make Range with a mix of Token and Keys, and frankly that will be a total mess to code.

bq. I don't see a good reason to not use a ""real"" hashcode implementation (Objects.hashCode is useful here)

Not sure I follow but ByteBuffer.hashCode() does hash the content of the buffer if that was what you meant.

bq. There's clearly a lot of places in this patch where it's causing special case ugliness though, independent of its status as ""max.""

Again, I would be open to better encoding. But is there really that much places? The patch tried to make it so that no code outside of DecoratedKey really have to deal with it. If not perfect, I actually think it's better contained that before the patch.

bq. I think the positives of making this Generic are outweighed by the negative of implying that minimum value for partitioner X depends on the RingPosition that is returning it. I think I'd rather accept the casting ugliness of having a Partitioner method that does instanceof checks to return the appropriate type.

I think you're right.

bq. Serializer code: How does DK, AB, etc. code deal w/ backwards compatibility issues?

Basically, old version only understand AbstractBounds of Token, while new version generates/accept AbstractBounds of either token, or keys. When old sends to new and keys are expected, new convert the range/bounds of token as range/bounds of keys. When new sends to old, it converts any range/bounds of keys to range/bounds of token.

bq. What if we added a Partitioner reference so we could just ask isMinimum()?

Do you mean to have the DK to have a reference to the partioner? If so, I agree that it's probably something we should, but it's nothing specific to that patch so I'd rather leave it to another ticket.
;;;","22/Nov/11 02:16;jbellis;bq. in practice, we inline calls to new DecoratedKey(t, null)

Right.  I must be missing something crucial, because that's exactly what it looks like we're still doing in this patch, only with a special constructor.;;;","22/Nov/11 05:17;slebresne;No, no, the patch does use the same think. I merely said that the patch does some attempt at a better encapsulation, as it seems better to use the Token.{upper|lower}BoundKey to creates those fake keys that inlining the call to the constructor all over the code (which we do now). It makes the use of null more of an internal detail of DecoratedKey (not completely, granted, but it's a little bit better). It also makes it simpler to check we don't accidentally construct a DK with a null key by accident (the goal of the assertion in the first DK constructor in the patch).

But let it be clear that I'm not making any claim that this patch ""cleans"" some ugliness in the current code. It mainly try to solve the problem at hand, which is basically to be able to do range_slices and getting the right result even when multiple keys have the same token.

This is not saying it wouldn't be good to fix any current ugliness at the same time if possible, but in truth, I don't find that using special DK to represent special keys is such an ugly hack (not either claiming it's super beautiful, I just don't have a particular hatred of this). Besides, I don't have tons of ideas to fix the issue at end (the priority) and make the code clearly cleaner. And I do think that whatever ugliness the current have, this patch doesn't make it worst.

Anyway, I'll try to see if I can improve the encapsulation of the Token.{upper|lower}BoundKey representation and see if I can come with something slightly cleaner.;;;","22/Nov/11 05:28;jbellis;bq. the patch does some attempt at a better encapsulation, as it seems better to use the Token.{upper|lower}BoundKey to creates those fake keys that inlining the call to the constructor all over the code (which we do now). 

Okay, I'll buy that.  It's an awful lot of code churn for IMO a relatively minor win, but I see where you're going with that.

Help me understand this patchset a different way: which is the part without which CASSANDRA-1600 is impossible?;;;","24/Nov/11 00:06;slebresne;bq. Help me understand this patchset a different way: which is the part without which CASSANDRA-1600 is impossible?

CASSANDRA-1600 requires that the row key range requested be known by CFS.getRangeSlice/search, while today it only gest the corresponding tokens.  We could possibly do what your first patch for CASSANDRA-1600 did and add the keys separately. You'll have to deal with wrapping and such, but that's probably doable.

What this patchset does is make getRangeSlice/search actually take keys, so this greatly simplify CASSANDRA-1600. But CASSANDRA-1600 is probably doable without this, it's just the logical first step before getting a clean implementation. Now for the specific parts, as said we need to be able to have keys for getRangeSlice/search, which basically require the bulk of this patchset (i.e. for CASSANDRA-1600, we could still have DecoratedKey.compareTo() to only compare the tokens and not the keys, but that's probably it)

But truth being told, CASSANDRA-1600 is by far not my main motivation for this. My main motivation is CASSANDRA-1684. For the latter, if we want to do it 'natively', we will have lots of key having the same token, so this ticket is an absolute requirement before even getting started. And there is also the problem of md5 collision :)
;;;","24/Nov/11 00:07;slebresne;I've tried finding a better encapsulation for the 'fake' keys of the patch.  The idea being to restrict DK to 'true' row key, i.e. the ones that can be written on disk and create a new class (Token.KeyBound) to represent the two ""fake"" key for each token representing the smallest/biggest key having the token. To make it work together, they share the new RowPosition interface.

Some of the methods accepts a RowPosition (instead of DecoratedKey) to indicate that it can accept a 'fake' key for purpose of selecting true keys.  So for instance SSTableReader.getPosition() accepts a RowPosition. However, SSTableReader.getCachedPosition() only accepts a DK, because the key cache can only contain a ""true"" row key.

Anyway, I actually end up liking this. With that, we never ever create a DK with a null key (nor even an empty one, which wouldn't be a true key either).  This is more clear and avoids mistakes. Unfortunately the patch got bigger :(
;;;","28/Nov/11 22:49;jbellis;+1 on the KeyBound approach.  This is exactly what I was hoping for.

Returning to a minor point:

bq. bq. I don't see a good reason to not use a ""real"" hashcode implementation (Objects.hashCode is useful here)

bq. Not sure I follow but ByteBuffer.hashCode() does hash the content of the buffer if that was what you meant.

I mean that straight addition is a weak hashcode combination since X + Y is the same as Y + X.  ""return Objects.hashCode(X, Y)"" is an easy way to do it ""right"" with no more code than the weak approach.  Doesn't matter much here but it's good practice imo.

Another nit: should we be using a enum for RowPosition.kind?

Meta observation: I'm glad we're not doing this a week before freeze. :);;;","29/Nov/11 23:10;slebresne;bq. I mean that straight addition is a weak hashcode combination since X + Y is the same as Y + X. ""return Objects.hashCode(X, Y)"" is an easy way to do it ""right"" with no more code than the weak approach. Doesn't matter much here but it's good practice imo.

Make sense. I made the DK hashcode be only based on the key hashcode though (since the token is just a cached value for getToken() :)). The hashCode method of Token.KeyBound don't use Objects.hasCode(), but I really think that in that case it doesn't matter at all and it avoids the boxing of the boolean. I can change it though if that's the only problem remaining.

bq. Another nit: should we be using a enum for RowPosition.kind?

What do you mean exactly by that? Are you talking of the kind use in RowPositionSerializer? To have an enum to distinguish between DK and Token.KeyBound instance of doing the instanceof? If so why not, but I'm not sure it buys us anything.;;;","29/Nov/11 23:24;jbellis;Okay, we can skip the hashcode change if you're worried about boxing.

Yes, that's what I'm referring to for ""kind.""  Seeing code like ""if kind == 0"" means I have to go back to the kind method to see what a return value of 0 means.;;;","30/Nov/11 18:44;slebresne;Attaching v3, rebased and using an enum for the RowPosition kind. I could have changed a few {{assert key instanceof DecoratedKey}} by {{assert key.kind() == RowPosition.Kind.ROW_KEY}} I suppose, but I prefered keeping the instanceof since each time the next line do a cast to DK, so this feels more coherent like that.;;;","30/Nov/11 22:05;jbellis;+1;;;","01/Dec/11 17:25;hudson;Integrated in Cassandra #1229 (See [https://builds.apache.org/job/Cassandra/1229/])
    remove assumption that key and token are in bijection
patch by slebresne; reviewed by jbellis for CASSANDRA-1034

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1208993
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/client/RingCache.java
* /cassandra/trunk/src/java/org/apache/cassandra/config/DatabaseDescriptor.java
* /cassandra/trunk/src/java/org/apache/cassandra/cql/QueryProcessor.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/DecoratedKey.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/HintedHandOffManager.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/IndexScanCommand.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/Memtable.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/RangeSliceCommand.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/RowIteratorFactory.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/RowPosition.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/CompactionManager.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/LeveledManifest.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/index/SecondaryIndexManager.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/index/SecondaryIndexSearcher.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/index/keys/KeysSearcher.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/marshal/LocalByPartionerType.java
* /cassandra/trunk/src/java/org/apache/cassandra/dht/AbstractBounds.java
* /cassandra/trunk/src/java/org/apache/cassandra/dht/AbstractByteOrderedPartitioner.java
* /cassandra/trunk/src/java/org/apache/cassandra/dht/AbstractPartitioner.java
* /cassandra/trunk/src/java/org/apache/cassandra/dht/BootStrapper.java
* /cassandra/trunk/src/java/org/apache/cassandra/dht/Bounds.java
* /cassandra/trunk/src/java/org/apache/cassandra/dht/IPartitioner.java
* /cassandra/trunk/src/java/org/apache/cassandra/dht/LocalPartitioner.java
* /cassandra/trunk/src/java/org/apache/cassandra/dht/OrderPreservingPartitioner.java
* /cassandra/trunk/src/java/org/apache/cassandra/dht/RandomPartitioner.java
* /cassandra/trunk/src/java/org/apache/cassandra/dht/Range.java
* /cassandra/trunk/src/java/org/apache/cassandra/dht/RingPosition.java
* /cassandra/trunk/src/java/org/apache/cassandra/dht/Token.java
* /cassandra/trunk/src/java/org/apache/cassandra/hadoop/ColumnFamilyInputFormat.java
* /cassandra/trunk/src/java/org/apache/cassandra/hadoop/ColumnFamilyRecordWriter.java
* /cassandra/trunk/src/java/org/apache/cassandra/io/sstable/IndexSummary.java
* /cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableBoundedScanner.java
* /cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableLoader.java
* /cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableReader.java
* /cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableScanner.java
* /cassandra/trunk/src/java/org/apache/cassandra/locator/AbstractReplicationStrategy.java
* /cassandra/trunk/src/java/org/apache/cassandra/locator/TokenMetadata.java
* /cassandra/trunk/src/java/org/apache/cassandra/net/MessagingService.java
* /cassandra/trunk/src/java/org/apache/cassandra/service/AntiEntropyService.java
* /cassandra/trunk/src/java/org/apache/cassandra/service/StorageProxy.java
* /cassandra/trunk/src/java/org/apache/cassandra/service/StorageService.java
* /cassandra/trunk/src/java/org/apache/cassandra/service/StorageServiceMBean.java
* /cassandra/trunk/src/java/org/apache/cassandra/streaming/StreamIn.java
* /cassandra/trunk/src/java/org/apache/cassandra/streaming/StreamOut.java
* /cassandra/trunk/src/java/org/apache/cassandra/streaming/StreamRequestMessage.java
* /cassandra/trunk/src/java/org/apache/cassandra/streaming/StreamingRepairTask.java
* /cassandra/trunk/src/java/org/apache/cassandra/thrift/CassandraServer.java
* /cassandra/trunk/src/java/org/apache/cassandra/thrift/ThriftValidation.java
* /cassandra/trunk/src/java/org/apache/cassandra/tools/BulkLoader.java
* /cassandra/trunk/src/java/org/apache/cassandra/utils/FBUtilities.java
* /cassandra/trunk/src/java/org/apache/cassandra/utils/MerkleTree.java
* /cassandra/trunk/test/unit/org/apache/cassandra/Util.java
* /cassandra/trunk/test/unit/org/apache/cassandra/db/CleanupTest.java
* /cassandra/trunk/test/unit/org/apache/cassandra/db/ColumnFamilyStoreTest.java
* /cassandra/trunk/test/unit/org/apache/cassandra/db/KeyCollisionTest.java
* /cassandra/trunk/test/unit/org/apache/cassandra/db/SerializationsTest.java
* /cassandra/trunk/test/unit/org/apache/cassandra/dht/AbstractBoundsTest.java
* /cassandra/trunk/test/unit/org/apache/cassandra/dht/BootStrapperTest.java
* /cassandra/trunk/test/unit/org/apache/cassandra/dht/PartitionerTestCase.java
* /cassandra/trunk/test/unit/org/apache/cassandra/dht/RangeTest.java
* /cassandra/trunk/test/unit/org/apache/cassandra/io/CompactSerializerTest.java
* /cassandra/trunk/test/unit/org/apache/cassandra/io/sstable/SSTableReaderTest.java
* /cassandra/trunk/test/unit/org/apache/cassandra/service/AntiEntropyServiceTestAbstract.java
* /cassandra/trunk/test/unit/org/apache/cassandra/service/MoveTest.java
* /cassandra/trunk/test/unit/org/apache/cassandra/service/SerializationsTest.java
* /cassandra/trunk/test/unit/org/apache/cassandra/service/StorageProxyTest.java
* /cassandra/trunk/test/unit/org/apache/cassandra/streaming/SerializationsTest.java
* /cassandra/trunk/test/unit/org/apache/cassandra/streaming/StreamingTransferTest.java
;;;","01/Dec/11 18:14;slebresne;Committed \o/;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
pending range collision between nodes,CASSANDRA-603,12442507,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jaakko,lenn0x,lenn0x,05/Dec/09 04:45,16/Apr/19 17:33,22/Mar/23 14:57,17/Dec/09 02:13,0.5,,,,0,,,,,,"We bootstrapped 5 nodes on the east coast from an existing cluster (5) on west. We waited at least 60 seconds before starting up each node so it would start bootstrapping. We started seeing these types of errors:

 INFO [GMFD:1] 2009-12-04 01:45:42,065 Gossiper.java (line 568) Node /X.X.X.140 has now joined.
ERROR [GMFD:1] 2009-12-04 01:46:14,371 DebuggableThreadPoolExecutor.java (line 127) Error in ThreadPoolExecutor
java.lang.RuntimeException: pending range collision between /X.X.X.139 and /X.X.X.140
        at org.apache.cassandra.locator.TokenMetadata.addPendingRange(TokenMetadata.java:242)
        at org.apache.cassandra.service.StorageService.updateBootstrapRanges(StorageService.java:481)
        at org.apache.cassandra.service.StorageService.onChange(StorageService.java:402)
        at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:692)
        at org.apache.cassandra.gms.Gossiper.applyApplicationStateLocally(Gossiper.java:657)
        at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:610)
        at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(Gossiper.java:978)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
ERROR [GMFD:1] 2009-12-04 01:46:14,378 CassandraDaemon.java (line 71) Fatal exception in thread Thread[GMFD:1,5,main]   
java.lang.RuntimeException: pending range collision between /X.X.X.139 and /X.X.X.140
java.lang.RuntimeException: pending range collision between /X.X.X.139 and /X.X.X.140
        at org.apache.cassandra.locator.TokenMetadata.addPendingRange(TokenMetadata.java:242)
        at org.apache.cassandra.service.StorageService.updateBootstrapRanges(StorageService.java:481)
        at org.apache.cassandra.service.StorageService.onChange(StorageService.java:402)
        at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:692)
        at org.apache.cassandra.gms.Gossiper.applyApplicationStateLocally(Gossiper.java:657)
        at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:610)
        at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(Gossiper.java:978)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619) ",,hammer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Dec/09 15:44;jaakko;603.patch;https://issues.apache.org/jira/secure/attachment/12428139/603.patch",,,,,,,,,,,,,,1.0,jaakko,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19774,,,Thu Dec 17 22:03:03 UTC 2009,,,,,,,,,,"0|i0fzy7:",91422,,,,,Normal,,,,,,,,,,,,,,,,,"07/Dec/09 08:47;jaakko;Pending range collision check is currently too trigger-happy and should be relaxed a bit. At the moment pending range collision happens if any of the node's ranges clash, although it should happen only if the primary range node is booting to clashes.

However, if your nodes already had finished bootstrapping, then this is another issue completely. Haven't seen such case myself, but I'll have a look.
;;;","07/Dec/09 21:02;jaakko;Could you please check if the nodes in question (.139 and .140) had already completed bootstrap (log entry ""Bootstrap completed..."" on INFO level)?
;;;","08/Dec/09 01:29;lenn0x;No they did not. The messages started showing up a few minutes after I believe, anticompaction was still running.;;;","08/Dec/09 21:16;jaakko;For now it is best to wait until one bootstrap has finished before starting another one. As mentioned pending ranges clash is relatively easy to get (happens if any of replica ranges are the same), so before this one is fixed, you might easily see this even ""without reason"".
;;;","08/Dec/09 21:28;jaakko;As for the fix, there are two (at least) two options I think:

(1) Add a list of pending primary ranges (or tokens) to token metadata. Currently primary and replica pending ranges are all in one list, so there is no way to check afterwards if primary ranges collide.

(2) Ditch pending ranges completely and convert it to pending tokens. Problem with pending ranges is that it is static structure (determined at the time of bootstrap/leaving) and does not react to token changes during the operation. This introduces a number of difficult-to-prove-that-it-works-correctly and difficult-to-handle-correctly corner cases regarding node movement as proved by various mail and JIRA discussions recently. If we had a list of pending tokens instead, it would adapt to any changes that happen during the move operation. There are currently issues in pending range handling (not cleaned up correctly in all cases, thread/atomicy issues, leaving coordination, etc) that would mostly go away if we swiched to pending tokens instead, I think. Might be that I'm overlooking something obvious here, but to me it seems like dynamically adapting pending token list would be more suitable for this.
;;;","09/Dec/09 01:26;lenn0x;Jaakko,

Just curious, for all the node movement coordination operations, would Zookeeper make any of this a bit easier to manage?;;;","10/Dec/09 21:23;jaakko;It certainly is worth considering. For now I think we can do without, but with automatic load balancing coordination issues will become more complex.

I'm going to have a look at this pending range issue tomorrow.
;;;","11/Dec/09 19:32;jaakko;Patch attached. Modifications:

Keep track of booting and leaving tokens and calculate pending ranges again every time there is status change. This will keep them up to date. To ensure that pending ranges cover node's final range, following reasoning is used in calculation:

(1) When in doubt, it is better to write too much to a node than too little. That is, if there are multiple nodes moving, calculate the biggest ranges a node could have. Cleaning up unneeded data afterwards is better than missing writes during movement.

(2) When a node leaves, ranges for other nodes can only grow (a node might get additional ranges, but it will not lose any of its current ranges as a result of a leave). Therefore we will first remove _all_ leaving tokens for the sake of calculation and then check what ranges would go where if all nodes are to leave. This way we get the biggest possible ranges with regard current leave operations, covering all subsets of possible final range values.

(3) When a node bootstraps, ranges of other nodes can only get smaller. Without doing complex calculations to see if multiple bootstraps overlap, we simply base calculations on the same token ring used before (reflecting situation after all leave operations have completed). Bootstrapping nodes will be added and removed one by one to that metadata and checked what their ranges would be. This will give us the biggest possible ranges the node could have. It might be that other bootstraps make our actual final ranges smaller, but it does not matter as we can clean up the data afterwards.

Bootstrap Token collision (old pending range collision) is thrown now only if bootstrap tokens are identical.

Calculating pending ranges is rather heavy operation, but since it is done only once when a node changes state in the cluster, it should be manageable.

This patch would make #572 cleaner to do, since we now know which way a node is going and can update pending ranges according to any changes.

Edit: this also removes nodeprobe cancelpendingranges. That would be pointless now. If there is a node/token that has not finished move operation, nodeprobe removetoken will do the trick.;;;","15/Dec/09 06:32;jbellis;1, 2, and 3 are all how the existing code works in my mind (which may not be how it works in reality :).  What does the extra tracking of bootstrap tokens & leaving endpoints buy us?  As you said, pending ranges shouldn't overlap in the first place unless there is a token collision.;;;","15/Dec/09 13:55;jaakko;Unfortunately it doesn't quite work that way :)

First the case of leaving nodes:

Problem with current implementation is that pending ranges is calculated only once at the time of leaving. Suppose there is a ring of nodes A, B, C, D and E with replication factor 2. Ring status is this:

(primary, replica)
E-A, D-E
A-B, E-A
B-C, A-B
C-D, B-C
D-E, C-D

Suppose C prepares to leave. After hearing STATE_LEAVING from C, ring status will be:

(primary, replica, pending)
E-A, D-E
A-B, E-A
B-C, A-B
C-D, B-C, A-B
D-E, C-D, B-C

Now suppose also B leaves. After receiving STATE_LEAVING, ring status with current implementation will be:
E-A, D-E
A-B, E-A
B-C, A-B, E-A
C-D, B-C, A-B
D-E, C-D, B-C

This is clearly wrong, as (1) E-A is being streamed to C, even though it is leaving and (2) D is not getting this range, even if it is supposed to.

In order to do this right, we will need to know at all times what nodes are leaving and calculate ranges accordingly. An anonymous pending ranges list is not enough, as that does not tell which node is leaving and/or if the ranges are there because of bootstrap or leave operation.


As for bootstrapping and pending range collision:

Suppose that there is a ring of nodes A, C and E, with replication factor 3. Node D bootstraps between C and E, so its pending ranges will be E-A, A-C and C-D. Now suppose node B bootstraps between A and C at the same time. Its pending ranges would be C-E, E-A and A-B. Now both nodes have pending range E-A in their list, which will cause pending range collision even though we're only talking about replica range, not even primary range. The same thing happens for any nodes that boot simultaneously between same two nodes. For this we cannot simply make pending ranges a multimap, since that would make us unable to notice the real problem of two nodes trying to boot using the same token. In order to do this properly, we need to know what tokens are booting at any time.
;;;","16/Dec/09 05:14;jbellis;Thanks for the explanation.  Committed, w/ parts of the above as comments to TokenMetadata.

One minor quibble is, I'd really prefer to avoid having this circular TM <-> ReplicationStrategy dependency cycle.  (Which is part of the reason the code was structured the way it was: RS would pass TM the info it needed to do its thing in a concurrency-safe fashion, w/o needing to reach into RS itself which makes auditing for thread-safety much harder).  So if you think of a way to refactor that, even better. :);;;","16/Dec/09 05:15;jbellis;Oops, I take back the ""committed"" part -- I'm getting test failures.  I think they just need to be updated to use the new method signatures.;;;","16/Dec/09 15:44;jaakko;New version:
- Moved calculatePendingRanges to StorageService
- fixed test errors;;;","17/Dec/09 02:13;jbellis;committed to 0.5 and trunk;;;","18/Dec/09 06:03;hudson;Integrated in Cassandra #291 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/291/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RandomPartitioner convertFromDiskFormat is slow,CASSANDRA-581,12441634,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,lenn0x,lenn0x,lenn0x,25/Nov/09 10:00,16/Apr/19 17:33,22/Mar/23 14:57,05/Dec/09 09:45,0.5,,,,1,,,,,,"convertFromDiskFormat in RandomPartitioner is slow. It uses split. We were testing with 1000+ keys using multi-get on a local node. We saw on average 200ms~, with the applied patch it went down to 76ms~.
",,johanoskarsson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Nov/09 10:12;lenn0x;0001-Make-convertFromDiskFormat-use-substring-over-split-.patch;https://issues.apache.org/jira/secure/attachment/12426053/0001-Make-convertFromDiskFormat-use-substring-over-split-.patch",,,,,,,,,,,,,,1.0,lenn0x,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19764,,,Sat Dec 05 12:34:26 UTC 2009,,,,,,,,,,"0|i0fztb:",91400,,,,,Low,,,,,,,,,,,,,,,,,"25/Nov/09 10:07;stuhood;+1 Looks good to me.;;;","25/Nov/09 10:14;lenn0x;Forgot the +1 on the splitPoint for second.;;;","25/Nov/09 10:49;jbellis;Can you add a test that catches the +1 bug?  This is definitely ""should be covered by the test suite"" area.;;;","05/Dec/09 09:45;lenn0x;Jonathan,

The test already exist, just my fault on that patch change. I verified the test under RandomPartitionerTest validates the +1 change I did.

Commited.;;;","05/Dec/09 20:34;hudson;Integrated in Cassandra #278 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/278/])
    Change convertFromDiskFormat to use substring splitting vs using split operation (slow). patch by goffinet; reviewed by stuhood for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
More schema migration race conditions,CASSANDRA-1715,12479261,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,gdusbabek,jbellis,jbellis,06/Nov/10 05:12,16/Apr/19 17:33,22/Mar/23 14:57,20/Nov/10 01:04,0.7.0 rc 1,,,,0,,,,,,"Related to CASSANDRA-1631.

This is still a bug with schema updates to an existing CF, since reloadCf is doing a unload/init cycle. So flushing + compaction is an issue there as well. Here is a stacktrace from during an index creation where it stubbed its toe on an incomplete sstable from an in-progress compaction (path names anonymized):
{code}
INFO [CompactionExecutor:1] 2010-11-02 16:31:00,553 CompactionManager.java (line 224) Compacting [org.apache.cassandra.io.sstable.SSTableReader(path='Standard1-e-6-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='Standard1-e-7-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='Standard1-e-8-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='Standard1-e-9-Data.db')]
...
ERROR [MigrationStage:1] 2010-11-02 16:31:10,939 ColumnFamilyStore.java (line 244) Corrupt sstable Standard1-tmp-e-10-<>=[Data.db, Index.db]; skipped
java.io.EOFException
        at org.apache.cassandra.utils.FBUtilities.skipShortByteArray(FBUtilities.java:308)
        at org.apache.cassandra.io.sstable.SSTable.estimateRowsFromIndex(SSTable.java:231)
        at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:286)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:202)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:235)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:443)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:431)
        at org.apache.cassandra.db.Table.initCf(Table.java:335)
        at org.apache.cassandra.db.Table.reloadCf(Table.java:343)
        at org.apache.cassandra.db.migration.UpdateColumnFamily.applyModels(UpdateColumnFamily.java:89)
        at org.apache.cassandra.db.migration.Migration.apply(Migration.java:158)
        at org.apache.cassandra.thrift.CassandraServer$2.call(CassandraServer.java:672)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
...
 INFO [CompactionExecutor:1] 2010-11-02 16:31:31,970 CompactionManager.java (line 303) Compacted to Standard1-tmp-e-10-Data.db.  213,657,983 to 213,657,983 (~100% of original) bytes for 626,563 keys.  Time: 31,416ms.
{code}

There is also a race between schema modification and streaming.",,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Nov/10 07:25;gdusbabek;ASF.LICENSE.NOT.GRANTED--v3-0001-take-drop-off-CompactionManager.txt;https://issues.apache.org/jira/secure/attachment/12459653/ASF.LICENSE.NOT.GRANTED--v3-0001-take-drop-off-CompactionManager.txt","16/Nov/10 07:25;gdusbabek;ASF.LICENSE.NOT.GRANTED--v3-0002-compaction-lock.txt;https://issues.apache.org/jira/secure/attachment/12459654/ASF.LICENSE.NOT.GRANTED--v3-0002-compaction-lock.txt","16/Nov/10 07:25;gdusbabek;ASF.LICENSE.NOT.GRANTED--v3-0003-migration-uses-locks.txt;https://issues.apache.org/jira/secure/attachment/12459655/ASF.LICENSE.NOT.GRANTED--v3-0003-migration-uses-locks.txt","16/Nov/10 07:25;gdusbabek;ASF.LICENSE.NOT.GRANTED--v3-0004-handle-moved-dropped-CF-prior-to-pending-compaction-st.txt;https://issues.apache.org/jira/secure/attachment/12459656/ASF.LICENSE.NOT.GRANTED--v3-0004-handle-moved-dropped-CF-prior-to-pending-compaction-st.txt","16/Nov/10 07:25;gdusbabek;ASF.LICENSE.NOT.GRANTED--v3-0005-CFS.reload-assumes-metadata-is-mutable.txt;https://issues.apache.org/jira/secure/attachment/12459657/ASF.LICENSE.NOT.GRANTED--v3-0005-CFS.reload-assumes-metadata-is-mutable.txt","16/Nov/10 07:25;gdusbabek;ASF.LICENSE.NOT.GRANTED--v3-0007-updateColumnFamily-uses-reload-remove-unneccesary-stru.txt;https://issues.apache.org/jira/secure/attachment/12459659/ASF.LICENSE.NOT.GRANTED--v3-0007-updateColumnFamily-uses-reload-remove-unneccesary-stru.txt","16/Nov/10 07:25;gdusbabek;ASF.LICENSE.NOT.GRANTED--v3-0008-perform-index-maintenance-outside-of-migration-locks-d.txt;https://issues.apache.org/jira/secure/attachment/12459660/ASF.LICENSE.NOT.GRANTED--v3-0008-perform-index-maintenance-outside-of-migration-locks-d.txt","16/Nov/10 07:25;gdusbabek;ASF.LICENSE.NOT.GRANTED--v3-0009-use-avro-structures-inside-UpdateColumnFamily.txt;https://issues.apache.org/jira/secure/attachment/12459661/ASF.LICENSE.NOT.GRANTED--v3-0009-use-avro-structures-inside-UpdateColumnFamily.txt","16/Nov/10 07:25;gdusbabek;ASF.LICENSE.NOT.GRANTED--v3-0010-remove-unused-fields-in-DropColumnFamily-DropKeyspace.txt;https://issues.apache.org/jira/secure/attachment/12459662/ASF.LICENSE.NOT.GRANTED--v3-0010-remove-unused-fields-in-DropColumnFamily-DropKeyspace.txt","18/Nov/10 18:05;jbellis;v3-0006-replace-modifiable-CFM-members-with-private-fields-a.patch;https://issues.apache.org/jira/secure/attachment/12459906/v3-0006-replace-modifiable-CFM-members-with-private-fields-a.patch","18/Nov/10 18:06;jbellis;v3-0011-make-addIndex-asynchronous-and-race-proof.patch;https://issues.apache.org/jira/secure/attachment/12459907/v3-0011-make-addIndex-asynchronous-and-race-proof.patch","19/Nov/10 22:23;jbellis;v3-0012-remove-locks-from-UpdateColumnFamily.patch;https://issues.apache.org/jira/secure/attachment/12460004/v3-0012-remove-locks-from-UpdateColumnFamily.patch",,,12.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20269,,,Tue Nov 23 02:15:03 UTC 2010,,,,,,,,,,"0|i0g6xz:",92555,,,,,Critical,,,,,,,,,,,,,,,,,"09/Nov/10 07:25;gdusbabek;Be prepared to throw up a little.

This approach puts a lock around CompactionManager. Migrations have to acquire it and Table.flushLock before proceeding.  Special care had to be taking when reloading a CFS since it places blocking jobs on the compaction manager.  This patch should handle the streaming race as well (when the directory specified in a Descriptor is no longer valid).;;;","09/Nov/10 07:33;jbellis;Also, out of curiosity, what were the main complications w/ mutable CFS.metadata?;;;","09/Nov/10 07:48;gdusbabek;bq. what were the main complications w/ mutable CFS.metadata?

There were a couple things. A new memtable would need to know about the updated meta settings for thresholds.  The timing here is tricky because of flushing (chances are you would have just flushed and have an empty memtable in anyway, but one can't be too sure).  Other things... Make sure secondary indexes are dealt with properly on updates (e.g.: not reloaded needlessly).  Efficiently dealing with SSTableReader instances--certain classes up updates wouldn't require messing with them at all, but others would (when files move).  Ideally, it would be nice to repoint a few instances of SSTable at new data files and have all caches, stats, etc. remain intact.;;;","09/Nov/10 10:02;jbellis;UpdateColumnFamily doesn't acquireLocks().  (Shouldn't Migration do that so the subclasses don't have to?)

bq. A new memtable would need to know about the updated meta settings for thresholds. The timing here is tricky because of flushing (chances are you would have just flushed and have an empty memtable in anyway, but one can't be too sure).

This gets a little messy code-wise (because we allow overriding memtable settings at runtime) but not too bad.  At worst we just set the CFS values to the new migration values during application.  I don't see any timing issues (Memtable.isThresholdViolated checks w/ the CFS each time, it doesn't cache locally).

bq. Make sure secondary indexes are dealt with properly on updates (e.g.: not reloaded needlessly).
 
Writing code to detect when indexes are added/dropped is a pain compared to just rebuilding it from scratch, but efficiency-wise it seems like a win. At least mutating you can avoid redoing the index sampling every time.  Stopping updates in their tracks while we reload, to change read_repair_chance, is really brutal.  (If UpdateCF doesn't actually need to acquireLocks then never mind, but I think it does.)

bq. Efficiently dealing with SSTableReader instances--certain classes up updates wouldn't require messing with them at all, but others would (when files move). 

What is making files move here?
;;;","09/Nov/10 10:10;gdusbabek;bq. What is making files move here?
Renaming. Sorry.  I know we'll need to implement it eventually, so I can't stop thinking about it. That one doesn't matter in this context.;;;","09/Nov/10 22:03;gdusbabek;bq. UpdateColumnFamily doesn't acquireLocks().
It does on line 82 in my checkout.
bq. Shouldn't Migration do that so the subclasses don't have to?
It doesn't make sense to lock on the Add* methods, but I agree it might be easier just to do the locking in the superclass. What do you think?;;;","11/Nov/10 00:21;gdusbabek;v2 shows what the CFS/CFM reload approach would probably look like for UpdateColumnFamily.  Unfortunately it touches a lot of code and might not be warranted at this late stage in the beta cycle.  

If left for 0.7.1, I need to explain that it changes the serialization format for Migrations in a non-backwards compatible way, which is not desirable.  The same kind of work would have to be done for the other Migration subclasses.

This is mainly a demonstration, but one thing I'd definitely change is to use the avro CfDef in the UpdateColumnFamily constructor instead of the thrift version.;;;","15/Nov/10 05:25;jbellis;The v2 approach looks great.  I think the main improvement we need is to not do blocking flushes while the locks are held.  For the purposes of creating a new memtable a nonblocking flush is fine.  For creating indexes we'll need to set up a callback to do the index building after the flush completes.  (We used to have code that took a callback arg as part of the flush call, I think I took it out but it should be relatively easy to resurrect.)  I agree that it touches a lot of code, but the core changes (i.e. not one-line things like encapsulating gcgraceseconds that are messy but not dangerous) aren't much larger than v1.  The huge improvement over waiting to re-sample indexes after UpdateCF is worth it imo.

I'm also fine with saying that changing the CFS will blow away any JMX-applied changes and reset values to what the new CFM says the should be.  But if you are happy with the Default* approach I am too.

bq. If left for 0.7.1, I need to explain that it changes the serialization format for Migrations in a non-backwards compatible way, which is not desirable

Is this saying that we'd need to tell beta3 users to rebuild their schemas if this goes in?  I am fine with that, I just want to make sure I understand.;;;","15/Nov/10 21:35;gdusbabek;bq. Is this saying that we'd need to tell beta3 users to rebuild their schemas if this goes in? I am fine with that, I just want to make sure I understand.
Yes, exactly.

I'll go ahead and finish this patch in the v2 direction.;;;","16/Nov/10 07:27;gdusbabek;0008 addresses the flushes-within-locks brought up by jonathan.  0009 and 0010 are cleanup.;;;","18/Nov/10 18:08;jbellis;new version of 06 to rebase.

11 makes addIndex asynchronous which unclogs the post-flush executor (which is single-threaded for safety, so a long index build would mean we don't clean up any commitlogs until it's done) and also makes restarting when a new index is not yet complete friendlier.

I don't think we need the locks on Update anymore since we're keeping the same Tracker and so forth so 12 removes that.  locks are still needed for Drop.

What do you think?;;;","19/Nov/10 03:49;gdusbabek;I think update still needs to acquire the locks for the case when secondary indexes are dropped.  The locking could conceivably be pushed down to the point in CFS when the indexes are dropped though, at which point we'd need to remove the assert from the beginning of CFS.reload() and make the members that get reset in reload() volatile (minCompactionThreshold, maxCompactionThreshold, etc.).;;;","19/Nov/10 03:59;gdusbabek;0012 causes several unit tests to fail.  You might want to take a closer look at it.;;;","19/Nov/10 22:23;jbellis;new 0012.

bq. the locking could conceivably be pushed down to the point in CFS when the indexes are dropped 

right, this is what 0011 does.  more accurately, it uses CSLM to avoid an explicit lock.

bq. at which point we'd need to remove the assert from the beginning of CFS.reload() and make the members that get reset in reload() volatile (minCompactionThreshold, maxCompactionThreshold, etc.). 

done.

bq. 0012 causes several unit tests to fail

one was the assert, one was an unrelated commitlog bug that got exposed by something unrelated.  fixed.;;;","19/Nov/10 22:57;gdusbabek;bq. right, this is what 0011 does. more accurately, it uses CSLM to avoid an explicit lock.
My argument was that it wasn't safe to call indexCfs.removeAllSSTables() without the flush lock, but it looks like SSTableTracker is properly synchronized to avoid any problems with flushing.  No problem here.

+1 I'll commit this shortly.;;;","20/Nov/10 01:04;gdusbabek;committed.;;;","20/Nov/10 04:05;hudson;Integrated in Cassandra-0.7 #19 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/19/])
    ;;;","20/Nov/10 20:48;hudson;Integrated in Cassandra #602 (See [https://hudson.apache.org/hudson/job/Cassandra/602/])
    remove locks from UpdateColumnFamily. patch by jbellis, reviewed by gdusbabek. CASSANDRA-1715
make addIndex asynchronous and race proof. patch by jbellis, reviewed by gdusbabek. CASSANDRA-1715
remove unused fields in DropColumnFamily, DropKeyspace. patch by gdusbabek, reviewe by jbellis. CASSANDRA-1715
use avro structures inside UpdateColumnFamily. patch by gdusbabek, reviewe by jbellis. CASSANDRA-1715
perform index maintenance outside of migration locks during CF update. patch by gdusbabek, reviewe by jbellis. CASSANDRA-1715
updateColumnFamily uses reload, remove unneccesary structures, fix bugs. patch by gdusbabek, reviewe by jbellis. CASSANDRA-1715
replace modifiable CFM members with private fields and public getters. patch by gdusbabek, reviewe by jbellis. CASSANDRA-1715
CFS.reload() assumes metadata is mutable. patch by gdusbabek, reviewe by jbellis. CASSANDRA-1715
handle moved/dropped CF prior to pending compaction/streams. patch by gdusbabek, reviewe by jbellis. CASSANDRA-1715
migration uses locks. patch by gdusbabek, reviewe by jbellis. CASSANDRA-1715
compaction lock. patch by gdusbabek, reviewe by jbellis. CASSANDRA-1715
take drop off CompactionManager. patch by gdusbabek, reviewe by jbellis. CASSANDRA-1715
;;;","23/Nov/10 09:32;tjake;During testing I hit this section of code:

CFMetaData.java:662 
{code}
  // remove the ones leaving.
        for (ByteBuffer indexName : toRemove)
            column_metadata.remove(indexName);
{code}

but column_metadata is defined as:

{code}
        this.column_metadata = Collections.unmodifiableMap(column_metadata);
{code}

So remove() will throw an exception.
;;;","23/Nov/10 10:07;jbellis;can you open a new ticket for that?;;;","23/Nov/10 10:15;tjake;CASSANDRA-1768;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hadoop Integration doesn't work when one node is down,CASSANDRA-1927,12494462,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,mck,uctopcu,uctopcu,03/Jan/11 09:15,16/Apr/19 17:33,22/Mar/23 14:57,04/Jan/11 03:43,0.7.1,,,,1,,,,,,"using the same directives in the sample code:

When I start the CFInputFormat to read a CF in a keyspace of RF=3 on a 4-node cluster:
- If all the nodes are all up, everything works fine and I don't have any problems walking through the all data in the CF, however
- If there's a node down, the hadoop job does not even start, just dies without any errors or exceptions.

So I'm really sorry for not being able to post any errors or exceptions, though it's really easy to reproduce. Just startup a cluster and take one node down and you're there :)",,mck,stuhood,uctopcu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Jan/11 01:41;mck;CASSANDRA-1927.patch;https://issues.apache.org/jira/secure/attachment/12467331/CASSANDRA-1927.patch",,,,,,,,,,,,,,1.0,mck,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20373,,,Mon Jan 03 20:23:29 UTC 2011,,,,,,,,,,"0|i0g89j:",92769,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"03/Jan/11 16:25;mck;Client side (hadoop job):

java.io.IOException: Could not get input splits
	at org.apache.cassandra.hadoop.ColumnFamilyInputFormat.getSplits(ColumnFamilyInputFormat.java:127)
	at org.apache.hadoop.mapred.JobClient.writeNewSplits(JobClient.java:885)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:779)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:432)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:447)
	at no.finntech.countstats.reduce.FakeAdCounterTableReduce.run(FakeAdCounterTableReduce.java:421)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
	at no.finntech.countstats.reduce.FakeAdCounterTableReduce.main(FakeAdCounterTableReduce.java:75)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
Caused by: java.util.concurrent.ExecutionException: java.io.IOException: unable to connect to server
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.hadoop.ColumnFamilyInputFormat.getSplits(ColumnFamilyInputFormat.java:123)
	... 13 more
Caused by: java.io.IOException: unable to connect to server
	at org.apache.cassandra.hadoop.ColumnFamilyInputFormat.createConnection(ColumnFamilyInputFormat.java:212)
	at org.apache.cassandra.hadoop.ColumnFamilyInputFormat.getSubSplits(ColumnFamilyInputFormat.java:187)
	at org.apache.cassandra.hadoop.ColumnFamilyInputFormat.access$200(ColumnFamilyInputFormat.java:74)
	at org.apache.cassandra.hadoop.ColumnFamilyInputFormat$SplitCallable.call(ColumnFamilyInputFormat.java:160)
	at org.apache.cassandra.hadoop.ColumnFamilyInputFormat$SplitCallable.call(ColumnFamilyInputFormat.java:145)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
Caused by: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused
	at org.apache.thrift.transport.TSocket.open(TSocket.java:185)
	at org.apache.thrift.transport.TFramedTransport.open(TFramedTransport.java:81)
	at org.apache.cassandra.hadoop.ColumnFamilyInputFormat.createConnection(ColumnFamilyInputFormat.java:208)
	... 9 more
Caused by: java.net.ConnectException: Connection refused
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:333)
	at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:195)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
	at java.net.Socket.connect(Socket.java:525)
	at org.apache.thrift.transport.TSocket.open(TSocket.java:180)
	... 11 more

;;;","03/Jan/11 16:27;mck;There's a todo comment in ColumnFamilyInputFormat
 // TODO handle failure of range replicas & retry

line 198 only tries the first endpoint. a loop on the TException trying the next endpoint is needed.;;;","03/Jan/11 18:45;mck;Utku: are you able to test this patch?

( It didn't work for me because RF was never really set to 3. using cassandra-cli ""describe keyspace xxx"" reported ""Replication Factor: 1"" )  :-$

;;;","03/Jan/11 19:00;uctopcu;Mck: Right now I can't access to our compilation server. However I can replace the running binaries and test them if I have the patched rc4. Can you somehow provide me the compiled package?;;;","03/Jan/11 19:08;mck;Sent DM. If it doesn't work you should at minimum see the job's IOException stacktrace change from ""unable to connect to server"" to ""failed connecting to all endpoints"".;;;","03/Jan/11 21:58;uctopcu;I'll be testing it in a few hours. I'll write down the results. something urgent came up.;;;","03/Jan/11 23:53;mck;After fixing my local RF problem this patch works for me.;;;","04/Jan/11 01:25;jbellis;It looks like this patch includes the code from CASSANDRA-1921, which is causing conflicts b/c it's already applied on 0.7 and trunk.  Can you create a patch for 1927 only?;;;","04/Jan/11 01:27;mck;Putting Stu as reviewer since he was for CASSANDRA-342 (which the TODO comment in question was added under).;;;","04/Jan/11 01:34;mck;Yeah, the patch had a lot of crap in it. sorry. will re-apply.;;;","04/Jan/11 01:38;mck;correct patch & license grant;;;","04/Jan/11 01:41;mck;third time lucky. removed unnecessary import.;;;","04/Jan/11 01:54;uctopcu;I've tested against the rc4+patch and it works.;;;","04/Jan/11 03:43;jbellis;committed, thanks!;;;","04/Jan/11 04:23;hudson;Integrated in Cassandra-0.7 #142 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/142/])
    retry hadoop split requests on connection failure
patch by mck; reviewed by jbellis for CASSANDRA-1927
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool scrub hangs or throws an exception,CASSANDRA-2240,12499601,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,kunda,kunda,24/Feb/11 23:32,16/Apr/19 17:33,22/Mar/23 14:57,02/Mar/11 00:10,0.7.3,,Tool/nodetool,,0,,,,,,"trying to run nodetool scrub hung or (only happened one time) threw the following exception:

ERROR [CompactionExecutor:1] 2011-02-28 10:26:26,620 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.AssertionError
        at org.apache.cassandra.dht.RandomPartitioner.convertFromDiskFormat(RandomPartitioner.java:62)
        at org.apache.cassandra.io.sstable.SSTableReader.decodeKey(SSTableReader.java:627)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:538)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:55)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:194)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
",using build #314 from hudson,kunda,wajam,,,,,,,,,,,,,,,,,,,,57600,57600,,0%,57600,57600,,,,,,,,,,,,CASSANDRA-2217,,,,,,,,"01/Mar/11 10:49;jbellis;2240-v2.txt;https://issues.apache.org/jira/secure/attachment/12472270/2240-v2.txt","01/Mar/11 12:45;jbellis;2240-v3.txt;https://issues.apache.org/jira/secure/attachment/12472278/2240-v3.txt","01/Mar/11 21:22;slebresne;2240-v5.patch;https://issues.apache.org/jira/secure/attachment/12472305/2240-v5.patch","01/Mar/11 22:46;jbellis;2240-v6.txt;https://issues.apache.org/jira/secure/attachment/12472313/2240-v6.txt","01/Mar/11 13:30;jbellis;2240.txt;https://issues.apache.org/jira/secure/attachment/12472279/2240.txt","01/Mar/11 07:16;jbellis;2240.txt;https://issues.apache.org/jira/secure/attachment/12472250/2240.txt","27/Feb/11 18:44;kunda;exception2.txt;https://issues.apache.org/jira/secure/attachment/12472102/exception2.txt","27/Feb/11 18:34;kunda;jstack1.txt;https://issues.apache.org/jira/secure/attachment/12472100/jstack1.txt","27/Feb/11 18:34;kunda;signatureBuckets-f-104.tar.gz;https://issues.apache.org/jira/secure/attachment/12472101/signatureBuckets-f-104.tar.gz","28/Feb/11 16:07;kunda;system.log.2.gz;https://issues.apache.org/jira/secure/attachment/12472152/system.log.2.gz","28/Feb/11 16:01;kunda;system.log.gz;https://issues.apache.org/jira/secure/attachment/12472151/system.log.gz","24/Feb/11 23:41;kunda;test-0.6.x-tables.tar.gz;https://issues.apache.org/jira/secure/attachment/12471844/test-0.6.x-tables.tar.gz","27/Feb/11 18:44;kunda;userChannelFilter-f-210.tar.gz;https://issues.apache.org/jira/secure/attachment/12472103/userChannelFilter-f-210.tar.gz",,13.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20521,,,Tue Mar 01 20:15:08 UTC 2011,,,,,,,,,,"0|i0ga6v:",93081,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"24/Feb/11 23:37;jbellis;what is going on in the compactionmanager when it's ""hung?""  (use jstack);;;","24/Feb/11 23:41;kunda;attached the tables that can be used to reproduce the hang;;;","25/Feb/11 00:03;jbellis;This is not a valid sstable.  It claims (from its lack of version string) that it contains encoded row keys, meaning <token>:<key>, but it actually does not.  scrub can't help you with that.;;;","25/Feb/11 00:07;jbellis;... on closer inspection it does have colon-delimited keys, but scrub doesn't see them.  ;;;","25/Feb/11 00:08;kunda;Here's the trace I got (narrowed down):

""CompactionExecutor:1"" prio=10 tid=0x000000001eb67800 nid=0x7fdb runnable [0x0000000040dc4000]
   java.lang.Thread.State: RUNNABLE
        at java.io.DataInputStream.readFully(DataInputStream.java:195)
        at java.io.DataInputStream.readLong(DataInputStream.java:416)
        at org.apache.cassandra.utils.BloomFilterSerializer.deserialize(BloomFilterSerializer.java:51)
        at org.apache.cassandra.utils.BloomFilterSerializer.deserialize(BloomFilterSerializer.java:30)
        at org.apache.cassandra.io.sstable.IndexHelper.defreezeBloomFilter(IndexHelper.java:108)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:86)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:549)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:55)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:194)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)

   Locked ownable synchronizers:
        - <0x00002aaabd5f71c8> (a java.util.concurrent.ThreadPoolExecutor$Worker)
        - <0x00002aaabd6cd008> (a java.util.concurrent.locks.ReentrantLock$NonfairSync);;;","25/Feb/11 03:37;jbellis;What do you see when you apply the patch for CASSANDRA-2241?

What create column family statement should I give the cli, to create this userActionUtilsKey CF?;;;","25/Feb/11 03:46;jbellis;also, can you attach the system.log from when you started cassandra?  we're trying to figure out why it's using the new-version BloomFilterSerializer, when it should be using LegacyBloomFilterSerializer.;;;","25/Feb/11 03:50;kunda;I didn't have time to apply the patch yet - but I will next week.
Regarding the example CF - as [~slebresne] commented in CASSANDRA-2217, it was indeed created in Cassandra 0.6.5 -
so the cli cannot be used to create it ;)

However, here is KS xml element that defined it in storage-conf.xml:
{code:xml} 
    <Keyspace Name=""generalUtils"">
      <ColumnFamily Name=""userActionUtilsKey"" CompareWith=""UTF8Type"" />
      <ColumnFamily Name=""facebookShowIds"" CompareWith=""UTF8Type"" />

      <ReplicaPlacementStrategy>org.apache.cassandra.locator.RackUnawareStrategy</ReplicaPlacementStrategy>
      <ReplicationFactor>1</ReplicationFactor>
      <EndPointSnitch>org.apache.cassandra.locator.EndPointSnitch</EndPointSnitch>
    </Keyspace>
{code};;;","25/Feb/11 03:50;jbellis;When I create a keyspace and CF w/ default settings and scrub it, I get

{noformat}
 INFO 13:49:17,594 Scrubbing SSTableReader(path='/var/lib/cassandra/data/Keyspace1/userActionUtilsKey-9-Data.db')
 INFO 13:49:17,856 Scrub of SSTableReader(path='/var/lib/cassandra/data/Keyspace1/userActionUtilsKey-9-Data.db') complete
{noformat};;;","25/Feb/11 04:14;kunda;I had to go back 26 x 20MiB logs consisting mostly of lines such as:
{code} INFO [CompactionExecutor:1] 2011-02-24 11:18:10,262 SSTableIdentityIterator.java (line 90) Invalid bloom filter in SSTableReader(path='/vm1/cassandraDB/data/Keyspace2/ruleGroup-f-243-Data.db'); will rebuild it {code}
which were preceded by a bunch of NegativeArraySizeExceptions, which were after the initialization log:
{code}
INFO [main] 2011-02-24 11:08:43,595 AbstractCassandraDaemon.java (line 77) Logging initialized
 INFO [main] 2011-02-24 11:08:43,605 AbstractCassandraDaemon.java (line 97) Heap size: 8330936320/8331984896
 INFO [main] 2011-02-24 11:08:43,606 CLibrary.java (line 61) JNA not found. Native methods will be disabled.
 INFO [main] 2011-02-24 11:08:43,613 DatabaseDescriptor.java (line 121) Loading settings from file:/usr/local/apache-cassandra-2011-02-24_02-21-51/conf/cassandra.yaml
 INFO [main] 2011-02-24 11:08:43,821 DatabaseDescriptor.java (line 181) DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
 INFO [main] 2011-02-24 11:08:43,893 SSTable.java (line 147) Deleted /vm1/cassandraDB/data/system/LocationInfo-f-210
 INFO [main] 2011-02-24 11:08:43,894 SSTable.java (line 147) Deleted /vm1/cassandraDB/data/system/LocationInfo-f-212
 INFO [main] 2011-02-24 11:08:43,894 SSTable.java (line 147) Deleted /vm1/cassandraDB/data/system/LocationInfo-f-211
 INFO [main] 2011-02-24 11:08:43,894 SSTable.java (line 147) Deleted /vm1/cassandraDB/data/system/LocationInfo-f-209
 INFO [main] 2011-02-24 11:08:43,919 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/system/IndexInfo-f-5
 INFO [main] 2011-02-24 11:08:43,937 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/system/Schema-f-89
 INFO [main] 2011-02-24 11:08:43,945 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/system/Migrations-f-85
 INFO [main] 2011-02-24 11:08:43,947 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/system/Migrations-f-87
 INFO [main] 2011-02-24 11:08:43,948 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/system/Migrations-f-86
 INFO [main] 2011-02-24 11:08:43,951 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/system/LocationInfo-f-213
 INFO [main] 2011-02-24 11:08:43,953 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/system/LocationInfo-f-214
 INFO [main] 2011-02-24 11:08:43,982 DatabaseDescriptor.java (line 461) Loading schema version b6fdb590-3e9e-11e0-8d0e-34b74a661156
 WARN [main] 2011-02-24 11:08:44,128 DatabaseDescriptor.java (line 493) Schema definitions were defined both locally and in cassandra.yaml. Definitions in cassandra.yaml were ignored.
 INFO [main] 2011-02-24 11:08:44,188 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace2/recommenders-f-176
 INFO [main] 2011-02-24 11:08:44,191 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace2/recommenders-f-177
 INFO [main] 2011-02-24 11:08:44,192 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace2/recommenders-f-178
 INFO [main] 2011-02-24 11:08:44,197 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace2/businessRule-f-200
 INFO [main] 2011-02-24 11:08:44,204 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace2/recommendTo-f-92
 INFO [main] 2011-02-24 11:08:44,205 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace2/recommendTo-f-93
 INFO [main] 2011-02-24 11:08:44,209 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace2/followers-f-175
 INFO [main] 2011-02-24 11:08:44,213 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace2/names-f-165
 INFO [main] 2011-02-24 11:08:44,218 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace2/ruleGroup-f-244
 INFO [main] 2011-02-24 11:08:44,218 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace2/ruleGroup-f-243
 INFO [main] 2011-02-24 11:08:44,225 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace2/recommendFrom-f-109
 INFO [main] 2011-02-24 11:08:44,233 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace2/channels-f-2866
 INFO [main] 2011-02-24 11:08:44,262 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace2/channels-f-2867
 INFO [main] 2011-02-24 11:08:44,286 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace2/recommendAgain-f-1425
 INFO [main] 2011-02-24 11:08:44,293 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/userProfile-f-816
 INFO [main] 2011-02-24 11:08:44,294 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/userProfile-f-815
 INFO [main] 2011-02-24 11:08:44,301 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/showData-f-6153
 INFO [main] 2011-02-24 11:08:44,302 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/showData-f-6039
 INFO [main] 2011-02-24 11:08:44,443 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/showData-f-6130
 INFO [main] 2011-02-24 11:08:44,498 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/showData-f-6040
 INFO [main] 2011-02-24 11:08:44,591 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/showData-f-6143
 INFO [main] 2011-02-24 11:08:44,608 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/showData-f-6036
 INFO [main] 2011-02-24 11:08:44,676 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/showData-f-6035
 INFO [main] 2011-02-24 11:08:44,852 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/showData-f-6038
 INFO [main] 2011-02-24 11:08:44,953 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/showData-f-6134
 INFO [main] 2011-02-24 11:08:44,960 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/showData-f-6114
 INFO [main] 2011-02-24 11:08:44,976 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/showData-f-6096
 INFO [main] 2011-02-24 11:08:45,040 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/showData-f-6147
 INFO [main] 2011-02-24 11:08:45,048 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/showData-f-6152
 INFO [main] 2011-02-24 11:08:45,062 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/cloudData-f-5990
 INFO [main] 2011-02-24 11:08:45,087 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/cloudData-f-6063
 INFO [main] 2011-02-24 11:08:45,109 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/cloudData-f-6109
 INFO [main] 2011-02-24 11:08:45,110 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/cloudData-f-5991
 INFO [main] 2011-02-24 11:08:45,136 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/cloudData-f-6108
 INFO [main] 2011-02-24 11:08:45,154 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/userChannelFilter-f-207
 INFO [main] 2011-02-24 11:08:45,154 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/userChannelFilter-f-208
 INFO [main] 2011-02-24 11:08:45,162 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/popularCloudIds-f-203
 INFO [main] 2011-02-24 11:08:45,166 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/userActions-f-1160
 INFO [main] 2011-02-24 11:08:45,177 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/headends-f-5793
 INFO [main] 2011-02-24 11:08:45,195 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/headends-f-5801
 INFO [main] 2011-02-24 11:08:45,196 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/headends-f-5761
 INFO [main] 2011-02-24 11:08:45,202 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/headends-f-5800
 INFO [main] 2011-02-24 11:08:45,213 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/userActionLikes-f-10
 INFO [main] 2011-02-24 11:08:45,218 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/userAttributes-f-2517
 INFO [main] 2011-02-24 11:08:45,219 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/userAttributes-f-2533
 INFO [main] 2011-02-24 11:08:45,221 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/userAttributes-f-2532
 INFO [main] 2011-02-24 11:08:45,230 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity/signatureBuckets-102
 INFO [main] 2011-02-24 11:08:45,607 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity/signatureBuckets-103
 INFO [main] 2011-02-24 11:08:45,799 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity/signatureBuckets-88
 INFO [main] 2011-02-24 11:08:46,319 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity/signatureBuckets-93
 INFO [main] 2011-02-24 11:08:46,846 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity/signatureBuckets-63
 INFO [main] 2011-02-24 11:08:49,879 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity/signatureBuckets-83
 INFO [main] 2011-02-24 11:08:51,340 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity/userSignatures-22
 INFO [main] 2011-02-24 11:08:51,351 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity/userSignatures-23
 INFO [main] 2011-02-24 11:08:51,355 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity/userSignatures-17
 INFO [main] 2011-02-24 11:08:51,385 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity/userSignatures-24
 INFO [main] 2011-02-24 11:08:51,390 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity/signatureFunctions-5
 INFO [main] 2011-02-24 11:08:51,394 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/generalUtils/userActionUtilsKey-9
 INFO [main] 2011-02-24 11:08:51,396 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/generalUtils/facebookShowIds-f-37
 INFO [main] 2011-02-24 11:08:51,398 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity2/signatureBuckets-6
 INFO [main] 2011-02-24 11:08:51,527 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity2/signatureBuckets-5
 INFO [main] 2011-02-24 11:08:51,903 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity2/userSignatures-1
 INFO [main] 2011-02-24 11:08:52,006 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity2/userSignatures-e-3
 INFO [main] 2011-02-24 11:08:52,009 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity2/userSignatures-2
 INFO [main] 2011-02-24 11:08:52,013 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity2/signatureFunctions-2
 INFO [main] 2011-02-24 11:08:52,013 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity2/signatureFunctions-1
 INFO [main] 2011-02-24 11:08:52,014 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity2/signatureFunctions-e-3
 INFO [main] 2011-02-24 11:08:52,019 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/realTimeSocial/facebookActions-f-186
 INFO [main] 2011-02-24 11:08:52,019 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/realTimeSocial/facebookActions-f-185
 INFO [main] 2011-02-24 11:08:52,022 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/realTimeSocial/watchingUsers-f-477
 INFO [main] 2011-02-24 11:08:52,038 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/realTimeSocial/showState-e-35
 INFO [main] 2011-02-24 11:08:52,039 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/realTimeSocial/showState-e-37
 INFO [main] 2011-02-24 11:08:52,039 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/realTimeSocial/showState-e-36
 INFO [main] 2011-02-24 11:08:52,042 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/realTimeSocial/watchingUserHistory-f-462
 INFO [main] 2011-02-24 11:08:52,042 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/realTimeSocial/watchingUserHistory-f-461
 INFO [main] 2011-02-24 11:08:52,043 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/realTimeSocial/watchingUserHistory-f-463
 INFO [main] 2011-02-24 11:08:52,045 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/chats/chatUsers-f-129
 INFO [main] 2011-02-24 11:08:52,046 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/chats/chatHistory-f-129
 INFO [main] 2011-02-24 11:08:52,055 CommitLogSegment.java (line 50) Creating new commitlog segment /vm1/cassandraDB/commitlog/CommitLog-1298538532055.log
 INFO [main] 2011-02-24 11:08:52,061 CommitLog.java (line 155) Replaying /vm1/cassandraDB/commitlog/CommitLog-1298537386811.log
 INFO [main] 2011-02-24 11:08:52,073 CommitLog.java (line 311) Finished reading /vm1/cassandraDB/commitlog/CommitLog-1298537386811.log
 INFO [main] 2011-02-24 11:08:52,074 ColumnFamilyStore.java (line 666) switching in a fresh Memtable for recommendAgain at CommitLogContext(file='/vm1/cassandraDB/commitlog/CommitLog-1298538532055.log', position=0)
 INFO [main] 2011-02-24 11:08:52,076 ColumnFamilyStore.java (line 977) Enqueuing flush of Memtable-recommendAgain@205025794(636 bytes, 7 operations)
 INFO [FlushWriter:1] 2011-02-24 11:08:52,077 Memtable.java (line 157) Writing Memtable-recommendAgain@205025794(636 bytes, 7 operations)
 INFO [FlushWriter:1] 2011-02-24 11:08:52,113 Memtable.java (line 164) Completed flushing /vm1/cassandraDB/data/Keyspace2/recommendAgain-f-1426-Data.db (859 bytes)
 INFO [main] 2011-02-24 11:08:52,118 CommitLog.java (line 163) Log replay complete
 INFO [main] 2011-02-24 11:08:52,131 StorageService.java (line 354) Cassandra version: 2011-02-24_02-21-51
 INFO [main] 2011-02-24 11:08:52,131 StorageService.java (line 355) Thrift API version: 19.4.0
 INFO [main] 2011-02-24 11:08:52,132 StorageService.java (line 368) Loading persisted ring state
 INFO [main] 2011-02-24 11:08:52,132 StorageService.java (line 404) Starting up server gossip
 INFO [main] 2011-02-24 11:08:52,137 ColumnFamilyStore.java (line 666) switching in a fresh Memtable for LocationInfo at CommitLogContext(file='/vm1/cassandraDB/commitlog/CommitLog-1298538532055.log', position=148)
 INFO [main] 2011-02-24 11:08:52,137 ColumnFamilyStore.java (line 977) Enqueuing flush of Memtable-LocationInfo@1068086436(29 bytes, 1 operations)
 INFO [FlushWriter:1] 2011-02-24 11:08:52,138 Memtable.java (line 157) Writing Memtable-LocationInfo@1068086436(29 bytes, 1 operations)
 INFO [FlushWriter:1] 2011-02-24 11:08:52,178 Memtable.java (line 164) Completed flushing /vm1/cassandraDB/data/system/LocationInfo-f-215-Data.db (80 bytes)
 INFO [main] 2011-02-24 11:08:52,198 StorageService.java (line 468) Using saved token 51653040247566871911249877869558549493
 INFO [main] 2011-02-24 11:08:52,199 ColumnFamilyStore.java (line 666) switching in a fresh Memtable for LocationInfo at CommitLogContext(file='/vm1/cassandraDB/commitlog/CommitLog-1298538532055.log', position=444)
 INFO [main] 2011-02-24 11:08:52,199 ColumnFamilyStore.java (line 977) Enqueuing flush of Memtable-LocationInfo@832074392(53 bytes, 2 operations)
 INFO [FlushWriter:1] 2011-02-24 11:08:52,199 Memtable.java (line 157) Writing Memtable-LocationInfo@832074392(53 bytes, 2 operations)
 INFO [FlushWriter:1] 2011-02-24 11:08:52,230 Memtable.java (line 164) Completed flushing /vm1/cassandraDB/data/system/LocationInfo-f-216-Data.db (163 bytes)
 INFO [CompactionExecutor:1] 2011-02-24 11:08:52,231 CompactionManager.java (line 395) Compacting [SSTableReader(path='/vm1/cassandraDB/data/system/LocationInfo-f-213-Data.db'),SSTableReader(path='/vm1/cassandraDB/data/system/LocationInfo-f-214-Data.db'),SSTableReader(path='/vm1/cassandraDB/data/system/LocationInfo-f-215-Data.db'),SSTableReader(path='/vm1/cassandraDB/data/system/LocationInfo-f-216-Data.db')]
 INFO [main] 2011-02-24 11:08:52,233 Mx4jTool.java (line 72) Will not load MX4J, mx4j-tools.jar is not in the classpath
 INFO [CompactionExecutor:1] 2011-02-24 11:08:52,292 CompactionManager.java (line 482) Compacted to /vm1/cassandraDB/data/system/LocationInfo-tmp-f-217-Data.db.  851 to 445 (~52% of original) bytes for 3 keys.  Time: 61ms.
 INFO [main] 2011-02-24 11:08:52,296 CassandraDaemon.java (line 112) Binding thrift service to /0.0.0.0:9160
 INFO [main] 2011-02-24 11:08:52,298 CassandraDaemon.java (line 126) Using TFastFramedTransport with a max frame size of 15728640 bytes.
 INFO [Thread-3] 2011-02-24 11:08:52,300 CassandraDaemon.java (line 153) Listening for thrift clients...
{code};;;","25/Feb/11 04:17;kunda;The previously mentioned tons of ""Invalid bloom filter in SSTableReader"" logs ended with a single stacktrace:
{code}
ERROR [CompactionExecutor:1] 2011-02-24 11:36:19,268 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.AssertionError
	at org.apache.cassandra.dht.RandomPartitioner.convertFromDiskFormat(RandomPartitioner.java:62)
	at org.apache.cassandra.io.sstable.SSTableReader.decodeKey(SSTableReader.java:627)
	at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:541)
	at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:55)
	at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:194)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
{code}
followed by what appears to be standard flushing/switching logs.;;;","25/Feb/11 04:20;kunda;I now restarted the server again, and two things happened:
1) performing a scrub on the example CF and a bunch of other CFs succeeded without hanging
2) performing a scrub on a different CF resulting in the following stack trace (on different retries):
{code}
ERROR [CompactionExecutor:1] 2011-02-24 22:02:30,329 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.io.IOError: java.io.EOFException
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:113)
	at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:549)
	at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:55)
	at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:194)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
Caused by: java.io.EOFException
	at java.io.RandomAccessFile.readInt(RandomAccessFile.java:776)
	at org.apache.cassandra.io.sstable.IndexHelper.skipBloomFilter(IndexHelper.java:47)
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:104)
	... 8 more
{code}
3) and on another CF:
{code}
ERROR [CompactionExecutor:1] 2011-02-24 22:28:28,002 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.AssertionError
        at org.apache.cassandra.dht.RandomPartitioner.convertFromDiskFormat(RandomPartitioner.java:62)
        at org.apache.cassandra.io.sstable.SSTableReader.decodeKey(SSTableReader.java:627)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:541)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:55)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:194)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
{code};;;","25/Feb/11 04:55;jbellis;I really can't help with just a stacktrace and none of the logs leading up to it.

Remember that scrub snapshots before it does its thing, so it's easy to restore the pre-scrubbed versions and try again.;;;","26/Feb/11 00:23;jbellis;Couldn't reproduce by scrubbing sstables produced by 0.6 stress.py;;;","27/Feb/11 18:28;kunda;After performing more tests, I realized why the problem could not be reproduced - the scrub process hang on a different CF, and afterwards any scrub operation would hang, until the server is restarted.
I was able to narrow down the problem to a specific sstable - I will soon post it along the stack trace.;;;","27/Feb/11 18:34;kunda;Here is the sstable that hangs the scrub process - ran on 0.7 build #325;;;","27/Feb/11 18:44;kunda;And here is another sstable that doesn't hang but throws an exception;;;","27/Feb/11 19:23;kunda;Update: restarted server and rerun scrub on the attached signatureBuckets CF, this time did not hung but gave the following error:
 INFO [CompactionExecutor:1] 2011-02-27 13:21:27,928 CompactionManager.java (line 511) Scrubbing SSTableReader(path='/vm1/cassandraDB/data/userSimilarity/signatureBuckets-f-104-Data.db')
 INFO [CompactionExecutor:1] 2011-02-27 13:21:28,430 SSTableIdentityIterator.java (line 90) Invalid bloom filter in SSTableReader(path='/vm1/cassandraDB/data/userSimilarity/signatureBuckets-f-104-Data.db'); will rebuild it
 INFO [CompactionExecutor:1] 2011-02-27 13:21:28,430 SSTableIdentityIterator.java (line 99) Invalid row summary in SSTableReader(path='/vm1/cassandraDB/data/userSimilarity/signatureBuckets-f-104-Data.db'); will rebuild it
ERROR [CompactionExecutor:1] 2011-02-27 13:21:28,434 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.io.IOError: java.io.EOFException: attempted to skip 758940931 bytes but only skipped 1400349
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:113)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:548)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:55)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:194)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.io.EOFException: attempted to skip 758940931 bytes but only skipped 1400349
        at org.apache.cassandra.io.sstable.IndexHelper.skipBloomFilter(IndexHelper.java:51)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:104)
        ... 8 more

;;;","27/Feb/11 20:05;kunda;Repeating the process now results in a different failure on a different and very big (>500MB) sstable of the same signatureBuckets CF:

ERROR [CompactionExecutor:1] 2011-02-27 13:26:01,307 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.AssertionError
        at org.apache.cassandra.dht.RandomPartitioner.convertFromDiskFormat(RandomPartitioner.java:62)
        at org.apache.cassandra.io.sstable.SSTableReader.decodeKey(SSTableReader.java:627)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:538)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:55)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:194)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)

I think it has something to do with [~jbellis]'s comment about missing colon delimiter -
if scrub cannot solve this, is there any way to fix this problem?
;;;","28/Feb/11 05:53;wajam;We are running into the same issue using #325 from Hudson. Exception everytime.

Everything started when we upgraded from 0.6.8 to 0.7.2. One of the node (17 nodes total) started to be slow, we realized one CF wasn't compacting and throw exception (Error in ThreadPoolExecutor java.io.IOError: org.apache.cassandra.db.ColumnSerializer$CorruptColumnException: invalid column name length 0).

So I gave the new scrub function a try and but looks like it doesnt work. We really need to have this fixed!

Thank you!;;;","28/Feb/11 06:45;jbellis;can someone post a log file from a failed scrub attempt?  ideally with log level debug but I'll take info if that's what you have.;;;","28/Feb/11 07:14;wajam;I tried to activate debug log but since this is a live node, it seems there is WAY too much going on. Here is the INFO log:

ERROR 23:10:35,105 Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.io.IOError: org.apache.cassandra.db.ColumnSerializer$CorruptColumnException: invalid column name length 0
        at org.apache.cassandra.io.util.ColumnIterator.deserializeNext(ColumnSortedMap.java:246)
        at org.apache.cassandra.io.util.ColumnIterator.next(ColumnSortedMap.java:262)
        at org.apache.cassandra.io.util.ColumnIterator.next(ColumnSortedMap.java:223)
        at java.util.concurrent.ConcurrentSkipListMap.buildFromSorted(ConcurrentSkipListMap.java:1521)
        at java.util.concurrent.ConcurrentSkipListMap.<init>(ConcurrentSkipListMap.java:1471)
        at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:366)
        at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:314)
        at org.apache.cassandra.db.ColumnFamilySerializer.deserializeColumns(ColumnFamilySerializer.java:129)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.getColumnFamilyWithColumns(SSTableIdentityIterator.java:172)
        at org.apache.cassandra.io.PrecompactedRow.<init>(PrecompactedRow.java:78)
        at org.apache.cassandra.io.CompactionIterator.getCompactedRow(CompactionIterator.java:139)
        at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:108)
        at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:43)
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:73)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
        at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
        at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
        at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:448)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:123)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:93)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: org.apache.cassandra.db.ColumnSerializer$CorruptColumnException: invalid column name length 0
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:68)
        at org.apache.cassandra.io.util.ColumnIterator.deserializeNext(ColumnSortedMap.java:242)
        ... 25 more
 INFO 23:10:35,106 Scrubbing SSTableReader(path='/var/lib/cassandra/data/Wajam/Comment-f-710-Data.db')
ERROR 23:10:37,489 Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.io.EOFException
        at java.io.RandomAccessFile.readFully(RandomAccessFile.java:416)
        at java.io.RandomAccessFile.readFully(RandomAccessFile.java:394)
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.readBytes(BufferedRandomAccessFile.java:268)
        at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:310)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:284)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:540)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:55)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:194)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)

And the exception that is output by nodetool scrub:

Error occured while scrubbing keyspace <keyspacename>
java.util.concurrent.ExecutionException: java.io.EOFException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
        at java.util.concurrent.FutureTask.get(FutureTask.java:111)
        at org.apache.cassandra.db.CompactionManager.performScrub(CompactionManager.java:203)
        at org.apache.cassandra.db.ColumnFamilyStore.scrub(ColumnFamilyStore.java:963)
        at org.apache.cassandra.service.StorageService.scrub(StorageService.java:1247)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:111)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:45)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:226)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:251)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:857)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:795)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1450)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:90)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1285)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1383)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:807)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
        at sun.rmi.transport.Transport$1.run(Transport.java:177)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.io.EOFException
        at java.io.RandomAccessFile.readFully(RandomAccessFile.java:416)
        at java.io.RandomAccessFile.readFully(RandomAccessFile.java:394)
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.readBytes(BufferedRandomAccessFile.java:268)
        at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:310)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:284)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:540)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:55)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:194)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        ... 3 more

;;;","28/Feb/11 16:01;kunda;Attached a system log leading up to a ""java.io.EOFException: attempted to skip X bytes but only skipped Y"" error during a scrub of a very big (probably uncompactable) sstable;;;","28/Feb/11 16:07;kunda;another system log, this time leading to a vanilla java.io.EOFException;;;","01/Mar/11 05:25;jbellis;Looked at userChannelFilter-f-210.tar.gz.  Data file does not match index even a little.  Scrub can't help there.
;;;","01/Mar/11 05:41;wajam;I have a 71MB SS table that scrub fail to fix so maybe you would be interested in having it ? There is private data in there so I could upload it somewhere so only you can download it. Let me know how we can work together.

Thank you!;;;","01/Mar/11 07:16;jbellis;Patch to make scrub less crashy.  It still can't magically fix severely corrupted files like the exhibits here, though.;;;","01/Mar/11 07:19;jbellis;Sebastien, go ahead and try scrub after rebuilding w/ the newest patch.

But if your example is like the others, you're seeing something other than the CASSANDRA-2211/CASSANDRA-2216 corruption that scrub is intended to deal with.

So far the only suggestion I have to track that down is to start over and run with snapshot_before_compaction turned on in cassandra.yaml, so when a corrupt sstable is generated we will know where it came from.;;;","01/Mar/11 08:57;wajam;Jonathan,

I'm pretty positive I'm running into CASSANDRA-2216.

Applied the patch and still having the same exception, maybe this is related to CASSANDRA-2256 ?

ERROR 00:51:55,214 Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.io.EOFException
        at java.io.RandomAccessFile.readFully(RandomAccessFile.java:416)
        at java.io.RandomAccessFile.readFully(RandomAccessFile.java:394)
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.readBytes(BufferedRandomAccessFile.java:268)
        at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:310)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:284)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:541)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:56)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:195)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)

;;;","01/Mar/11 10:49;jbellis;2211/2216 would leave valid row keys written, which is where yours is corrupt.  That is why I think this is something else.

Here is a v2 though that will catch the error and try the next row from the index.;;;","01/Mar/11 11:58;wajam;
Now it seems to hang for a while on a 1.4GB sstable. Eventually I get this exception...

ERROR 03:56:29,413 Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.NullPointerException
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:569)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:56)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:195)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
;;;","01/Mar/11 12:18;wajam;Looking at the data directory, it looks like it was working as excepted before the exception happened. The tmp SStable is 1.0GB... Looks like we are getting close!;;;","01/Mar/11 12:45;jbellis;v3 adds support for the index file ending before the data file;;;","01/Mar/11 12:57;wajam;I will try v3 soon and let you know. While looking at the patch, I found a typo:

logger.warn(""No valid rows found while scrubbing "" + sstable + ""; it is marked for deltion now. If you want to attempt manual recovery, you can find a copy in the pre-scrub snapshot"");

Should be ""deletion"" instead of ""deltion""... no big deal :);;;","01/Mar/11 13:02;wajam;Now I'm getting this:

ERROR 04:58:53,338 Error reading index file.  Scrub does not (yet) know how to recover from corrupt index files; you can try rebuilding it offline.  See http://www.mail-archive.com/user@cassandra.apache.org/msg03325.html
ERROR 04:58:53,338 Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.RuntimeException: java.io.EOFException
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:565)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:56)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:195)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.io.EOFException
        at java.io.RandomAccessFile.readFully(RandomAccessFile.java:416)
        at java.io.RandomAccessFile.readFully(RandomAccessFile.java:394)
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.readBytes(BufferedRandomAccessFile.java:268)
        at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:310)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:284)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:559)
        ... 7 more

And

root@WajamCassandra12:/usr/share/cassandra# nodetool -h 127.0.0.1 scrub Wajam Wajam
Error occured while scrubbing keyspace Wajam
java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.io.EOFException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
        at java.util.concurrent.FutureTask.get(FutureTask.java:111)
        at org.apache.cassandra.db.CompactionManager.performScrub(CompactionManager.java:204)
        at org.apache.cassandra.db.ColumnFamilyStore.scrub(ColumnFamilyStore.java:963)
        at org.apache.cassandra.service.StorageService.scrub(StorageService.java:1247)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:111)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:45)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:226)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:251)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:857)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:795)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1450)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:90)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1285)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1383)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:807)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
        at sun.rmi.transport.Transport$1.run(Transport.java:177)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.RuntimeException: java.io.EOFException
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:565)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:56)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:195)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        ... 3 more
Caused by: java.io.EOFException
        at java.io.RandomAccessFile.readFully(RandomAccessFile.java:416)
        at java.io.RandomAccessFile.readFully(RandomAccessFile.java:394)
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.readBytes(BufferedRandomAccessFile.java:268)
        at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:310)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:284)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:559)
        ... 7 more
;;;","01/Mar/11 13:30;jbellis;v4 attached.  (might be my last for the night; getting late here.  but I'll check back first thing in the morning);;;","01/Mar/11 13:51;wajam;Scrub is running with v4 right now, we will see what happen, no exception so far. There is these things being displayed about fifty millions time in the log tho:) :

 INFO 05:33:29,521 Invalid row summary in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:29,548 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:29,859 Invalid row summary in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:29,882 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:31,223 Invalid row summary in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:31,249 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:31,644 Invalid row summary in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:31,678 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:32,102 Invalid row summary in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:32,129 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:32,393 Invalid row summary in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:32,425 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:32,713 Invalid row summary in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:32,734 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:33,004 Invalid row summary in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:33,031 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:33,305 Invalid row summary in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:33,331 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:33,599 Invalid row summary in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:33,623 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:33,897 Invalid row summary in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:33,917 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:34,330 Invalid row summary in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:34,375 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:35,504 Invalid row summary in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:35,525 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:35,933 Invalid row summary in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it

Either way, as long as it fix my ss table I will be happy :) I will let you know in the morning if there is any more issue

Thank you!
;;;","01/Mar/11 21:22;slebresne;Stepping in while Jonathan takes some well deserved rest and attaching v5 (based on last version attached). This makes the following changes:

* In doScrub(), move the first indexFile.readLong() out of the assert.
* Fix computation of dataStartFromIndex (was missing th 4 or 8 bytes for the data size).
* IndexHelper.defreezeBloomFilter don't leave the file pointer after the bloomFilter for new BF since it reads directly from the file (instead of reading the bytes at once and deserializing from that). Correct this.
* Log if a row has been correctly read the first time but index start and size are different from data start and size (since index should then be manually rebuilt).
* Do a retry if dataStart == dataStartFromIndex but dataSize != dataSizeFromIndex (in case the row size only has been corrupted).

Lastly, a minor remarks: the patch removes a flush in BF.serialize(). Maybe this belongs to another ticket ?
;;;","01/Mar/11 21:42;wajam;Thank you Sylvain

Unfortunatly, i'm unable to apply v5 patch for some reason, is it a svn or git patch ?;;;","01/Mar/11 21:57;slebresne;It's a git patch, git apply or 'patch -p1 -i 2240-v5.patch' should do the trick (it does here). It's based on current cassandra-0.7 branch. ;;;","01/Mar/11 22:46;jbellis;v6 applies Sylvain's fixes to v4 (which mysteriously disappeared from jira);;;","01/Mar/11 22:46;jbellis;I also backed out the read-directly-from-file change in IndexHelper.  Will create a new ticket for that.;;;","01/Mar/11 22:47;wajam;Yep, that worked. So it's currently running with your patch, and so far I get no output from scrub after running for like 5 mins, I usually get the ""Scrubbing SSTABLE"" in the first min so I'm not sure if that's normal. There is the ""tmp"" files in the data directory but looks like they are stuck at 0 byte.

Oh, looks like I will try v6 now then!

Jonathan, v4 is there, you named it 2240.txt :);;;","01/Mar/11 23:00;jbellis;bq. v4 is there, you named it 2240.txt

Oops -- that was the original patch, not actually v4.  Must have been tired.;;;","01/Mar/11 23:28;slebresne;Alright, v6 looks good, +1. Though, we may want to wait to see if it works alright for Sebastien too.;;;","01/Mar/11 23:36;wajam;I'm testing it as we speak!

Log are flooded with this:

 INFO 15:31:15,783 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-164-Data.db'); will rebuild it
 INFO 15:31:15,783 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-164-Data.db'); will rebuild it
 INFO 15:31:15,783 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-164-Data.db'); will rebuild it
 INFO 15:31:15,783 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-164-Data.db'); will rebuild it

I'm not sure why it's scrubbing ""WebsiteWajams"" CF as I asked for ""Wajam"" (nodetool -h 127.0.0.1 scrub Wajam Wajam) ... Either way, this sstable is messed up too so that's fine.

Hopefully it won't just do like v4 that ran all night and was still echoing invalid bloom filter for the same sstable in the log this morning, Looks like it was stuck in a infinite loop or something.

It looks like it's scrubbing another sstable now so that's already an improvment.

Will keep you guys updated!


;;;","01/Mar/11 23:48;wajam; INFO 15:42:52,690 Scrub of SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-155-Data.db') complete: 3271113 in new sstable

""3271113 in new sstable""... I'm not sure it's 3271113 what... Looks like ""good rows"" according to the patch, might want to specify! :)

I think it's the first time I see this line tho so it's good news. Is there any way the ""invalid bloom filter, will rebuild it"" line can only appear once per sstable ? I'm guessing its generating useless IO writing about 1000 of those lines/second in the log!

This should be definitly included in 0.7.3 which hopefully will be released soon!

Good work guys, very appreciated :);;;","02/Mar/11 00:10;wajam;I did see a few exceptions every now and then. Not sure if you need to do anything about this. Sorry for all these posts, hopefully it help :):

224574- INFO 16:06:40,744 Retrying from row index; data is 245642090 bytes starting at 42
224657- WARN 16:06:40,744 Retry failed too.  Skipping to next row (retry's stacktrace follows)
224745-java.io.IOError: java.io.EOFException
224783- at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:117)
224884: at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:610)
224966- at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:56)
225050- at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:195)
225131- at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
225202- at java.util.concurrent.FutureTask.run(FutureTask.java:166)
225263- at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
225347- at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
225431- at java.lang.Thread.run(Thread.java:636)
225473-Caused by: java.io.EOFException
225505- at java.io.DataInputStream.readInt(DataInputStream.java:392)
225567- at org.apache.cassandra.utils.BloomFilterSerializer.deserialize(BloomFilterSerializer.java:47);;;","02/Mar/11 00:10;jbellis;committed w/ update to goodRows message and moving the BF/row header messages to debug level

thanks!;;;","02/Mar/11 00:14;jbellis;bq. I did see a few exceptions every now and then

Did you see any ""Error reading index file"" lines before that?  That's the only way I can think that it would come up with such a strange row size.
;;;","02/Mar/11 00:22;wajam;Yep I did see error reading index file before that.

Any chance to include this in 0.7.3 release ? I'm sure it would help a ton of people :);;;","02/Mar/11 00:30;jbellis;Yes, we'll get this into 0.7.3.;;;","02/Mar/11 00:53;hudson;Integrated in Cassandra-0.7 #336 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/336/])
    make nodetool scrub more robust
patch by jbellis and slebresne; tested by Sébastien Giroux for CASSANDRA-2240
;;;","02/Mar/11 04:15;kunda;Thank you so much, my sstables are finally clean!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Windows: Test org.apache.cassandra.streaming.SerializationsTest FAILED,CASSANDRA-2335,12501507,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Duplicate,,bcoverston,bcoverston,16/Mar/11 04:25,16/Apr/19 17:33,22/Mar/23 14:57,15/Oct/11 23:41,,,,,0,,,,,,,Windows,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/11 05:05;vloncar;2335-windows-serialization-tests.patch;https://issues.apache.org/jira/secure/attachment/12490394/2335-windows-serialization-tests.patch","06/Jul/11 00:23;jbellis;2335.txt;https://issues.apache.org/jira/secure/attachment/12485299/2335.txt",,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20567,,,Tue Oct 25 22:02:54 UTC 2011,,,,,,,,,,"0|i0garz:",93176,,,,,Low,,,,,,,,,,,,,,,,,"16/Mar/11 04:50;bcoverston;Including Stack Traces:
    [junit] Testsuite: org.apache.cassandra.streaming.SerializationsTest
    [junit] Tests run: 4, Failures: 3, Errors: 0, Time elapsed: 1.203 sec
    [junit] 
    [junit] Testcase: testPendingFileRead(org.apache.cassandra.streaming.SerializationsTest):	FAILED
    [junit] Filename must include parent directory.
    [junit] junit.framework.AssertionFailedError: Filename must include parent directory.
    [junit] 	at org.apache.cassandra.io.sstable.Descriptor.fromFilename(Descriptor.java:116)
    [junit] 	at org.apache.cassandra.streaming.PendingFile$PendingFileSerializer.deserialize(PendingFile.java:126)
    [junit] 	at org.apache.cassandra.streaming.SerializationsTest.testPendingFileRead(SerializationsTest.java:71)
    [junit] 
    [junit] 
    [junit] Testcase: testStreamHeaderRead(org.apache.cassandra.streaming.SerializationsTest):	FAILED
    [junit] Filename must include parent directory.
    [junit] junit.framework.AssertionFailedError: Filename must include parent directory.
    [junit] 	at org.apache.cassandra.io.sstable.Descriptor.fromFilename(Descriptor.java:116)
    [junit] 	at org.apache.cassandra.streaming.PendingFile$PendingFileSerializer.deserialize(PendingFile.java:126)
    [junit] 	at org.apache.cassandra.streaming.StreamHeader$StreamHeaderSerializer.deserialize(StreamHeader.java:90)
    [junit] 	at org.apache.cassandra.streaming.StreamHeader$StreamHeaderSerializer.deserialize(StreamHeader.java:72)
    [junit] 	at org.apache.cassandra.streaming.SerializationsTest.testStreamHeaderRead(SerializationsTest.java:105)
    [junit] 
    [junit] 
    [junit] Testcase: testStreamRequestMessageRead(org.apache.cassandra.streaming.SerializationsTest):	FAILED
    [junit] Filename must include parent directory.
    [junit] junit.framework.AssertionFailedError: Filename must include parent directory.
    [junit] 	at org.apache.cassandra.io.sstable.Descriptor.fromFilename(Descriptor.java:116)
    [junit] 	at org.apache.cassandra.streaming.PendingFile$PendingFileSerializer.deserialize(PendingFile.java:126)
    [junit] 	at org.apache.cassandra.streaming.StreamRequestMessage$StreamRequestMessageSerializer.deserialize(StreamRequestMessage.java:153)
    [junit] 	at org.apache.cassandra.streaming.StreamRequestMessage$StreamRequestMessageSerializer.deserialize(StreamRequestMessage.java:123)
    [junit] 	at org.apache.cassandra.streaming.SerializationsTest.testStreamRequestMessageRead(SerializationsTest.java:170)
    [junit] ;;;","21/Apr/11 03:31;gdusbabek;I strongly suspect this error is due to the paths in the serialized PendingFiles.  They hate Windows.;;;","04/Jul/11 07:44;dallsopp;The assertion failure comes from Descriptor.java:

{noformat}
    public static Descriptor fromFilename(String filename)
    {
        int separatorPos = filename.lastIndexOf(File.separatorChar);
        assert separatorPos != -1 : ""Filename must include parent directory."";
        ...
{noformat};;;","06/Jul/11 00:23;jbellis;patch attached to fix hardcoded / in path names;;;","28/Jul/11 04:31;jbellis;David, does that work for you?;;;","01/Aug/11 04:50;dallsopp;Um, not sure, as I'm having a lot of trouble getting Cassandra to build on Windows reliably (or at all), which is why I've been rather slow on some other tickets. Eclipse gives a StackOverflowError when it gets to the Target maven-ant-tasks-retrieve-build (have applied https://issues.apache.org/jira/browse/CASSANDRA-2687 but doesn't seem to help here). My Eclipse also crashes intermittently, just to keep life interesting ;-)

Update - I just found https://issues.apache.org/jira/browse/CASSANDRA-2640 which probably explains the Eclipse problem as my Eclipse's Ant is out of date.

I eventually gave up and did a fresh install of Ant and Maven (1.8.2, 3.0.3) so I could build from the command line. For Cassandra 0.7 I am getting the problem described at http://cassandra-user-incubator-apache-org.3065146.n2.nabble.com/Problem-compiling-td6438062.html - I get
{noformat}
Cassandra-0.7\build.xml:287: artifact:pom doesn't support the ""groupId"" attribute
{noformat}

The change _looks_ good to me, but I can't test at present.;;;","04/Aug/11 01:24;jbellis;Stephen, can you elucidate the groupid error?;;;","04/Aug/11 08:03;stephenc;Certainly I will look into it.

If somebody could stick the patched build.xml onto gist and paste the link here just so I can be sure I'm looking at the same thing;;;","04/Aug/11 09:48;jbellis;It's just the vanilla build.xml from the 0.7 branch;;;","15/Aug/11 05:05;vloncar;These tests fail because of the hard-coded path ""path/doesn't/matter"" in streaming.PendingFile.bin, streaming.StreamHeader.bin and streaming.StreamRequestMessage.bin. Since Descriptor only looks for the existence of separator char anywhere in the path (and the path doesn't matter :)), changing one ""/"" to ""\"" in those files seems to make all platforms happy.

Attached patch builds upon previous and adds corrected streaming.*.bin files.

NOTE: Created on windows with svn diff, let me know if there are any issues with d2u when applying this.;;;","23/Aug/11 02:28;jbellis;bq. Since Descriptor only looks for the existence of separator char anywhere in the path (and the path doesn't matter ), changing one ""/"" to ""\"" in those files seems to make all platforms happy.

That makes sense, but patch does not apply to svn 0.7.  Is it against trunk?

bq. adds corrected streaming.*.bin files

What changes to the bin file itself are being made?  svn ignored them in the patch file.;;;","15/Oct/11 23:41;jbellis;SerializationsTest is passing in Windows now on trunk.  Good enough for me.;;;","26/Oct/11 06:02;dallsopp;Incidentally, the error: {code}artifact:pom doesn't support the ""groupId"" attribute{code}  can be fixed by downloading the latest _Maven Ant Tasks_ jarfile from http://maven.apache.org/ant-tasks/download.html - Having the latest Maven and the latest Ant isn't enough, in some cases.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"restarting node crashes with NPE when, while replaying the commitlog, the cfMetaData is requested",CASSANDRA-995,12462287,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,gdusbabek,tzz,tzz,17/Apr/10 04:31,16/Apr/19 17:33,22/Mar/23 14:57,21/Apr/10 21:51,0.7 beta 1,,,,0,,,,,,"Removing the commitlog directory completely fixes this.   I can reliably reproduce it by 1) starting and configuring a schema with one keyspace, one super CF with LongType supercolumns; 2) inserting data; 3) shutting down and restarting the node.

Here's my schema expressed in cassidy.pl, should be obvious what the parameters are:
./cassidy.pl -server X -port Y -keyspace system 'kdefine test org.apache.cassandra.locator.RackUnawareStrategy 2 org.apache.cassandra.locator.EndPointSnitch'
./cassidy.pl -server X -port Y -keyspace test 'fdefine Status Super LongType BytesType comment=statuschanges,row_cache_size=0,key_cache_size=20000'

The problem seems to be related to CASSANDRA-44 as it happens when the CF metadata is requested but I don't know what's causing it.

10/04/16 15:25:11 INFO commitlog.CommitLog: Replaying /home/cassandra/commitlog/CommitLog-1271449410100.log, /home/cassandra/commitlog/CommitLog-1271449378151.log, /home/cassandra/commitlog/CommitLog-1271449415800.log
java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:160)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.Table.<init>(Table.java:261)
        at org.apache.cassandra.db.Table.open(Table.java:102)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:233)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:172)
        at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:104)
        at org.apache.cassandra.thrift.CassandraDaemon.init(CassandraDaemon.java:151)
        ... 5 more
",SVN rev 935070,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Apr/10 22:23;gdusbabek;0001-include-all-keyspaces-when-creating-the-schema-migra.patch;https://issues.apache.org/jira/secure/attachment/12442298/0001-include-all-keyspaces-when-creating-the-schema-migra.patch","20/Apr/10 22:23;gdusbabek;0002-Use-RackUnawareStrategy-in-unit-tests-because-it-doe.patch;https://issues.apache.org/jira/secure/attachment/12442299/0002-Use-RackUnawareStrategy-in-unit-tests-because-it-doe.patch","17/Apr/10 05:11;tzz;ASF.LICENSE.NOT.GRANTED--crashlog-995;https://issues.apache.org/jira/secure/attachment/12442006/ASF.LICENSE.NOT.GRANTED--crashlog-995","17/Apr/10 06:29;gdusbabek;ASF.LICENSE.NOT.GRANTED--run_1.txt;https://issues.apache.org/jira/secure/attachment/12442014/ASF.LICENSE.NOT.GRANTED--run_1.txt","17/Apr/10 06:29;gdusbabek;ASF.LICENSE.NOT.GRANTED--run_2.txt;https://issues.apache.org/jira/secure/attachment/12442015/ASF.LICENSE.NOT.GRANTED--run_2.txt","20/Apr/10 00:46;tzz;Tester.java;https://issues.apache.org/jira/secure/attachment/12442199/Tester.java",,,,,,,,,6.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19946,,,Tue Apr 20 17:00:59 UTC 2010,,,,,,,,,,"0|i0g2cv:",91812,,,,,Critical,,,,,,,,,,,,,,,,,"17/Apr/10 04:39;jbellis;If this is just saying that ""I can't replay a commitlog written before -44, after applying 44 and restarting,"" then that is expected.  Use nodetool drain before upgrading (or simply r/m the commitlog as you say).;;;","17/Apr/10 04:45;tzz;I wouldn't report it as a bug if it wasn't on a commitlog written by the same version of the software.  All the steps (1,2,3) are on the same SVN revision I stated in the Environment section.;;;","17/Apr/10 05:11;tzz;This is the complete log.  First start is fresh without commitlog or data.  Second start is after shutting down completely by killing jsvc (through /etc/init.d/cassandra restart on a Debian system).;;;","17/Apr/10 05:19;tzz;My complete schema, although only Status and Property are used.  Sorry I edited the original but it was not substantially different.

./cassidy.pl -server X -port 9170 -keyspace system 'kdefine HB3-prod org.apache.cassandra.locator.RackUnawareStrategy 2 org.apache.cassandra.locator.EndPointSnitch'
./cassidy.pl -server X -port 9170 -keyspace HB3-prod 'fdefine Status Super LongType BytesType comment=statuschanges,row_cache_size=0,key_cache_size=20000'
./cassidy.pl -server X -port 9170 -keyspace HB3-prod 'fdefine Property Super LongType BytesType comment=statuschanges,row_cache_size=0,key_cache_size=20000'
./cassidy.pl -server X -port 9170 -keyspace HB3-prod 'fdefine Analysis Super LongType BytesType comment=statuschanges,row_cache_size=0,key_cache_size=20000'
./cassidy.pl -server X -port 9170 -keyspace HB3-prod 'fdefine Relationships Super LongType BytesType comment=statuschanges,row_cache_size=0,key_cache_size=20000'
./cassidy.pl -server X -port 9170 -keyspace HB3-prod 'fdefine Knowledge Super LongType BytesType comment=statuschanges,row_cache_size=0,key_cache_size=20000'

./cassidy.pl -server X -port 9170 -keyspace system 'kdefine CM org.apache.cassandra.locator.RackUnawareStrategy 2 org.apache.cassandra.locator.EndPointSnitch'
./cassidy.pl -server X -port 9170 -keyspace CM 'fdefine Inventory Super LongType BytesType comment=statuschanges,row_cache_size=0,key_cache_size=20000'

When I tried to trigger the error manually with cassidy with just a few inserts like this (the below inserts test=abc into SC 0 in the Status CF):

./cassidy.pl -server X -port 9170 -keyspace HB3-prod 'ins Status testkey 0 test=abc'
./cassidy.pl -server X -port 9170 -keyspace HB3-prod 'get Status testkey 0'

the problem doesn't happen.  I don't know how much data causes it.  My typical writes that trigger this, over a minute, are about 20-30 columns per supercolumn, about 2500 keys, about 5 SCs per key.  I can reliably trigger it with a one-minute population run.

Thanks and I hope this is enough information to replicate the bug.  If not I'll try to give you more including a better test case.;;;","17/Apr/10 06:29;gdusbabek;I couldn't reproduce this.  run_1.txt is a log from defining a KS with a SC and then inserting 100 rows.  run_2.txt is what happens after a restart.

Ted, can you reproduce this outside of cassidy.pl?;;;","19/Apr/10 23:12;tzz;I'm working on replicating the inserts I do from Perl but so far haven't been able to replicate in Java.  I'll keep trying.;;;","20/Apr/10 00:43;tzz;OK, I can prepare a proper test if you need it but the atteched Tester.java definitely causes the NPE and should work with minor changes for you (makeOpenLoginClient and getLongAsBytes are really simple helper methods).  I do ""sudo /etc/init.d/cassandra stop; rm -rf *; sudo /etc/init.d/cassandra start"", run the test, then ""sudo /etc/init.d/cassandra restart"" and get the NPE I showed earlier.  This is with today's SVN (r935659).

It definitely is a combination of the number of keyspaces and CFs together with the number of inserts.  If I do less that 1000 iterations or if I define fewer CFs and keyspaces, the problem doesn't happen on restart.  So it took me a long time to hunt it down.
;;;","20/Apr/10 03:34;gdusbabek;Thanks Ted.  I can reproduce this problem now.;;;","20/Apr/10 22:23;gdusbabek;Migration code was only storing the most recently modified keyspace when recording a migration instead of all of them.;;;","21/Apr/10 00:01;jbellis;+1 except don't commit the .iml :);;;","21/Apr/10 01:00;tzz;I tested and the fix works for me. Thank you.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
allow overriding existing token owner with a new IP,CASSANDRA-1118,12465236,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,gdusbabek,jbellis,jbellis,24/May/10 10:26,16/Apr/19 17:33,22/Mar/23 14:57,08/Jun/10 04:26,0.6.3,0.7 beta 1,,,0,,,,,,"We'd like to support replacing one node with another at the same IP (e.g. when the data is on Amazon's EBS and can easily be mounted to a new host), as noted in CASSANDRA-872.  But in practice this is reported to not work w/o a cluster restart (can't find the ML thread now ... ?)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Jun/10 21:47;gdusbabek;0001-use-start-time-to-resolve-node-token-reassignment-di.patch;https://issues.apache.org/jira/secure/attachment/12446144/0001-use-start-time-to-resolve-node-token-reassignment-di.patch","05/Jun/10 13:10;gdusbabek;v2-0001-use-generation-time-to-resolve-node-token-reassignme.patch;https://issues.apache.org/jira/secure/attachment/12446400/v2-0001-use-generation-time-to-resolve-node-token-reassignme.patch","07/Jun/10 23:02;gdusbabek;v3-0001-use-generation-time-to-resolve-node-token-reassignme.patch;https://issues.apache.org/jira/secure/attachment/12446489/v3-0001-use-generation-time-to-resolve-node-token-reassignme.patch",,,,,,,,,,,,3.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19997,,,Tue Jun 08 12:45:21 UTC 2010,,,,,,,,,,"0|i0g33z:",91934,,,,,Low,,,,,,,,,,,,,,,,,"29/May/10 03:38;gdusbabek;I'm guessing the description should read ""We'd like to support replacing one node with another at *a different* IP..."";;;","29/May/10 04:00;jbellis;Yes, that's right.  (oops.);;;","02/Jun/10 06:15;gdusbabek;Basically for my notes so I don't forget this...

This feature is already implemented, but I see two bugs.  First, there is a flaw in the state logic that assumes any time a node is seen for the first time that is the canonical host for a given token.  It is manifest when NodeB(tokenX) comes online intending to replace NodeA(tokenX).  when B sees A for the first time during the normal course of gossip, B incorrectly assumes that A is replacing it, which messes up B's view of the ring.  Second, partitions are not addressed (Node C, returning from a partition will not see that B has replaced A) and will have an incorrect view of the ring.;;;","02/Jun/10 21:47;gdusbabek;Use start time to resolve node token reassignment disagreement. ;;;","02/Jun/10 21:48;gdusbabek;Patch is against trunk.;;;","04/Jun/10 12:23;jbellis;would it be simpler to use the gossip ""generation"" to detect the more recently started host?;;;","04/Jun/10 12:32;gdusbabek;That would be better.;;;","05/Jun/10 13:10;gdusbabek;Modified patch to use generation time.;;;","06/Jun/10 10:19;jbellis;you can get the generation from endpointstate.heartbeatstate without needing to add an extra applicationstate;;;","07/Jun/10 23:02;gdusbabek;Use heatbeat generation time instead of something different.;;;","08/Jun/10 02:43;jbellis;+1;;;","08/Jun/10 20:45;hudson;Integrated in Cassandra #459 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/459/])
    use generation time to resolve node token reassignment disagreement. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1118
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BytesType and batch mutate causes encoded bytes of non-printable characters to be dropped,CASSANDRA-1235,12468005,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,messi,tnine,tnine,28/Jun/10 11:02,16/Apr/19 17:33,22/Mar/23 14:57,14/Aug/10 01:26,0.6.5,,,,0,,,,,,"When running the two tests, individual column insert works with the values generated.  However, batch insert with the same values causes an encoding failure on the key.  It appears bytes are dropped from the end of the byte array that represents the key value.  See the attached unit test","Java 1.6 sun JDK 
Java(TM) SE Runtime Environment (build 1.6.0_20-b02)
Java HotSpot(TM) 64-Bit Server VM (build 16.3-b01, 

Ubuntu 10.04 64 bit",stuhood,tjake,tnine,univ,uschindler,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-767,,,,,,,,,,,,,,,"13/Aug/10 03:56;univ;TestByteKeys.py;https://issues.apache.org/jira/secure/attachment/12451944/TestByteKeys.py","28/Jun/10 11:03;tnine;TestEncodedKeys.java;https://issues.apache.org/jira/secure/attachment/12448170/TestEncodedKeys.java","01/Aug/10 00:28;messi;rowmutation-key-trimming.patch;https://issues.apache.org/jira/secure/attachment/12450963/rowmutation-key-trimming.patch",,,,,,,,,,,,3.0,messi,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20041,,,Fri Aug 13 18:29:49 UTC 2010,,,,,,,,,,"0|i0g3tj:",92049,,,,,Critical,,,,,,,,,,,,,,,,,"28/Jun/10 11:03;tnine;This file demonstrates the broken input.  Notice that the first test passes with clean input.  The second one fails utilizing batch write for the same input keys.;;;","28/Jun/10 23:30;jbellis;Please don't mess with the issue metadata.;;;","29/Jun/10 03:38;tnine;No worries, sorry about that, I just realized the affected version was incorrect.  Where can I look to begin fixing this? Unfortunately this issue has caused our development to a halt since we depend on the functionality of numeric range queries in Lucene/Lucandra.  Ideally I'd like to create a patch that applies to 0.6.2 so we can roll our own build with the patch and get running again.  I'm assuming it's an issue with the thrift server, but I don't want to start tweaking things without a good idea on where I should be looking for this issue.

Here's an example in hex.  The left is what I pass as bytes in UTF-8 for the key, the right is what I get back during get_range_slice.

http://pastebin.com/KM8Ze794
;;;","08/Jul/10 23:27;jbellis;I believe that

                return new String(buffer, 0, len);

will treat buffer as UTF-16, not UTF-8.  you want

                return new String(buffer, 0, len, ""UTF8"");

I'm not at all sure that longToPrefixCoded is going to generate valid UTF-8, either.;;;","09/Jul/10 00:15;uschindler;buffer is char[], so there is no conversion at all, new String(char[]) only copies the char[] to the internal String's char[]. longToPrefixCoded is definitely correct, large parts of Lucene Java are based on this :-)

(from the Lucene Generics and Unicode Policeman);;;","09/Jul/10 03:45;tnine;While I'm in agreement with Uwe, my bigger concern is that two tests that are functionally equivalent return different results based on the mutation operations.  Performing a batch mutate with the same insertion data as a single write should insert and the same bytes.  Unfortunately batch mutate appears to be randomly dropping bytes.  If it were a true UTF8 issue, wouldn't it drop bytes on the single column writes as well batch mutate?;;;","09/Jul/10 04:21;jbellis;That has to be a Thrift bug, then -- the insert and batch_mutate method both end up calling StorageProxy.mutate or mutateBlocking after converting the Thrift objects into RowMutations;;;","27/Jul/10 05:47;todd@spidertracks.co.nz;I'm currently out of the office and will return on 2010-07-27.  If
this is an urgent request, please mail support@spidertracks.com.
;;;","01/Aug/10 00:28;messi;This patch fixes the problem but I don't know if other problems will arise.;;;","01/Aug/10 09:42;jbellis;nice fix.  (It's possible that this would break someone relying on it, but it's clearly broken the way it is.)  committed.;;;","13/Aug/10 03:58;univ;Still experiencing some problems with byte keys. The file ""TestByteKeys.py"" demonstrates the problem.
Tested with revision 984926.;;;","14/Aug/10 01:26;jbellis;0.6 row keys are _strings_ which means they must be utf-8 encoded, although your version of thrift for python doesn't enforce that (see THRIFT-395).;;;","14/Aug/10 02:29;uschindler;For Lucandra/Lucene this is fine, too (at the moment, as all terms are strings here. Even binary numbers are correcty UTF-8 encoded terms).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some Thrift Exceptions not passed down to Client,CASSANDRA-711,12445854,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,lenn0x,lenn0x,19/Jan/10 04:16,16/Apr/19 17:33,22/Mar/23 14:57,02/Mar/10 00:22,0.6,,,,0,,,,,,"We still don't pass all exceptions down to client via Thrift. We have seen a few of these when working on our client library:

org.apache.thrift.protocol.TProtocolException: Required field 'start' was not present! Struct: SliceRange(start:null, finish:null, reversed:false, count:100)

Would be good if those exceptions were passed down, instead of 'TSocket Read 0 Bytes'.
",,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Feb/10 11:28;jbellis;711-test.txt;https://issues.apache.org/jira/secure/attachment/12435405/711-test.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19828,,,Mon Mar 01 16:22:39 UTC 2010,,,,,,,,,,"0|i0g0lr:",91528,,,,,Low,,,,,,,,,,,,,,,,,"19/Jan/10 04:18;jbellis;can you include full ST?

it sounds like a thrift bug, not ours.;;;","19/Jan/10 05:00;lenn0x;You might be right... Hmm...

ERROR [pool-1-thread-34] 2010-01-18 12:13:35,252 TThreadPoolServer.java (line 257) Thrift error occurred during processing of message.
org.apache.thrift.protocol.TProtocolException: Required field 'start' was not present! Struct: SliceRange(start:null, finish:null, reversed:false, count:100)
        at org.apache.cassandra.service.SliceRange.validate(SliceRange.java:587)
        at org.apache.cassandra.service.SliceRange.read(SliceRange.java:515)
        at org.apache.cassandra.service.SlicePredicate.read(SlicePredicate.java:366)
        at org.apache.cassandra.service.Cassandra$get_slice_args.read(Cassandra.java:3063)
        at org.apache.cassandra.service.Cassandra$Processor$get_slice.process(Cassandra.java:937)
        at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:895)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619);;;","26/Jan/10 04:35;jbellis;fix attached to THRIFT-689.;;;","10/Feb/10 11:28;jbellis;test to demonstrate problem.  may need rebasing.;;;","25/Feb/10 23:31;nicktelford;Another example of this:

ERROR - Thrift error occurred during processing of message.
org.apache.thrift.protocol.TProtocolException: Required field 'timestamp' was not found in serialized data! Struct: Column(name:null, value:null, timestamp:0)
	at org.apache.cassandra.service.Column.read(Column.java:382)
	at org.apache.cassandra.service.SuperColumn.read(SuperColumn.java:317)
	at org.apache.cassandra.service.ColumnOrSuperColumn.read(ColumnOrSuperColumn.java:295)
	at org.apache.cassandra.service.Cassandra$batch_insert_args.read(Cassandra.java:10447)
	at org.apache.cassandra.service.Cassandra$Processor$batch_insert.process(Cassandra.java:1084)
	at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:817)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)

Seems to arise when you don't pass a field that was marked as ""required"" in the structs Thrift interface spec. Thrift appears to do the checks on the server-side and not properly handle the exception.;;;","25/Feb/10 23:56;jbellis;yes, that is why I submitted a fix to thrift and linked the thrift ticket two comments above yours;;;","01/Mar/10 14:12;stuhood;The patch for THRIFT-689 was committed in Thrift SVN r916825. What are the next steps here? Updating Cassandra's thrift to that exact revision?;;;","01/Mar/10 21:24;jbellis;yeah, update and test for regressions :)

I'll do that today.;;;","02/Mar/10 00:22;jbellis;Done, thanks for the Thrift help Stu.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Consistency QUORUM does not work anymore (hector:Could not fullfill request on this host),CASSANDRA-2081,12497245,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,amorton,tbritz,tbritz,01/Feb/11 03:40,16/Apr/19 17:33,22/Mar/23 14:57,08/Feb/11 00:06,0.6.12,0.7.1,,,0,,,,,,"I'm using apache-cassandra-2011-01-28_20-06-01.jar and hector 7.0.25.

Using consistency level Quorum won't work anymore (tested it on read). Consisteny level ONE still works though

I have tried this with one dead node in my cluster.

If I restart cassandra with an older svn revision (apache-cassandra-2011-01-28_20-06-01.jar), I can access the cluster with consistency level QUORUM again, while still using apache-cassandra-2011-01-28_20-06-01.jar and hector 7.0.25 in my application.


11/01/31 19:54:38 ERROR connection.CassandraHostRetryService: Downed intr1n18(192.168.0.18):9160 host still appears to be down: Unable to open transport to intr1n18(192.168.0.18):9160 , java.net.NoRouteToHostException: No route to host
11/01/31 19:54:38 INFO connection.CassandraHostRetryService: Downed Host retry status false with host: intr1n18(192.168.0.18):9160
11/01/31 19:54:45 ERROR connection.HConnectionManager: Could not fullfill request on this host CassandraClient<intr1n11:9160-483>

intr1n11 is marked as up however and I can also access the node through the cassandra cli.


192.168.0.1     Up     Normal  8.02 GB         5.00%   0cc
192.168.0.2     Up     Normal  7.96 GB         5.00%   199
192.168.0.3     Up     Normal  8.24 GB         5.00%   266
192.168.0.4     Up     Normal  4.94 GB         5.00%   333
192.168.0.5     Up     Normal  5.02 GB         5.00%   400
192.168.0.6     Up     Normal  5 GB            5.00%   4cc
192.168.0.7     Up     Normal  5.1 GB          5.00%   599
192.168.0.8     Up     Normal  5.07 GB         5.00%   666
192.168.0.9     Up     Normal  4.78 GB         5.00%   733
192.168.0.10    Up     Normal  4.34 GB         5.00%   7ff
192.168.0.11    Up     Normal  5.01 GB         5.00%   8cc
192.168.0.12    Up     Normal  5.31 GB         5.00%   999
192.168.0.13    Up     Normal  5.56 GB         5.00%   a66
192.168.0.14    Up     Normal  5.82 GB         5.00%   b33
192.168.0.15    Up     Normal  5.57 GB         5.00%   c00
192.168.0.16    Up     Normal  5.03 GB         5.00%   ccc
192.168.0.17    Up     Normal  4.77 GB         5.00%   d99
192.168.0.18    Down   Normal  ?               5.00%   e66
192.168.0.19    Up     Normal  4.78 GB         5.00%   f33
192.168.0.20    Up     Normal  4.83 GB         5.00%   ffffffffffffffff




","linux, hector + cassandra",cburroughs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Feb/11 21:18;amorton;2081-2.txt;https://issues.apache.org/jira/secure/attachment/12470443/2081-2.txt","03/Feb/11 05:26;amorton;2081-logging.patch;https://issues.apache.org/jira/secure/attachment/12470072/2081-logging.patch","03/Feb/11 06:29;jbellis;2081.txt;https://issues.apache.org/jira/secure/attachment/12470078/2081.txt",,,,,,,,,,,,3.0,amorton,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20431,,,Mon Feb 07 16:06:12 UTC 2011,,,,,,,,,,"0|i0g973:",92920,,jbellis,,jbellis,Critical,,,,,,,,,,,,,,,,,"01/Feb/11 03:55;jbellis;What kind of ""doesn't work"" are you seeing?;;;","01/Feb/11 04:01;tbritz;My application hangs/blocks forever as I catch all the Hector exceptions and retry when there was an error.

Above log file messages will repeat itself again and again.

There are also no error messages in the cassandra log file.

Also ""Could not fullfill request on this host CassandraClient"" is an error message I have never seen before. ;;;","01/Feb/11 04:38;jbellis;Is this RF=3?

What do you see in the Cassandra log when you set log level to debug, for the queries that Hector gives up on?

What are the versions you tried that works/doesn't work?  (In description above both versions are given as apache-cassandra-2011-01-28_20-06-01.jar.);;;","01/Feb/11 05:00;tbritz;RF=3

I will enable the debug log level tomorrow for cassandra, switch back to apache-cassandra-2011-01-28_20-06-01.jar and post you the results.

The last version that I tried that worked was apache-cassandra-2011-01-24_06-01-26.jar. apache-cassandra-2011-01-28_20-06-01.jar doesn't work anymore.
;;;","01/Feb/11 05:22;brandon.williams;I'm not able to reproduce with contrib/stress, can you try that?;;;","01/Feb/11 12:46;amorton;I've sort of stumbled onto something similar with an 0.7 install. I need to go home now so cannot dig any deeper and rule out human error, but this is what I have.

5 node 0.7.0 install

1) Load data in using

python stress.py -d jb-cass1,jb-cass2,jb-cass3,jb-cass4,jb-cass5 -o insert -n 1000000 -e QUORUM -t 10 -i 1 -l 3
(use all 5 nodes, insert 1,000,000 rows with RF 3 and QUORUM and 10 threads, report progress every second)

2) Read back using 

python stress.py -d jb-cass2,jb-cass3,jb-cass4,jb-cass5 -o read -n 1000000 -e QUORUM -t 10 -i 1
(note that jb-cass1 is removed from the list)

3) make big bang

Once the read has run a few seconds I ran ""reboot -f"" on node 1. I expect the read operations to complete, output was 

11270,1315,1315,0.00839671943578,9
11631,361,361,0.00746133188792,11
11631,0,0,NaN,12
11631,0,0,NaN,13
11631,0,0,NaN,14
11631,0,0,NaN,15
11631,0,0,NaN,16
11631,0,0,NaN,17
11631,0,0,NaN,18
11631,0,0,NaN,19
Process Reader-10:
Traceback (most recent call last):
  File ""/vol/apps/python-2.6.4_64/lib/python2.6/multiprocessing/process.py"", line 232, in _bootstrap
    self.run()
  File ""stress.py"", line 279, in run
    r = self.cclient.get_slice(key, parent, p, consistency)
  File ""/local1/frameworks/cassandra/apache-cassandra-0.7.0-src/contrib/py_stress/cassandra/Cassandra.py"", line 432, in get_slice
    return self.recv_get_slice()
  File ""/local1/frameworks/cassandra/apache-cassandra-0.7.0-src/contrib/py_stress/cassandra/Cassandra.py"", line 462, in recv_get_slice
    raise result.te

All clients died. stress.py is not setting a timeout on the thrift socket, so am guessing this is server side.

I was running DEBUG on all the nodes (but had turned off the line numbers), this is from one. the 114.63 machine is obviously the one I killed. 


DEBUG [pool-1-thread-2] 2011-02-01 17:14:08,186 StorageService.java (line org.apache.cassandra.service.StorageService) Sorted endpoints are /192.168.114.63,jb08.wetafx.co.nz/192.168.114.67,/192.168.114.64
DEBUG [pool-1-thread-2] 2011-02-01 17:14:08,186 QuorumResponseHandler.java (line org.apache.cassandra.service.QuorumResponseHandler) QuorumResponseHandler blocking for 2 responses
DEBUG [pool-1-thread-2] 2011-02-01 17:14:08,186 StorageProxy.java (line org.apache.cassandra.service.StorageProxy) strongread reading digest for SliceFromReadCommand(table='Keyspace1', key='30323334343534', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5) from 6624@jb08.wetafx.co.nz/192.168.114.67
DEBUG [pool-1-thread-2] 2011-02-01 17:14:08,187 StorageProxy.java (line org.apache.cassandra.service.StorageProxy) strongread reading data for SliceFromReadCommand(table='Keyspace1', key='30323334343534', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5) from 6623@/192.168.114.63
DEBUG [pool-1-thread-2] 2011-02-01 17:14:08,187 StorageProxy.java (line org.apache.cassandra.service.StorageProxy) strongread reading digest for SliceFromReadCommand(table='Keyspace1', key='30323334343534', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5) from 6624@/192.168.114.64
DEBUG [ReadStage:19] 2011-02-01 17:14:08,187 SliceQueryFilter.java (line org.apache.cassandra.db.filter.SliceQueryFilter) collecting 0 of 5: 4330:false:34@1296532428248604
DEBUG [ReadStage:19] 2011-02-01 17:14:08,187 SliceQueryFilter.java (line org.apache.cassandra.db.filter.SliceQueryFilter) collecting 1 of 5: 4331:false:34@1296532428248637
DEBUG [ReadStage:19] 2011-02-01 17:14:08,187 SliceQueryFilter.java (line org.apache.cassandra.db.filter.SliceQueryFilter) collecting 2 of 5: 4332:false:34@1296532428248640
DEBUG [ReadStage:19] 2011-02-01 17:14:08,187 SliceQueryFilter.java (line org.apache.cassandra.db.filter.SliceQueryFilter) collecting 3 of 5: 4333:false:34@1296532428248642
DEBUG [ReadStage:19] 2011-02-01 17:14:08,187 SliceQueryFilter.java (line org.apache.cassandra.db.filter.SliceQueryFilter) collecting 4 of 5: 4334:false:34@1296532428248656
DEBUG [ReadStage:19] 2011-02-01 17:14:08,187 ReadVerbHandler.java (line org.apache.cassandra.db.ReadVerbHandler) digest is 220b82e28c2bb4be869c168243d75f01
DEBUG [ReadStage:19] 2011-02-01 17:14:08,187 ReadVerbHandler.java (line org.apache.cassandra.db.ReadVerbHandler) Read key 30323334343534; sending response to 7D8FA1FD-A2FE-6A54-7BB0-3B129206D1E1@jb08.wetafx.co.nz/192.168.114.67
DEBUG [RequestResponseStage:13] 2011-02-01 17:14:08,188 ResponseVerbHandler.java (line org.apache.cassandra.net.ResponseVerbHandler) Processing response on a callback from 7D8FA1FD-A2FE-6A54-7BB0-3B129206D1E1@jb08.wetafx.co.nz/192.168.114.67
DEBUG [RequestResponseStage:13] 2011-02-01 17:14:08,188 ReadResponseResolver.java (line org.apache.cassandra.service.ReadResponseResolver) Preprocessed digest response
DEBUG [RequestResponseStage:16] 2011-02-01 17:14:08,188 ResponseVerbHandler.java (line org.apache.cassandra.net.ResponseVerbHandler) Processing response on a callback from 7D8FA1FD-A2FE-6A54-7BB0-3B129206D1E1@/192.168.114.64
DEBUG [RequestResponseStage:16] 2011-02-01 17:14:08,188 ReadResponseResolver.java (line org.apache.cassandra.service.ReadResponseResolver) Preprocessed digest response
 INFO [ScheduledTasks:1] 2011-02-01 17:14:15,438 Gossiper.java (line org.apache.cassandra.gms.Gossiper) InetAddress /192.168.114.63 is now dead.
DEBUG [ScheduledTasks:1] 2011-02-01 17:14:15,440 MessagingService.java (line org.apache.cassandra.net.MessagingService) Resetting pool for /192.168.114.63
DEBUG [WRITE-/192.168.114.63] 2011-02-01 17:14:17,442 OutboundTcpConnection.java (line org.apache.cassandra.net.OutboundTcpConnection) attempting to connect to /192.168.114.63
DEBUG [pool-1-thread-1] 2011-02-01 17:14:18,183 CassandraServer.java (line org.apache.cassandra.thrift.CassandraServer) ... timed out
DEBUG [pool-1-thread-2] 2011-02-01 17:14:18,189 CassandraServer.java (line org.apache.cassandra.thrift.CassandraServer) ... timed out
DEBUG [pool-1-thread-1] 2011-02-01 17:14:18,232 ClientState.java (line org.apache.cassandra.service.ClientState) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-2] 2011-02-01 17:14:18,232 ClientState.java (line org.apache.cassandra.service.ClientState) logged out: #<User allow_all groups=[]>
DEBUG [ScheduledTasks:1] 2011-02-01 17:14:29,811 StorageLoadBalancer.java (line org.apache.cassandra.service.StorageLoadBalancer) Disseminating load info ...
DEBUG [ScheduledTasks:1] 2011-02-01 17:15:29,814 StorageLoadBalancer.java (line org.apache.cassandra.service.StorageLoadBalancer) Disseminating load info ...
DEBUG [WRITE-/192.168.114.63] 2011-02-01 17:15:53,637 OutboundTcpConnection.java (line org.apache.cassandra.net.OutboundTcpConnection) attempting to connect to /192.168.114.63
DEBUG [WRITE-/192.168.114.63] 2011-02-01 17:16:08,667 OutboundTcpConnection.java (line org.apache.cassandra.net.OutboundTcpConnection) attempting to connect to /192.168.114.63
DEBUG [WRITE-/192.168.114.63] 2011-02-01 17:16:23,697 OutboundTcpConnection.java (line org.apache.cassandra.net.OutboundTcpConnection) attempting to connect to /192.168.114.63
DEBUG [ScheduledTasks:1] 2011-02-01 17:16:29,816 StorageLoadBalancer.java (line org.apache.cassandra.service.StorageLoadBalancer) Disseminating load info ...
DEBUG [WRITE-/192.168.114.63] 2011-02-01 17:16:36,723 OutboundTcpConnection.java (line org.apache.cassandra.net.OutboundTcpConnection) attempting to connect to /192.168.114.63
DEBUG [WRITE-/192.168.114.63] 2011-02-01 17:16:50,751 OutboundTcpConnection.java (line org.apache.cassandra.net.OutboundTcpConnection) attempting to connect to /192.168.114.63
DEBUG [WRITE-/192.168.114.63] 2011-02-01 17:17:03,775 OutboundTcpConnection.java (line org.apache.cassandra.net.OutboundTcpConnection) attempting to connect to /192.168.114.63
DEBUG [WRITE-/192.168.114.63] 2011-02-01 17:17:19,807 OutboundTcpConnection.java (line org.apache.cassandra.net.OutboundTcpConnection) attempting to connect to /192.168.114.63
DEBUG [ScheduledTasks:1] 2011-02-01 17:17:29,818 StorageLoadBalancer.java (line org.apache.cassandra.service.StorageLoadBalancer) Disseminating load info ...
DEBUG [WRITE-/192.168.114.63] 2011-02-01 17:17:32,834 OutboundTcpConnection.java (line org.apache.cassandra.net.OutboundTcpConnection) attempting to connect to /192.168.114.63
DEBUG [WRITE-/192.168.114.63] 2011-02-01 17:17:42,852 OutboundTcpConnection.java (line org.apache.cassandra.net.OutboundTcpConnection) attempting to connect to /192.168.114.63
DEBUG [WRITE-/192.168.114.63] 2011-02-01 17:17:56,880 OutboundTcpConnection.java (line org.apache.cassandra.net.OutboundTcpConnection) attempting to connect to /192.168.114.63


need to leave work now so may not be able to get further into this until tomorrow. ;;;","01/Feb/11 16:48;amorton;Had a read through the code for think I have an idea what my problem was, not sure if it applies to the previous issue and not sure if its a real bug. 

o.a.c.service.ReadCallback.response() will only signal the o.a.c.utils.SimpleCondition if the data request has been received. If the signal is not set after rpc_timeout then ReadCallback.get() will raise a j.u.c.TimeoutException() which comes out of the StorageProxy and is caught in CassandraServer and turned into a o.a.c.thrift.TimedOutException. 

So if the node that is asked for the data fails to return, the entire request will timeout even if there are enough nodes to serve the request. I think I've seen this discussed before as by design, and the client should just retry in response to the timeout. Is that correct ? ;;;","01/Feb/11 19:04;tbritz;Brandon, I haven't yet run stress test. I can reproduce this error every single time with a single thread accessing my idle cluster.

I also reverted to an older version of hector, but this won't help. As noted before, this error doesn't occur running apache-cassandra-2011-01-24_06-01-26.jar.

Here is the debug output of one of the nodes timing out in my application and not returning an answer:

I only try to access (read/iterator) one single table  ""table_usersources"".

The application runs on node intr1n5 (192.168.0.5) and I added the debug output of intr1n19 (192.168.0.19). The node intr1n18(192.168.0.18) is down and not responding.

Please let me know if you need more information in order to fix this bug. Thanks!


Intr1n19:

 INFO [HintedHandoff:1] 2011-02-01 11:47:39,772 HintedHandOffManager.java (line 249) Finished hinted handoff of 0 rows to endpoint /192.168.0.15
 INFO [ScheduledTasks:1] 2011-02-01 11:47:40,773 Gossiper.java (line 205) InetAddress /192.168.0.10 is now dead.
DEBUG [ScheduledTasks:1] 2011-02-01 11:47:40,773 MessagingService.java (line 176) Resetting pool for /192.168.0.10
 INFO [HintedHandoff:1] 2011-02-01 11:47:40,775 HintedHandOffManager.java (line 192) Started hinted handoff for endpoint /192.168.0.10
 INFO [GossipStage:1] 2011-02-01 11:47:40,775 Gossiper.java (line 579) InetAddress /192.168.0.10 is now UP
 INFO [HintedHandoff:1] 2011-02-01 11:47:40,775 HintedHandOffManager.java (line 249) Finished hinted handoff of 0 rows to endpoint /192.168.0.10
DEBUG [WRITE-intr1n4/192.168.0.4] 2011-02-01 11:47:41,479 OutboundTcpConnection.java (line 159) attempting to connect to intr1n4/192.168.0.4
DEBUG [WRITE-intr1n20/192.168.0.20] 2011-02-01 11:47:42,729 OutboundTcpConnection.java (line 159) attempting to connect to intr1n20/192.168.0.20
DEBUG [WRITE-intr1n11/192.168.0.11] 2011-02-01 11:47:42,776 OutboundTcpConnection.java (line 159) attempting to connect to intr1n11/192.168.0.11
DEBUG [WRITE-intr1n18/192.168.0.18] 2011-02-01 11:47:46,781 OutboundTcpConnection.java (line 159) attempting to connect to intr1n18/192.168.0.18
DEBUG [WRITE-intr1n15/192.168.0.15] 2011-02-01 11:47:50,786 OutboundTcpConnection.java (line 159) attempting to connect to intr1n15/192.168.0.15
DEBUG [WRITE-intr1n10/192.168.0.10] 2011-02-01 11:47:53,698 OutboundTcpConnection.java (line 159) attempting to connect to intr1n10/192.168.0.10

DEBUG [ScheduledTasks:1] 2011-02-01 11:48:15,413 GCInspector.java (line 135) GC for ParNew: 32 ms, 124741344 reclaimed leaving 880060312 used; max is 3289776128
DEBUG [ScheduledTasks:1] 2011-02-01 11:48:24,853 FileUtils.java (line 48) Deleting LocationInfo-f-79-Index.db
DEBUG [ScheduledTasks:1] 2011-02-01 11:48:24,853 FileUtils.java (line 48) Deleting LocationInfo-f-79-Filter.db
DEBUG [ScheduledTasks:1] 2011-02-01 11:48:24,854 FileUtils.java (line 48) Deleting LocationInfo-f-79-Statistics.db
 INFO [ScheduledTasks:1] 2011-02-01 11:48:24,854 SSTable.java (line 147) Deleted /hd2/cassandra_md5/data/system/LocationInfo-f-79
DEBUG [WRITE-intr1n18/192.168.0.18] 2011-02-01 11:48:28,820 OutboundTcpConnection.java (line 159) attempting to connect to intr1n18/192.168.0.18
DEBUG [pool-1-thread-1] 2011-02-01 11:48:28,936 CassandraServer.java (line 445) range_slice
DEBUG [pool-1-thread-1] 2011-02-01 11:48:28,943 StorageProxy.java (line 514) RangeSliceCommand{keyspace='table_usersources', column_family='table_usersources_meta', super_column=null, predicate=SlicePredicate(column_names:[java.nio.HeapByteBuffer[pos=76 lim=88 cap=65536]]), range=[,], max_keys=250}
DEBUG [pool-1-thread-1] 2011-02-01 11:48:28,943 StorageProxy.java (line 705) restricted ranges for query [,] are [[,0cc], (0cc,199], (199,266], (266,333], (333,400], (400,4cc], (4cc,599], (599,666], (666,733], (733,7ff], (7ff,8cc], (8cc,999], (999,a66], (a66,b33], (b33,c00], (c00,ccc], (ccc,d99], (d99,e66], (e66,f33], (f33,ffffffffffffffff], (ffffffffffffffff,]]
DEBUG [pool-1-thread-1] 2011-02-01 11:48:28,949 ReadCallback.java (line 58) ReadCallback blocking for 2 responses
DEBUG [pool-1-thread-1] 2011-02-01 11:48:28,949 StorageProxy.java (line 562) reading RangeSliceCommand{keyspace='table_usersources', column_family='table_usersources_meta', super_column=null, predicate=SlicePredicate(column_names:[java.nio.HeapByteBuffer[pos=76 lim=88 cap=65536]]), range=[,0cc], max_keys=250} from 269@/192.168.0.1
DEBUG [pool-1-thread-1] 2011-02-01 11:48:28,949 StorageProxy.java (line 562) reading RangeSliceCommand{keyspace='table_usersources', column_family='table_usersources_meta', super_column=null, predicate=SlicePredicate(column_names:[java.nio.HeapByteBuffer[pos=76 lim=88 cap=65536]]), range=[,0cc], max_keys=250} from 269@/192.168.0.2
DEBUG [pool-1-thread-1] 2011-02-01 11:48:28,950 StorageProxy.java (line 562) reading RangeSliceCommand{keyspace='table_usersources', column_family='table_usersources_meta', super_column=null, predicate=SlicePredicate(column_names:[java.nio.HeapByteBuffer[pos=76 lim=88 cap=65536]]), range=[,0cc], max_keys=250} from 269@/192.168.0.3
DEBUG [RequestResponseStage:1] 2011-02-01 11:48:28,954 ResponseVerbHandler.java (line 48) Processing response on a callback from 269@/192.168.0.1
DEBUG [ScheduledTasks:1] 2011-02-01 11:48:38,771 StorageLoadBalancer.java (line 349) Disseminating load info ...
DEBUG [pool-1-thread-1] 2011-02-01 11:48:38,950 CassandraServer.java (line 483) ... timed out
DEBUG [pool-1-thread-1] 2011-02-01 11:48:38,957 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [WRITE-intr1n18/192.168.0.18] 2011-02-01 11:48:44,835 OutboundTcpConnection.java (line 159) attempting to connect to intr1n18/192.168.0.18
DEBUG [pool-1-thread-3] 2011-02-01 11:48:56,275 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-6] 2011-02-01 11:48:56,275 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-7] 2011-02-01 11:48:56,278 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-11] 2011-02-01 11:48:56,278 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-5] 2011-02-01 11:48:56,278 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-2] 2011-02-01 11:48:56,277 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-8] 2011-02-01 11:48:56,278 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-10] 2011-02-01 11:48:56,277 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-12] 2011-02-01 11:48:56,278 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-15] 2011-02-01 11:48:56,278 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-14] 2011-02-01 11:48:56,278 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-9] 2011-02-01 11:48:56,278 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-16] 2011-02-01 11:48:56,278 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-13] 2011-02-01 11:48:56,278 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-4] 2011-02-01 11:48:56,278 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [WRITE-intr1n18/192.168.0.18] 2011-02-01 11:48:57,845 OutboundTcpConnection.java (line 159) attempting to connect to intr1n18/192.168.0.18


Application:

11:48:25,616 INFO  ~ Registering JMX me.prettyprint.cassandra.service:ServiceType=hector,MonitorType=hector
11:48:25,652 INFO  ~ get connection for table_lists: consistency: ONE
11:48:25,695 INFO  ~ get connection for table_lists: consistency: ONE
11:48:25,825 INFO  ~ Downed Host Retry service started with queue size -1 and retry delay 10s
11:48:28,887 ERROR ~ Unable to open transport to intr1n18(192.168.0.18):9160
org.apache.thrift.transport.TTransportException: java.net.NoRouteToHostException: No route to host
        at org.apache.thrift.transport.TSocket.open(TSocket.java:185)
        at org.apache.thrift.transport.TFramedTransport.open(TFramedTransport.java:81)
        at me.prettyprint.cassandra.connection.HThriftClient.open(HThriftClient.java:111)
        at me.prettyprint.cassandra.connection.ConcurrentHClientPool.<init>(ConcurrentHClientPool.java:44)
        at me.prettyprint.cassandra.connection.HConnectionManager.<init>(HConnectionManager.java:63)
        at me.prettyprint.cassandra.service.AbstractCluster.<init>(AbstractCluster.java:62)
        at me.prettyprint.cassandra.service.AbstractCluster.<init>(AbstractCluster.java:58)
        at me.prettyprint.cassandra.service.ThriftCluster.<init>(ThriftCluster.java:17)
        at me.prettyprint.hector.api.factory.HFactory.createCluster(HFactory.java:157)
        at me.prettyprint.hector.api.factory.HFactory.getOrCreateCluster(HFactory.java:136)
     ....
Caused by: java.net.NoRouteToHostException: No route to host
        at java.net.PlainSocketImpl.socketConnect(Native Method)
        at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:333)
        at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:195)
        at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:182)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
        at java.net.Socket.connect(Socket.java:529)
        at org.apache.thrift.transport.TSocket.open(TSocket.java:180)
        ... 23 more
11:48:28,889 ERROR ~ Could not start connection pool for host intr1n18(192.168.0.18):9160
11:48:28,889 INFO  ~ Host detected as down was added to retry queue: intr1n18(192.168.0.18):9160
11:48:28,897 INFO  ~ get connection for table_usersources: consistency: QUORUM
11:48:38,014 ERROR ~ Unable to open transport to intr1n18(192.168.0.18):9160
org.apache.thrift.transport.TTransportException: java.net.NoRouteToHostException: No route to host
        at org.apache.thrift.transport.TSocket.open(TSocket.java:185)
        at org.apache.thrift.transport.TFramedTransport.open(TFramedTransport.java:81)
        at me.prettyprint.cassandra.connection.HThriftClient.open(HThriftClient.java:111)
        at me.prettyprint.cassandra.connection.CassandraHostRetryService$RetryRunner.verifyConnection(CassandraHostRetryService.java:116)
        at me.prettyprint.cassandra.connection.CassandraHostRetryService$RetryRunner.run(CassandraHostRetryService.java:96)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.net.NoRouteToHostException: No route to host
        at java.net.PlainSocketImpl.socketConnect(Native Method)
        at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:333)
        at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:195)
        at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:182)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
        at java.net.Socket.connect(Socket.java:529)
        at org.apache.thrift.transport.TSocket.open(TSocket.java:180)
        ... 13 more
11:48:38,019 ERROR ~ Downed intr1n18(192.168.0.18):9160 host still appears to be down: Unable to open transport to intr1n18(192.168.0.18):9160 , java.net.NoRouteToHostException: No route to host
11:48:38,020 INFO  ~ Downed Host retry status false with host: intr1n18(192.168.0.18):9160
11:48:38,956 ERROR ~ Could not fullfill request on this host CassandraClient<intr1n19:9160-594>
11:48:38,956 ERROR ~ Exception:
me.prettyprint.hector.api.exceptions.HTimedOutException: TimedOutException()
        at me.prettyprint.cassandra.service.ExceptionsTranslatorImpl.translate(ExceptionsTranslatorImpl.java:32)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl$3.execute(KeyspaceServiceImpl.java:161)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl$3.execute(KeyspaceServiceImpl.java:143)
        at me.prettyprint.cassandra.service.Operation.executeAndSetResult(Operation.java:101)
        at me.prettyprint.cassandra.connection.HConnectionManager.operateWithFailover(HConnectionManager.java:159)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl.operateWithFailover(KeyspaceServiceImpl.java:129)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl.getRangeSlices(KeyspaceServiceImpl.java:165)
        at me.prettyprint.cassandra.model.thrift.ThriftRangeSlicesQuery$1.doInKeyspace(ThriftRangeSlicesQuery.java:67)
        at me.prettyprint.cassandra.model.thrift.ThriftRangeSlicesQuery$1.doInKeyspace(ThriftRangeSlicesQuery.java:63)
        at me.prettyprint.cassandra.model.KeyspaceOperationCallback.doInKeyspaceAndMeasure(KeyspaceOperationCallback.java:20)
        at me.prettyprint.cassandra.model.ExecutingKeyspace.doExecute(ExecutingKeyspace.java:85)
        at me.prettyprint.cassandra.model.thrift.ThriftRangeSlicesQuery.execute(ThriftRangeSlicesQuery.java:62)
    ...
Caused by: TimedOutException()
        at org.apache.cassandra.thrift.Cassandra$get_range_slices_result.read(Cassandra.java:12104)
        at org.apache.cassandra.thrift.Cassandra$Client.recv_get_range_slices(Cassandra.java:732)
        at org.apache.cassandra.thrift.Cassandra$Client.get_range_slices(Cassandra.java:704)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl$3.execute(KeyspaceServiceImpl.java:149)
        ... 23 more
11:48:48,961 ERROR ~ Could not fullfill request on this host CassandraClient<intr1n17:9160-577>
11:48:48,961 ERROR ~ Exception:
me.prettyprint.hector.api.exceptions.HTimedOutException: TimedOutException()
        at me.prettyprint.cassandra.service.ExceptionsTranslatorImpl.translate(ExceptionsTranslatorImpl.java:32)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl$3.execute(KeyspaceServiceImpl.java:161)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl$3.execute(KeyspaceServiceImpl.java:143)
        at me.prettyprint.cassandra.service.Operation.executeAndSetResult(Operation.java:101)
        at me.prettyprint.cassandra.connection.HConnectionManager.operateWithFailover(HConnectionManager.java:159)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl.operateWithFailover(KeyspaceServiceImpl.java:129)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl.getRangeSlices(KeyspaceServiceImpl.java:165)
        at me.prettyprint.cassandra.model.thrift.ThriftRangeSlicesQuery$1.doInKeyspace(ThriftRangeSlicesQuery.java:67)
        at me.prettyprint.cassandra.model.thrift.ThriftRangeSlicesQuery$1.doInKeyspace(ThriftRangeSlicesQuery.java:63)
        at me.prettyprint.cassandra.model.KeyspaceOperationCallback.doInKeyspaceAndMeasure(KeyspaceOperationCallback.java:20)
        at me.prettyprint.cassandra.model.ExecutingKeyspace.doExecute(ExecutingKeyspace.java:85)
        at me.prettyprint.cassandra.model.thrift.ThriftRangeSlicesQuery.execute(ThriftRangeSlicesQuery.java:62)
       ...
Caused by: TimedOutException()
        at org.apache.cassandra.thrift.Cassandra$get_range_slices_result.read(Cassandra.java:12104)
        at org.apache.cassandra.thrift.Cassandra$Client.recv_get_range_slices(Cassandra.java:732)
        at org.apache.cassandra.thrift.Cassandra$Client.get_range_slices(Cassandra.java:704)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl$3.execute(KeyspaceServiceImpl.java:149)
        ... 23 more
11:48:51,025 ERROR ~ Unable to open transport to intr1n18(192.168.0.18):9160
org.apache.thrift.transport.TTransportException: java.net.NoRouteToHostException: No route to host
        at org.apache.thrift.transport.TSocket.open(TSocket.java:185)
        at org.apache.thrift.transport.TFramedTransport.open(TFramedTransport.java:81)
        at me.prettyprint.cassandra.connection.HThriftClient.open(HThriftClient.java:111)
        at me.prettyprint.cassandra.connection.CassandraHostRetryService$RetryRunner.verifyConnection(CassandraHostRetryService.java:116)
        at me.prettyprint.cassandra.connection.CassandraHostRetryService$RetryRunner.run(CassandraHostRetryService.java:96)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.net.NoRouteToHostException: No route to host
        at java.net.PlainSocketImpl.socketConnect(Native Method)
        at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:333)
        at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:195)
        at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:182)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
        at java.net.Socket.connect(Socket.java:529)
        at org.apache.thrift.transport.TSocket.open(TSocket.java:180)
        ... 13 more
11:48:51,025 ERROR ~ Downed intr1n18(192.168.0.18):9160 host still appears to be down: Unable to open transport to intr1n18(192.168.0.18):9160 , java.net.NoRouteToHostException: No route to host
11:48:51,025 INFO  ~ Downed Host retry status false with host: intr1n18(192.168.0.18):9160



Address         Status State   Load            Owns    Token
                                                       ffffffffffffffff
192.168.0.1     Up     Normal  11.26 GB        5.00%   0cc
192.168.0.2     Up     Normal  11.23 GB        5.00%   199
192.168.0.3     Up     Normal  11.58 GB        5.00%   266
192.168.0.4     Up     Normal  6.77 GB         5.00%   333
192.168.0.5     Up     Normal  6.86 GB         5.00%   400
192.168.0.6     Up     Normal  6.81 GB         5.00%   4cc
192.168.0.7     Up     Normal  6.88 GB         5.00%   599
192.168.0.8     Up     Normal  6.84 GB         5.00%   666
192.168.0.9     Up     Normal  6.52 GB         5.00%   733
192.168.0.10    Up     Normal  5.17 GB         5.00%   7ff
192.168.0.11    Up     Normal  6.75 GB         5.00%   8cc
192.168.0.12    Up     Normal  7.06 GB         5.00%   999
192.168.0.13    Up     Normal  7.27 GB         5.00%   a66
192.168.0.14    Up     Normal  7.71 GB         5.00%   b33
192.168.0.15    Up     Normal  7.46 GB         5.00%   c00
192.168.0.16    Up     Normal  6.94 GB         5.00%   ccc
192.168.0.17    Up     Normal  6.45 GB         5.00%   d99
192.168.0.18    Down   Normal  ?               5.00%   e66
192.168.0.19    Up     Normal  6.26 GB         5.00%   f33
192.168.0.20    Up     Normal  6.33 GB         5.00%   ffffffffffffffff

;;;","02/Feb/11 04:35;amorton;My understanding here is the 0.19 node is sending read requests to the 0.1, 0.2 and 0.3 nodes and only getting a reply from the 0.1 node before timing out. The 0.1 node is the first node the request is sent to, so this is the data request the others are digest. 

The timeout is the rpc_timeout, and can be seen here...

DEBUG [pool-1-thread-1] 2011-02-01 11:48:28,949 ReadCallback.java (line 58) ReadCallback blocking for 2 responses
...10 seconds... 
DEBUG [pool-1-thread-1] 2011-02-01 11:48:38,950 CassandraServer.java (line 483) ... timed out

Whats happening on the 0.2 and 0.3 nodes at this point? Are they logging errors or WARN messages about dropped messages ? Can you see any logs about processing messages from the 0.19 node? I'm not sure the down 0.18 node is a factor here.

The client should be retrying when it gets a timeout, which I think you said Hector was doing. 

 ;;;","02/Feb/11 05:46;tbritz;There is absolutely no load on the cluster, so it's strange that the timeout is being triggered. I will put the other nodes into debug mode tomorrow as well and post the DEBUG output of 0.1, 0.2, and 0.3

;;;","03/Feb/11 00:08;tbritz;Debug Output:



========================================
192.168.0.1
--------------------
2011-02-02 12:54:29,117 DEBUG [ScheduledTasks:1] StorageLoadBalancer.java (line 349) Disseminating load info ...

2011-02-02 12:54:35,016 DEBUG [ReadStage:8] RangeSliceVerbHandler.java (line 55) Sending RangeSliceReply{rows=} to 567@/192.168.0.7
2011-02-02 12:54:45,015 DEBUG [ReadStage:9] RangeSliceVerbHandler.java (line 55) Sending RangeSliceReply{rows=} to 600@/192.168.0.7

2011-02-02 12:54:49,679 DEBUG [pool-1-thread-5] ClientState.java (line 91) logged out: #<User allow_all groups=[]>

========================================
192.168.0.2
--------------------
2011-02-02 12:54:30,147 DEBUG [ScheduledTasks:1] StorageLoadBalancer.java (line 349) Disseminating load info ...

2011-02-02 12:54:34,789 DEBUG [MutationStage:5] RowMutationVerbHandler.java (line 52) Applying RowMutation(keyspace='table_lists', key='32383663623561363162363164306231333162326264656232303730303866625f7777772e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313432315f36346334613666322d333737392d343331642d623334382d663636383533316233656334', modifications=[ColumnFamily(table_lists [7461626c655f6c69737473:false:116@1296647674390000!2419199,]), ColumnFamily(table_lists_meta [6e6578745f72657175657374:false:8@1296647674391000!2419199,])])
2011-02-02 12:54:34,790 DEBUG [MutationStage:5] Table.java (line 397) applying mutation of row 32383663623561363162363164306231333162326264656232303730303866625f7777772e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313432315f36346334613666322d333737392d343331642d623334382d663636383533316233656334
2011-02-02 12:54:34,792 DEBUG [MutationStage:5] RowMutationVerbHandler.java (line 81) RowMutation(keyspace='table_lists', key='32383663623561363162363164306231333162326264656232303730303866625f7777772e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313432315f36346334613666322d333737392d343331642d623334382d663636383533316233656334', modifications=[ColumnFamily(table_lists [7461626c655f6c69737473:false:116@1296647674390000!2419199,]), ColumnFamily(table_lists_meta [6e6578745f72657175657374:false:8@1296647674391000!2419199,])]) applied.  Sending response to 566@/192.168.0.7

2011-02-02 12:54:34,953 DEBUG [ReadStage:8] RangeSliceVerbHandler.java (line 55) Sending RangeSliceReply{rows=} to 567@/192.168.0.7
2011-02-02 12:54:44,965 DEBUG [ReadStage:9] RangeSliceVerbHandler.java (line 55) Sending RangeSliceReply{rows=} to 600@/192.168.0.7

2011-02-02 12:54:45,057 DEBUG [ScheduledTasks:1] GCInspector.java (line 135) GC for ParNew: 14 ms, 13086296 reclaimed leaving 2087311488 used; max is 4856348672
2011-02-02 12:54:49,613 DEBUG [pool-1-thread-11] ClientState.java (line 91) logged out: #<User allow_all groups=[]>

========================================
192.168.0.3
--------------------
2011-02-02 12:54:30,920 DEBUG [ScheduledTasks:1] StorageLoadBalancer.java (line 349) Disseminating load info ...

2011-02-02 12:54:34,368 DEBUG [MutationStage:8] RowMutationVerbHandler.java (line 52) Applying RowMutation(keyspace='table_lists', key='36306463373934666139396136366665303539636130373063333366323066615f2e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313636345f65626263303033372d356639612d343066382d383833382d356436336433616233366165', modifications=[ColumnFamily(table_lists [7461626c655f6c69737473:false:116@1296647674314000!2419199,]), ColumnFamily(table_lists_meta [6e6578745f72657175657374:false:8@1296647674322000!2419199,])])
2011-02-02 12:54:34,369 DEBUG [MutationStage:8] Table.java (line 397) applying mutation of row 36306463373934666139396136366665303539636130373063333366323066615f2e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313636345f65626263303033372d356639612d343066382d383833382d356436336433616233366165
2011-02-02 12:54:34,371 DEBUG [MutationStage:8] RowMutationVerbHandler.java (line 81) RowMutation(keyspace='table_lists', key='36306463373934666139396136366665303539636130373063333366323066615f2e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313636345f65626263303033372d356639612d343066382d383833382d356436336433616233366165', modifications=[ColumnFamily(table_lists [7461626c655f6c69737473:false:116@1296647674314000!2419199,]), ColumnFamily(table_lists_meta [6e6578745f72657175657374:false:8@1296647674322000!2419199,])]) applied.  Sending response to 562@/192.168.0.7
2011-02-02 12:54:34,381 DEBUG [MutationStage:9] RowMutationVerbHandler.java (line 52) Applying RowMutation(keyspace='table_lists', key='32383663623561363162363164306231333162326264656232303730303866625f7777772e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313432315f36346334613666322d333737392d343331642d623334382d663636383533316233656334', modifications=[ColumnFamily(table_lists [7461626c655f6c69737473:false:116@1296647674390000!2419199,]), ColumnFamily(table_lists_meta [6e6578745f72657175657374:false:8@1296647674391000!2419199,])])
2011-02-02 12:54:34,382 DEBUG [MutationStage:9] Table.java (line 397) applying mutation of row 32383663623561363162363164306231333162326264656232303730303866625f7777772e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313432315f36346334613666322d333737392d343331642d623334382d663636383533316233656334
2011-02-02 12:54:34,383 DEBUG [MutationStage:9] RowMutationVerbHandler.java (line 81) RowMutation(keyspace='table_lists', key='32383663623561363162363164306231333162326264656232303730303866625f7777772e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313432315f36346334613666322d333737392d343331642d623334382d663636383533316233656334', modifications=[ColumnFamily(table_lists [7461626c655f6c69737473:false:116@1296647674390000!2419199,]), ColumnFamily(table_lists_meta [6e6578745f72657175657374:false:8@1296647674391000!2419199,])]) applied.  Sending response to 564@/192.168.0.7

2011-02-02 12:54:34,539 DEBUG [ReadStage:8] RangeSliceVerbHandler.java (line 55) Sending RangeSliceReply{rows=} to 567@/192.168.0.7
2011-02-02 12:54:44,540 DEBUG [ReadStage:9] RangeSliceVerbHandler.java (line 55) Sending RangeSliceReply{rows=} to 600@/192.168.0.7

2011-02-02 12:54:49,202 DEBUG [pool-1-thread-13] ClientState.java (line 91) logged out: #<User allow_all groups=[]>

========================================
192.168.0.7
--------------------
2011-02-02 12:54:34,059 DEBUG [ScheduledTasks:1] StorageLoadBalancer.java (line 349) Disseminating load info ...

2011-02-02 12:54:34,356 DEBUG [pool-1-thread-1] CassandraServer.java (line 355) batch_mutate
2011-02-02 12:54:34,375 DEBUG [pool-1-thread-1] StorageProxy.java (line 154) insert writing key 36306463373934666139396136366665303539636130373063333366323066615f2e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313636345f65626263303033372d356639612d343066382d383833382d356436336433616233366165 to 561@/192.168.0.5
2011-02-02 12:54:34,375 DEBUG [pool-1-thread-1] StorageProxy.java (line 154) insert writing key 36306463373934666139396136366665303539636130373063333366323066615f2e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313636345f65626263303033372d356639612d343066382d383833382d356436336433616233366165 to 562@/192.168.0.3
2011-02-02 12:54:34,381 DEBUG [pool-1-thread-1] StorageProxy.java (line 154) insert writing key 36306463373934666139396136366665303539636130373063333366323066615f2e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313636345f65626263303033372d356639612d343066382d383833382d356436336433616233366165 to 563@/94.242.198.13
2011-02-02 12:54:34,384 DEBUG [RequestResponseStage:3] ResponseVerbHandler.java (line 48) Processing response on a callback from 561@/192.168.0.5
2011-02-02 12:54:34,386 DEBUG [RequestResponseStage:5] ResponseVerbHandler.java (line 48) Processing response on a callback from 563@/192.168.0.4
2011-02-02 12:54:34,386 DEBUG [RequestResponseStage:4] ResponseVerbHandler.java (line 48) Processing response on a callback from 562@/192.168.0.3

2011-02-02 12:54:34,391 DEBUG [pool-1-thread-2] CassandraServer.java (line 355) batch_mutate
2011-02-02 12:54:34,393 DEBUG [pool-1-thread-2] StorageProxy.java (line 154) insert writing key 32383663623561363162363164306231333162326264656232303730303866625f7777772e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313432315f36346334613666322d333737392d343331642d623334382d663636383533316233656334 to 564@/192.168.0.3
2011-02-02 12:54:34,394 DEBUG [pool-1-thread-2] StorageProxy.java (line 154) insert writing key 32383663623561363162363164306231333162326264656232303730303866625f7777772e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313432315f36346334613666322d333737392d343331642d623334382d663636383533316233656334 to 565@/192.168.0.4
2011-02-02 12:54:34,394 DEBUG [pool-1-thread-2] StorageProxy.java (line 154) insert writing key 32383663623561363162363164306231333162326264656232303730303866625f7777772e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313432315f36346334613666322d333737392d343331642d623334382d663636383533316233656334 to 566@/192.168.0.2
2011-02-02 12:54:34,398 DEBUG [RequestResponseStage:6] ResponseVerbHandler.java (line 48) Processing response on a callback from 564@/192.168.0.3
2011-02-02 12:54:34,398 DEBUG [RequestResponseStage:7] ResponseVerbHandler.java (line 48) Processing response on a callback from 565@/192.168.0.4
2011-02-02 12:54:34,399 DEBUG [RequestResponseStage:8] ResponseVerbHandler.java (line 48) Processing response on a callback from 566@/192.168.0.2

2011-02-02 12:54:34,531 DEBUG [pool-1-thread-3] CassandraServer.java (line 445) range_slice
2011-02-02 12:54:34,533 DEBUG [pool-1-thread-3] StorageProxy.java (line 514) RangeSliceCommand{keyspace='table_usersources', column_family='table_usersources_meta', super_column=null, predicate=SlicePredicate(column_names:[java.nio.HeapByteBuffer[pos=76 lim=88 cap=65536]]), range=[,], max_keys=250}
2011-02-02 12:54:34,534 DEBUG [pool-1-thread-3] StorageProxy.java (line 705) restricted ranges for query [,] are [[,24], (24,49], (49,6d], (6d,92], (92,b6], (b6,db], (db,ffffffffffffffff], (ffffffffffffffff,]]
2011-02-02 12:54:34,536 DEBUG [pool-1-thread-3] ReadCallback.java (line 58) ReadCallback blocking for 2 responses
2011-02-02 12:54:34,536 DEBUG [pool-1-thread-3] StorageProxy.java (line 562) reading RangeSliceCommand{keyspace='table_usersources', column_family='table_usersources_meta', super_column=null, predicate=SlicePredicate(column_names:[java.nio.HeapByteBuffer[pos=76 lim=88 cap=65536]]), range=[,24], max_keys=250} from 567@/192.168.0.1
2011-02-02 12:54:34,537 DEBUG [pool-1-thread-3] StorageProxy.java (line 562) reading RangeSliceCommand{keyspace='table_usersources', column_family='table_usersources_meta', super_column=null, predicate=SlicePredicate(column_names:[java.nio.HeapByteBuffer[pos=76 lim=88 cap=65536]]), range=[,24], max_keys=250} from 567@/192.168.0.2
2011-02-02 12:54:34,537 DEBUG [pool-1-thread-3] StorageProxy.java (line 562) reading RangeSliceCommand{keyspace='table_usersources', column_family='table_usersources_meta', super_column=null, predicate=SlicePredicate(column_names:[java.nio.HeapByteBuffer[pos=76 lim=88 cap=65536]]), range=[,24], max_keys=250} from 567@/192.168.0.3
2011-02-02 12:54:34,554 DEBUG [RequestResponseStage:1] ResponseVerbHandler.java (line 48) Processing response on a callback from 567@/192.168.0.3
2011-02-02 12:54:44,537 DEBUG [pool-1-thread-3] CassandraServer.java (line 483) ... timed out
2011-02-02 12:54:44,548 DEBUG [pool-1-thread-3] ClientState.java (line 91) logged out: #<User allow_all groups=[]>

2011-02-02 12:54:44,549 DEBUG [pool-1-thread-4] CassandraServer.java (line 445) range_slice
2011-02-02 12:54:44,550 DEBUG [pool-1-thread-4] StorageProxy.java (line 514) RangeSliceCommand{keyspace='table_usersources', column_family='table_usersources_meta', super_column=null, predicate=SlicePredicate(column_names:[java.nio.HeapByteBuffer[pos=76 lim=88 cap=65536]]), range=[,], max_keys=250}
2011-02-02 12:54:44,550 DEBUG [pool-1-thread-4] StorageProxy.java (line 705) restricted ranges for query [,] are [[,24], (24,49], (49,6d], (6d,92], (92,b6], (b6,db], (db,ffffffffffffffff], (ffffffffffffffff,]]
2011-02-02 12:54:44,550 DEBUG [pool-1-thread-4] ReadCallback.java (line 58) ReadCallback blocking for 2 responses
2011-02-02 12:54:44,552 DEBUG [pool-1-thread-4] StorageProxy.java (line 562) reading RangeSliceCommand{keyspace='table_usersources', column_family='table_usersources_meta', super_column=null, predicate=SlicePredicate(column_names:[java.nio.HeapByteBuffer[pos=76 lim=88 cap=65536]]), range=[,24], max_keys=250} from 600@/192.168.0.1
2011-02-02 12:54:44,552 DEBUG [pool-1-thread-4] StorageProxy.java (line 562) reading RangeSliceCommand{keyspace='table_usersources', column_family='table_usersources_meta', super_column=null, predicate=SlicePredicate(column_names:[java.nio.HeapByteBuffer[pos=76 lim=88 cap=65536]]), range=[,24], max_keys=250} from 600@/192.168.0.2
2011-02-02 12:54:44,552 DEBUG [pool-1-thread-4] StorageProxy.java (line 562) reading RangeSliceCommand{keyspace='table_usersources', column_family='table_usersources_meta', super_column=null, predicate=SlicePredicate(column_names:[java.nio.HeapByteBuffer[pos=76 lim=88 cap=65536]]), range=[,24], max_keys=250} from 600@/192.168.0.3
2011-02-02 12:54:44,554 DEBUG [RequestResponseStage:5] ResponseVerbHandler.java (line 48) Processing response on a callback from 600@/192.168.0.1
2011-02-02 12:54:54,550 DEBUG [pool-1-thread-4] CassandraServer.java (line 483) ... timed out
2011-02-02 12:54:54,552 DEBUG [pool-1-thread-4] ClientState.java (line 91) logged out: #<User allow_all groups=[]>


;;;","03/Feb/11 05:25;amorton;There is something odd about the way the messages are been described in the logs, e.g. yours say

2011-02-02 12:54:44,554 DEBUG [RequestResponseStage:5] ResponseVerbHandler.java (line 48) Processing response on a callback from 600@/192.168.0.1

mine say

DEBUG [RequestResponseStage:16] 2011-02-01 17:14:08,188 ResponseVerbHandler.java (line org.apache.cassandra.net.ResponseVerbHandler) Processing response on a callback from 7D8FA1FD-A2FE-6A54-7BB0-3B129206D1E1@/192.168.114.6 

The MessagingService.sendRR() overload that takes an array (and is called when there are multiple messages) should be updating the message ID for all messages to a shared GUID before sending. This happens the log message about ""StorageProxy.java (line 562) reading RangeSliceCommand..."" It should be present in the ResponseVerbHandler log message.

Perhaps this means the callback for the message cannot be found. To test the theory can you apply the 2081-logging.patch to the node your are running the request on? It updates ResponseVernHandler to log when a message is lost.


;;;","03/Feb/11 05:26;amorton;adds logging to ResponseVerbHandler to log is the message is received but there is no callback;;;","03/Feb/11 05:31;jbellis;Thibaut is using a 0.7.1 prerelease version where the uuid messages from sendRR have been replaced with unique per-host messages (which are standard int IDs like the old non-shared messages were).;;;","03/Feb/11 05:58;tbritz;Is there an archive of older builds from hudson? I could try out a few versions between 24th january and the 28th to pinpoint the revision that is causing this. This won't take long.;;;","03/Feb/11 06:17;amorton;Looking at the right code now. 

In StorageProxy.fetchRows() line 395 the digestMessage is shared for all non local digest requests. 

And I just realised the log messages above are coming from StorageProxy.getRangeSlice() where the message object is shared for all endpoints. The log messages from Thibaut show the same message ID on nodes 0.1 0.2 and 0.3.

My reading of the MessagingService and ResponseVerbHandler is that once a message with ID X has been received, if others are received with the same ID they will be ignored. Is that correct? If so this looks like a bug in the two methods above. ;;;","03/Feb/11 06:28;jbellis;bq. In StorageProxy.fetchRows() line 395 the digestMessage is shared for all non local digest requests. 

This is fixed in CASSANDRA-2094, but shouldn't break quorum for RF=3 (since a single digest is all we need)

bq. the log messages above are coming from StorageProxy.getRangeSlice() where the message object is shared for all endpoints

Aha, I didn't notice that.  I think this is our bug.  Patch attached.;;;","03/Feb/11 06:39;amorton;Out of interest, how about making the ExpiringMap check the return of NonBlockingHashMap.put() and assert it was the value passed in. May catch future problems. 
;;;","03/Feb/11 18:11;mconrad;Jonathan, I tried your patch and it fixes the problem. Thanks.;;;","03/Feb/11 20:59;tbritz;As Michel noted, this fixes the problem. Thanks :-);;;","04/Feb/11 00:02;jbellis;Thanks for helping track that down everyone!;;;","04/Feb/11 00:16;hudson;Integrated in Cassandra-0.7 #237 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/237/])
    fix range slice ConsistencyLevel > ONE
patch by jbellis; tested by Michel Conrad and Thibaut for CASSANDRA-2081
;;;","04/Feb/11 00:24;hudson;Integrated in Cassandra-0.6 #56 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/56/])
    fix range slice ConsistencyLevel > ONE
patch by jbellis; tested by Michel Conrad and Thibaut for CASSANDRA-2081
;;;","04/Feb/11 00:37;hudson;Integrated in Cassandra-0.7 #238 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/238/])
    add debug log message for missing callback
patch by Aaron Morton and jbellis for CASSANDRA-2081
;;;","07/Feb/11 20:48;amorton;StorageProxy.scan() is also reusing Message objects, with the same result of requests at QUORUM timing out for get_indexed_slice()

Log messages below show a get_indexed_slice() in a 3 node cluster @ QUORUM. The new log message shows the callback has already been removed. 



DEBUG [pool-1-thread-12] 2011-02-08 01:46:32,893 CassandraServer.java (line 509) scan
DEBUG [pool-1-thread-12] 2011-02-08 01:46:32,894 StorageProxy.java (line 703) restricted ranges for query [-1,-1] are [[-1,85070591730234615865843651857942052864], (85070591730234615865843651857942052864,95070591730234615865843651857942052864], (95070591730234615865843651857942052864,108074891939685041992920030907211891412], (108074891939685041992920030907211891412,-1]]
DEBUG [pool-1-thread-12] 2011-02-08 01:46:32,919 StorageProxy.java (line 796) scan ranges are [-1,85070591730234615865843651857942052864],(85070591730234615865843651857942052864,95070591730234615865843651857942052864],(95070591730234615865843651857942052864,108074891939685041992920030907211891412],(108074891939685041992920030907211891412,-1]
DEBUG [pool-1-thread-12] 2011-02-08 01:46:32,919 ReadCallback.java (line 58) ReadCallback blocking for 2 responses
DEBUG [pool-1-thread-12] 2011-02-08 01:46:32,923 StorageProxy.java (line 819) reading org.apache.cassandra.db.IndexScanCommand@26a1b248 from 6289@/127.0.0.3
DEBUG [pool-1-thread-12] 2011-02-08 01:46:32,923 StorageProxy.java (line 819) reading org.apache.cassandra.db.IndexScanCommand@26a1b248 from 6289@/127.0.0.2
DEBUG [RequestResponseStage:1] 2011-02-08 01:46:32,926 ResponseVerbHandler.java (line 51) Processing response on a callback from 6289@/127.0.0.3
DEBUG [RequestResponseStage:2] 2011-02-08 01:46:32,928 ResponseVerbHandler.java (line 41) Callback already removed for 6289
;;;","07/Feb/11 20:48;amorton;Am trying to fix this now. ;;;","07/Feb/11 21:17;amorton;patch 2081-2.txt is against the current 0.7 branch. I've added an assert in the MessageService that looks like this when a callback is added for an existing message ID. 

RROR [pool-1-thread-1] 2011-02-08 02:07:46,763 Cassandra.java (line 2918) Internal error processing get_indexed_slices
java.lang.AssertionError
	at org.apache.cassandra.net.MessagingService.addCallback(MessagingService.java:262)
	at org.apache.cassandra.net.MessagingService.sendRR(MessagingService.java:278)
	at org.apache.cassandra.service.StorageProxy.scan(StorageProxy.java:817)
	at org.apache.cassandra.thrift.CassandraServer.get_indexed_slices(CassandraServer.java:520)
	at org.apache.cassandra.thrift.Cassandra$Processor$get_indexed_slices.process(Cassandra.java:2910)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2555)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)


Have also modified StorageProxy.scan() to create a message for each endpoint. 

I've read through StorageProxy and think their might be an problem in .sendMessages() line 236, it looks like it sends the same message to all endpoints in the local DC. It's late/early and I'm not sure so can someone else take a look please. 
;;;","07/Feb/11 22:47;hudson;Integrated in Cassandra-0.7 #252 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/252/])
    avoid re-using Message object in index queries
patch by Aaron Morton; reviewed by jbellis for CASSANDRA-2081
;;;","07/Feb/11 22:48;jbellis;bq. it looks like it sends the same message to all endpoints in the local DC

It sends the same message to each entry in the list...  which is always going to be a list of size one.  So the good news is I don't think it's incorrect, but the bad news is we broke CASSANDRA-1530.;;;","08/Feb/11 00:06;jbellis;will re-open 1530;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gossiper ConcurrentModificationException after Decommissioning,CASSANDRA-1494,12473825,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,dretzlaff,dretzlaff,11/Sep/10 00:48,16/Apr/19 17:33,22/Mar/23 14:57,13/Sep/10 12:39,,,,,0,,,,,,"After decommissioning 192.168.2.147, the Gossiper caused a ConcurrentModificationException in 192.168.2.55. This cascaded into 192.168.2.55 thinking that 192.168.2.148 and 192.168.2.149 repeatedly went UP and then DOWN. Eventually this left so many intranode (storage port) TCP connections in CLOSE_WAIT that other nodes started failing with ""too many open files"" exceptions.

 INFO [Timer-0] 2010-09-08 17:00:02,398 Gossiper.java (line 402) FatClient /192.168.2.147 has been silent for 3600000ms, removing from gossip
ERROR [Timer-0] 2010-09-08 17:00:02,418 Gossiper.java (line 99) Gossip error
java.util.ConcurrentModificationException
        at java.util.Hashtable$Enumerator.next(Hashtable.java:1031)
        at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:383)
        at org.apache.cassandra.gms.Gossiper$GossipTimerTask.run(Gossiper.java:93)
        at java.util.TimerThread.mainLoop(Timer.java:512)
        at java.util.TimerThread.run(Timer.java:462)
 INFO [Timer-0] 2010-09-08 17:00:12,398 Gossiper.java (line 180) InetAddress /192.168.2.148 is now dead.
 INFO [Timer-0] 2010-09-08 17:00:14,399 Gossiper.java (line 180) InetAddress /192.168.2.149 is now dead.
 INFO [GMFD:1] 2010-09-08 17:00:19,400 Gossiper.java (line 578) InetAddress /192.168.2.149 is now UP
 INFO [HINTED-HANDOFF-POOL:1] 2010-09-08 17:00:19,400 HintedHandOffManager.java (line 165) Started hinted handoff for endPoint /192.168.2.149
 INFO [HINTED-HANDOFF-POOL:1] 2010-09-08 17:00:19,401 HintedHandOffManager.java (line 222) Finished hinted handoff of 0 rows to endpoint /192.168.2.149
 INFO [Timer-0] 2010-09-08 17:00:20,399 Gossiper.java (line 180) InetAddress /192.168.2.149 is now dead.
 INFO [GMFD:1] 2010-09-08 17:00:43,409 Gossiper.java (line 578) InetAddress /192.168.2.148 is now UP
 INFO [HINTED-HANDOFF-POOL:1] 2010-09-08 17:00:43,409 HintedHandOffManager.java (line 165) Started hinted handoff for endPoint /192.168.2.148
 INFO [HINTED-HANDOFF-POOL:1] 2010-09-08 17:00:43,410 HintedHandOffManager.java (line 222) Finished hinted handoff of 0 rows to endpoint /192.168.2.148
 INFO [Timer-0] 2010-09-08 17:00:44,404 Gossiper.java (line 180) InetAddress /192.168.2.148 is now dead.
 INFO [GMFD:1] 2010-09-08 17:01:18,415 Gossiper.java (line 578) InetAddress /192.168.2.149 is now UP

(UP/DOWN cycle repeats until the target node *really* goes DOWN due to too many TCP sockets in CLOSE_WAIT.)",Linux 2.6.33.8-149.fc13.x86_64 #1 SMP Tue Aug 17 22:53:15 UTC 2010 x86_64 x86_64 x86_64 GNU/Linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20166,,,Mon Sep 13 15:01:07 UTC 2010,,,,,,,,,,"0|i0g5en:",92306,,,,,Normal,,,,,,,,,,,,,,,,,"13/Sep/10 11:24;jbellis;Not sure how a single error could cascade like that after CASSANDRA-1289.  Thoughts?;;;","13/Sep/10 12:22;dretzlaff;The two nodes going up/down are actually running Cassandra 0.6.1. (Sorry for omitting this fact earlier.) I'm new to Cassandra, but it seems entirely possible that they also hit the ConcurrentModificationException and the then-broken timer task caused repeated, faulty FailureDetector evictions. The failure mode bit us twice (we decommissioned two nodes) so if there's still uncertainty I'm sure I can reproduce. Unfortunately the log files on those two machines have rotated since the failures so I can't look for more evidence from those events.;;;","13/Sep/10 12:39;jbellis;Having one timertask fail occasionally isn't a big deal, except that in pre-1289 versions like 0.6.1 a single task with an uncaught exception will take down Gossip permanently.  Upgrading to 0.6.5 should take care of this.;;;","13/Sep/10 12:46;dretzlaff;Okay. I'd suggest at least following the removeEndpoint() call with a ""break"" at least on aesthetic grounds, since otherwise that for loop will cause a ConcurrentModificationException every time.;;;","13/Sep/10 21:42;jbellis;added break in r996527, although it's a bit silly (CME aborts the loop just as effectively :)

[this is fixed better in 0.7, btw];;;","13/Sep/10 23:01;dretzlaff;:) Thanks for humoring me.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrapped range generated by getRestrictedRanges,CASSANDRA-1724,12479616,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,stuhood,stuhood,stuhood,11/Nov/10 02:07,16/Apr/19 17:33,22/Mar/23 14:57,16/Nov/10 23:05,0.7.0 rc 1,,,,0,,,,,,"{noformat}
I have 7 suse nodes running Cassandra0.7 branch (latest as of the morning of Nov 9). I've loaded 10 rows with one column family(replication factor=4) and 100 super columns. Using the ColumnFamilyInputFormat with mapreduce (LocalJobRunner) to retrieve all the rows gives me the following exception:

10/11/10 10:33:15 WARN mapred.LocalJobRunner: job_local_0001
java.lang.RuntimeException: org.apache.thrift.TApplicationException: Internal error processing get_range_slices
       at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$RowIterator.maybeInit(ColumnFamilyRecordReader.java:277)
       at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$RowIterator.computeNext(ColumnFamilyRecordReader.java:292)
       at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$RowIterator.computeNext(ColumnFamilyRecordReader.java:189)
       at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
       at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
       at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.nextKeyValue(ColumnFamilyRecordReader.java:148)
       at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:423)
       at org.apache.hadoop.mapreduce.MapContext.nextKeyValue(MapContext.java:67)
       at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:143)
       at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:621)
       at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
       at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)
Caused by: org.apache.thrift.TApplicationException: Internal error processing get_range_slices
       at org.apache.thrift.TApplicationException.read(TApplicationException.java:108)
       at org.apache.cassandra.thrift.Cassandra$Client.recv_get_range_slices(Cassandra.java:724)
       at org.apache.cassandra.thrift.Cassandra$Client.get_range_slices(Cassandra.java:704)
       at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$RowIterator.maybeInit(ColumnFamilyRecordReader.java:255)
       ... 11 more

The server has the following exception:
ERROR [pool-1-thread-11] 2010-11-10 10:35:58,839 Cassandra.java (line 2876) Internal error processing get_range_slices
java.lang.AssertionError: (150596448267070854052355226693835429313,18886431880788352792108545029372560769]
       at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1200)
       at org.apache.cassandra.service.StorageProxy.getRangeSlice(StorageProxy.java:429)
       at org.apache.cassandra.thrift.CassandraServer.get_range_slices(CassandraServer.java:513)
       at org.apache.cassandra.thrift.Cassandra$Processor$get_range_slices.process(Cassandra.java:2868)
       at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2555)
       at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:619)

Any help would be appreciated.

Thanks.

AD
{noformat}",,amuralidharan@nisc.coop,chrusty,patrik.modesto,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Nov/10 05:23;amuralidharan@nisc.coop;cassandra.command.line.ps.output;https://issues.apache.org/jira/secure/attachment/12459285/cassandra.command.line.ps.output","11/Nov/10 04:38;amuralidharan@nisc.coop;cassandra.server.exception.debug.log;https://issues.apache.org/jira/secure/attachment/12459279/cassandra.server.exception.debug.log","11/Nov/10 03:27;amuralidharan@nisc.coop;cassandra.server.exception.log;https://issues.apache.org/jira/secure/attachment/12459268/cassandra.server.exception.log",,,,,,,,,,,,3.0,stuhood,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20273,,,Fri Nov 26 14:55:19 UTC 2010,,,,,,,,,,"0|i0g6zz:",92564,,,,,Normal,,,,,,,,,,,,,,,,,"11/Nov/10 02:35;stuhood;Waiting for more info from the user: due to CASSANDRA-1725, we're fairly sure a pre-1700 version of Cassandra is in use, but we need to confirm.;;;","11/Nov/10 03:27;amuralidharan@nisc.coop;Attached log lines preceding the server exception;;;","11/Nov/10 04:11;stuhood;Aditya: would it be possible to get the logs in DEBUG mode?;;;","11/Nov/10 04:38;amuralidharan@nisc.coop;Attached server log with loglevel set to DEBUG;;;","11/Nov/10 04:43;stuhood;Thanks for the help Aditya! But it looks like this line indicates that the node you posted logs from is not running a version of Cassandra that includes CASSANDRA-1700:
{quote}
DEBUG [pool-1-thread-7] 2010-11-10 14:29:22,721 StorageProxy.java (line 593) restricted ranges for query (150596448267070854052355226693835429313,18886431880788352792108545029372560769] are [(150596448267070854
052355226693835429313,18886431880788352792108545029372560769]]
{quote}

Also, you might want to just wait until CASSANDRA-1725 has been committed as well: if that update fixes this issue, we can close it. Thanks for your patience!;;;","11/Nov/10 05:23;amuralidharan@nisc.coop;Thanks for looking at it. Attached the ps output showing the command line, which includes the SNAPSHOT jar I built from rev 1032781 of the cassandra-0.7 branch. What minimum revision should I be using?;;;","15/Nov/10 02:59;jbellis;CASSANDRA-1725 was r1033713.  You should just get the latest 0.7 branch.;;;","16/Nov/10 23:05;jbellis;(this is another manifestation of CASSANDRA-1700);;;","17/Nov/10 04:19;amuralidharan@nisc.coop;I continue to see this problem, even on revision 1035750 of the cassandra-0.7 branch. How should I go about addressing this issue specifically for my situation?

Any help would be appreciated.;;;","17/Nov/10 05:54;stuhood;Aditya: would you mind posting updated DEBUG logs from the new deployment?;;;","25/Nov/10 23:56;patrik.modesto;We have the same problem here, from version 0.7.0beta3 and still in 0.7-rc1. Attached is the DEBUG log of the exception:

DEBUG 16:51:48,653 range_slice
DEBUG 16:51:48,653 RangeSliceCommand{keyspace='TEST', column_family='Url', super_column=null, predicate=SlicePredicate(column_names:[java.nio.HeapByteBuffer[pos=57 lim=67 cap=177]]), range=(162950022446285318630909295651345252065,9481098247439719900692337295923514899], max_keys=4096}
DEBUG 16:51:48,653 restricted ranges for query (162950022446285318630909295651345252065,9481098247439719900692337295923514899] are [(162950022446285318630909295651345252065,9481098247439719900692337295923514899]]
DEBUG 16:51:48,653 local range slice
ERROR 16:51:48,653 Internal error processing get_range_slices
java.lang.AssertionError: (162950022446285318630909295651345252065,9481098247439719900692337295923514899]
        at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1264)
        at org.apache.cassandra.service.StorageProxy.getRangeSlice(StorageProxy.java:429)
        at org.apache.cassandra.thrift.CassandraServer.get_range_slices(CassandraServer.java:514)
        at org.apache.cassandra.thrift.Cassandra$Processor$get_range_slices.process(Cassandra.java:2868)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2555)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
DEBUG 16:51:48,838 logged out: #<User allow_all groups=[]>

The problem occurs while doing mapreduce task with more than one cassandra node.;;;","26/Nov/10 22:55;chrusty;I'm experiencing exactly the same issue, with the following versions:

0.7-rc1
0.7-beta3

contrib/word_count works fine with one node, but i get the same exceptions when trying with more than one node.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup can create sstables whose contents do not match their advertised version,CASSANDRA-2211,12499284,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,22/Feb/11 07:28,16/Apr/19 17:33,22/Mar/23 14:57,22/Feb/11 22:53,0.7.3,,,,0,,,,,,"Since cleanup switched to per-sstable operation (CASSANDRA-1916), the main loop looks like this:

{code}
                    if (Range.isTokenInRanges(row.getKey().token, ranges))
                    {
                        writer = maybeCreateWriter(sstable, compactionFileLocation, expectedBloomFilterSize, writer);
                        writer.append(new EchoedRow(row));
                        totalkeysWritten++;
                    }
                    else
                    {
                        while (row.hasNext())
                        {
                            IColumn column = row.next();
                            if (indexedColumns.contains(column.name()))
                                Table.cleanupIndexEntry(cfs, row.getKey().key, column);
                        }
                    }
{code}

... that is, rows that haven't changed we copy to the new sstable without deserializing, with EchoedRow.  But, the new sstable is created with CURRENT_VERSION which may not be what the old data consisted of.

(This could cause symptoms similar to CASSANDRA-2195 but I do not think it is the cause of that bug; IIRC the cluster in question there was not upgraded from an older Cassandra.)",,cburroughs,stuhood,,,,,,,,,,,,,,,,,,,,14400,14400,,0%,14400,14400,,,,,,,,,,,,,,,,,,,,"22/Feb/11 20:40;slebresne;0001-2211-v3.patch;https://issues.apache.org/jira/secure/attachment/12471610/0001-2211-v3.patch","22/Feb/11 13:43;jbellis;2211-v2.txt;https://issues.apache.org/jira/secure/attachment/12471599/2211-v2.txt","22/Feb/11 07:49;jbellis;2211.txt;https://issues.apache.org/jira/secure/attachment/12471584/2211.txt",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20507,,,Tue Feb 22 15:20:55 UTC 2011,,,,,,,,,,"0|i0g9zz:",93050,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"22/Feb/11 07:44;jbellis;This bug was introduced in 0.7.0 but in practice was not a problem until we changed sstable formats again in 0.7.1 for CASSANDRA-1555.;;;","22/Feb/11 10:41;jbellis;A better fix would be to have it echo if the data is on the current version, otherwise rewrite.  This would (a) be a better fit with our policy of not having to keep code around to write old versions and (b) allow a better upgrade path to version N + 1 (that doesn't support the old version sstables) than major compaction. I'll see if I can do that tonight.;;;","22/Feb/11 13:43;jbellis;v2 as described above.;;;","22/Feb/11 20:40;slebresne;+1 on the patch. I'm just attaching a v3 that simply use getDefaultGcBefore() throughout CompactionManager (to make things cleaner)

Sadly, this is not the only place where we echo data wrongfully, cf. CASSANDRA-2216;;;","22/Feb/11 22:53;jbellis;committed;;;","22/Feb/11 23:20;hudson;Integrated in Cassandra-0.7 #303 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/303/])
    fix for cleanup writing old-format data into new-version sstable
patch by jbellis; reviewed by slebresne for CASSANDRA-2211
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EOFException with Cassandra.Client.get_range_slices() API,CASSANDRA-1073,12464163,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,frankdu,frankdu,11/May/10 02:23,16/Apr/19 17:33,22/Mar/23 14:57,29/May/10 00:34,0.6.3,,,,0,,,,,,"Below is the snippet I tried to run. The keyspace is named 'Keyspac1', with only 1 column family named 'CF1'. 

		ColumnParent cp = new ColumnParent(""CF1"");
		
		SlicePredicate predicate = new SlicePredicate();
		SliceRange sliceRange = new SliceRange();
		sliceRange.setStart(new byte[0]);
		sliceRange.setFinish(new byte[0]);
		predicate.setSlice_range(sliceRange);
		
		KeyRange range = new KeyRange(10);
		range.setStart_key("""".getBytes());
		range.setEnd_key("""".getBytes());
		
		client.set_keyspace(keyspace);
		List<KeySlice> slices = client.get_range_slices(cp, predicate, range, ConsistencyLevel.ONE);


Then an EOFException was spit out. For readability, it is included in the next comment.",,frankdu,suguru,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-1088,,,,,,,,"26/May/10 08:22;mdennis;CASSANDRA-1073.patch;https://issues.apache.org/jira/secure/attachment/12445513/CASSANDRA-1073.patch","26/May/10 08:36;mdennis;CASSANDRA-1073.patch2;https://issues.apache.org/jira/secure/attachment/12445515/CASSANDRA-1073.patch2",,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19983,,,Fri May 28 16:34:57 UTC 2010,,,,,,,,,,"0|i0g2tz:",91889,,,,,Normal,,,,,,,,,,,,,,,,,"18/May/10 12:22;frankdu;The previous steps are vague. Here are the steps to reproduce the issue. I go through them with latest source codes:

1. Keyspace1 definition is not shown, though it is defined in cassandra.yaml. Therefore I added a keyspace 'FrankDu', and call client.system_add_keyspace(ks_def) method to add it. Below code is added in main() method of CliMain.

List<CfDef> cf_defs = new ArrayList<CfDef>();
cf_defs.add(new CfDef(""FrankDu"", ""CF1"").setComment(""CF1"").setComparator_type(""BytesType""));
KsDef ks_def = new KsDef(""FrankDu"", ""org.apache.cassandra.locator.RackUnawareStrategy"", 1, cf_defs);
thriftClient_.system_add_keyspace(ks_def);

2. Ran cassandra inside eclipse first. Then started CliMain. The keyspace 'FrankDu' got created. Every commands works fine: {{show}}, {{describe}}, {{set}}, {{get}}. So I inserted some rows.

3. Stopped both programs. Disabled the code in Step 1. Then ran cassandra first, and then started CliMain. Every commands works fine: {{show}}, {{describe}}, {{set}}, {{get}}. 

But get_range_slices() API cannot work, because of the EOFException. Below is the code to call the API:

ColumnParent cp = new ColumnParent(""CF1"");

SlicePredicate predicate = new SlicePredicate();
SliceRange sliceRange = new SliceRange();
sliceRange.setStart(new byte[0]);
sliceRange.setFinish(new byte[0]);
predicate.setSlice_range(sliceRange);

KeyRange range = new KeyRange(10);
range.setStart_key("""".getBytes());
range.setEnd_key("""".getBytes());

String keyspace = ""FrankDu"";
client.set_keyspace(keyspace);
List<KeySlice> slices = client.get_range_slices(cp, predicate, range, ConsistencyLevel.ONE);

The cassandra error log is attached below.

Best Regards,
Frank

Cassandra Console Log:
===============================
10/05/17 23:51:53 INFO config.DatabaseDescriptor: Auto DiskAccessMode determined to be mmap
10/05/17 23:51:54 INFO db.ColumnFamilyStore: Removing orphan /Users/fdu/Documents/workspace/working/CassandraProject/var/data/system/Schema-tmp-c-5-Index.db
10/05/17 23:51:54 INFO sstable.SSTableReader: Sampling index for /Users/fdu/Documents/workspace/working/CassandraProject/var/data/system/Schema-c-1-<>
10/05/17 23:51:54 INFO sstable.SSTableReader: Sampling index for /Users/fdu/Documents/workspace/working/CassandraProject/var/data/system/Schema-c-2-<>
10/05/17 23:51:54 INFO sstable.SSTableReader: Sampling index for /Users/fdu/Documents/workspace/working/CassandraProject/var/data/system/Schema-c-3-<>
10/05/17 23:51:54 INFO sstable.SSTableReader: Sampling index for /Users/fdu/Documents/workspace/working/CassandraProject/var/data/system/Schema-c-4-<>
10/05/17 23:51:54 INFO sstable.SSTableReader: Sampling index for /Users/fdu/Documents/workspace/working/CassandraProject/var/data/system/Migrations-c-1-<>
10/05/17 23:51:54 INFO sstable.SSTableReader: Sampling index for /Users/fdu/Documents/workspace/working/CassandraProject/var/data/system/Migrations-c-2-<>
10/05/17 23:51:54 INFO sstable.SSTable: Deleted /Users/fdu/Documents/workspace/working/CassandraProject/var/data/system/LocationInfo-c-1-Data.db
10/05/17 23:51:54 INFO sstable.SSTable: Deleted /Users/fdu/Documents/workspace/working/CassandraProject/var/data/system/LocationInfo-c-2-Data.db
10/05/17 23:51:54 INFO sstable.SSTable: Deleted /Users/fdu/Documents/workspace/working/CassandraProject/var/data/system/LocationInfo-c-3-Data.db
10/05/17 23:51:54 INFO sstable.SSTable: Deleted /Users/fdu/Documents/workspace/working/CassandraProject/var/data/system/LocationInfo-c-4-Data.db
10/05/17 23:51:54 INFO sstable.SSTableReader: Sampling index for /Users/fdu/Documents/workspace/working/CassandraProject/var/data/system/LocationInfo-c-5-<>
10/05/17 23:51:54 INFO config.DatabaseDescriptor: Loading schema version 5903fd78-6230-11df-92b1-e700f669bcfc
10/05/17 23:51:54 INFO sstable.SSTableReader: Sampling index for /Users/fdu/Documents/workspace/working/CassandraProject/var/data/FrankDu/CF1-c-1-<>
10/05/17 23:51:54 INFO sstable.SSTableReader: Sampling index for /Users/fdu/Documents/workspace/working/CassandraProject/var/data/Keyspace1/CF1-c-1-<>
10/05/17 23:51:54 WARN config.DatabaseDescriptor: Schema definitions were defined both locally and in cassandra.yaml. Definitions in cassandra.yaml were ignored.
10/05/17 23:51:54 INFO commitlog.CommitLog: Replaying /Users/fdu/Documents/workspace/working/CassandraProject/var/commitlog/CommitLog-1274154704096.log
10/05/17 23:51:54 INFO commitlog.CommitLog: Log replay complete
10/05/17 23:51:54 INFO db.CompactionManager: Compacting [org.apache.cassandra.io.sstable.RowIndexedReader(path='/Users/fdu/Documents/workspace/working/CassandraProject/var/data/system/Schema-c-1-Data.db'),org.apache.cassandra.io.sstable.RowIndexedReader(path='/Users/fdu/Documents/workspace/working/CassandraProject/var/data/system/Schema-c-2-Data.db'),org.apache.cassandra.io.sstable.RowIndexedReader(path='/Users/fdu/Documents/workspace/working/CassandraProject/var/data/system/Schema-c-3-Data.db'),org.apache.cassandra.io.sstable.RowIndexedReader(path='/Users/fdu/Documents/workspace/working/CassandraProject/var/data/system/Schema-c-4-Data.db')]
10/05/17 23:51:54 INFO db.SystemTable: Saved Token found: 128912333422951617350993337813831967297
10/05/17 23:51:54 INFO db.SystemTable: Saved ClusterName found: Test Cluster
10/05/17 23:51:54 INFO commitlog.CommitLogSegment: Creating new commitlog segment /Users/fdu/Documents/workspace/working/CassandraProject/var/commitlog/CommitLog-1274154714387.log
10/05/17 23:51:54 INFO service.StorageService: Starting up server gossip
10/05/17 23:51:54 INFO db.CompactionManager: Compacted to /Users/fdu/Documents/workspace/working/CassandraProject/var/data/system/Schema-tmp-c-6-Data.db.  2093/1307 bytes for 3 keys.  Time: 162ms.
10/05/17 23:51:54 INFO thrift.CassandraDaemon: Binding thrift service to localhost/127.0.0.1:9160
10/05/17 23:51:54 INFO thrift.CassandraDaemon: Cassandra starting up...
10/05/17 23:51:54 INFO utils.Mx4jTool: Will not load MX4J, mx4j-tools.jar is not in the classpath
10/05/17 23:52:12 ERROR concurrent.JMXEnabledThreadPoolExecutor: Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.RuntimeException: java.io.EOFException
	at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:54)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:41)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:637)
Caused by: java.lang.RuntimeException: java.io.EOFException
	at org.apache.cassandra.db.filter.SSTableSliceIterator$ColumnGroupReader.pollColumn(SSTableSliceIterator.java:200)
	at org.apache.cassandra.db.filter.SSTableSliceIterator.computeNext(SSTableSliceIterator.java:133)
	at org.apache.cassandra.db.filter.SSTableSliceIterator.computeNext(SSTableSliceIterator.java:1)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:135)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:130)
	at org.apache.commons.collections.iterators.CollatingIterator.anyHasNext(CollatingIterator.java:364)
	at org.apache.commons.collections.iterators.CollatingIterator.hasNext(CollatingIterator.java:217)
	at org.apache.cassandra.db.RowIteratorFactory$3.getReduced(RowIteratorFactory.java:138)
	at org.apache.cassandra.db.RowIteratorFactory$3.getReduced(RowIteratorFactory.java:1)
	at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:73)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:135)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:130)
	at org.apache.cassandra.db.RowIterator.hasNext(RowIterator.java:49)
	at org.apache.cassandra.db.ColumnFamilyStore.getRangeRows(ColumnFamilyStore.java:877)
	at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:925)
	at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:42)
	... 4 more
Caused by: java.io.EOFException
	at java.io.RandomAccessFile.readFully(RandomAccessFile.java:383)
	at java.io.RandomAccessFile.readFully(RandomAccessFile.java:361)
	at org.apache.cassandra.utils.FBUtilities.readByteArray(FBUtilities.java:322)
	at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:84)
	at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:1)
	at org.apache.cassandra.db.filter.SSTableSliceIterator$ColumnGroupReader.getNextBlock(SSTableSliceIterator.java:235)
	at org.apache.cassandra.db.filter.SSTableSliceIterator$ColumnGroupReader.pollColumn(SSTableSliceIterator.java:195)
	... 19 more
10/05/17 23:52:12 ERROR thrift.CassandraDaemon: Fatal exception in thread Thread[ROW-READ-STAGE:2,5,main]
java.lang.RuntimeException: java.lang.RuntimeException: java.io.EOFException
	at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:54)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:41)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:637)
Caused by: java.lang.RuntimeException: java.io.EOFException
	at org.apache.cassandra.db.filter.SSTableSliceIterator$ColumnGroupReader.pollColumn(SSTableSliceIterator.java:200)
	at org.apache.cassandra.db.filter.SSTableSliceIterator.computeNext(SSTableSliceIterator.java:133)
	at org.apache.cassandra.db.filter.SSTableSliceIterator.computeNext(SSTableSliceIterator.java:1)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:135)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:130)
	at org.apache.commons.collections.iterators.CollatingIterator.anyHasNext(CollatingIterator.java:364)
	at org.apache.commons.collections.iterators.CollatingIterator.hasNext(CollatingIterator.java:217)
	at org.apache.cassandra.db.RowIteratorFactory$3.getReduced(RowIteratorFactory.java:138)
	at org.apache.cassandra.db.RowIteratorFactory$3.getReduced(RowIteratorFactory.java:1)
	at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:73)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:135)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:130)
	at org.apache.cassandra.db.RowIterator.hasNext(RowIterator.java:49)
	at org.apache.cassandra.db.ColumnFamilyStore.getRangeRows(ColumnFamilyStore.java:877)
	at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:925)
	at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:42)
	... 4 more
Caused by: java.io.EOFException
	at java.io.RandomAccessFile.readFully(RandomAccessFile.java:383)
	at java.io.RandomAccessFile.readFully(RandomAccessFile.java:361)
	at org.apache.cassandra.utils.FBUtilities.readByteArray(FBUtilities.java:322)
	at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:84)
	at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:1)
	at org.apache.cassandra.db.filter.SSTableSliceIterator$ColumnGroupReader.getNextBlock(SSTableSliceIterator.java:235)
	at org.apache.cassandra.db.filter.SSTableSliceIterator$ColumnGroupReader.pollColumn(SSTableSliceIterator.java:195)
	... 19 more
ç;;;","21/May/10 00:21;frankdu;I figured out what specific condition will cause the exception:

1. The steps will cause the issue.
I have latest source code setup in eclipse. I use cassandra-cli tool, insert some data, and terminate cli by clicking Terminate icon in eclipse console. It means that the method CliMain.disconnect() is not invoked. Terminate cassandra, and start it up again. Now the exception occurs every time I invoke get_range_slices() API.

2. The steps won't cause the issue.
I inserted some data by typing {{set}} commands. Then I quit cassandra-cli by typing {{exit}}. It means that the method CliMain.disconnect() is invoked. Restart cassandra, and everything works greatly!

So, I guess that the server side code (corresponding to TTransport.close method) may not handle well situations when a client drops unexpectedly.

Please let me know if any supplemental information is required to fix the issue. 

- Frank;;;","24/May/10 08:44;jbellis;Matt, can you see if you can turn this into a failing system test?;;;","25/May/10 04:29;mdennis;Frank, is this deterministic for you?  

I have a test that puts a thrift client into a loop inserting data which is then sent a SIGKILL.  Sometimes this results in the EOF error in system log and TimedOutException in the client, other times it doesn't.

;;;","25/May/10 10:52;frankdu;Matt, it happens in some conditions. Based on experiments, it tends to occur when I insert 3+ rows, and more columns. Below are some rows that result in the exception all the time:

RowKey: fdu1
=> (column=name, value=Frank Du, timestamp=1274754157811000)
=> (column=city, value=NYC, timestamp=1274754122237000)

RowKey: fdu2
=> (column=name, value=FRANK, timestamp=1274754170971000)

RowKey: fdu3
=> (column=name, value=cassandra, timestamp=1274754183107000)

The steps are as described previously. I start to wonder if it is appropriate to terminate cassandra this way, because it seems that some operation is missed at the termination. 

Best Regards,
Frank;;;","25/May/10 11:02;jbellis;matt / frank, when you see this happen, does sstable2json work?  what about sstablekeys?;;;","25/May/10 11:03;mdennis;The response() method in WriteResponseHandler (specifically the call to condition.signal()) is the culprit.  With the call commented out, my test is able to run indefinitely (or as best as I can approximate ""indefinite"" anyway) without failure.  As soon as the call to condition.signal() is added back in, the test fails within just a few iterations (often on the first or second run).  

I'm still looking into what the exact cause is (which is somewhat complicated as much of the code is wrapped up in Thrift itself) but my guess at this point is that the client is waiting on this condition and when it is killed, the condition and/or waiting thread doesn't correctly handle this.;;;","25/May/10 11:33;mdennis;sstablekeys appears broken for me after any tests run, failure or not.  It works fine if executed on something in the system keyspace data in /var/lib/cassandra/etc

mdennis@toptop:~/mdev/trunk$ bin/sstablekeys build/test/cassandra/data/Keyspace1/Standard1-c-1-Data.db 
Exception in thread ""main"" java.lang.StringIndexOutOfBoundsException: String index out of range: -1
	at java.lang.String.checkBounds(String.java:397)
	at java.lang.String.<init>(String.java:482)
	at org.apache.cassandra.dht.RandomPartitioner.convertFromDiskFormat(RandomPartitioner.java:63)
	at org.apache.cassandra.tools.SSTableExport.enumeratekeys(SSTableExport.java:161)
	at org.apache.cassandra.tools.SSTableExport.main(SSTableExport.java:370)
;;;","25/May/10 11:45;jbellis;matthew, i think you and frank are looking at two different things.  it sounds to me like frank is saying, ""if I do thus and so [which involves killing the SERVER] I get corrupt sstables that generate EOFexception when read, so no response is ever sent for WRH to see.""

you're saying ""if i kill the client the server logs EOFexception somewhere in the thrift code or otherwise errors out trying to send the reply to the client [your WRH thing]"" which makes total sense.;;;","25/May/10 12:03;mdennis;Just to be clear, the test I automated:

parent process starts Cassandra server
parent process starts python thrift client in separate process
client loops inserting data
parent process sends SIGKILL to client in the middle of looping
parent process sends SIGTERM to server (is this not a clean shutdown?)
parent process starts server again
parent process connects to server over thrift from python
parent process issues get_range_slice and receives TimedOutException

As I understood Frank, he believed the issue to be that CliMain didn't call disconnect because it was killed from Eclipse.  When he explicitly typed ""exit"" in the CLI, CliMain called disconnect and the problem did not present himself (even though presumably he restarted Cassandra in the same way).

Should I be looking at killing the server instead?

;;;","25/May/10 12:04;frankdu;Sounds like what I mean. One interesting point: after inserting the 3 rows, get_range_slices() will result in EOFException. However, get() can read out 3 rows one by one, with no problem.

Best,
Frank;;;","26/May/10 08:34;jbellis;can you reverse engineer from the patch to a reproducible system (or more likely unit) test?;;;","26/May/10 08:36;mdennis;patch2 corrects a small typo in the original patch - please use patch2 and ignore the original patch;;;","26/May/10 08:42;mdennis;patch2 against -r948213;;;","29/May/10 00:02;slebresne;I believe the patch attached to #1130 may fix this.;;;","29/May/10 00:34;jbellis;This is a second manifestation of CASSANDRA-1130.  (Or 1130 is a second manifestation of this.)  Sylvain has a fix in that ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool drain attempts to delete a deleted file,CASSANDRA-1408,12471949,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,jhermes,jhermes,19/Aug/10 05:49,16/Apr/19 17:33,22/Mar/23 14:57,11/Dec/10 01:36,0.6.9,0.7 beta 2,,,0,,,,,,"Running `nodetool drain` presented me with a pretty stack-trace.
The drain itself finished successfully and nothing showed up in the system.log.

{noformat}
$ bin/nodetool -h 127.0.0.1 -p 8080 drain
Exception in thread ""main"" java.lang.AssertionError: attempted to delete non-existing file CommitLog-1282166457787.log
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:40)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:178)
	at org.apache.cassandra.service.StorageService.drain(StorageService.java:1653)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1427)
	at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:788)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
	at sun.rmi.transport.Transport$1.run(Transport.java:159)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
{noformat}",sun-jdk-1.6/Ubuntu 10.04,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Dec/10 06:50;brandon.williams;1408-0.6.txt;https://issues.apache.org/jira/secure/attachment/12465949/1408-0.6.txt","07/Sep/10 22:15;jbellis;1408.txt;https://issues.apache.org/jira/secure/attachment/12454016/1408.txt",,,,,,,,,,,,,2.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20123,,,Fri Dec 10 18:48:42 UTC 2010,,,,,,,,,,"0|i0g4vj:",92220,,gdusbabek,,gdusbabek,Low,,,,,,,,,,,,,,,,,"19/Aug/10 05:57;jbellis;this may be a problem in 0.6 as well.;;;","03/Sep/10 06:42;jbellis;why are we even calling CommitLog.recover() during drain?;;;","03/Sep/10 20:54;gdusbabek;No good reason that I can think of.  At the time I remember the complaint was that shutting down left uncommitted updates in the CL, but flushing solves that.;;;","03/Sep/10 22:26;jbellis;I think the recover can be necessary b/c we don't shut down mutations until after the flushes.  Attached patch fixes ordering there (and adds wait on postFlushExecutor for maximum correctness).

(As for the error on recover() itself, that is/will be addressed in CASSANDRA-1348.);;;","07/Sep/10 21:45;gdusbabek;patch needs rebase.;;;","07/Sep/10 22:15;jbellis;rebased;;;","07/Sep/10 23:52;gdusbabek;+1;;;","08/Sep/10 00:27;jbellis;committed;;;","19/Sep/10 10:30;phuongcsa;java.lang.AssertionError: attempted to delete non-existing file Walls-e-25-Statistics.db
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:43)
        at org.apache.cassandra.io.sstable.SSTable.deleteIfCompacted(SSTable.java:135)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:174)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:343)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:309)
        at org.apache.cassandra.db.Table.initCf(Table.java:296)
        at org.apache.cassandra.db.Table.<init>(Table.java:245)
        at org.apache.cassandra.db.Table.open(Table.java:104)
        at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:461)
        at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:105)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:98)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:216)


I have installed this patch, however bug is not resolved.

Cassandra 0.7 beta1, Ubuntu 10.04;;;","10/Dec/10 06:35;brandon.williams;Reopened for backport to 0.6;;;","10/Dec/10 07:26;lenn0x;Brandon,

I verified this with 0.6 branch. Error occurs without patch. Applied with patch, no more errors. Thx!;;;","11/Dec/10 00:33;jbellis;+1;;;","11/Dec/10 01:36;brandon.williams;Committed.;;;","11/Dec/10 02:48;hudson;Integrated in Cassandra-0.6 #22 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/22/])
    correct ordering of drain operations so CL.recover is no longer necessary.  Patch by jbellis and brandonwilliams, reviewed by jbellis for CASSANDRA-1408
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create keyspace example in the help menu of the command line client doesn't work,CASSANDRA-1562,12475539,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Duplicate,,mingfai,mingfai,01/Oct/10 03:21,16/Apr/19 17:33,22/Mar/23 14:57,15/Feb/11 03:31,,,Legacy/Tools,,0,,,,,,"For the nightly build, the ""create keyspace"" command from the help doesn't work. 
{code}
create keyspace foo with replication_factor = 1 and  placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy' 
{code}

replication factor is changed to 1 to allow simple local testing. The command results as:
{quote}
line 1:73 mismatched input ''org.apache.cassandra.locator.SimpleStrategy'' expecting IntegerLiteral
816169c8-ccc6-11df-af0c-6d517e313560
{quote}

""describe keyspace foo"" shows there is no placement strategy. 

If the order of attributes are swapped, i.e.
{code}
create keyspace foo with placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy' and replication_factor = 1 
{code}
there won't be error message but the keyspace still won't be created with any placement strategy.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20201,,,Tue Feb 22 16:47:16 UTC 2011,,,,,,,,,,"0|i0g5zr:",92401,,,,,Low,,,,,,,,,,,,,,,,,"01/Oct/10 03:22;mingfai;shouldn't be major :-);;;","01/Oct/10 03:30;mingfai;nm. swapping the order of attributes won't work. It results as no error but keyspace won't be created with any placement strategy. Looks like the command line client can't be used to create KS for now. ;;;","15/Feb/11 03:31;jbellis;fixed by 0.7.0 final;;;","23/Feb/11 00:47;eranda;I try to track on this problem and stopped at org.apache.cassandra.cli.CliClient in the method executeAddKeySpace. I tried and still cannot figure it out where this method call to (thriftClient.system_add_keyspace(updateKsDefAttributes(statement, ksDef)) ) ) . If you can support me on this that would be great.
thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
repair leaving FDs unclosed,CASSANDRA-1752,12480189,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,thobbs,jbellis,jbellis,17/Nov/10 17:15,16/Apr/19 17:33,22/Mar/23 14:57,02/Dec/10 06:42,0.6.9,,,,0,,,,,,"""We noticed that after a `nodetool repair` was ran, several of our nodes reported high disk usage; -- even one node hit 100% disk usage. After a restart of that node, disk usage drop instantly by 80 gigabytes -- well that was confusing, but we quickly formed the theory that Cassandra must of been holding open references to deleted file descriptors.

""Later, i found this node as an example, it is using about 8-10 gigabytes more than it should be -- 118 gigabytes reported by df, yet du reports only 106 gigabytes in the cassandra directory (nothing else on the mahcine). As you can see from the lsof listing, it is holding open FDs to files that no longer exist on the filesystem, and there are no open streams or as far as I can tell other reasons for the deleted sstable to be open.

""This seems to be related to running a repair, as we haven't seen it in any other situations before.""

A quick check of FileStreamTask shows that the obvious base is covered:
{code}
        finally
        {
            try
            {
                raf.close();
            }
            catch (IOException e)
            {
                throw new AssertionError(e);
            }
        }
{code}

So it seems that either the transfer loop is never finishing to get to that finally block (in which case why isn't it showing up in outbound streams?) or something else is the problem.",,johanoskarsson,mdennis,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Dec/10 12:23;thobbs;1752-0.6-v2.txt;https://issues.apache.org/jira/secure/attachment/12465023/1752-0.6-v2.txt","02/Dec/10 03:34;thobbs;1752-0.6-v3.txt;https://issues.apache.org/jira/secure/attachment/12465061/1752-0.6-v3.txt","30/Nov/10 07:58;thobbs;1752-0.6.txt;https://issues.apache.org/jira/secure/attachment/12464921/1752-0.6.txt",,,,,,,,,,,,3.0,thobbs,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20291,,,Wed Dec 01 23:59:27 UTC 2010,,,,,,,,,,"0|i0g767:",92592,,mdennis,,mdennis,Normal,,,,,,,,,,,,,,,,,"30/Nov/10 03:15;thobbs;This appears to be a large part of the problem: http://bugs.sun.com/view_bug.do?bug_id=4724038;;;","30/Nov/10 03:44;jbellis;But the unmapping is supposed to take place at finalization time, which is also when we actually issue the unlink.;;;","30/Nov/10 04:04;thobbs;Ah, when StreamOut.transferSSTables() blocks on waitForStreamCompletion(), the list of SSTableReaders is still in scope, so they aren't garbage collected.;;;","30/Nov/10 08:47;thobbs;The temporary files that are streamed get deleted whenever the node receives a message saying that the file was streamed successfully.  There isn't a need for SSTableReaders at all in this case; only the names of the files produced by the anticompaction are needed for streaming.  The fix here is to to simply close the SSTableWriter without opening an SSTableReader after anticompaction and return a the list of filenames for use with streaming instead.  This way, if waitForStreamCompletion() hangs indefinitely, there are no SSTRs around to keep the FDs open.;;;","30/Nov/10 12:37;mdennis;+1 

(but CompactionManager.java:355-358 are superfluous given the loop check after that statement and the added return at the end of the method);;;","01/Dec/10 02:51;jbellis;can't we leave the timing and logging code inside the Helper compaction method to reduce duplication in its callers?;;;","01/Dec/10 02:52;jbellis;where do the temporary files get deleted post-stream with this patch?;;;","01/Dec/10 09:53;thobbs;Files are deleted post-stream in StreamOutManager.finishAndStartNext().  I'll clean up the code a bit and post a new patch shortly.;;;","01/Dec/10 12:23;thobbs;Cleaned up version of patch attached.;;;","02/Dec/10 01:51;jbellis;committed;;;","02/Dec/10 02:01;jbellis;reverted -- tests fail to build;;;","02/Dec/10 02:29;hudson;Integrated in Cassandra-0.6 #13 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/13/])
    avoid opening readers on anticompacted to-be-streamed temporary files
patch by thobbs; reviewed by mdennis and jbellis for CASSANDRA-1752
;;;","02/Dec/10 03:34;thobbs;Unit tests are fixed in the v3 patch.;;;","02/Dec/10 06:42;jbellis;re-committed, thanks;;;","02/Dec/10 07:59;hudson;Integrated in Cassandra-0.6 #14 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/14/])
    avoid opening readers on anticompacted to-be-streamed temporary files
patch by thobbs; reviewed by mdennis and jbellis for CASSANDRA-1752
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
split commitlog into header + mutations files,CASSANDRA-1179,12466697,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,mdennis,jbellis,jbellis,11/Jun/10 07:02,16/Apr/19 17:33,22/Mar/23 14:57,20/Jun/10 00:09,0.7 beta 1,,,,0,,,,,,"As mentioned in CASSANDRA-1119, it seems possible that a commitlog header could be corrupted by a power loss during update of the header, post-flush.  We could try to make it more robust (by writing the size of the commitlogheader first, and skipping to the end if we encounter corruption) but it seems to me that the most foolproof method would be to split the log into two files: the header, which we'll overwrite, and the data, which is truly append only.  If If the header is corrupt on reply, we just reply the data from the beginning; the header allows us to avoid replaying data redundantly, but it's strictly an optimization and not required for correctness.",,hammer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jun/10 05:15;jbellis;1179-v2.txt;https://issues.apache.org/jira/secure/attachment/12447172/1179-v2.txt","16/Jun/10 06:55;mdennis;trunk-1179-v3.txt;https://issues.apache.org/jira/secure/attachment/12447181/trunk-1179-v3.txt","17/Jun/10 03:19;mdennis;trunk-1179-v4.txt;https://issues.apache.org/jira/secure/attachment/12447266/trunk-1179-v4.txt","16/Jun/10 03:17;mdennis;trunk-1179.txt;https://issues.apache.org/jira/secure/attachment/12447163/trunk-1179.txt",,,,,,,,,,,4.0,mdennis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20023,,,Sun Jun 20 12:47:27 UTC 2010,,,,,,,,,,"0|i0g3hb:",91994,,,,,Normal,,,,,,,,,,,,,,,,,"16/Jun/10 05:06;jbellis;made some minor changes, primarily using BRAF in writeCommitLogHeader (you don't get buffering w/ raw FileOutputStream, and BRAF is simpler than doing the FOS/BufferedOutputStream/FileChannel dance).  also added RecoveryManager3Test to test the .header missing entirely.

todo: still needs to delete the .headers after a successful replay as well as the .log.

more severe: after running ""bin/cassandra -f"" and C-c-ing several times in a row, I get

ERROR 16:02:14,537 Exception encountered during startup.
java.lang.ArrayIndexOutOfBoundsException
	at java.lang.System.arraycopy(Native Method)
	at org.apache.cassandra.io.util.BufferedRandomAccessFile.read(BufferedRandomAccessFile.java:332)
	at java.io.RandomAccessFile.readFully(RandomAccessFile.java:381)
	at java.io.RandomAccessFile.readFully(RandomAccessFile.java:361)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:213)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:172)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:120)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:90)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:221)

(This looks like BRAF is throwing AIOOBE when it should really be EOFException)
;;;","16/Jun/10 05:15;jbellis;(fixed trying to be to clever w/ metadata fsync -- we actually do need to include that the first time we write the file);;;","16/Jun/10 06:55;mdennis;trunk-1179-v3.txt deletes header files after successful replay, handle short entries and garbage size writes;;;","16/Jun/10 08:04;mdennis;need to add a test for the corrupted / partially flushed segment;;;","16/Jun/10 09:54;jbellis;I'd rather fix BRAF to generate correct EOFExceptions in case other code runs into this.  (And by removing the EOFException check, we introduce a new bug that if the size int is incomplete, we die again.);;;","16/Jun/10 10:01;jbellis;(actually BRAF.read should be returning -1, so that RAF.readFully throws EOFException);;;","17/Jun/10 03:19;mdennis;{quote}
made some minor changes, primarily using BRAF in writeCommitLogHeader (you don't get buffering w/ raw FileOutputStream, and BRAF is simpler than doing the FOS/BufferedOutputStream/FileChannel dance).
{quote}

FOS doesn't sync on flush/close and as headers are ""optional"" now there is no reason to waste the IO.  Just to be sure I was remembering this correctly, I just now tested it.  It provides 80+% improvement over BRAF, even more on a heavily loaded system.  This was clearly a failure on my part to document it at as such.  The header is so small (56 bytes I think) the OS will cache it just fine and not using buffered output will avoid both the memcopies and GC from the buffers.

{quote}
todo: still needs to delete the .headers after a successful replay as well as the .log.
{quote}

thank you, I hadn't realized there were two places the logs were getting removed.  Done.

{quote}
I'd rather fix BRAF to generate correct EOFExceptions in case other code runs into this. (And by removing the EOFException check, we introduce a new bug that if the size int is incomplete, we die again.)

(actually BRAF.read should be returning -1, so that RAF.readFully throws EOFException) 
{quote}

It was not at EOF, the buffer the data was supposed to be written into was zero length.  There was data in the file, but no where to write it in the buffer (because the size read was 0, new byte[size] resulted in a zero length array was was then supposed to be filled by BRAF.readFully).

I've added tests to catch this problem (as well as other related ones) and also changed BRAF to throw a more reasonable exception (but not EOF).  I believe BRAF.readFully will already throw EOF if it is at the end of the file.

The size of the log entry is now CRCed on it's own.  Whlie testing with random garbage at the end of a commit log, I had written a really large int to the size field which resulted in recover() trying to allocate a massive byte[] and getting OOM.

{quote}
by removing the EOFException check, we introduce a new bug that if the size int is incomplete, we die again.
{quote}

good catch.  I have no idea WTF I was thinking, there was even a comment that warned about it that got removed when the try/catch was removed.  I was probably trying to test something and removed it so it'd spew but forgot to put it back.
;;;","20/Jun/10 00:09;jbellis;committed, minus the BRAF change;;;","20/Jun/10 20:47;hudson;Integrated in Cassandra #471 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/471/])
    split commitlog header into separate file and add size checksum to mutations.  patch by mdennis and jbellis for CASSANDRA-1179
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CassandraStorage LoadPushDown implementation causes heisenbugs,CASSANDRA-2484,12504329,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jeromatron,jeromatron,jeromatron,15/Apr/11 08:49,16/Apr/19 17:33,22/Mar/23 14:57,18/Apr/11 05:11,0.7.5,,,,0,hadoop,pig,,,,"After pulling hair out about why weird errors were happening loading data from cassandra with seemingly irrelevant changes to the pig scripts (mostly changing the script trying to debug other problems), it looks like the weird errors were because of the implementation we currently have for LoadPushDaown in CassandraStorage.  Unless there is a good reason to implement it, I feel like we should just remove the few lines that are in there until we can spend some serious time doing an implementation of it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Apr/11 02:51;jeromatron;2484-trunk.txt;https://issues.apache.org/jira/secure/attachment/12476645/2484-trunk.txt","15/Apr/11 08:55;jeromatron;2484.txt;https://issues.apache.org/jira/secure/attachment/12476399/2484.txt",,,,,,,,,,,,,2.0,jeromatron,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20647,,,Mon Apr 18 18:51:21 UTC 2011,,,,,,,,,,"0|i0gbn3:",93316,,,,,Normal,,,,,,,,,,,,,,,,,"15/Apr/11 08:55;jeromatron;Removed LoadStoreFunc because it was causing heisenbugs.  Also updated JavaDocs a bit.;;;","16/Apr/11 04:01;brandon.williams;Can you provide the errors you encountered?  At the least, it seems like RequiredFieldResponse should essentially be a no-op.;;;","16/Apr/11 04:09;jeromatron;What would happen was we would be troubleshooting a bug in a complicated script and if we took out part of it we would get errors with isEmpty (something we were using but unrelated at all to the change) and then investigate more and Cassandra wouldn't return anything at all but only with that particular portion of the script running.  Then someone else was going through another kind of complicated script and getting odd null pointer exceptions and traced it back to something similar.  That made us think that pig was trying to ""optimize"" or something behind the scenes.  Then we thought it might be trying to project certain data out of the column family based on the changed script.  That led us to wonder if it had something to do with the LoadPushDown - since that is called when pig thinks it can project things out of a data store/file if it doesn't need the rest.  That would explain some of the odd behavior at least.  We commented out it out like in the patch and both errors that were independent and unexplainable any other way, were gone.

I know that's kind of a round about way of going about it, but it seems like good evidence to me that something is up with it - and if it's essentially a no-op, and it works without it, then I didn't see why we wouldn't just take it out.;;;","18/Apr/11 05:11;brandon.williams;Committed.;;;","18/Apr/11 05:37;hudson;Integrated in Cassandra-0.7 #436 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/436/])
    Remove LoadPushDown methods from pig storage.
Patch by Jeremy Hanna, reviewed by brandonwilliams for CASSANDRA-2484
;;;","19/Apr/11 02:51;jeromatron;Attaching a patch for trunk - not sure why the other one didn't apply.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot get range slice of super columns in reversed order,CASSANDRA-2212,12499291,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,muga_nishizawa,muga_nishizawa,22/Feb/11 10:36,16/Apr/19 17:33,22/Mar/23 14:57,24/Feb/11 01:55,0.6.13,0.7.3,,,1,,,,,,"I cannot get range slice of super columns in reversed order.  These data are stored in Cassandra in advance.  On the other hand, range slice of these data in normal order can be acquired.

You can reproduce the bug by executing attached programs.  
- 1. Start Cassandra daemon on localhost (number of thrift port is 9160)
- 2. Create keyspace and column family, according to ""create_table.cli"", 
- 3. Execute ""cassandra_sample_insert.py"", storing pairs of row keys and super columns
- 4. Execute ""cassandra_sample_rangeslice.py"" and get range slice of stored super columns
""cassandra_sample_insert.py"" and ""cassandra_sample_rangeslice.py"" require pycassa.  

You will need to execute 4.""cassandra_sample_rangeslice.py"" with following options so that you get range slice of super columns in reversed order.  

 % python cassandra_sample_rangeslice.py -r 00082 00083

On the other hand, to get range slice in normal order, you will need to use following options.  

 % python cassandra_sample_rangeslice.py -f 00082 00083

00082 and 00083 are the specified key range.  Range slice can be acquired in normal order but, I cannot get it in reversed order.  

I assume that there may be a bug within the code for acquiring the index block of specified range.  In fact, 00083 is included in gap between lastName of index block and firstName of next index block.   ","Fedore 11, Intel Core i5",paulrbrown,skamio,terjem,,,,,,,,,,,,,,,,,,,21600,21600,,0%,21600,21600,,,,,,,,,,,,,,,,,,,,"23/Feb/11 22:53;slebresne;0001-Fix-IndexHelp.indexFor-for-reverse-query.patch;https://issues.apache.org/jira/secure/attachment/12471739/0001-Fix-IndexHelp.indexFor-for-reverse-query.patch","18/Mar/11 22:17;slebresne;2212_0.6.patch;https://issues.apache.org/jira/secure/attachment/12473997/2212_0.6.patch","22/Feb/11 10:39;muga_nishizawa;cassandra_sample_insert.py;https://issues.apache.org/jira/secure/attachment/12471596/cassandra_sample_insert.py","22/Feb/11 10:40;muga_nishizawa;cassandra_sample_rangeslice.py;https://issues.apache.org/jira/secure/attachment/12471597/cassandra_sample_rangeslice.py","22/Feb/11 10:39;muga_nishizawa;create_table.cli;https://issues.apache.org/jira/secure/attachment/12471595/create_table.cli",,,,,,,,,,5.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20508,,,Fri Mar 18 18:11:34 UTC 2011,,,,,,,,,,"0|i0ga07:",93051,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"22/Feb/11 13:11;thobbs;I am able to reproduce this with revision r1072164 (from Feb. 18, a couple days after 0.7.2) on a single node.  Other column ranges work as expected for reversed slices.;;;","23/Feb/11 22:53;slebresne;Thanks a lot Muga for the test script, and you are right, this a bug in getting the index block during reverse queries.

Patch attached with unit tests for IndexHelper.;;;","24/Feb/11 01:15;jbellis;Sylvain, can you summarize the bug and how this patch fixes it?;;;","24/Feb/11 01:32;slebresne;Sure.

The problem is that we were not picking the right index slot for reverse query. Let's take the example from the unit test, and say your index look like this:
  [0..5][10..15][20..25]

And say you look for the slice [13..17]. When doing forward slice, we we doing a binary search comparing 13 (the start of the query) to the lastName part of the index slot, which is fine. You'll end up with the ""first"" slot, going from left to right, that may contain the start.

When doing a reverse query, we were doing the same thing, only using as a start column the end of the query, aka 17 in my example. However, comparing 17 with the lastName of each index slot, you end up selecting the last slot, which is wrong (the slice exit early since 17 is not in the range).

What you want to do is pick the ""first"" slot, but now going from right to left, that may contain start. So you want to find the slot where firstName > start and take the slot just before.

I hope I'm clear. Anyway, that's what the patch does. ;;;","24/Feb/11 01:55;jbellis;committed, with your explanation added as a comment to indexFor :);;;","24/Feb/11 02:54;hudson;Integrated in Cassandra-0.7 #311 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/311/])
    fix reversed slice queries on large rows
patch by slebresne; reviewed by jbellis for CASSANDRA-2212
;;;","18/Mar/11 16:13;paulrbrown;I think that I have a situation where this occurs against 0.6.8 as well.  Is this fix suitable for backporting onto 0.6?;;;","18/Mar/11 22:17;slebresne;Attaching patch against 0.6;;;","19/Mar/11 02:11;jbellis;committed for 0.6.13;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ColumnFamilyRecordReader returns duplicate rows,CASSANDRA-1042,12463494,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,joosto,joosto,02/May/10 00:18,16/Apr/19 17:33,22/Mar/23 14:57,20/Jul/10 03:53,0.6.5,,,,0,hadoop,mapreduce,,,,"There's a bug in ColumnFamilyRecordReader that appears when processing a single split (which happens in most tests that have small number of rows), and potentially in other cases.  When the start and end tokens of the split are equal, duplicate rows can be returned.

Example with 5 rows:
token (start and end) = 53193025635115934196771903670925341736

Tokens returned by first get_range_slices iteration (all 5 rows):
 16955237001963240173058271559858726497
 40670782773005619916245995581909898190
 99079589977253916124855502156832923443
 144992942750327304334463589818972416113
 166860289390734216023086131251507064403

Tokens returned by next iteration (first token is last token from
previous, end token is unchanged)
 16955237001963240173058271559858726497
 40670782773005619916245995581909898190

Tokens returned by final iteration  (first token is last token from
previous, end token is unchanged)
 [] (empty)

In this example, the mapper has processed 7 rows in total, 2 of which
were duplicates.

",,anty,bryantower,cbiocca,greglu,johanoskarsson,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jun/10 07:41;jeromatron;1042-0_6.txt;https://issues.apache.org/jira/secure/attachment/12448109/1042-0_6.txt","11/Jul/10 03:30;jbellis;1042-test.txt;https://issues.apache.org/jira/secure/attachment/12449166/1042-test.txt","19/Jul/10 23:49;jbellis;1042-v2.txt;https://issues.apache.org/jira/secure/attachment/12449851/1042-v2.txt","28/May/10 00:29;jeromatron;CASSANDRA-1042-trunk.patch.txt;https://issues.apache.org/jira/secure/attachment/12445670/CASSANDRA-1042-trunk.patch.txt","28/May/10 00:29;jeromatron;Cassandra-1042-0_6-branch.patch.txt;https://issues.apache.org/jira/secure/attachment/12445671/Cassandra-1042-0_6-branch.patch.txt","21/May/10 07:32;jeromatron;cassandra.tar.gz;https://issues.apache.org/jira/secure/attachment/12445117/cassandra.tar.gz","02/Jul/10 00:33;jeromatron;duplicate_keys.rtf;https://issues.apache.org/jira/secure/attachment/12448502/duplicate_keys.rtf",,,,,,,,7.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19970,,,Thu Aug 05 12:40:54 UTC 2010,,,,,,,,,,"0|i0g2nb:",91859,,,,,Normal,,,,,,,,,,,,,,,,,"02/May/10 01:02;jbellis;This sounds like something we could make a unit test for, without having to get Hadoop itself involved.;;;","04/May/10 00:38;cbiocca;The basic issue is that the thrift server's return value is sorted by the absolute value of the tokens, while the CassandraRecordReader assumes that the order is the one given by traversal of the range (that is, we get the smallest value greater than start_token in first position, and the greatest value smaller than or equal to end_token in last position. 
Now I don't know which is correct, as the API docs I've looked at don't suggest which order is supposed to be returned, but if the server's implementation is correct, then the record reader needs to iterate over the returned tokens to figure out which one is actually the last token for iteration purposes. Otherwise, switching the server's implementation to return keys in the iteration order will work.;;;","14/May/10 23:02;jeromatron;Hmm, I was able to reproduce this with the contrib/word_count piece on trunk.  It appears to double count rows in ranges that have a single row as well as those that have more - in this case 1000.;;;","15/May/10 06:58;jeromatron;Just some more data - when I use an OrderPreservingPartioner, the word count works fine...;;;","18/May/10 06:25;jeromatron;For the describe_splits call it makes, it returns 3 sub splits, the first of which is a wrapping split.  Sounds like it's buggy on the server side.  Will check with Jonathan.;;;","18/May/10 23:11;jeromatron;Jonathan:

Inputs to client.describe_splits() - ColumnFamilyInputSplit:185:
range.start_token: 85469146195799762548951268272529359452
range.end_token: 85469146195799762548951268272529359452
splitsize: 65536

output:
splits - arraylist<String>:
0: 85469146195799762548951268272529359452
1: 85469146195799762548951268272529359452

Seems like that is the bug right there, but not familiar enough with what it's supposed to do in that case?

Btw, there are only 4 rows in the CF.;;;","19/May/10 05:45;jeromatron;Appears to be something server related in the splits themselves.;;;","21/May/10 07:32;jeromatron;In order to facilitate reproducing the problem, I'm attaching my cassandra data directory tar/gzed up.

There are 4 rows in the cassandra instance.  If you modify WordCountSetup to change TEST_COUNT from 4 to 1, then run WordCount, you will find that Cassandra trunk will count 7 occurrences instead of 4.  You can also debug on the line I mentioned previously to see what describe_splits receives and then outputs.

Just wanted to facilitate reproducing the problem.;;;","27/May/10 07:05;jeromatron;Adding a patch that does the following:

1. Removes an ordering section in StorageProxy that messes with the wrapping range for a get_range_slice call - thereby messing up the order of the records returned.  That led to having the initial wrapping range returned in token order instead of wrapping order.  So there was a second call going from last token as far as natural ordering goes, all the way to the initial start token.  So if the server's token were 5, and there were 10 tokens, it would list 1-10, then 1-5 again.  With this fix, the return order of the tokens is 6-10, then 1-5, which is correct - the order of the wrapped range, then in token order.

2. A few instances of token.toString should have been TokenFactory.toString(token) - fixed.

3. There was a method in StorageService - getStringEndpointMap - that is never call - removed that.

4. Updated WordCountSetup with the latest trunk to use new Clock(System.currentTimeMillis());;;","27/May/10 07:08;jeromatron;Tx to Stu Hood for helping me narrow this down.;;;","27/May/10 07:12;jeromatron;To clarify: to fix the problem - this removes some ordering in StorageProxy.getRangeIterator since getRestricedRanges should already have returned the right thing.;;;","27/May/10 10:03;jbellis;Looks good to me.  Nice work, Jeremy and Stu.

Can you submit a version against 0.6 branch too?;;;","28/May/10 00:29;jeromatron;Attaching patches for 0.6-branch and trunk.;;;","28/May/10 00:31;jeromatron;Jonathan - I updated the trunk patch to not add a couple of unused imports that snuck in while I was messing with WordCount.;;;","28/May/10 00:34;jeromatron;Also - I didn't remove StorageService.getStringEndpointMap in the 0.6 branch version because CassandraServer.get_string_property still calls it.  get_string_property was removed on trunk as part of CASSANDRA-965;;;","28/May/10 02:59;jbellis;committed, thanks!;;;","28/May/10 03:50;jbellis;done;;;","26/Jun/10 02:51;jbellis;re-opening in light of CASSANDRA-1198;;;","26/Jun/10 07:41;jeromatron;Unwrapped the tokens in the first place ensuring that the splits would not contain wraps.  Works fine now.;;;","26/Jun/10 07:41;jeromatron;Attaching new patch.;;;","26/Jun/10 07:43;jeromatron;The patch should apply cleanly to 0.7/trunk as well;;;","26/Jun/10 08:30;stuhood;+1;;;","26/Jun/10 08:33;jbellis;do we have a theory as to why wrapped ranges should cause bugs?

i worry that if we're just trying code out and it seems to work, that we may not be fixing the real problem;;;","26/Jun/10 08:48;jeromatron;Good point.

From what I could tell in this instance, it would go through the input splits and on the last input split, it would have an incorrect last value.  So it would go back through and take that value to the end of the input list.  I would imagine that is where it had wrapped.  I'm not sure why it had the incorrect last value as the last value in the wrapped input split though.  If someone is wiser than I in these matters, please chime in.  But it appears that normalizing how the splits are done so one split does not wrap internally, it solves the problem.

To reproduce easily and with a small dataset: If you don't apply the patch and run the word_count_setup with only 10 values for text3, usually that will be enough to manifest the problem when running wordcount.

Also, I might think that if the wrap could be detected when creating the splits, as with this patch, then it makes sense that wrapping could be detected when reading the rows in the ColumnFamilyRecordReader.  That could be another way to resolve it.  But I think it's sixes when it comes to the solution.

Like I said, I'm not certain why that incorrect ordering happens on the wrapped split.;;;","02/Jul/10 00:33;jeromatron;Adding the output of word count with duplicate tokens.  It appears to happen when the input split contains a wrapped key range.  That's why the updated patch splits wrapped key ranges (fixing the problem).;;;","07/Jul/10 04:16;jeromatron;Sorry if this is redundant but pasting in a thought we had a while ago that motivated the attached patch.  If we make sure that the splits are always in ring order and never wrap, it solves the problem.

""Token ranges may also wrap -- that is, the end token may be less than the start one. Thus, a range from keyX to keyX is a one-element range, but a range from tokenY to tokenY is the full ring.""

It does not say what order they will be in when it wraps.  Some clients assume that the ordering is natural order while the hadoop client interactions assume that it will be ring order.

For example:
-- a list of tokens (1,2,3,4,5,6,7,8,9)
-- a get_range_slice call with start_token = 5, end_token = 5
Natural order meaning token order from start to finish, returning the results (1,2,3,4,5,6,7.8,9).
Ring order or wrapping order meaning it would return the results (5,6,7,8,9,1,2,3,4).;;;","07/Jul/10 04:20;jbellis;the ""correct"" order when tokens are involved is ring order

(when start_key is used instead of start_token, you can't have a wrapping range so it should be moot);;;","11/Jul/10 02:07;jbellis;patching CFIF isn't the answer, we need any client using the API to get the right results;;;","11/Jul/10 03:30;jbellis;it seems that the root of the problem is, as Jeremy said, rows getting returned in token order instead of ring order.  if, in joost's original example, the rows were returned in order of

99079589977253916124855502156832923443
144992942750327304334463589818972416113
166860289390734216023086131251507064403
16955237001963240173058271559858726497
40670782773005619916245995581909898190

then doing an extra query for (40670782773005619916245995581909898190, 53193025635115934196771903670925341736]

would return the desired result of nothing.

but I am unable to reproduce this behavior in a unit test (against 0.6 branch, attached).  trying jeremy's data dir (also against 0.6 branch), I get ""java.io.IOException: Found system table files, but they couldn't be loaded. Did you change the partitioner?"" ;;;","12/Jul/10 09:11;jbellis;Jeremy pointed out that the sorting that removed by the original patch here is sorting in raw token order rather than taking into account the requested start token.  I think that's our problem, although I'm not sure why my unit test isn't running into that.;;;","19/Jul/10 22:41;jbellis;ah, the unit test hits CFS directly instead of going through StorageProxy (where the sort happens)...;;;","19/Jul/10 23:49;jbellis;v2 attached:

 - removes wrapped-range handling from CFS.getRangeSlice, since StorageProxy always unwraps first
 - adds (initially failing) system test exercising wrapped-range path
 - adds sorting of unwrapped, restricted ranges relative to the original query range [this is the bug fix]
;;;","20/Jul/10 03:38;jeromatron;+1;;;","20/Jul/10 03:53;jbellis;committed;;;","05/Aug/10 20:40;jbellis;Backported a related fix from CASSANDRA-1156 to 0.6.5 in r982580;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
handle old gossip properly,CASSANDRA-572,12441433,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jaakko,jaakko,jaakko,23/Nov/09 17:20,16/Apr/19 17:33,22/Mar/23 14:57,22/Dec/09 01:19,0.5,,,,0,,,,,,"(1) If a node has been moving in the ring, further bootstraps by other nodes will cause errors as they are handling STATE_LEAVING gossip without having such member in token metadata.

(2) When a node bootstraps, it handles all ep states in the order they happen to arrive. If the first one to arrive has moved in the past (that is, it has STATE_LEAVING in its ep state), getNaturalEndpoint will throw ArrayIndexOutOfBounds exception as sortedTokens.size() == 0.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Dec/09 17:31;jaakko;572-01-fix-index-out-of-bounds.patch;https://issues.apache.org/jira/secure/attachment/12428610/572-01-fix-index-out-of-bounds.patch","21/Dec/09 17:31;jaakko;572-02-use-one-move-state.patch;https://issues.apache.org/jira/secure/attachment/12428611/572-02-use-one-move-state.patch","21/Dec/09 17:31;jaakko;572-03-unit-tests.patch;https://issues.apache.org/jira/secure/attachment/12428612/572-03-unit-tests.patch",,,,,,,,,,,,3.0,jaakko,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19759,,,Mon Dec 21 17:19:19 UTC 2009,,,,,,,,,,"0|i0fzrb:",91391,,,,,Normal,,,,,,,,,,,,,,,,,"23/Nov/09 17:23;jaakko;(1) Modified StorageMetadata.onChange to handle STATE_LEAVING and STATE_LEFT only if isMember is true

(2) We might fix this by simply making sure we're calling getNaturalEndpoints for members only, but imho it is better to fix at the source.
;;;","24/Nov/09 00:19;jbellis;See CASSANDRA-559 for more discussion.

Jaakko, can you check on the mailing list to see if anyone actually needs this, before we commit to making our lives (a little bit) harder for 0.5 -> 0.5.1.;;;","24/Nov/09 00:20;jbellis;Sorry, I was thrown off by the title, assuming old==0.4.  But that is actually not what this is about.;;;","24/Nov/09 00:35;jbellis;This feels like we're solving the wrong problem.  A bootstrapping node should not be added to the list of nodes clients can connect to (however you want to manage that) until after it is done bootstrapping.  I'd rather add

if (StorageService.instance().isBootstrapMode())
    throw new UnavailableException()

to the appropriate StorageProxy methods.;;;","24/Nov/09 08:50;jaakko;I probably described the problem a bit vaguely, another try:

Suppose all nodes in the cluster are running normally and none of them have moved. Their EP state includes STATE_BOOTSTRAPPING (if they were bootstrapped to the ring) and STATE_NORMAL in this order. Suppose there is nodeA, which gets loadbalanced. It goes through leaving, left and bootstrapping back to normal. After this its EP state includes (in this order!) LEAVING, LEFT, BOOTSTRAPPING, NORMAL. The important thing is that EP state can have only one of each state, and they will be handled by other nodes in the order added originally. This is fine for nodes that already were in the ring, as they have seen the _old_ NORMAL state. However, if we ever want to bootstrap another node to the ring, it will cause errors, as they will start to handle states from LEAVING. They have no knowledge of this node's state before they handle NORMAL, so we must handle LEAVING and LEFT properly. That is, we must do nothing if we do not have knowledge of the node.

So this is not related to the new node serving requests, only to handle state gossip from other nodes properly. My term old gossip was obviously badly chosen, perhaps old state information would be more appropriate.
;;;","01/Dec/09 04:06;jbellis;(Waiting on this while I mull over the rack aware bootstrap mailing list thread, since this is irrelevant if we have a problem there and need to add a ""coordinator"" node to fix it.);;;","02/Dec/09 16:26;jaakko;I don't think this is related to move coordination, but a separate issue. This will happen for every new node that enters the cluster (bootstrapping or not) after a node has already moved. Problem is, handling STATE_LEAVING and STATE_LEFT will cause errors for a node that is not in token metadata, and a node will not end up being there before STATE_NORMAL has been seen for it. Since states are handled in the order they were added, STATE_LEAVING and STATE_LEFT will be handled before BOOTSTRAPPING and NORMAL. LEAVING and LEFT in this case have no meaning as they refer to the old already-gone token, but since application state can only grow, these states will remain part of gossip and new nodes must handle them properly.

Edit: This can wait, as there is no point to add now. Let's check the situation after gossiping analysis is done;;;","03/Dec/09 02:23;jbellis;ok, I understand now.  for some reason I had a really hard time wrapping my head around this.

trying to make sure we ignore old applicationstate correctly in all cases seems like fixing the symptom instead of the real cause.

wouldn't it be simpler to have a single STATE aplicationstate object whose value would be a json (or whatever) tuple of [NORMAL|LEAVING|LEFT|BOOTSTRAPPING],ARG+?  that way only the most recent one would be present in the gossiper.;;;","03/Dec/09 10:45;jaakko;Funny thing, I was just thinking about the same thing during breakfast. Have to eat more often :)

The problem with this is that handling state changes will become somewhat more complex as we must be prepared to handle transitions between any two states in any order. Current gossip model leaves a trace of what the node has node, and even in the face of network partitions we can ""play back"" the transitions when they eventually arrive. That is, if a node moves, we will still see LEAVING, LEFT, BOOTSTRAPPING and NORMAL and construct token metadata according to that. If we only have one value to represent node's current state, we might go from, say NORMAL to NORMAL, or even LEFT to LEAVING without seeing any of the intermediate steps. Of course this can be done, but needs extra care. Don't know how much, though. Might very well be that in the end this would be better than the current way.

But even this would not remove the need to handle old application state correctly. If a node enters the ring when another node is just LEAVING or LEFT, that state will be the first one to be seen, and it must be ignored since there is nothing that can be done if NORMAL has not been seen. I think the real cause is there in any case, so we can't avoid fixing the symptoms that arrive with it.

I'll try this out now that I'm working on the gossiping part anyway so we'll have some more insight on what it would look like.
;;;","03/Dec/09 16:40;jaakko;OK, here's a patch that uses same state name (NODE_STATE) to gossip all movement information. Format is (BOOTSTRAPPING|NORMAL|LEAVING|LEFT)|token.

The main things caused by this modification to the state machine were:
(1) When a node is bootstrapping, we should clear pending ranges for this endpoint, as well as remove it from token metadata. These checks are not strictly necessary (I think), but are there to help transition from LEAVING -> BOOTSTRAPPING in case we missed LEFT due to network partition.
(2) For handleStateLeaving and handleStateLeft remove pending ranges for this endpoint before doing anything else. If we missed NORMAL, there might be obsolete pending ranges from BOOTSTRAP. Distant possibility, but possibility nonetheless.

Following additional check is not directly related to gossip format change and could happen even using the current model. This is a very unlikely event, but in a large (say, 200+ nodes) multi-DC cluster with lots of node movement, this could very well happen even with relatively short DC-to-DC network outage:
(1) Added a check to handleStateLeaving and handleStateLeft for the case that a node has made NORMAL -> LEAVING -> LEFT -> BOOTSTRAP -> NORMAL -> LEAVING [->LEFT] movement cycle without us seeing the intermediate stages. In this case we have information for the old token and now the node is leaving _new_ token. We cannot simply assert this, as it is possible this happens.

Now of course this already touches the subject what conditions we must take care of and what should be left to operators to handle. Some of them (like removing all references to the endpoint before continuing to handle bootstrapping) are questionable and might relax safety precautions, but if we do not do that, a modest 30s network outage might cause us not to see STATE_LEFT and we'd end up having strange pending ranges.

I don't expect this patch to be included as it is, but let's see what people think of this gossip change and then discuss what checks should be made :)
;;;","08/Dec/09 04:34;jbellis;The fundamental question is, is it ever dangerous to overwrite intermediate states, such that a node who has been down or partitioned does something broken when it gets the latest [partial] information?

> if we do not do that, a modest 30s network outage might cause us not to see STATE_LEFT

Trying to figure out what you're referring to here...  We can't miss a STATE_LEFT from a node that is leaving permanently since that will remain its last state and will be gossiped forever.  And if we miss a STATE_LEFT from a node that is moving, a down node that comes back up later will get the new token location and ""snap"" it to the right spot immediately.

I assume that as usual I am slow on the uptake here. :);;;","10/Dec/09 11:22;jaakko;From the small experiment (patch) it would seem that it is not too dangerous, just needs a bit of extra care. From #617 it would seem that reducing total amount of data in the gossiper would be beneficial, so current move gossip model needs some attention. There are basically two options:

(1) use one state for all moves, and use tuple/triple in application state string to identify what the node is doing (as suggested by jbellis earlier)
(2) return to the ""old"" gossip model: one state to broadcast the token and separate small states to express what the token means. The reason for moving out of this model was to make all needed information part of one gossip message and not rely on them arriving at the same time. With small addition to StorageService, we might keep arriving token ""somewhere"" and only add it to token metadata when the actual mode gossip arrives. This would take care of the race condition and would also reduce the amount of data to be gossiped to bare minimum.

> I assume that as usual I am slow on the uptake here. :) 

I think the problem is in my ability to clearly express an idea :)
;;;","10/Dec/09 12:01;jbellis;I think I would prefer option (1), since from experience I can attest that trying to write a 100% correct ""state machine"" for (2) is quite difficult.;;;","10/Dec/09 21:28;jaakko;Run into one tricky issue with STATE_LEAVING. I'll check that tomorrow with #603 as they are connected.;;;","19/Dec/09 00:30;jbellis;What was the tricky issue?  With CASSANDRA-603 and CASSANDRA-639 committed, this needs rebasing at least.;;;","19/Dec/09 08:13;jaakko;Sorry about the delay. I did this part again yesterday (somewhat different now after #603), but had insufficient time to test it properly. The patch would seem OK, so as soon as I have tested it a bit more, I'll submit it.

The problem earlier was how to handle state jump to 'leaving'. Now that pending ranges is calculated every time and we know what direction a node was going last time, this is not a problem anymore. You can expect a patch soon.
;;;","21/Dec/09 17:31;jaakko;01:
Fix ArrayIndex exception in case the first gossip a new node sees is leaving or left.

02:
Use one state for all movement related gossip
Fix also some bugs related to pending ranges on the leaving node itself

03:
A bunch of unit tests for node movement;;;","22/Dec/09 01:19;jbellis;committed to 0.5 and trunk (only change was turning isLeaving writeLock calls to readLock);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
json2sstable/sstable2json don't export/import correct column names when the column family is of BytesType ordering,CASSANDRA-618,12442906,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,urandom,rrabah,rrabah,10/Dec/09 04:51,16/Apr/19 17:33,22/Mar/23 14:57,15/Dec/09 05:10,0.5,,Legacy/Tools,,0,,,,,,"Easy to reproduce.
1- start with an empty node.
2- run: client.insert(""Keyspace1"",
                          key_user_id,
                          new ColumnPath(""Standard1"", null, ""name"".getBytes(""UTF-8"")),
                          ""Ramzi"".getBytes(""UTF-8""),
                          timestamp,
                          ConsistencyLevel.ONE)
3- flush to get sstable
4- sstable2json and export the sstable to a file
5- delete sstable
6- json2sstable and import the json into a new sstable.
7- sstable2json on new sstable, you will see that the name is different than the name in the original json file. 
Also do a get on the column and it will return no result. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Dec/09 03:59;urandom;ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-618-unittest-that-demonstrates-bug.txt;https://issues.apache.org/jira/secure/attachment/12427952/ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-618-unittest-that-demonstrates-bug.txt","15/Dec/09 03:59;urandom;ASF.LICENSE.NOT.GRANTED--v2-0002-de-serialize-columns-to-from-hex-regardless-of-compara.txt;https://issues.apache.org/jira/secure/attachment/12427953/ASF.LICENSE.NOT.GRANTED--v2-0002-de-serialize-columns-to-from-hex-regardless-of-compara.txt","15/Dec/09 03:59;urandom;ASF.LICENSE.NOT.GRANTED--v2-0003-do-trivial-arithmetic-correctly.txt;https://issues.apache.org/jira/secure/attachment/12427954/ASF.LICENSE.NOT.GRANTED--v2-0003-do-trivial-arithmetic-correctly.txt","10/Dec/09 09:54;rrabah;unittest.patch;https://issues.apache.org/jira/secure/attachment/12427554/unittest.patch",,,,,,,,,,,4.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19784,,,Thu Dec 17 22:03:03 UTC 2009,,,,,,,,,,"0|i0g01j:",91437,,,,,Low,,,,,,,,,,,,,,,,,"10/Dec/09 05:10;jbellis;Ramzi, could you add a test to SSTableExportTest illustrating the problem?;;;","10/Dec/09 09:54;rrabah;Attached is a unit test for exporting/importing to json that reproduces this problem;;;","12/Dec/09 06:17;urandom;The attached patch(s) addresses this by serializing all column names to hex (instead of using the comparators toString()).;;;","12/Dec/09 09:05;rrabah;Applying the patch I am getting this error when I run ant clean build test:
 [junit] Testcase: testImportSuperCf(org.apache.cassandra.tools.SSTableImportTest):	Caused an ERROR
    [junit] Invalid localDeleteTime read: -2140491435
    [junit] java.io.IOException: Invalid localDeleteTime read: -2140491435
    [junit] 	at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:368)
    [junit] 	at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:325)
    [junit] 	at org.apache.cassandra.db.filter.SSTableNamesIterator.<init>(SSTableNamesIterator.java:103)
    [junit] 	at org.apache.cassandra.db.filter.NamesQueryFilter.getSSTableColumnIterator(NamesQueryFilter.java:69)
    [junit] 	at org.apache.cassandra.tools.SSTableImportTest.testImportSuperCf(SSTableImportTest.java:63)
;;;","12/Dec/09 22:34;jbellis;I'm getting that error even w/o the patch, even rolling back to code from dec 9 when i know the tests were passing.  possibly a change in wall clock time is what is causing the test failure?;;;","15/Dec/09 04:01;urandom;Apparently there is something on the order of 1,000 milliseconds in a second. Who knew.;;;","15/Dec/09 04:34;rrabah;in SSTableExport.java  for super column name we still use asKey(comparator.getString(column.name()))
change to byteToHex() besides that, looks good;;;","15/Dec/09 05:10;urandom;Right you are. Man, I'm batting a thousand with this ticket. Thanks Ramzi!;;;","18/Dec/09 06:03;hudson;Integrated in Cassandra #291 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/291/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Using KeysCached=""xx%"" results in Key cache capacity: 1",CASSANDRA-1129,12465392,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,rantav,rantav,26/May/10 01:23,16/Apr/19 17:33,22/Mar/23 14:57,02/Jun/10 03:26,0.6.3,,,,0,,,,,,"I don't know if this is a general bug or only something related to my instance, but for me (v0.6.1) I've noticed that when defining KeysCached=""50%"" (or KeysCached=""100%"" and I didn't test other values with %) then cfstats reports Key cache capacity: 1

      <ColumnFamily CompareWith=""BytesType"" Name=""KvAds""
        KeysCached=""100%""
        RowsCached=""10000""
        />


                Column Family: KvAds
                SSTable count: 7
                Space used (live): 797535964
                Space used (total): 797535964
                Memtable Columns Count: 42292
                Memtable Data Size: 10514176
                Memtable Switch Count: 24
                Read Count: 2563704
                Read Latency: 4.590 ms.
                Write Count: 1963804
                Write Latency: 0.025 ms.
                Pending Tasks: 0
                Key cache capacity: 1
                Key cache size: 1
                Key cache hit rate: 0.0
                Row cache capacity: 10000
                Row cache size: 10000
                Row cache hit rate: 0.2206178354382234
                Compacted row minimum size: 386
                Compacted row maximum size: 9808
                Compacted row mean size: 616

I'll attach one of the sstable files from this CF",,jhermes,schubertzhang,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/May/10 13:42;jhermes;0001-CASSANDRA1129.patch;https://issues.apache.org/jira/secure/attachment/12445832/0001-CASSANDRA1129.patch","31/May/10 15:26;jhermes;0002-CASSANDRA1129.patch;https://issues.apache.org/jira/secure/attachment/12445907/0002-CASSANDRA1129.patch","01/Jun/10 23:33;jhermes;0003-CASSANDRA1129.patch;https://issues.apache.org/jira/secure/attachment/12446030/0003-CASSANDRA1129.patch","26/May/10 03:04;rantav;KvAds-84.zip;https://issues.apache.org/jira/secure/attachment/12445489/KvAds-84.zip","02/Jun/10 14:42;jhermes;TOTRUNK-2-CASSANDRA1129.patch;https://issues.apache.org/jira/secure/attachment/12446118/TOTRUNK-2-CASSANDRA1129.patch","02/Jun/10 08:50;jhermes;TOTRUNK-CASSANDRA1129.patch;https://issues.apache.org/jira/secure/attachment/12446084/TOTRUNK-CASSANDRA1129.patch",,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20001,,,Fri Jun 04 02:38:45 UTC 2010,,,,,,,,,,"0|i0g367:",91944,,,,,Low,,,,,,,,,,,,,,,,,"26/May/10 01:33;rantav;The smallest sstable file I could find of this CF.;;;","26/May/10 01:37;jbellis;can you include index and filter parts too, as a zip or tarball?;;;","26/May/10 03:04;rantav;zip file with index, data and filter;;;","27/May/10 21:38;jbellis;So to troubleshoot this you would

* check out the 0.6 branch
* add the CF definition above to your conf/storage-conf.xml
* unzip the sstable files into your data/Keyspace1 directory
* set a breakpoint in DatabaseDescriptor.getKeysCachedFor and see why it's calculating 1 instead of the number of rows in the sstable;;;","29/May/10 13:42;jhermes;Status for first patch:
M       test/unit/org/apache/cassandra/db/CompactionsPurgeTest.java
M       test/conf/storage-conf.xml
M       src/java/org/apache/cassandra/db/ColumnFamilyStore.java
M       src/java/org/apache/cassandra/utils/FBUtilities.java
M       src/java/org/apache/cassandra/cache/InstrumentedCache.java
M       src/java/org/apache/cassandra/io/SSTableTracker.java

This bug was two bugs:
# FBUtilities#absoluteFromFraction(double,long) is a bit ambiguous. The method reads 100% as *absolute* 1, so the keyCacheCapacity is always going to be 1. This fix is already in 0.7.
# The bigger bug is that the keyCacheCapacity was not getting changed during runtime. There was a boolean capacityModified that controlled access to the capacity. It gets set true the first time the capacity is modified (read: creation of table/cfstore) and then never goes back to false. By removing this bool, the cache size gets updated on flushes and on compactions correctly.

Also included are tests in CompactionsPurgeTest that shows this fix works for both 50% and 100% (that the keyCacheCapacity changes dynamically).

The update is O(1), so it shouldn't matter for performance that it's being called on every flush and every compaction.;;;","29/May/10 19:59;jbellis;committed the absoluteFromFraction fix.

for the rest, we need to preserve the boolean, but make it only get set when the cache capacity is set manually from JMX (see CASSANDRA-1079);;;","31/May/10 15:26;jhermes;Removed the FBUtil change from patch.

setCapacity(int) changed to setCapacity(int,bool) in both interface and object.
The boolean in the object is now capacityFrozen with the same logic as before.
setCapacity from the automatic update (flushing/compaction) pass in a false to not freeze the capacity afterward, setCapacity from the JMX command pass in a true to do so.

(To remedy a previous statement, the running time is not O(1) overall. It's O(|SSTables|) with constant time per table.);;;","01/Jun/10 22:15;jbellis;instead of overloading setCapacity to be the interface for both normal adjustments and manual overrides, let's split it up.  have setCapacity(value) be the mbean interface, setting a capacitySetManually boolean, and make a method updateCapacity(value) for internal use.  ;;;","01/Jun/10 23:33;jhermes;capacitySetManually is in place.
setCapacity(int), updateCapacity(int) work as described above.;;;","02/Jun/10 03:26;jbellis;committed, thanks!;;;","02/Jun/10 03:31;jbellis;could you submit a version of this patch against trunk, too?;;;","02/Jun/10 08:50;jhermes;Here's the patch for trunk.
The testconf.xml is now a testconf.yaml.
The test itself uses trunk-style clock, decorated keys.

io/SSTableTracker changes go to io/sstable/SSTableTracker.

The other changes patched easily.;;;","02/Jun/10 11:34;jbellis;I'm getting test failures on trunk (but not 0.6):

    [junit] Testcase: testKeyCache50(org.apache.cassandra.db.CacheSizeTest):	FAILED
    [junit] 128
    [junit] junit.framework.AssertionFailedError: 128
    [junit] 	at org.apache.cassandra.db.CacheSizeTest.testKeyCache(CacheSizeTest.java:93)
    [junit] 	at org.apache.cassandra.db.CacheSizeTest.testKeyCache50(CacheSizeTest.java:49)

(I moved it to a separate CacheSizeTest class, in case the others in CompactionsPurgeTest were messing with it.  Didn't help.);;;","02/Jun/10 14:42;jhermes;All right, the difference is between SSTableReader.estimatedKeys() in 0.6 and RowIndexedReader.estimatedKeys() in 0.7 -- RIR increments the size once during estimation whereas SSTR does not.
Knowing this, I'm making the test explicitly catch 128/256.;;;","02/Jun/10 14:53;jhermes;By the way, the RIR increment looks unintentional to me. It might be a bug; I can't see a reason why it gets incremented for CASS-777 when SSTableReader went to RIReader.
If this is the case, then the +1 can be reverted and the test can go back to catching 64/128.;;;","04/Jun/10 10:38;jbellis;committed the original version for trunk, and removed the extra +1 from estimatedKeys.  Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table.open has a broken lock in it,CASSANDRA-734,12446409,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jmhodges,jmhodges,23/Jan/10 11:52,16/Apr/19 17:33,22/Mar/23 14:57,26/Jan/10 00:55,0.5,,,,0,,,,,,Table.open's lock is used around the Map#put method call but not the #get. This makes it a source of spurious bugs. The attached patch synchronizes the entire Table.open method and removes the unused createLock static.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Jan/10 11:12;jbellis;734-nbhm.txt;https://issues.apache.org/jira/secure/attachment/12431272/734-nbhm.txt","23/Jan/10 11:53;jmhodges;broken_lock_in_table_open.patch;https://issues.apache.org/jira/secure/attachment/12431190/broken_lock_in_table_open.patch",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19843,,,Mon Jan 25 16:55:41 UTC 2010,,,,,,,,,,"0|i0g0qv:",91551,,,,,Low,,,,,,,,,,,,,,,,,"23/Jan/10 12:38;jbellis;I don't think introducing full synchronized lock into a method called for basically every operation is a good idea.  We could use nonblockinghashmap for the container instead, which would take care of the get problem.  (we could just use NBHM with putIfAbsent if Table creation weren't something we want to avoid doing twice for the same keyspace in case of a race.)  what do Real Java Programmers do for ""singleton cache?"";;;","24/Jan/10 18:50;jmhodges;To start with, synchronization being slow is mostly a scary story left around from the bad old days of Java 1.3 and lower. http://www.ibm.com/developerworks/java/library/j-jtp04223.html

Second, any thing we build instead of using synchronized will be nearly exactly duplicating synchronized's behavior except broken and slower. There seems to be nothing that compiles down to just ""lock around this get and, if that's null, create this other thing and then put it in there"" in Java bytecode. The only other way to make this work is load all the tables at boot time. Which, of course, is a non-starter. However, synchronized does say ""all this work has to be done together"" which fixes our bug and has years of JVM hackers behind it making it as fast as possible. 

Third, we should be aiming as much for correctness as we can. A Cassandra node is eventually consistent, but its codebase is not. Fixing a bug that will, eventually, kick Twitter and Digg and Rackspace's ass now is better than holding off until a ""faster"" way can be found in some possible future where unicorns live and candy mountains are not just scary things in creepy guys basements.

synchronized does a bang up job of fixing this bug now and doing so in a way that is more performant than other ""correct"" ways.

After this patch goes in, we should be re-evaluate all of these calls to Table.open(), though. I'm going to bet that in most cases it would make more sense for the client object to hold on to a reference to the Table if they need it and not let go every time like they do currently after the Table.open() call goes out of scope.

Edited for a friggin' ""it's"" grammar problem.;;;","24/Jan/10 20:54;gdusbabek;> The only other way to make this work is load all the tables at boot time.
See CassandraDaemon.setup().

Why not this:
1.  Create a synchronized initTables() method that is called from CD, that sets an initDone flag and blows up it is ever called again.
2.  Take the locking  and synchronicity out of Table.open() (there really is no point as long as the backing collection is unmodifiable), and turn it into a purely 'getter'-type method.;;;","25/Jan/10 03:00;jmhodges;Sweet. I hadn't seen that call to  Table.getAllTableNames() in CD.setup(). If the laziness constraint can be relaxed, that's fine by me! I'll write up a patch.;;;","25/Jan/10 03:11;jbellis;> synchronization being slow is mostly a scary story left around from the bad old days of Java 1.3 and lower

That's true for _uncontested_ syncs but that is not what we have here.  The JVM isn't going to be able to optimize those away, and it's going to be several orders of magnitude slower than w/o the sync.

> any thing we build instead of using synchronized will be nearly exactly duplicating synchronized's behavior except broken and slower

NBHM is lock-free (which actually means it uses lower-level CAS which is much cheaper).

> we should be aiming as much for correctness as we can

I never said otherwise.  But let's do it without causing unnecessary performance regressions.
;;;","25/Jan/10 03:14;jbellis;> Take the locking and synchronicity out of Table.open() 

wouldn't we just have to undo that for CASSANDRA-44?;;;","25/Jan/10 04:17;jmhodges;bq. NBHM is lock-free (which actually means it uses lower-level CAS which is much cheaper). 

How does a NBHM solve the problem get-and-then-put-but-only-instantiate-the-object-at-all-if-get-is-null? I haven't seen any docs on get with conditional set and conditional instantiation.


bq. wouldn't we just have to undo that for CASSANDRA-44?

Not if we do the initTable work, and then later turn it in a NBHM. With initTable in place, and we go to update a Table, we would only have to do a put without the conditional instantiation.;;;","25/Jan/10 04:43;jmhodges;bq. Not if we do the initTable work, and then later turn it in a NBHM. With initTable in place, and we go to update a Table, we would only have to do a put without the conditional instantiation.

(And it seems to me that if it's possible to construct a Table in more than one thread in our solution to CASSANDRA-44, we're very likely solving CASSANDRA-44 wrong.);;;","25/Jan/10 09:15;jbellis;> How does a NBHM solve the problem get-and-then-put-but-only-instantiate-the-object-at-all-if-get-is-null?

It doesn't: it solves the problem of doing get() on a thread-unsafe object while remaining high performance.  I'm saying, we can use Table.open in close to its current form by replacing the current HashMap w/ a NBHM, and continuing to use a synchronized block for if the get() is null.

> Not if we do the initTable work, and then later turn it in a NBHM.

True enough, but is that then really simpler than just fixing Table.open?;;;","25/Jan/10 10:54;jmhodges;bq. It doesn't: it solves the problem of doing get() on a thread-unsafe object while remaining high performance. I'm saying, we can use Table.open in close to its current form by replacing the current HashMap w/ a NBHM, and continuing to use a synchronized block for if the get() is null. 

You've forgotten about instantiating the Table twice. One thread notices that the get is null and in another thread the same happens before the first thread manages to do a put.;;;","25/Jan/10 11:12;jbellis;that's why you have to do the second check once you synchronize.  it's a double-checked locking variant, using NBHM to provide thread safety on the initial get() [like you would with volatile, in standard non-broken DCL]

patch attached since i'm clearly not explaining this very well :);;;","25/Jan/10 12:49;jmhodges;Works for me. Must have missed the last of your sentence.;;;","26/Jan/10 00:55;jbellis;committed to 0.5 branch (for 0.5.1) and trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception after about 2000 inserts,CASSANDRA-151,12424834,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,nk11,nk11,08/May/09 01:13,16/Apr/19 17:33,22/Mar/23 14:57,11/May/09 22:54,,,,,0,,,,,,"Wtih this client code and default configuration:

int max = 100000;
Random random = new Random();
for (int a = 0; a < max; a  ) {
     System.out.println(a);
     client.insert(""Table1"", ""k1:""   random.nextInt(Integer.MAX_VALUE), ""Super1:x"", new byte[] { (byte) 1 }, 0);
}

I get after about 2000 inserts

DEBUG [pool-1-thread-1] 2009-05-07 20:04:30,942 StorageProxy.java (line 120) insert writing key k1:1355213513 to [127.0.0.1:7000]
ERROR [ROW-MUTATION-STAGE:4] 2009-05-07 20:04:30,942 RowMutationVerbHandler.java (line 99) Error in row mutation
java.io.EOFException
	at java.io.DataInputStream.readInt(Unknown Source)
	at org.apache.cassandra.db.SuperColumnSerializer.fillSuperColumn(SuperColumn.java:368)
	at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:349)
	at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:314)
	at org.apache.cassandra.db.ColumnFamily$ColumnFamilySerializer.deserialize(ColumnFamily.java:515)
	at org.apache.cassandra.db.ColumnFamily$ColumnFamilySerializer.deserialize(ColumnFamily.java:455)
	at org.apache.cassandra.db.RowMutationSerializer.defreezeTheMaps(RowMutation.java:374)
	at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:384)
	at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:337)
	at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:69)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:46)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/May/09 04:06;jbellis;151.patch;https://issues.apache.org/jira/secure/attachment/12407571/151.patch",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19570,,,Wed May 13 09:26:39 UTC 2009,,,,,,,,,,"0|i0fx6n:",90974,,,,,Normal,,,,,,,,,,,,,,,,,"08/May/09 01:17;nk11;correction, I get it after much less, but at around 2000 inserts it freezes;;;","08/May/09 01:27;jbellis;you need to specify three values for the column ""path"" in a supercolumn insert -- ColumnFamily:SuperColumn:subcolumn.  you are only specifying two (Super1:x).

I will add a check for this on the server.;;;","08/May/09 02:25;nk11;right... that was the exception, my fault.
the freeze is still there after the fix unfortunately. no exception in the logs this time;;;","08/May/09 03:07;jbellis;the freeze is probably memory pressure like I said in IRC.  reduce your memtable settings to flush more often, or increase -Xmx in bin/cassandra.in. 

but first you should upgrade, i noticed your client code was out of date.;;;","08/May/09 03:30;nk11;you were right, lowering the MemtableSizeInMB did it.;;;","08/May/09 04:06;jbellis;more robust parameter checking for get_column;;;","08/May/09 06:52;junrao;Patch looks good to me.
;;;","11/May/09 22:54;jbellis;committed;;;","13/May/09 17:26;hudson;Integrated in Cassandra #73 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/73/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Indexes: Auto-generating the CFname may collide with user-generated names,CASSANDRA-1761,12480462,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jhermes,jhermes,jhermes,20/Nov/10 06:51,16/Apr/19 17:33,22/Mar/23 14:57,16/Mar/11 00:27,0.7.5,,,,0,,,,,,"{noformat}column_families:
  - name: CF
    comparator: BytesType
    column_metadata: 
      - name: foo
        index_name: 626172
        index_type: KEYS
      - name: bar
        index_type: KEYS{noformat}

Auto-generated versus user-supplied names collide in the YAML above. The code:
{code}cfname = parentCf + ""."" + (info.getIndexName() == null ? FBUtilities.bytesToHex(info.name) : info.getIndexName()){code}

From the first ColumnDefinition, we create cfname = ""CF.626172"" (from the fail clause of the ternany, user-supplied name)
From the second ColumnDefinition, we create cfname = ""CF.626172"" (from the pass clause of the ternary, we generate the name)

They're in hex form. This is possible, but fairly unlikely that someone will do this.",,,,,,,,,,,,,,,,,,,,,,,57600,57600,,0%,57600,57600,,,,,,,,,,,,,,,,,,,,"15/Mar/11 08:11;jhermes;1761-0.7.txt;https://issues.apache.org/jira/secure/attachment/12473640/1761-0.7.txt","15/Mar/11 08:00;jhermes;1761.txt;https://issues.apache.org/jira/secure/attachment/12473638/1761.txt","15/Mar/11 08:03;jhermes;repro.cli;https://issues.apache.org/jira/secure/attachment/12473639/repro.cli",,,,,,,,,,,,3.0,jhermes,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20298,,,Tue Mar 15 16:46:45 UTC 2011,,,,,,,,,,"0|i0g787:",92601,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"20/Nov/10 08:05;jhermes;CF.bar -> CF.626172.;;;","17/Dec/10 06:33;jbellis;This will require some re-organization to solve.  Currently, ColumnDefinition index_name is just copied from CfDef and may be null; final index name is determined by CFMetaData.newIndexMetadata, which has no context as to what other indexes exist in the CF.

What we should do is create the final index name at the CassandraServer level, and validate at that point that it does not cause conflicts with existing ones.  (Also: index_name and index_type fields of CD become final.)

(This lets us return appropriate InvalidRequest exceptions to the client instead of failing with an internal error, too.)

Then by the time we get to addIndex/newIndexMetadata, all we need to do is a final sanity check as a defense against users violating the one-schema-change-at-a-time rule.;;;","15/Mar/11 07:59;jhermes;Because the column_metadata has to be passed in full, it can be fully validated in ThriftValidation of the cf_def in the system_{update,add}_column_family and system_add_keyspace calls.

In related news, I also found out that we never validated CfDefs in system_update_column_family, so I'm surprised to say the least and glad it was found in an innocuous bug instead of something more serious.

As for the one-schema-change-at-a-time rule, this is now enforced by default with validateSchemaAgreement() calls in CassandraServer.;;;","15/Mar/11 08:03;jhermes;Also attaching repro script.

Without patch you should see an error trying to register an MBean twice, and the first index will no longer work.
After patch you should see an IRE.;;;","15/Mar/11 08:11;jhermes;1761.txt is for trunk.
1761-0.7.txt is for 0.7*.

(Slight difference in imports makes it not apply cleanly.);;;","16/Mar/11 00:27;jbellis;committed w/ some improvements:

- added a system test
- fix migration failure to throw InternalError back to client instead of InvalidRequestException (a bug found by the new test)
- use Set instead of List for .contains checks;;;","16/Mar/11 00:46;hudson;Integrated in Cassandra-0.7 #382 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/382/])
    validate index names
patch by jhermes and jbellis for CASSANDRA-1761
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix low-hanging scoping and other issues picked up by findbugs,CASSANDRA-338,12432084,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,euphoria,euphoria,euphoria,04/Aug/09 03:51,16/Apr/19 17:33,22/Mar/23 14:57,09/Dec/09 09:40,0.6,,,,0,,,,,,"Findbugs currently reports 266 bugs against trunk.  Many of these are invalid, but many are genuine.  This issue will involve fixing the genuine ones that are fixable in time for 0.4.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Aug/09 06:47;dehora;CASSANDRA-338-CCE.diff;https://issues.apache.org/jira/secure/attachment/12415901/CASSANDRA-338-CCE.diff","04/Aug/09 03:53;euphoria;cassandra-findbugs-1.diff;https://issues.apache.org/jira/secure/attachment/12415397/cassandra-findbugs-1.diff","04/Aug/09 04:27;euphoria;cassandra-findbugs-2.diff;https://issues.apache.org/jira/secure/attachment/12415405/cassandra-findbugs-2.diff","05/Aug/09 21:12;jbellis;findbugs-2nd-addendum.diff;https://issues.apache.org/jira/secure/attachment/12415599/findbugs-2nd-addendum.diff","05/Aug/09 12:58;euphoria;findbugs-2ndset-1.diff;https://issues.apache.org/jira/secure/attachment/12415576/findbugs-2ndset-1.diff",,,,,,,,,,5.0,euphoria,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19641,,,Wed Dec 09 01:40:27 UTC 2009,,,,,,,,,,"0|i0fybr:",91159,,,,,Low,,,,,,,,,,,,,,,,,"04/Aug/09 03:53;euphoria;Attached patch brings count from 266 -> 250 bugs by fixing some scopes, removing some dead code, and properly implementing Serializable;;;","04/Aug/09 04:07;jbellis;why make Row serializable?;;;","04/Aug/09 04:12;euphoria;Row already implemented the functionality for Serializable, it just didn't declare itself as such.  It should, because ReadResponse is Serializable and contains a Row.  Our serializer implementation for ReadResponse just knows about Row's serializer, but findbugs was compelling me to make Java know about it as well.;;;","04/Aug/09 04:15;jbellis;ah, so the real bug is that ReadResponse implements Serializable, when it doesn't need to (relic of really old FB code);;;","04/Aug/09 04:27;euphoria;Incorporates jbellis's more rational way of solving the Row serialization problem.;;;","04/Aug/09 04:32;jbellis;committed;;;","04/Aug/09 20:35;hudson;Integrated in Cassandra #157 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/157/])
    brings findbugs count from 266 -> 250 bugs by fixing some scopes, removing some dead code, and not implementing Serializable unnecessarily.  patch by Michael Greene; reviewed for  by jbellis
;;;","05/Aug/09 04:05;jbellis;(does that resolve this issue?);;;","05/Aug/09 04:21;euphoria;Well, there are plenty of other ones to pick off in the report.  I'd rather keep this open until we're planning on wrapping up 0.4 rather than open up new issues for each set.  I have 2 more off the 250 in a local branch.;;;","05/Aug/09 12:58;euphoria;This is a second set of fixes to potential problems caught by findbugs.  Brings bug count from 250 -> 239.

For what it's worth, about 150 of the remaining are out of our control through ANTLR or Thrift.;;;","05/Aug/09 21:12;jbellis;addendup to patch 2:

instead of adding hashcode to classes that override equals unnecessarily, r/m the equals method.

instead of adding null checks to variables that Should Never Be Null, add assert != null.

look ok?;;;","05/Aug/09 21:19;euphoria;Yes in both cases if the presumptions are true.;;;","05/Aug/09 21:40;jbellis;committed;;;","06/Aug/09 21:05;hudson;Integrated in Cassandra #159 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/159/])
    fix more findbugs complaints.  patch by Michael Greene; reviewed by jbellis for 
;;;","08/Aug/09 06:43;dehora;Fixes two bad classcasts in StorageService. But I'm wondering if this hasn't blown up to date whether the path ever gets executed?;;;","08/Aug/09 06:47;dehora;Fixes two bad classcasts in StorageService.;;;","08/Aug/09 07:27;jbellis;applied (minus the unit tests -- the effort is appreciated, but it doesn't actually exercise any code in StorageService), thanks!

> I'm wondering if this hasn't blown up to date whether the path ever gets executed? 

It isn't.  Sandeep, is that code fragment (StorageService.forceHandoff) going to be useful at all for CASSANDRA-195?  If not let's nuke it.;;;","08/Aug/09 20:35;hudson;Integrated in Cassandra #161 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/161/])
    fix broken casts.  patch by Bill de hOra; reviewed by jbellis for 
;;;","09/Aug/09 10:49;jbellis;removing 0.4 tag since it's more open-ended than that;;;","10/Aug/09 02:20;eribeiro;Nice to know that findbugs is being used to check the code base cleanness. I've posted and patched some issues discovered by findbugs in the past.;;;","10/Nov/09 02:56;jbellis;should this be closed?;;;","10/Nov/09 04:00;euphoria;I have another round of a few that we can add in, but after that I
think it should.  There are always going to be small things to fix
that can be found by static analysis, but most of the things it picks
up now are out of our control.

;;;","10/Nov/09 04:04;jbellis;ok, moving to release-after-0.5 so i remember to bug you in that time frame :);;;","09/Dec/09 09:40;jbellis;closing in favor of Stu's more recent findbugs tickets;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OOM intermittently during compaction,CASSANDRA-208,12426691,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jbellis,jihuang,jihuang,30/May/09 00:49,16/Apr/19 17:33,22/Mar/23 14:57,10/Jun/09 01:47,0.4,,,,0,,,,,,"jvm crashes intermittently during compaction. Our test data set is not that big, less than 10 GB.
When jvm is about to crash, we see that it consumes a lot of memory (exceeding the max heap size).

The excessive memory usage during compaction is caused by the maintenance of blockIndexes_ in SSTable. this blockIndexes_ was only introduced to the apache version.","arch: x86_64
os: Linux version 2.6.18-92.1.22.el5 
java: nio2-ea-bin-b99-linux-x64-05_feb_2009
",bendiken,johanoskarsson,lenn0x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Jun/09 11:19;jbellis;0004-fix-ColumnReader-add-tests.patch;https://issues.apache.org/jira/secure/attachment/12409922/0004-fix-ColumnReader-add-tests.patch","06/Jun/09 01:11;jbellis;0005-fix-test-order-dependent-failures.patch;https://issues.apache.org/jira/secure/attachment/12410003/0005-fix-test-order-dependent-failures.patch","09/Jun/09 05:56;jbellis;208-2.patch;https://issues.apache.org/jira/secure/attachment/12410180/208-2.patch","09/Jun/09 23:39;jbellis;208-3.patch;https://issues.apache.org/jira/secure/attachment/12410228/208-3.patch","09/Jun/09 05:55;jbellis;208.patch;https://issues.apache.org/jira/secure/attachment/12410179/208.patch","05/Jun/09 06:30;jbellis;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-208-cleanup.txt;https://issues.apache.org/jira/secure/attachment/12409910/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-208-cleanup.txt","05/Jun/09 06:30;jbellis;ASF.LICENSE.NOT.GRANTED--0002-r-m-touch.txt;https://issues.apache.org/jira/secure/attachment/12409911/ASF.LICENSE.NOT.GRANTED--0002-r-m-touch.txt","05/Jun/09 06:30;jbellis;ASF.LICENSE.NOT.GRANTED--0003-split-sstable-into-data-index-and-bloom-filter-files.txt;https://issues.apache.org/jira/secure/attachment/12409912/ASF.LICENSE.NOT.GRANTED--0003-split-sstable-into-data-index-and-bloom-filter-files.txt",,,,,,,8.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19594,,,Wed Jun 10 13:12:06 UTC 2009,,,,,,,,,,"0|i0fxj3:",91030,,,,,Critical,,,,,,,,,,,,,,,,,"30/May/09 01:31;junrao;In your data set, do you have a large row? Today, during compaction, a full row may have to be buffered in memory. See https://issues.apache.org/jira/browse/CASSANDRA-16 for more on this.;;;","30/May/09 03:43;jihuang;we don't have a large row, definitely not to a size which cannot fit in memory.;;;","30/May/09 07:25;jbellis;blockindexes_ keeps the indexes in memory as the flush or compaction is performed, so they can be dumped at the end of the SSTable file.

The old code on code.google dumped each index at the end of its block which avoided this problem (only one block's index would ever be in memory at once).  I'm not sure why FB changed this behavior.  The only reason I can think of is that reading the index later won't require as many seeks, but this seems like a bad optimization to make since index reading is done once per sstable.

I couldn't tell at first why this should cause OOM -- in both the google and apache versions we have indexMetadataMap_ which contains the same block index info, right?  Wrong, the difference is that iMM only contains the first entry from each block index.  So much less memory is used there.

I guess we should switch back to the old, interleaved block indexing method.;;;","02/Jun/09 07:22;jbellis;Or, how about this: we just stop storing multiple kinds of data in the SSTable file, and instead store the index entries and the bloom filter in separate on-disk files.

Gains (vs interleaving):
 - simplicity (don't have to skip non-data keys, EOF is really EOF)
 - no more hard-coded ""keys"" in the sstable that will result in very weird-ass bugs if a client every uses one
 - behaves more like the FS cache expects since each section (index, data, bloom) is homogeneous and the ""if I read block A, I'm more likely to need the next block"" assumption holds more often
 - retains goal of loading index on server start w/o seeking

Losses:
 - some seeking during flush/compaction to switch between writing data blocks and index blocks

Although the ""no seeks on writes, at all"" claim is a cool one, in practice the amount of seeks we'll be doing is still negligible when buffering is done, i.e., still a huge win over traditional B-tree design where every update requires a seek.

Thoughts?;;;","02/Jun/09 07:24;jbellis;IMO we need to address the hard-coded-""keys"" problem sooner or later, and having separate files is really the only sane solution.;;;","02/Jun/09 09:22;junrao;Even if we take out the row index and BF, data is still mixed with column index.

Interestingly, HBase's sstable format started with MapFile (1 file for index, 1 file for BF and 1 file for data) and has recently moved to the TFile (a spec can be found at https://issues.apache.org/jira/browse/HADOOP-3315) format (1 file for everything). Although HBase probably did that mainly for reducing # of files (problem for HDFS master), it's probably worth while to take a look at their new design (https://issues.apache.org/jira/browse/HBASE-61).;;;","02/Jun/09 09:41;jbellis;Thanks for the links, that is interesting context.;;;","02/Jun/09 10:06;jbellis;Another related link: http://wiki.apache.org/hadoop/Hbase/NewFileFormat

My take is that the designs are different enough that their reasons for moving to a single file don't really apply to cassandra.

 - the old MapFile has a bunch of properties that make it general enough for Hadoop core but inefficient for hbase (e.g. storing the CF name once per key, keys appearing multiple times in the index)
 - they only index one key per block, so their index is much much smaller than ours, and they can get away with storing the index at the end of the file as cassandra currently does

> Even if we take out the row index and BF, data is still mixed with column index. 

Not at the SSTable key/value level.  To sstable the value is just byte[] so the fact that CF serializes with indexes is an implementation detail.  (To the degree that SSTable or SF does care, that is an encapsulation violation -- one of the reasons this code is one of the less pleasant parts of cassandra to work in.)

I will get a patch together that will implement the file splitting I proposed and we will see how that looks.  I think that's going to get us to a stable 0.3 fastest; if we want to radically re-think how indexing works (so we can go back to index-at-the-end-of-one-file) then I think that is a change to make in 0.4.  (The non-sparse index Cassandra uses may be necessary if you want to support large CF rows, or you will waste too much time scanning through those rows looking for keys when you only get to within 128 keys from the index.)

One thing that piqued my curiosity: what is the hbase ""row index?""  Looks like their ""key index"" is like our sstable indexes (with the difference mentioned above, that it only indexes one key per block).;;;","02/Jun/09 23:34;junrao;When HBase was using the old format, rows are stored as <key,value> pairs in MapFile, where a key is rowkey:columnName:timestamp. The keys for every 128 rows are promoted to the row index. The benefit is that it's simple. There is only a single-level index (compared with row index and column index within a row in cassandra) and it can be used to efficiently look up a full row, a column within a row, or a version of a column in a row. On the other hand, if you make the index dense, the row keys are duplicated for columns within the same row.;;;","04/Jun/09 05:39;jbellis;03
    split sstable into data, index, and bloom filter files

02
    r/m 'touch' code that populates a cache that is never used (and never updated on compaction either, so it's buggy too)

01
    cleanup.  mostly just moving code around so its position makes more sense to me
;;;","04/Jun/09 07:03;junrao;can't apply patch to trunk. could you rebase?
;;;","04/Jun/09 07:35;junrao;just realize this is off 0.3 branch, never mind.;;;","04/Jun/09 23:51;jbellis;Jiansheng: can you give us more details about your data set?  We are trying to figure out how likely it is for others to run into this, leaning towards moving this patch to trunk/0.4.

The relevant data are
 - how many keys in the CF that fails to compact
 - average / max row size (column size x count) in that CF
 - jvm memory settings, and how much of the heap is free (according to e.g. jconsole) before compaction starts;;;","05/Jun/09 01:59;junrao;Just browsed through the patch for now and found the following bug.

In SSTable.afterAppend, the position added to indexEntries should be the position in the index file, not the position in the data file.
;;;","05/Jun/09 02:14;jbellis;Good catch.

I will add a test for this in the version for trunk.;;;","05/Jun/09 02:24;jihuang;To answer Jonathan's questions: (1) we have tested around several million keys, (2) avg row size ~ 2k, max row size ~ 10k. (3) tried several Xmx settings, max tried is 5G. Before compaction starts, pretty much most of the heap is free. I think the problem is easy to run into if the system is run with continuous traffic for over a week or so given that our test dataset has been fairly small.;;;","05/Jun/09 02:53;junrao;Jiansheng, what's the average key size of a row?
;;;","05/Jun/09 03:18;jihuang;Jun, our key size is about 20 characters.;;;","05/Jun/09 04:18;junrao;Hmm, with a key size of 40 bytes, even with 10 million keys, the space needed for all row index entries is about 400MB. That's way smaller than 5G. So, not sure whether the new SSTable format really solves this particular OOME problem.
;;;","05/Jun/09 04:54;jbellis;You're forgetting the BlockMetadata longs (extra 16 bytes per key) and Map overhead (32 bytes per key or more), which would be 880MB for 10M keys.

Still smaller than 5G, though.  I guess we can get Jiansheng to test after applying the patch. :);;;","05/Jun/09 06:31;jbellis;removed patches against 0.3 (to avoid confusion, i still have copies should they become necessary).

added patches against trunk.;;;","05/Jun/09 07:07;jihuang;Ok, then scale up the number of keys by 5 times, you will run into the problem. :);;;","05/Jun/09 07:35;junrao;Some comments:
1. In SSTable.loadIndexFile, indexEntries could be empty if there are fewer than indexInterval keys. I don't see code dealing with empty indexEntries. A simple solution is to add the last index key to indexEntries when indexEntries is empty. Similarly, you have to fix the same case when an SSTable is first created. You have to remember the last index key being appended.

2. In SSTable.next, need to deal with the case that getPosition() returns -1. 

3. In SSTable.getColumnGroupReader, change getNearestPostion() to getPostion(). Similar to 2, need to deal with returned position being -1.


Also, we need to remember opening another issue to get rid of partitioner from KeyPosition.

;;;","05/Jun/09 09:08;jbellis;1. empty memtables are not flushed, so there will be at least one key, and the first key will always be in indexEntries since 0 % interval == 0 for any interval.  lots of the tests have only one row.  I don't think there is a problem here.

2. ok

3. Hmm, if you have columns B C and D in a row, and you do a slice_from starting with A, don't you want B C D to be returned?  getNearestPosition will start the scan at B, but vanilla getPosition will return -1 if there is not an exact match.;;;","05/Jun/09 09:23;junrao;3. That's true, but at the column level. At the row level, get_slice_from only cares about exact row match.;;;","05/Jun/09 09:26;jbellis;Ah, thanks.  I got mixed up. :);;;","05/Jun/09 11:19;jbellis;2. is handled by FileReader.next being a no-op if position == -1

3. attached 04 patch to fix, and add more tests.  Also, makes sliceFrom able to handle """" as start column.;;;","05/Jun/09 23:47;junrao;Were you able to run unit tests successfully? I saw a bunch of failures in tests like CompactionsTest and NameSortTest.
;;;","06/Jun/09 01:11;jbellis;I did have NameSortTest and TimeSortTest failing because they were extending CFST and hence running those tests as well, which caused problems since those sets of tests are supposed to be run independently.  Making NST and TST not inherit from CFST (which was only done for convenience) fixes that.   Here is the patch.

I haven't seen compactions et al ever fail, no.;;;","06/Jun/09 02:03;junrao;My ant test succeeds on linux, but fails under cygwin on windows. Here are some of the failures that I get. Those tests succeed on windows without your patch.

 [junit] Testcase: testCompactions(org.apache.cassandra.db.CompactionsTest): FAILED
 [junit] attempted to delete non-existing file Table1-Standard1-14-Filter.db
 [junit] junit.framework.AssertionFailedError: attempted to delete non-existing file Table1-Standard1-14-Filter.db
 [junit]     at org.apache.cassandra.io.SSTable.deleteWithConfirm(SSTable.java:103)
 [junit]     at org.apache.cassandra.io.SSTable.delete(SSTable.java:98)
 [junit]     at org.apache.cassandra.db.ColumnFamilyStore.doFileCompaction(ColumnFamilyStore.java:1373)
 [junit]     at org.apache.cassandra.db.ColumnFamilyStore.doCompaction(ColumnFamilyStore.java:849)
 [junit]     at org.apache.cassandra.db.CompactionsTest.testCompactions(CompactionsTest.java:63)

[junit] Testcase: testNameSort10(org.apache.cassandra.db.NameSortTest):     FAILED
[junit] null
[junit] junit.framework.AssertionFailedError
[junit]     at org.apache.cassandra.db.NameSortTest.validateNameSort(NameSortTest.java:111)
[junit]     at org.apache.cassandra.db.NameSortTest.testNameSort(NameSortTest.java:89)
[junit]     at org.apache.cassandra.db.NameSortTest.testNameSort10(NameSortTest.java:43);;;","09/Jun/09 00:17;jbellis;do those tests pass after applying just 01 and 02?;;;","09/Jun/09 01:50;junrao;The tests pass with just 01 and 02. They start failing with 03.;;;","09/Jun/09 03:45;jbellis;committed 01 to avoid rebase hell.  (02 was merged via CASSANDRA-222 from 0.3.);;;","09/Jun/09 05:55;jbellis;208.patch integrates the old 03, 04, and 05 for convenience;;;","09/Jun/09 05:56;jbellis;208-2 fixes syncing of the bloom filter files.  this fixes the compactions failure and all the others except timesort, which is not actually a new bug.

CASSANDRA-223 shows that the timesort failure is caused by a bug in TimeFilter that dates back to the FB import.;;;","09/Jun/09 06:25;junrao;Ok, now all tests pass except timesort.
;;;","09/Jun/09 20:34;hudson;Integrated in Cassandra #103 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/103/])
    cleanup SSTable-related code.  patch by jbellis; reviewed by Jun Rao for 
;;;","09/Jun/09 23:39;jbellis;208-3 patches TST to only test what it was intended to test, not CASSANDRA-223.;;;","10/Jun/09 01:21;junrao;Patch 208-3 looks fine. All tests pass.;;;","10/Jun/09 01:47;jbellis;committed 208 and 208-2 as a single commit, and 208-3 separately.

jiansheng, does this resolve your OOM problems?;;;","10/Jun/09 21:12;hudson;Integrated in Cassandra #104 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/104/])
    apply rows atomically, rather than one-column-at-a-time.  this avoids exposing the bug in time-sorted
columns discussed in #223.
patch by jbellis; reviewed by Jun Rao for 
split sstable into data, index, and bloom filter files.  this allows us to avoid saving up index chunks
in memory until the sstable data is completely written, while still allowing fast scanning of the index
on startup.  it also simplifies the sstable/sequencefile code considerably.
patch by jbellis; reviewed by Jun Rao for  
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Column family deletion time is not always reseted after gc_grace,CASSANDRA-2317,12501241,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,12/Mar/11 19:19,16/Apr/19 17:33,22/Mar/23 14:57,04/Jul/11 22:36,1.0.0,,,,0,,,,,,"Follow up of CASSANDRA-2305.
Reproducible (thanks to Jeffrey Wang) by: 

Create a CF with gc_grace_seconds = 0 and no row cache.
Insert row X, col A with timestamp 0.
Insert row X, col B with timestamp 2.
Remove row X with timestamp 1 (expect col A to disappear, col B to stay).
Wait 1 second.
Force flush and compaction.
Insert row X, col A with timestamp 0.
Read row X, col A (see nothing).",,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,"14/Mar/11 21:47;slebresne;0001-Add-AbstractColumnContainer-to-factor-common-parts-o.patch;https://issues.apache.org/jira/secure/attachment/12473561/0001-Add-AbstractColumnContainer-to-factor-common-parts-o.patch","14/Mar/11 21:47;slebresne;0002-Add-unit-test.patch;https://issues.apache.org/jira/secure/attachment/12473562/0002-Add-unit-test.patch","14/Mar/11 21:47;slebresne;0003-Reset-CF-and-SC-deletion-time-after-compaction.patch;https://issues.apache.org/jira/secure/attachment/12473563/0003-Reset-CF-and-SC-deletion-time-after-compaction.patch",,,,,,,,,,,,3.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20558,,,Mon Jul 04 15:20:32 UTC 2011,,,,,,,,,,"0|i0ganz:",93158,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"12/Mar/11 19:20;slebresne;Adding patch against 0.7.

First patch is a unit test to reproduce the failure, patch 2 is the fix.;;;","12/Mar/11 19:54;slebresne;I realize this fix don't take the cache into account. I'll attach an updated patch asap.;;;","12/Mar/11 21:12;jbellis;How do you have CF objects around at all post-purge?;;;","14/Mar/11 21:47;slebresne;bq. How do you have CF objects around at all post-purge?

The problem is actually with cf objects that don't get fully purged. Those still retain their markedForDeleteAt and localDeletionTime after compaction even though it could be way past gc_grace. That is, a deletion can easily live way past gc_grace + compaction.

As it turns out, super columns also suffers for the same problem.

Because it felt a bit annoying to have to fix the problem in 2 places, and because that's not the first time that happens, the first attached is a refactoring one, that introduces an AbstractColumnContainer class that factor common code to ColumnFamily and SuperColumn.

The second patch introduces unit tests for column family and super columns and third patch is the fix. It introduces a new structure to hold both markedDeletedAt and localDeletionTime so that we are able to set both of those together atomically. This is necessary for the second part of CASSANDRA-2305.  I think that anyhow it was not fully correct to update them non atomically.

Note that the third patch depends on the patch for CASSANDRA-2279.

All patches are against 0.7.
;;;","14/Mar/11 22:27;jbellis;Oh, so the problem is that we're not actually losing information (when a CF contains non-deleted data) that we may lose otherwise?

Is that really worth adding a bunch of complexity to ""fix?"";;;","14/Mar/11 23:01;slebresne;This is clearly not of big importance.

I however think that we should fix this for coherence sake. Tombstones have side effects on client queries and tombstone collection too (it's not just an internal detail). As such, we should make the behavior as coherent as possible. That a deletion always has effect until the first compaction after gc_grace would be more coherent that the current status quo. The fact that Jeffrey was surprised and that we assume right away that it was a bug proves that, I think.

And the fix is actually fairly simple, even though my patch doesn't do it justice. The first patch is really just a refactoring that I think should have been done a long time ago. I'm happy to create a ticket for this specifically if we prefer, but I just think it is stupid to not factor that code.

The second part of the patch is to put markedDeletedAt and localDeletionTime in a common structure that we CAS for changes. Again, this is because right now I think we have a race condition when updating those two values. That is, we could end up with the markedDeleteAt of a given operation but the localDeletionTime of another one. This could also be put on another ticket (I just tend to be lazy so I've put everything here, sorry).

Once those are done, the fix for this specific ticket is really just the maybeResetDeletionTime() function.;;;","14/Jun/11 03:16;jbellis;doesn't this mean that for a CF w/ no tombstone, we create a new deletioninfo every call to maybeReset?

bq. if (current.localDeletionTime > gcBefore || deletionInfo.compareAndSet(current, new DeletionInfo()))

otherwise, +1 for trunk.;;;","23/Jun/11 06:33;paladin8;Does anyone know whether this is fixed in 0.8? We are thinking of upgrading soon, but I don't want to try to apply the 0.7 patch to 0.8...;;;","23/Jun/11 07:19;jbellis;""Unresolved"" means not fixed anywhere yet.;;;","04/Jul/11 22:37;slebresne;Committed to trunk (as I agree this should really go there).

bq. doesn't this mean that for a CF w/ no tombstone, we create a new deletioninfo every call to maybeReset?

You're right, I've included a current.localDeletionTime == Integer.MIN_VALUE in the condition to escape early in that case.;;;","04/Jul/11 23:20;hudson;Integrated in Cassandra #948 (See [https://builds.apache.org/job/Cassandra/948/])
    Reset CF and SC deletion time after gc_grace
patch by slebresne; reviewed by jbellis for CASSANDRA-2317

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1142690
Files : 
* /cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamily.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/filter/QueryFilter.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/RowMutation.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/SuperColumn.java
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/test/unit/org/apache/cassandra/service/RowResolverTest.java
* /cassandra/trunk/test/unit/org/apache/cassandra/db/RowTest.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/IColumnContainer.java
* /cassandra/trunk/test/unit/org/apache/cassandra/db/compaction/CompactionsPurgeTest.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/AbstractColumnContainer.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilySerializer.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Running on Windows XP, get ""Incorrect number of parameters: and""",CASSANDRA-307,12430946,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,euphoria,jcarver,jcarver,21/Jul/09 00:46,16/Apr/19 17:33,22/Mar/23 14:57,21/Jul/09 23:23,0.4,,Legacy/Tools,,0,,,,,,"My eclipse workspace is in ""C:\Documents and Settings\Jason\My Documents""

A couple lines in cassandra.bat missed using double-quotes to protect against folders with spaces in the name.",Windows XP,,,,,,,,,,,,,,,,,,,,,,300,300,,0%,300,300,,,,,,,,,,,,,,,,,,,,"21/Jul/09 03:38;euphoria;307_v2.diff;https://issues.apache.org/jira/secure/attachment/12414029/307_v2.diff",,,,,,,,,,,,,,1.0,euphoria,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19627,,,Tue Jul 21 15:23:23 UTC 2009,,,,,,,,,,"0|i0fy4v:",91128,,,,,Low,,,,,,,,,,,,,,,,,"21/Jul/09 00:47;jcarver;This fixes two lines that need double quotes on Windows XP:

from: svn diff bin\cassandra.bat


Index: bin/cassandra.bat
===================================================================
--- bin/cassandra.bat   (revision 795691)
+++ bin/cassandra.bat   (working copy)
@@ -45,7 +45,7 @@
 REM ***** CLASSPATH library setting *****

 REM Shorten lib path for old platforms
-subst P: %CASSANDRA_HOME%\lib
+subst P: ""%CASSANDRA_HOME%\lib""
 P:
 set CLASSPATH=P:\

@@ -58,7 +58,7 @@

 :okClasspath
 set CASSANDRA_CLASSPATH=%CASSANDRA_HOME%;%CASSANDRA_CONF%;%CLASSPATH%;%CASSANDR
A_HOME%\build\classes
-set CASSANDRA_PARAMS=-Dcassandra -Dstorage-config=%CASSANDRA_CONF%
+set CASSANDRA_PARAMS=-Dcassandra -Dstorage-config=""%CASSANDRA_CONF%""
 goto runDaemon

 :runDaemon
;;;","21/Jul/09 01:49;euphoria;With this patch applied, I still get errors if I put spaces in the Cassandra path, on the loading of the storage conf.  See this issue: http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6506304

Still tracking this down, but for now I'd recommend avoiding spaces in Windows development.;;;","21/Jul/09 02:35;jcarver;Interesting, my JDK was old (1.6.0_05) so I couldn't reproduce the problem.  Perhaps the workaround on the link you sent me:

=====
Submitted On 09-OCT-2008
David.F

I had a line with same exception :
		
          document = builder.parse(translationFileName);
		
I succeed to get rid of error with 'new File(...)':
		
          document = builder.parse(new File(translationFileName));
=====


Applied like:

Index: src/java/org/apache/cassandra/utils/XMLUtils.java
===================================================================
--- src/java/org/apache/cassandra/utils/XMLUtils.java   (revision 795691)
+++ src/java/org/apache/cassandra/utils/XMLUtils.java   (working copy)
@@ -43,8 +43,13 @@
     {
         DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();
         DocumentBuilder db = dbf.newDocumentBuilder();
-        document_ = db.parse(xmlSrc);

+        //Prevent issue where spaces break xml source path on JDK 6/Windows
+        //     by wrapping xml path in a File obj
+        File xmlFile = new File(xmlSrc);
+
+        document_ = db.parse(xmlFile);
+
         XPathFactory xpathFactory = XPathFactory.newInstance();
         xpath_ = xpathFactory.newXPath();
     };;;","21/Jul/09 03:17;euphoria;This was the path I was going down.  I asked this on IRC but haven't received a response yet.

<michaelgreene> Currently we parse the storage file location as a URI
<michaelgreene> but parse(File) is another option and will fix 307
<michaelgreene> anyone need the URI version for some reason?;;;","21/Jul/09 03:38;euphoria;Looks like we can use a File just fine.

Attached patch works from the following path and is a consolidation of the code posted so far in this issue: E:\space path\cassandra-307>bin\cassandra.bat;;;","21/Jul/09 23:23;urandom;committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mutation replies are not correctly deserialized by originator,CASSANDRA-120,12424213,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jbellis,jbellis,jbellis,30/Apr/09 04:23,16/Apr/19 17:33,22/Mar/23 14:57,05/May/09 03:05,0.3,,,,0,,,,,,"these lines in WriteResponseResolver

			Object[] body = response.getMessageBody();
			WriteResponse writeResponse = (WriteResponse) body[0];

cause this exception

java.lang.ClassCastExceptionException: [B cannot be cast to org.apache.cassandra.db.WriteResponse
	at org.apache.cassandra.service.WriteResponseResolver.resolve(WriteResponseResolver.java:50)
	at org.apache.cassandra.service.WriteResponseResolver.resolve(WriteResponseResolver.java:31)
	at org.apache.cassandra.service.QuorumResponseHandler.get(QuorumResponseHandler.java:101)
	at org.apache.cassandra.service.StorageProxy.insertBlocking(StorageProxy.java:132)

because of course only byte[] is sent over the wire",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-34,,,,,,,,,,,,,,,"01/May/09 03:58;jbellis;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-120-demonstrate-the-problem-by-checking-inse.txt;https://issues.apache.org/jira/secure/attachment/12406946/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-120-demonstrate-the-problem-by-checking-inse.txt","01/May/09 03:58;jbellis;ASF.LICENSE.NOT.GRANTED--0002-add-constructor-to-DTPE-for-most-commonly-used-values.txt;https://issues.apache.org/jira/secure/attachment/12406947/ASF.LICENSE.NOT.GRANTED--0002-add-constructor-to-DTPE-for-most-commonly-used-values.txt","01/May/09 03:58;jbellis;ASF.LICENSE.NOT.GRANTED--0003-refactor-HttpConnection-to-use-its-own-executor-instea.txt;https://issues.apache.org/jira/secure/attachment/12406948/ASF.LICENSE.NOT.GRANTED--0003-refactor-HttpConnection-to-use-its-own-executor-instea.txt","01/May/09 03:58;jbellis;ASF.LICENSE.NOT.GRANTED--0004-r-m-DataImporter-it-s-not-useful-outside-of-FB.txt;https://issues.apache.org/jira/secure/attachment/12406949/ASF.LICENSE.NOT.GRANTED--0004-r-m-DataImporter-it-s-not-useful-outside-of-FB.txt","01/May/09 03:58;jbellis;ASF.LICENSE.NOT.GRANTED--0005-change-Message-body-to-byte-.-this-reveals-where-the.txt;https://issues.apache.org/jira/secure/attachment/12406950/ASF.LICENSE.NOT.GRANTED--0005-change-Message-body-to-byte-.-this-reveals-where-the.txt",,,,,,,,,,5.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19555,,,Tue May 05 13:43:26 UTC 2009,,,,,,,,,,"0|i0fwzz:",90944,,,,,Critical,,,,,,,,,,,,,,,,,"01/May/09 03:59;jbellis;05
    change Message body to byte[].  this reveals where there are problems: everything thatis using any of the send messages already needs to be doing that or they're broken.  the good ones I just unwrap; the broken ones I fixed except for MoveMessage which doesn't even have a byte[] serializer yet, so I left that one broken.  writeResponseResolver is the specific case that caused the bug report.

04
    r/m DataImporter; it's not useful outside of FB

03
    refactor HttpConnection to use its own executor instead of abusing MessagingService.  This will let us refactor Message body to a byte[].

02
    add constructor to DTPE for most commonly used values (single thread, no timeout, LinkedBlockingQueue)

01
    CASSANDRA-120 demonstrate the problem by checking insert return values (see CASSANDRA-102 for how to run the system tests)
;;;","05/May/09 02:31;nk11;Patches seem ok.
Tested also for one node only and error did not reproduce.;;;","05/May/09 03:05;jbellis;thanks for the review!  committed.;;;","05/May/09 21:43;hudson;Integrated in Cassandra #59 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/59/])
    change Message body to byte[].  this reveals where there are problems: everything that is using any of the send messages already needs to be doing that or they're broken.  the good ones I just unwrap; the broken ones I fixed except for MoveMessage which doesn't even have a byte[] serializer yet, so I left that one broken.  writeResponseResolver is the specific case that caused the bug report.
patch by jbellis; reviewed by nk11 for 
r/m DataImporter; it's not useful outside of FB
patch by jbellis; reviewed by nk11 for 
refactor HttpConnection to use its own executor instead of abusing MessagingService.  This will let us refactor Message body to a byte[].
patch by jbellis; reviewed by nk11 for 
add constructor to DTPE for most commonly used values (single thread, no timeout, LinkedBlockingQueue)
patch by jbellis; reviewed by nk11 for 
demonstrate problem with _blocking methods by checking insert return values.
patch by jbellis; reviewed by nk11 for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reading with CL > ONE returns multiple copies of the same column per key.,CASSANDRA-1145,12465813,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jeromatron,ajslater,ajslater,01/Jun/10 07:23,16/Apr/19 17:33,22/Mar/23 14:57,05/Aug/10 22:09,0.6.5,,,,0,,,,,,"Testing with 0.6-trunk today:

Reading with CL > ONE returns multiple copies of the same column per key consistent with the replicas queried before return. i.e, for RC=3, a QUORUM read yields 2 copies and an ALL read returns 3.
This is with pycassa get_range() which is using get_range_slice()

I see the same behavior with 0.6.1 and 0.6.2 debs

If my experience is not unique, anyone using get_range_slice is now deluged with duplicate data.
",ubuntu jaunty,ajslater,anty,cw_krebs,johanoskarsson,joosto,schubertzhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Aug/10 04:44;jeromatron;0001-Added-a-unit-test.patch;https://issues.apache.org/jira/secure/attachment/12451265/0001-Added-a-unit-test.patch","05/Aug/10 07:45;jeromatron;0001-Sort-keys.patch;https://issues.apache.org/jira/secure/attachment/12451281/0001-Sort-keys.patch","05/Aug/10 20:54;jbellis;1145-v2.txt;https://issues.apache.org/jira/secure/attachment/12451327/1145-v2.txt","01/Jun/10 07:28;ajslater;bugtest.py;https://issues.apache.org/jira/secure/attachment/12445975/bugtest.py","05/Aug/10 06:03;messi;demo.patch;https://issues.apache.org/jira/secure/attachment/12451271/demo.patch","01/Jun/10 07:28;ajslater;storage-conf.xml;https://issues.apache.org/jira/secure/attachment/12445976/storage-conf.xml",,,,,,,,,6.0,jeromatron,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20006,,,Thu Aug 05 14:09:20 UTC 2010,,,,,,,,,,"0|i0g39r:",91960,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"01/Jun/10 07:28;ajslater;attached is a storage-conf and example python program. 
it requires the python thrift bindings and pycassa: http://github.com/vomjom/pycassa

usage:  ./bugtest.py [TEXT1 TEXT2..]

arguments on the command line are inserted into cassandra and then range_get is used to get and pretty print them, with timestamps.

it counts the multiple copies of the same rows at the end.;;;","01/Jun/10 07:31;jeromatron;Hi AJ, could you try to replicate it with the latest 0.6 branch just to see if it wasn't fixed by another fix (cassandra-1042)?  Just wondering if it's a the same bug but showing itself in more than one place.

http://svn.apache.org/repos/asf/cassandra/branches/cassandra-0.6/;;;","01/Jun/10 07:34;ajslater;I saw the same behavior with cassandra-0.6 svn revision 949892;;;","01/Jun/10 07:40;jeromatron;Okay yeah - that fix was in revision 948934, so it looks like it's a different issue.  Tx.;;;","17/Jun/10 05:18;joosto;We're getting duplicate rows from get_range_slice when NOT using ConsistencyLevel.ONE.  That is to say, when using QUORUM or ALL, we're getting duplicate rows, but when using ONE, we are not. Is this case misnamed?;;;","17/Jun/10 05:25;jbellis;Joost: :) no, it really is a bug

Jeremy: could you have a look?;;;","07/Jul/10 07:45;jeromatron;Hi AJ, taking another look at this.  I'm getting errors when I try to put ('localhost:9160') in the SERVERS variable in bugtest.py.  Specifically, I'm seeing that pycassa is giving a NoServerAvailable exception when trying to connect.  So I'm unable to reproduce the problem.

Can you see if you can get it to run and hopefully replicate the problem just with the localhost?  I'm not sure if I'm just configuring that incorrectly either. ;;;","07/Jul/10 08:00;ajslater;Looks like pycassa has a problem with tuples with one element, which is another problem. So to fix, use a list:

SERVERS= ['localhost:9160']

Also I ran into your NoServerAvailable problem, using localhost:9160.
Cassandra is likely not really listening on 'localhost:9160', try 'myServerName:9160'  or use the exact IP you specify to thrift in storage-conf.xml
;;;","07/Jul/10 10:18;jeromatron;Sweet sassafras - thanks AJ, that was it - matching the localhost in the SERVERS variable to localhost in the rpc address in storage-conf.xml.  Looking at it more now.;;;","05/Aug/10 04:44;jeromatron;Added a unit test that will expose the problem.;;;","05/Aug/10 06:03;messi;CollatingIterator expects both collections to be sorted. See demo.patch.;;;","05/Aug/10 06:26;jeromatron;Folke - you're right.  I'll focus on the creation of the list of rows.  I had thought we looked at the ordering, but I think since they were in the same order, we didn't think twice.  Good catch.;;;","05/Aug/10 07:45;jeromatron;Adding a fix patch that will sort the keys before getRangeSlice uses them to get their associated rows.  That will keep them sorted when they are sent back to the client, for the CollatingIterator.;;;","05/Aug/10 20:54;jbellis;The order we read them in CFS [token order] is the ""right"" order.  Stomping on that will take as back to CASSANDRA-1042 type bugs.  

Does v2 (attached) fix the problem?;;;","05/Aug/10 21:50;jeromatron;Yes - I wondered if what I had done would affect other things, such as reverting other changes.  Thanks.  I could see where the problem was, but wasn't 100% sure where to make the change.  Sounds like they were sorted correctly wrt to token, which our CollatingIterator needed to take into account.

I ran the same tests again that could formerly reproduce the problem (with several permutations) and it fixes the problem.

I also ran `ant test` and `nosetests` to make sure everything else was happy.

+1 from me.;;;","05/Aug/10 22:09;jbellis;committed, with a slight modification to allow the unit test to pass in a different partitioner than the global one;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing logging for some exceptions,CASSANDRA-2061,12496895,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,stuhood,stuhood,27/Jan/11 12:29,16/Apr/19 17:33,22/Mar/23 14:57,17/Aug/11 04:53,1.0.0,,,,0,,,,,,"{quote}Since you are using ScheduledThreadPoolExecutor.schedule(), the exception was swallowed by the FutureTask.

You will have to perform a get() method on the ScheduledFuture, and you will get ExecutionException if there was any exception occured in run().{quote}",,cburroughs,,,,,,,,,,,,,,,,,,,,,28800,28800,,0%,28800,28800,,,,,,,,,,,,,,,,,,,,"28/Jan/11 23:32;jbellis;2061-0.7.txt;https://issues.apache.org/jira/secure/attachment/12469679/2061-0.7.txt","15/Aug/11 13:01;jbellis;2061-v3.txt;https://issues.apache.org/jira/secure/attachment/12490402/2061-v3.txt","28/Jan/11 23:33;jbellis;2061.txt;https://issues.apache.org/jira/secure/attachment/12469681/2061.txt",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20421,,,Tue Aug 16 21:23:12 UTC 2011,,,,,,,,,,"0|i0g933:",92902,,xedin,,xedin,Low,,,,,,,,,,,,,,,,,"27/Jan/11 23:36;jbellis;As near as I can tell, an exception thrown on a scheduled task will never kill the executor, just like in TPE.  I don't remember why the author [me] wrote that code -- probably because it was replacing Timer and TimerTask, and an uncaught exception in a TimerTask _will_ kill the timer.

Patch removes RetryingSTPE and replaces with DebuggableSTPE that has an afterExecute copied from DTPE.;;;","27/Jan/11 23:44;jbellis;new patch also updates afterExecute in both classes to log error if default uncaught exception handler is null;;;","27/Jan/11 23:46;jbellis;patch for 0.7;;;","28/Jan/11 04:44;stuhood;* 2061.txt doesn't completely remove RetryingSTPE.java, and doesn't replace the usage in CFStore
* 2061-0.7.txt doesn't apply to the 0.7 branch

Also, will we need a separate patch for trunk?;;;","28/Jan/11 23:27;jbellis;bq. 2061.txt doesn't completely remove RetryingSTPE.java

that's just how svn diff works.

bq. and doesn't replace the usage in CFStore

fixed.
;;;","28/Jan/11 23:32;jbellis;bq. 2061-0.7.txt doesn't apply to the 0.7 branch

fixed.  also applies to trunk.;;;","31/Jan/11 13:03;stuhood;Based on anecdotal evidence (it exposed an exception I was expecting), this looks good. But it looks like we can probably merge Debuggable(Scheduled)ThreadPool... they are appear to be essentially identical now.;;;","31/Jan/11 13:09;jbellis;From the STPE javadoc, it sounds like STPE is more heavyweight than TPE and you don't want to use the former when all you need API-wise is the latter.  I have not done the code diving to confirm this though.;;;","31/Jan/11 13:12;jbellis;STPE also notes,

bq. While this class inherits from ThreadPoolExecutor, a few of the inherited tuning methods are not useful for it. In particular, because it acts as a fixed-sized pool using corePoolSize threads and an unbounded queue, adjustments to maximumPoolSize have no useful effect.

We've wanted bounded queues in the past, and we definitely still use growable pools in places, so that's another reason to keep both.;;;","04/Feb/11 01:42;jbellis;committed;;;","04/Feb/11 04:04;jbellis;reverted because of DynamicEndpointSnitchTest failure.  Not sure what is going on there -- I suspect some scheduled task is taking too long and keeping the DES update from happening, but why that should be affected by this patch is obscure to me.;;;","04/Feb/11 04:16;hudson;Integrated in Cassandra-0.7 #243 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/243/])
    ;;;","15/Aug/11 13:00;jbellis;Figured out the problem.  Here's the new version of logExceptionsAfterExecute that fixes it:

{code}
     public static void logExceptionsAfterExecute(Runnable r, Throwable t)
     {
-        // exceptions wrapped by FutureTask
-        if (r instanceof FutureTask<?>)
+        // Check for exceptions wrapped by FutureTask.  We do this by calling get(), which will
+        // cause it to throw any saved exception.
+        //
+        // Complicating things, calling get() on a ScheduledFutureTask will block until the task
+        // is cancelled.  Hence, the extra isDone check beforehand.
+        if ((r instanceof Future<?>) && ((Future<?>) r).isDone())
         {
             try
             {
-                ((FutureTask<?>) r).get();
+                ((Future<?>) r).get();
             }
{code};;;","15/Aug/11 13:01;jbellis;v3 attached for trunk.;;;","17/Aug/11 04:53;xedin;committed.;;;","17/Aug/11 05:23;hudson;Integrated in Cassandra #1027 (See [https://builds.apache.org/job/Cassandra/1027/])
    Fix missing logging for some exceptions
patch by Jonathan Ellis; reviewed by Pavel Yaskevich for CASSANDRA-2061

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1158439
Files : 
* /cassandra/trunk/src/java/org/apache/cassandra/service/StorageService.java
* /cassandra/trunk/src/java/org/apache/cassandra/concurrent/RetryingScheduledThreadPoolExecutor.java
* /cassandra/trunk/src/java/org/apache/cassandra/concurrent/DebuggableScheduledThreadPoolExecutor.java
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/gms/Gossiper.java
* /cassandra/trunk/test/unit/org/apache/cassandra/locator/DynamicEndpointSnitchTest.java
* /cassandra/trunk/src/java/org/apache/cassandra/concurrent/DebuggableThreadPoolExecutor.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get_range_slice() behavior is inconsistent with get_slice() and multiget_slice() when super_column is set in ColumnParent,CASSANDRA-649,12444014,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,vomjom,vomjom,22/Dec/09 22:23,16/Apr/19 17:33,22/Mar/23 14:57,26/Dec/09 21:43,0.5,,,,0,,,,,,"Here's an example using my python library ( http://github.com/vomjom/pycassa ):

>>> import pycassa
>>> connect = pycassa.connect()
>>> cf = pycassa.ColumnFamily(connect, 'Test Keyspace', 'Test Super', super=True)
>>> cf.insert('key1', {'2': {'sub3': 'val3', 'sub4': 'val4'}})

>>> cf.get('key1')
{'2': {'sub4': 'val4', 'sub3': 'val3'}}
>>> cf.get('key1', super_column='2')
{'sub4': 'val4', 'sub3': 'val3'}

>>> cf.multiget(['key1'])
{'key1': {'2': {'sub4': 'val4', 'sub3': 'val3'}}}
>>> cf.multiget(['key1'], super_column='2')
{'key1': {'sub4': 'val4', 'sub3': 'val3'}}

>>> list(cf.get_range())
[('key1', {'2': {'sub4': 'val4', 'sub3': 'val3'}})]
>>> list(cf.get_range(super_column='2'))
[('key1', {'2': {'sub4': 'val4', 'sub3': 'val3'}})]

In the last case, I expected:
[('key1', {'sub4': 'val4', 'sub3': 'val3'})]

If the super_column argument is supplied, then all of these make a ColumnParent with:

cp = ColumnParent(column_family=self.column_family, super_column=super_column)

Or basically, in the KeySlice returned by get_range_slice(), if super_column was set in the passed in the ColumnParent, the columns member of the KeySlice should be a list of respective SuperColumn.columns and not a list of SuperColumn.

Another way to describe the problem:
get_slice(), multiget_slice(), and get_range_slice() all return:
list<ColumnOrSuperColumn> in their return values in some way or another.

If super_column is set in the ColumnParent then:
1. get_slice() and multiget_slice() return the list of SuperColumn.columns each wrapped in a ColumnOrSuperColumn
2. The KeySlice in get_range_slice() returns a list of ONE SuperColumn wrapped in a ColumnOrSuperColumn",Linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Dec/09 00:37;jbellis;649.patch;https://issues.apache.org/jira/secure/attachment/12428955/649.patch","25/Dec/09 20:07;vomjom;649_unit_test.patch;https://issues.apache.org/jira/secure/attachment/12428951/649_unit_test.patch",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19801,,,Sat Dec 26 13:43:08 UTC 2009,,,,,,,,,,"0|i0g08f:",91468,,,,,Normal,,,,,,,,,,,,,,,,,"25/Dec/09 04:39;jbellis;I'd be a lot more sure that this is a bug in Cassandra rather than an under-tested client library if you stuck to raw Thrift.;;;","25/Dec/09 06:41;jbellis;the patch for CASSANDRA-647 makes get_range_slice use the same convert-to-thrift format as get_slice and multiget_slice.  If that doesn't fix it, please add a Thrift test to test_server.py showing the problem.  (Running the thrift tests is described in http://wiki.apache.org/cassandra/HowToContribute);;;","25/Dec/09 20:07;vomjom;I've attached a patch to test_server.py to unit test the bug.

The old test case depended on the behavior that I indicated, so I replaced that with the expected behavior.

I tested the latest trunk and it seems to still have this issue.;;;","26/Dec/09 00:37;jbellis;thanks for the test.

patch extracts thriftifyColumnFamily and applies to getSlice and get_slice_range;;;","26/Dec/09 01:43;vomjom;+1

Works for me.

Thanks.;;;","26/Dec/09 21:43;jbellis;committed to 0.5 and trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race conditions when reinitialisating nodes (OOM + Nullpointer),CASSANDRA-2228,12499491,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,gdusbabek,tbritz,tbritz,23/Feb/11 23:50,16/Apr/19 17:33,22/Mar/23 14:57,04/Mar/11 06:59,0.7.4,,,,0,,,,,,"I had a corrupt system table which wouldn't compact anymore and I deleted the files and restarted cassandra and let it take the same token/ip address.

I experienced the same errors when I'm adding a newly installed node under the same token/ip address before calling repair.

1)
After a few seconds/minutes, I get a OOM error:


 INFO [FlushWriter:1] 2011-02-23 16:40:28,958 Memtable.java (line 164) Completed flushing /cassandra/data/system/Schema-f-15-Data.db (8037 bytes)
 INFO [MigrationStage:1] 2011-02-23 16:40:28,965 Migration.java (line 133) Applying migration 3e30e76b-1e3f-11e0-8369-5a9c1faed4ae Add keyspace: table_userentriesrep factor:3rep strategy:SimpleStrategy{org.apache.cassandra.config.CFMetaData@58925d9[cfId=1024,tableName=table_userentries,cfName=table_userentries,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType@b44dff0,subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=200000.0,readRepairChance=0.0,gcGraceSeconds=86400,defaultValidator=org.apache.cassandra.db.marshal.BytesType@b44dff0,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=3600,memtableFlushAfterMins=60,memtableThroughputInMb=64,memtableOperationsInMillions=10.0,column_metadata={}], org.apache.cassandra.config.CFMetaData@11ab7246[cfId=1025,tableName=table_userentries,cfName=table_userentries_meta,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType@b44dff0,subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=200000.0,readRepairChance=0.0,gcGraceSeconds=86400,defaultValidator=org.apache.cassandra.db.marshal.BytesType@b44dff0,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=3600,memtableFlushAfterMins=60,memtableThroughputInMb=64,memtableOperationsInMillions=10.0,column_metadata={}]}
 INFO [MigrationStage:1] 2011-02-23 16:40:28,965 ColumnFamilyStore.java (line 666) switching in a fresh Memtable for Migrations at CommitLogContext(file='/cassandra/commitlog/CommitLog-1298475572022.log', position=226075)
 INFO [MigrationStage:1] 2011-02-23 16:40:28,966 ColumnFamilyStore.java (line 977) Enqueuing flush of Memtable-Migrations@2121008793(12529 bytes, 1 operations)
 INFO [FlushWriter:1] 2011-02-23 16:40:28,966 Memtable.java (line 157) Writing Memtable-Migrations@2121008793(12529 bytes, 1 operations)
 INFO [MigrationStage:1] 2011-02-23 16:40:28,966 ColumnFamilyStore.java (line 666) switching in a fresh Memtable for Schema at CommitLogContext(file='/cassandra/commitlog/CommitLog-1298475572022.log', position=226075)
 INFO [MigrationStage:1] 2011-02-23 16:40:28,967 ColumnFamilyStore.java (line 977) Enqueuing flush of Memtable-Schema@139610466(8370 bytes, 15 operations)
 INFO [ScheduledTasks:1] 2011-02-23 16:40:28,972 StatusLogger.java (line 89) table_sourcedetection.table_sourcedetection                 0,0                 0/0            0/200000
ERROR [FlushWriter:1] 2011-02-23 16:41:01,240 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[FlushWriter:1,5,main]
java.lang.OutOfMemoryError: Java heap space
        at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:39)
        at java.nio.ByteBuffer.allocate(ByteBuffer.java:312)
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:126)
        at org.apache.cassandra.io.sstable.SSTableWriter.<init>(SSTableWriter.java:75)
        at org.apache.cassandra.db.Memtable.writeSortedContents(Memtable.java:158)
        at org.apache.cassandra.db.Memtable.access$000(Memtable.java:51)
        at org.apache.cassandra.db.Memtable$1.runMayThrow(Memtable.java:176)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)





2) If I restart then, I'm getting an Nullpointer exception. The OOM error will only appear once.

ERROR [main] 2011-02-23 16:42:32,782 AbstractCassandraDaemon.java (line 333) Exception encountered during startup.
java.lang.NullPointerException
        at java.util.concurrent.ConcurrentHashMap.get(ConcurrentHashMap.java:768)
        at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:925)
        at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:105)
        at org.apache.cassandra.service.MigrationManager.applyMigrations(MigrationManager.java:161)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:185)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)


Killing and restarting the node multiple times will eventually ""fix"" these errors.


Steps to reproduce. Remove complete data directory and restart node with same token/ip.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Mar/11 05:32;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-initialize-localEndpoint_.txt;https://issues.apache.org/jira/secure/attachment/12472612/ASF.LICENSE.NOT.GRANTED--v1-0001-initialize-localEndpoint_.txt",,,,,,,,,,,,,,1.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20516,,,Thu Mar 03 23:32:31 UTC 2011,,,,,,,,,,"0|i0ga47:",93069,,,,,Normal,,,,,,,,,,,,,,,,,"23/Feb/11 23:56;tbritz;The normal heap usage of the node is about half of the allocated heap:

Gossip active    : true
Load             : 30.99 GB
Generation No    : 1298476238
Uptime (seconds) : 308
Heap Memory (MB) : 1544.41 / 2493.38
;;;","24/Feb/11 00:58;jbellis;The OOM is because flushing allocates a buffer the size of in_memory_compaction_limit.  Sounds like you need to lower that.

The NPE looks like a bug.;;;","03/Mar/11 04:58;gdusbabek;What is listen_address in cassandra.yaml?  I'm having a hard time conceiving of a way for FBUtilities.getLocalAddress() to return null.;;;","03/Mar/11 04:59;thibaut.britz@trendiction.com;Hi,

I'm on vacation until March 14th.

For urgent matters, please contact:
Christophe Folschette (christophe.folschette@trendiction.com)
+352 20 33 35 32
;;;","04/Mar/11 00:09;jbellis;It looks like we're doing Migration stuff before Gossiper.start is called.

One possible solution: initialize Gossiper.localEndpoint in constructor instead of in start.;;;","04/Mar/11 01:14;gdusbabek;CHM throws NPE when the key is null.  To me, then, the bug is that FBU.getLocalAddress() is somehow returning null.  

I checked yesterday: even if local_address is unspecified, it ends up returning localhost.;;;","04/Mar/11 01:18;jbellis;My point was that we can call put before initializing localEndpoint_ to FBU.gLA, so it's naturally going to be null.;;;","04/Mar/11 04:46;gdusbabek;aha. I was looking in the trunk code (no chance of null).  I see the problem in the 0.7 branch.;;;","04/Mar/11 06:03;jbellis;+1;;;","04/Mar/11 06:59;gdusbabek;committed;;;","04/Mar/11 07:32;hudson;Integrated in Cassandra-0.7 #344 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/344/])
    initialize localendpoint in gossiper earlier. patch by gdusbabek, reviewed by jbellis. CASSANDRA-2228
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Invalid Response count 4,CASSANDRA-937,12460750,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,dispalt,dispalt,31/Mar/10 06:00,16/Apr/19 17:33,22/Mar/23 14:57,01/Apr/10 09:48,0.6.1,,,,0,,,,,,"2010-03-30_21:59:04.64973 ERROR - Error in ThreadPoolExecutor
2010-03-30_21:59:04.64973 java.lang.AssertionError: invalid response count 4
2010-03-30_21:59:04.64973 	at org.apache.cassandra.service.ReadResponseResolver.<init>(ReadResponseResolver.java:54)
2010-03-30_21:59:04.64973 	at org.apache.cassandra.service.ConsistencyManager$DigestResponseHandler.doReadRepair(ConsistencyManager.java:89)
2010-03-30_21:59:04.64973 	at org.apache.cassandra.service.ConsistencyManager$DigestResponseHandler.handleDigestResponses(ConsistencyManager.java:75)
2010-03-30_21:59:04.64973 	at org.apache.cassandra.service.ConsistencyManager$DigestResponseHandler.response(ConsistencyManager.java:60)
2010-03-30_21:59:04.64973 	at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:35)
2010-03-30_21:59:04.64973 	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:40)
2010-03-30_21:59:04.64973 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
2010-03-30_21:59:04.64973 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
2010-03-30_21:59:04.64973 	at java.lang.Thread.run(Thread.java:636)
2010-03-30_21:59:04.64973 ERROR - Fatal exception in thread Thread[RESPONSE-STAGE:5,5,main]
2010-03-30_21:59:04.64973 java.lang.AssertionError: invalid response count 4
2010-03-30_21:59:04.64973 	at org.apache.cassandra.service.ReadResponseResolver.<init>(ReadResponseResolver.java:54)
2010-03-30_21:59:04.64973 	at org.apache.cassandra.service.ConsistencyManager$DigestResponseHandler.doReadRepair(ConsistencyManager.java:89)
2010-03-30_21:59:04.64973 	at org.apache.cassandra.service.ConsistencyManager$DigestResponseHandler.handleDigestResponses(ConsistencyManager.java:75)
2010-03-30_21:59:04.64973 	at org.apache.cassandra.service.ConsistencyManager$DigestResponseHandler.response(ConsistencyManager.java:60)
2010-03-30_21:59:04.64973 	at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:35)
2010-03-30_21:59:04.64973 	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:40)
2010-03-30_21:59:04.64973 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
2010-03-30_21:59:04.64973 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
2010-03-30_21:59:04.64973 	at java.lang.Thread.run(Thread.java:636)
",replication factor is set to 3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Apr/10 04:51;jbellis;937-2.txt;https://issues.apache.org/jira/secure/attachment/12440403/937-2.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19927,,,Thu Apr 01 01:48:24 UTC 2010,,,,,,,,,,"0|i0g1zz:",91754,,,,,Normal,,,,,,,,,,,,,,,,,"31/Mar/10 06:05;jbellis;were you moving nodes around at all?;;;","31/Mar/10 06:09;dispalt;No, just restarting machines;;;","31/Mar/10 06:20;jbellis;I think this in doReadRepair is the culprit

            replicas_.add(FBUtilities.getLocalAddress());

to make up for

                endpoints.remove(FBUtilities.getLocalAddress());

in ReadVerbHandler.

The problem is, in the case where local node isn't actually a replica for the read in question it will cause this bug.;;;","01/Apr/10 04:51;jbellis;Repair is always invoked by a node that has a copy of the data, or thinks it should (that is how it gets the ""initial row"" to compare digests against), but if the definition of who should have replicas changes either from bootstrap or during rebuilding of the ring after a cluster restart then local node could be an ""extra"" replica at repair time.

This patch changes repair to keep the replica set constant throughout the process.  It also fixes repair to start comparing digests immediately rather than waiting for all responses first for no reason, and avoids an extra read from the local replica, re-using the one that we've been comparing digests with.;;;","01/Apr/10 05:27;gdusbabek;looks good. +1;;;","01/Apr/10 09:48;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
slice offset breaks read repair,CASSANDRA-286,12429945,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,09/Jul/09 22:26,16/Apr/19 17:33,22/Mar/23 14:57,16/Jul/09 08:39,0.4,,,,0,,,,,,"[code]
        int liveColumns = 0;
        int limit = offset + count;

        for (IColumn column : reducedColumns)
        {
            if (liveColumns >= limit)
                break;
            if (!finish.isEmpty()
                && ((isAscending && column.name().compareTo(finish) > 0))
                    || (!isAscending && column.name().compareTo(finish) < 0))
                break;
            if (!column.isMarkedForDelete())
                liveColumns++;

            if (liveColumns > offset)
                returnCF.addColumn(column);
        }
[code]

The problem is that for offset to return the correct ""live"" columns, it has to ignore tombstones it scans before the first live one post-offset.

This means that instead of being corrected within a few ms of a read, a node can continue returning deleted data indefinitely (until the next anti-entropy pass).

Coupled with offset's inherent inefficiency (see CASSANDRA-261) I think this means we should take it out and leave offset to be computed client-side (which, for datasets under which it was reasonable server-side, will still be reasonable).",,eweaver,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jul/09 06:07;jbellis;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-286-r-m-offset-from-slice-api-we-could-live.txt;https://issues.apache.org/jira/secure/attachment/12413602/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-286-r-m-offset-from-slice-api-we-could-live.txt","16/Jul/09 06:07;jbellis;ASF.LICENSE.NOT.GRANTED--0002-fix-not-including-tombstone-only-keys-in-keyRange.txt;https://issues.apache.org/jira/secure/attachment/12413603/ASF.LICENSE.NOT.GRANTED--0002-fix-not-including-tombstone-only-keys-in-keyRange.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19621,,,Thu Jul 16 13:20:21 UTC 2009,,,,,,,,,,"0|i0fy07:",91107,,,,,Normal,,,,,,,,,,,,,,,,,"09/Jul/09 23:30;junrao;This is a problem with any APIs relying on offset, instead of value. All columns before the offset affect the outcome. So, if there is any incorrect column (whether it's missing deletes or missing inserts) before the offset doesn't get fixed immediately, the outcome will be incorrect.

One potential fix is to include all columns before offset in the repair logic, but not in thrift return. This won't affect performance much since we already have to scan those columns. This may complicates the overall logic a bit though.
 ;;;","10/Jul/09 00:04;jbellis;Yes, that's the brute force fix, but it means that in the case of mass deletes in a given CF we could very possibly OOM collecting all the tombstones for a large offset.

Again, my rule of thumb is: features that allow the user to do something that slow things down are ok; features that allow the user to crash the server, are not. :);;;","10/Jul/09 01:17;sandeep_tata;I agree ... offset makes it hard to understand the cost of a get_slice. While it is very convenient for pagination, dropping it from the API is probably the right choice.
;;;","16/Jul/09 06:08;jbellis;patch 2 fixes the bug with tombstone handling in get_key_range that Evan noticed in CASSANDRA-139;;;","16/Jul/09 06:55;eweaver;Twitter collective is in favor of killing offset.;;;","16/Jul/09 06:58;jbellis;does that mean you're +1 on the patch, or the idea? :);;;","16/Jul/09 07:02;eweaver;I meant idea. But now I also mean patch.

Fixes my bugs. Code looks fine; ship it!;;;","16/Jul/09 08:39;jbellis;committed;;;","16/Jul/09 21:20;hudson;Integrated in Cassandra #139 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/139/])
    fix not including tombstone-only keys in keyRange.
patch by jbellis; reviewed by Evan Weaver for 
r/m offset from slice api; we could live with being inefficient but not with breaking read repair.
patch by jbellis; reviewed by Evan Weaver for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Risk of counter over-count when recovering commit log,CASSANDRA-2419,12503450,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,06/Apr/11 04:49,16/Apr/19 17:33,22/Mar/23 14:57,08/May/11 09:42,0.8.0,,,,1,counters,,,,,"When a memtable was flush, there is a small delay before the commit log replay position gets updated. If the node fails during this delay, all the updates of this memtable will be replay during commit log recovery and will end-up being over-counts.",,cburroughs,rcoli,stuhood,,,,,,,,,,,,,,,,,,,28800,28800,,0%,28800,28800,,,,,,,,,,,,,,,,,,,,"29/Apr/11 21:57;slebresne;0001-Record-CL-replay-infos-alongside-sstables-v2.patch;https://issues.apache.org/jira/secure/attachment/12477754/0001-Record-CL-replay-infos-alongside-sstables-v2.patch","07/Apr/11 08:38;slebresne;0001-Record-and-use-sstable-replay-position.patch;https://issues.apache.org/jira/secure/attachment/12475649/0001-Record-and-use-sstable-replay-position.patch","03/May/11 03:53;jbellis;2419-v3.txt;https://issues.apache.org/jira/secure/attachment/12477981/2419-v3.txt","03/May/11 09:57;jbellis;2419-v4.txt;https://issues.apache.org/jira/secure/attachment/12478017/2419-v4.txt","03/May/11 23:37;jbellis;2419-v6.txt;https://issues.apache.org/jira/secure/attachment/12478061/2419-v6.txt",,,,,,,,,,5.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20613,,,Tue May 10 22:30:29 UTC 2011,,,,,,,,,,"0|i0gb9j:",93255,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"06/Apr/11 05:02;slebresne;One solution I see to this problem would be to record along with the replay position the time when we last updated this replay position. The during recover, we would first look at all the sstables (for the CF) and if a sstable is freshly flushed (which implies that we have a marker to know that a sstable was never compacted) and have a modification time higher that the last time we updated the replay position, then we'll just remove the sstable since we know it will be fully replayed.

Note that to work correctly we also need a way to mark a freshly flushed sstable as 'non compactable' during the time it takes to mark the commit log.

We would probably only do this for counter CF just to be on the safe side.

Opinions ?;;;","06/Apr/11 05:25;jbellis;What if instead of the CL ""header"" we record the CL context as part of an sstable footer?  (footer is less likely to cause bugs w/ sstable math that assumes 0 = start of first row.)  then there is no race.;;;","06/Apr/11 05:34;jbellis;Hmm, I think we need both the CL header and this information, since this flush footer would only give us when we flushed which is not the same as ""do I need to replay.""

For instance: if there is no flush marker for a commitlog segment in any existing sstable, that does not necessarily mean no data is in the commitlog for that CF.

So replay position would be max(dirty at from CL header, flushed at from sstable footers).

(You would need to allow multiple flush contexts in a single sstable footer, to preserve them during compaction.);;;","06/Apr/11 05:39;slebresne;What about a new component .metadata for each sstable instead of a footer. I actually think we will have a use for other sstable metadata at some point anyway. For instance we could keep the file format version. That way we wouldn't rely so much on the data file name.;;;","06/Apr/11 05:44;jbellis;A separate component is a better idea.;;;","06/Apr/11 05:52;slebresne;Actually I don't think this really solves the race condition. We really shouldn't compact the newly flushed sstable until we marked the commit log, because if we compact it, even if we're able to detect that 'some parts' of a sstable will be replay during recover, there is nothing we can do about it.;;;","06/Apr/11 06:01;jbellis;I don't understand the problem.  Say we have this situation:

CommitLog-1302036825548.log: [full of writes to CF Foo counters, up to position 100.  header reads dirty-at 50, our last flush position]
Foo-g-45-Metadata.db: [flushed at position (1302036825548, 50)]
Foo-g-46-Metadata.db: [flushed at position (1302036825548, 100)]

We compact and get
Foo-g-47-Metadata.db: [flushed at position (1302036825548, 100)]

If we die and restart here we will correctly start reply of Foo at position 100 in this segment.

(we can combine to a single flushed-at entry in this case since they were from the same CL segment.  if they were from different segments we would keep both.);;;","06/Apr/11 06:10;slebresne;Right, that was just me not getting you idea at first. Make sense, sorry.;;;","07/Apr/11 08:38;slebresne;Attaching patch implementing Jonathan's idea to record the CL replay position along with the sstable.;;;","07/Apr/11 08:40;slebresne;I'm wondering, couldn't we just drop the commit log header if we do that.;;;","07/Apr/11 09:56;jbellis;Yes, I think we can.;;;","15/Apr/11 05:39;jbellis;bq. I'm wondering, couldn't we just drop the commit log header if we do that.

Were you planning to update w/ that change?;;;","29/Apr/11 21:57;slebresne;v2 removes commit log header completely in favor of sstable metadata about where to replay (patch against 0.8).

This differs from v1 in that instead of keeping every (segment, replay_position) pair, we keep for a given sstable, only the position for the most recent segment (that is, we leverage the fact that we use increasing timestamps for commit logs).

The reason for this is twofold:
  # this more compact (and simple)
  # if we remove the commit log header, we need to be able to say if a given segment is dirty or not for a given column family. That is, we don't want to know if some replay position existed on this segment, but if a relevant one still exist. So for a given column family we really only care about the newest (segment, replay_position) pair.

Now there is the question of the update path. With this patch, the (existing) commit log headers will be ignored. This means that ideally before updating to a version having this patch people would use drain. If they do not, then the commit logs will be fully replayed. Pre-0.8, it's not a big deal. With counters, this could mean over-counts (that's exactly what this ticket is about). So I would be in favor of putting this for 0.8.0, since it is a bug fix and it will avoids the problem of upgrading from a version already having counters. But I would admit this is not trivial patch, so ...
;;;","03/May/11 03:53;jbellis;v3 attached with some changes:

- SSTableMetadata removed; replayposition becomes a field in SSTable that is serialized w/ statistics
- RP is final and part of the SSTable constructor
- RP implements Comparable instead of a one-off resolve API; RP.getReplayPosition encapsulates the find-replay-point logic
- RP moved to a top-level class and replaces CLContext

I'd like to make the metadata a json blob so we can extend it more easily, so I probably need to re-introduce SSTM. Consider v3 a work in progress.;;;","03/May/11 09:57;jbellis;I tried two ways of storing metadata as yaml -- first with the metadata as java beans that were stored directly as yaml, and second half-manually serializing to a yaml Map<String, Object> -- and both feel clunkier than just using the version field to deal with adding things. (Especially when you need to do version checks anyway when modifying things rather than just adding new fields.)

So, v4 is substantially the same as v3 but Descriptor version is bumped to g and we use that instead of EOF to when reading RP. (Also, writeStatistics is renamed to writeMetadata.);;;","03/May/11 20:32;slebresne;v4 looks good, I like those changes (note: I committed r1099037 to fix CFSTest, it had a test with hardcoded sstable filenames using version 'f' and thus was failing);;;","03/May/11 23:00;jbellis;v5 updates CL replay to use RP.getReplayPosition as well;;;","03/May/11 23:37;jbellis;v6 fixes a test failure in v5;;;","04/May/11 01:05;slebresne;Minor nitpick: in CommitLog.java:recover(), was there a reason to create a new List<ReplayPosition> positions instead of using cfPositions.values() ?

but other than that, +1 on v6.;;;","04/May/11 01:35;jbellis;bq. in CommitLog.java:recover(), was there a reason to create a new List<ReplayPosition> positions instead of using cfPositions.values()

Nope. I'll fix that before commit.

Asked Paul Cannon to also review first though, since obviously we want to be extra sure not to cause regressions here.;;;","08/May/11 09:42;jbellis;committed;;;","10/May/11 03:12;stuhood;Thanks a ton for this work! Transactionality here we come.;;;","11/May/11 06:30;hudson;Integrated in Cassandra-0.8 #93 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/93/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
suggest avoiding broken openjdk6 on Debian as build-dep,CASSANDRA-1575,12475803,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,urandom,scode,scode,05/Oct/10 04:28,16/Apr/19 17:33,22/Mar/23 14:57,07/Oct/10 05:13,0.6.6,0.7 beta 3,Packaging,,0,,,,,,"I ran into this myself and then today someone was reporting having the same problem on IRC; there is a packaging bug in openjdk6 in lenny:

   http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=501487

The effect is that when ant tries to download files over SSL, it fails complaining about:

   ""java.security.InvalidAlgorithmParameterException: the trustAnchors parameter must be non-empty""

It turns out this works fine with the Sun JVM. I'm attaching a patch which makes Cassandra build on both lenny and squeeze; however, I am not sure whether other platforms may be negatively affected. The patch just requires an openjdk sufficiently new that the lenny openjdk won't quality. If there are other platforms where we do want an older openjdk, this patch might break that.

In addition, I removed the ""java6-sdk"" as a sufficient dependency because that resolved to openjdk-6-jdk on lenny.

I think it's a good idea to consider changing this just to decrease the initial threshold of adoption for those trying to build from source.

So: This does fix the build issue on lenny, and doesn't seem to break squeeze, but I cannot promise anything about e.g. ubuntu.

For the record, I'm also attaching a small self-contained test case which, when run, tries to download one of the offending pom files. It can be used to easily test weather the SSL download with work with a particular JVM.",Debian lenny,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Oct/10 04:30;scode;Trunk1575Test.java;https://issues.apache.org/jira/secure/attachment/12456317/Trunk1575Test.java","05/Oct/10 04:29;scode;trunk-1575.txt;https://issues.apache.org/jira/secure/attachment/12456316/trunk-1575.txt",,,,,,,,,,,,,2.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20209,,,Wed Oct 06 22:33:17 UTC 2010,,,,,,,,,,"0|i0g62n:",92414,,urandom,,urandom,Low,,,,,,,,,,,,,,,,,"05/Oct/10 05:44;scode;And I should stress that this will presumably only help normalized build environments that install build dependencies as required. It doesn't help a user trying to 'ant build', nor a user trying to 'debuild' and happens to have other JDK:s installed, but I'm not sure how to address that in a clean fashion.

Maybe add a note under ""requirements"" in the README?

(I realize this is catering to a very specific platform, but lenny is presumably pretty common.);;;","07/Oct/10 05:13;urandom;First off, thanks for the report, and the background research on it.

To summarize this issue for others, the openjdk-6 package in Lenny is missing the cacerts keystore needed to establish ""trust"" with SSL enabled servers.  I'm guessing this is because it was stripped from Sun's original code dump, because later versions of the package depend on ca-certificates-java which simply maintains a keystore made up of the Debian installed CAs.

Where this creates a problem for Cassandra is in the retrieval of build dependencies with Ivy, where those deps are located on SSL-enabled remote servers. This _only_ occurs on Lenny though, later versions are fine.

As to the attached patch, I'm not convinced that the cure here isn't worse than the disease.  Here' s why:

* The problem is only with building a Debian source package, and only on Lenny.  I believe this to be a small subset of all users.
* The situation isn't impossible for those that want to build the source package on Lenny.  They simply need to install sun-java6 first (or set it to default using update-alternatives if openjdk-6 is already installed).
* The attached patch will result in an uninstallable package for anyone who doesn't have the non-free repository enabled.  This is everyone who went through the default installation process.
* Unattended installs of sun-java6 (think chef, puppet, et. al.) are difficult at best because the package prompts for user acceptance of the license.
* If possible, we want to use the same packaging for all versions of Debian and derivatives, and there has been a lot of talk of removing the sun packages from archives. 

I think it'd be better to simply document this at http://wiki.apache.org/cassandra/DebianPackaging and leave things as they are.  If you disagree, feel free to reopen the report.;;;","07/Oct/10 06:33;scode;Sounds reasonable.

That said, maybe the set of people who would try 'ant build' on lenny is significantly larger than those building Debian packages with debuild. For those, a note in README might be helpful.

But again I realize this is catering to a very specific problem. Maybe it's just not worth it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cql driver jar,CASSANDRA-2263,12500226,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,urandom,urandom,urandom,03/Mar/11 04:59,16/Apr/19 17:33,22/Mar/23 14:57,31/May/11 01:55,1.0.0,,Legacy/CQL,,1,cql,,,,,"Work was done in CASSANDRA-1848  to create a jar for the CQL Java driver.  The generated Thrfit code was broken out into it's own jar as well, since that is a dependency for both servers and clients. However, based on the work currently happening in CASSANDRA-2262 and CASSANDRA-2124, it seems that additional dependencies will exist, and new jar(s) will need to be created.

The easiest way to fix this will probably be to put copies of all of {{o.a.c.db.marshal}} and {{o.a.c.utils}}, and a copy of {{o.a.c.config.ConfigurationException}} into the CQL driver jar (a split along those lines to create another jar doesn't make sense IMO).",,,,,,,,,,,,,,,,,,,,,,,0,0,,0%,0,0,,,,,,,,,,,,,,,,,,,,"30/May/11 23:53;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2263-don-t-ship-jdbc-in-main-jar.txt;https://issues.apache.org/jira/secure/attachment/12480856/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2263-don-t-ship-jdbc-in-main-jar.txt",,,,,,,,,,,,,,1.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20533,,,Tue May 31 22:43:04 UTC 2011,,,,,,,,,,"0|i0gabz:",93104,,,,,Low,,,,,,,,,,,,,,,,,"29/May/11 03:45;larswunderlich;I don't know whether this issue should be documented in a separate bug for 0.8.0-rc1, but since this one deals with separation of jar file, I add the comment here.

The following code fails on my machine:

Class.forName(""org.apache.cassandra.cql.jdbc.CassandraDriver"");
Connection c = DriverManager.getConnection(""jdbc:cassandra:/@localhost:9160/testspace"");

As of apache-cassandra-0.8.0-rc1 together with apache-cassandra-cql-1.0.2.jar the connection to local host couldn't be established, even though it was running and the keyspace was fine, because of the following reasons:

1.) cql jar requires direct classpath relationship to apache-cassandra-0.8.0-rc1.jar, without it cannot run at all, what contradicts server implementation encapsulation/secret in my mind to attach core jar file: 

Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/cassandra/db/marshal/AbstractType
	at org.apache.cassandra.cql.jdbc.Connection.execute(Connection.java:142)
	at org.apache.cassandra.cql.jdbc.Connection.execute(Connection.java:124)
	at org.apache.cassandra.cql.jdbc.CassandraConnection.<init>(CassandraConnection.java:83)
	at org.apache.cassandra.cql.jdbc.CassandraDriver.connect(CassandraDriver.java:86)
	at java.sql.DriverManager.getConnection(Unknown Source)
	at java.sql.DriverManager.getConnection(Unknown Source)
	at TestConnection.main(TestConnection.java:18)

2.) apache-cassandra-0.8.0-rc1.jar internally contains org.apache.cassandra.cql.jdbc package a second time, which might conflict with the cql standalone jar version of the driver in terms of class compatibility.

3.) Using CassandraDriver from core apache-cassandra-0.8.0-rc1.jar results in an error message that cassandra.yaml couldn't be found, but cassandra.yaml is not required from my point of view for clients anyway.

Cannot locate cassandra.yaml
Fatal configuration error; unable to start server.  See log for stacktrace.;;;","30/May/11 02:56;larswunderlich;Connection problem could be solved by passing parameter -Dcassandra.config=cassandra.yaml when starting test case and adding cassandra.yaml file as copy to classpath. However, cql driver couldn't be started out of the box and I haven't found documentation including code examples how to successfully setup a working JDBC connection as of now.  ;;;","30/May/11 23:55;urandom;Ultimately, we don't want to have to depend on the main jar, but the fact that this is the case now, is known.  I think the problem here (as you already suggested), is that there is also a copy in the cassandra jar.  The attached patch remedies that.;;;","31/May/11 00:16;jbellis;+1;;;","31/May/11 01:55;urandom;committed;;;","31/May/11 11:56;jbellis;I think this broke the jdbc tests post-clean:

{noformat}
build-test:
    [javac] /Users/jonathan/projects/cassandra/svn-0.8.0/build.xml:973: warning: 'includeantruntime' was not set, defaulting to build.sysclasspath=last; set to false for repeatable builds
    [javac] Compiling 124 source files to /Users/jonathan/projects/cassandra/svn-0.8.0/build/test/classes
    [javac] /Users/jonathan/projects/cassandra/svn-0.8.0/drivers/java/test/org/apache/cassandra/cql/jdbc/PreparedStatementTest.java:357: cannot find symbol
    [javac] symbol  : class CassandraPreparedStatement
    [javac] location: class org.apache.cassandra.cql.jdbc.PreparedStatementTest
    [javac]         CassandraPreparedStatement stmt = (CassandraPreparedStatement)con.prepareStatement(q);
    [javac]         ^
{noformat};;;","01/Jun/11 06:43;urandom;Gah, I'd fixed this locally but forgot to commit.  It's there now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dropping column families and keyspaces races with compaction and flushing,CASSANDRA-1631,12477746,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,gdusbabek,gdusbabek,gdusbabek,19/Oct/10 22:10,16/Apr/19 17:33,22/Mar/23 14:57,06/Nov/10 05:12,0.7 beta 3,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-1585,,,,,,"20/Oct/10 05:14;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-fix-drop-race-with-compaction.txt;https://issues.apache.org/jira/secure/attachment/12457596/ASF.LICENSE.NOT.GRANTED--v1-0001-fix-drop-race-with-compaction.txt","20/Oct/10 05:14;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0002-fix-drop-race-with-flush.txt;https://issues.apache.org/jira/secure/attachment/12457597/ASF.LICENSE.NOT.GRANTED--v1-0002-fix-drop-race-with-flush.txt",,,,,,,,,,,,,2.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20226,,,Fri Nov 05 21:12:51 UTC 2010,,,,,,,,,,"0|i0g6f3:",92470,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"20/Oct/10 05:29;jbellis;i don't think 02 actually prevents an update that is making its way through the system pre-drop from causing a flush after the lock is released.  probably setting the memtable to frozen while we hold the lock would fix this.;;;","20/Oct/10 05:37;gdusbabek;The null check in CFS.maybeSwitchMemtable was intended to handle that, i.e., if the flush goes through after the drop, it ends up doing nothing because the CF def can't be found.;;;","20/Oct/10 09:45;jbellis;ah, right.  +1;;;","20/Oct/10 23:14;hudson;Integrated in Cassandra #571 (See [https://hudson.apache.org/hudson/job/Cassandra/571/])
    fix drop race with flush. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1631
fix drop race with compaction. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1631
;;;","04/Nov/10 23:56;jbellis;This is still a bug with schema updates to an existing CF, since reloadCf is doing a unload/init cycle.  So flushing + compaction is an issue there as well.  Here is a stacktrace from during an index creation where it stubbed its toe on an incomplete sstable from an in-progress compaction (path names anonymized):

{code}
 INFO [CompactionExecutor:1] 2010-11-02 16:31:00,553 CompactionManager.java (line 224) Compacting [org.apache.cassandra.io.sstable.SSTableReader(path='Standard1-e-6-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='Standard1-e-7-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='Standard1-e-8-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='Standard1-e-9-Data.db')]
...
ERROR [MigrationStage:1] 2010-11-02 16:31:10,939 ColumnFamilyStore.java (line 244) Corrupt sstable Standard1-tmp-e-10-<>=[Data.db, Index.db]; skipped
java.io.EOFException
        at org.apache.cassandra.utils.FBUtilities.skipShortByteArray(FBUtilities.java:308)
        at org.apache.cassandra.io.sstable.SSTable.estimateRowsFromIndex(SSTable.java:231)
        at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:286)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:202)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:235)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:443)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:431)
        at org.apache.cassandra.db.Table.initCf(Table.java:335)
        at org.apache.cassandra.db.Table.reloadCf(Table.java:343)
        at org.apache.cassandra.db.migration.UpdateColumnFamily.applyModels(UpdateColumnFamily.java:89)
        at org.apache.cassandra.db.migration.Migration.apply(Migration.java:158)
        at org.apache.cassandra.thrift.CassandraServer$2.call(CassandraServer.java:672)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
...
 INFO [CompactionExecutor:1] 2010-11-02 16:31:31,970 CompactionManager.java (line 303) Compacted to Standard1-tmp-e-10-Data.db.  213,657,983 to 213,657,983 (~100% of original) bytes for 626,563 keys.  Time: 31,416ms.
{code};;;","05/Nov/10 03:57;gdusbabek;This isn't going to be as simple as it was last time (shifting work on to the CompactionManager and blocking).  Part of the unload/init code, you guessed it, shifts work of the CompactionManager and blocks (index creation to be precise).  

I'm looking into ways of doing this that don't involve deadlock or refactoring large pieces of code.  Part of the problem is that we've started treating the CompactionManager as a way to synchronize access to sstables.  The problem with that is that it is very course and the jobs submitted to it do a lot more things that pure FS work.;;;","05/Nov/10 23:03;gdusbabek;Realized streaming can be affected by drop/renames as well.;;;","06/Nov/10 05:12;jbellis;created CASSANDRA-1715 for these new bugs;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
column.name.remaining() <= 0 serializing row mutation (+ maybe other effects),CASSANDRA-1860,12493161,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,scode,scode,14/Dec/10 17:50,16/Apr/19 17:33,22/Mar/23 14:57,15/Dec/10 17:43,0.7.0 rc 3,,,,0,,,,,,"I suspect there is a lingering ByteBuffer consumption issue somewhere. I'll start with the most direct indication of this, which is that I see this (on yesterday's 0.7):

2010-12-14T08:11:43.656+00:00 127.0.0.1 java.lang.AssertionError
2010-12-14T08:11:43.656+00:00 127.0.0.1 at org.apache.cassandra.db.ColumnSerializer.serialize(ColumnSerializer.java:44)
2010-12-14T08:11:43.656+00:00 127.0.0.1 at org.apache.cassandra.db.ColumnSerializer.serialize(ColumnSerializer.java:35)
2010-12-14T08:11:43.656+00:00 127.0.0.1 at org.apache.cassandra.db.ColumnFamilySerializer.serializeForSSTable(ColumnFamilySerializer.java:87)
2010-12-14T08:11:43.656+00:00 127.0.0.1 at org.apache.cassandra.db.ColumnFamilySerializer.serialize(ColumnFamilySerializer.java:73)
2010-12-14T08:11:43.656+00:00 127.0.0.1 at org.apache.cassandra.db.RowMutationSerializer.freezeTheMaps(RowMutation.java:367)
2010-12-14T08:11:43.656+00:00 127.0.0.1 at org.apache.cassandra.db.RowMutationSerializer.serialize(RowMutation.java:378)
2010-12-14T08:11:43.656+00:00 127.0.0.1 at org.apache.cassandra.db.RowMutationSerializer.serialize(RowMutation.java:356)
2010-12-14T08:11:43.656+00:00 127.0.0.1 at org.apache.cassandra.db.RowMutation.makeRowMutationMessage(RowMutation.java:223)
2010-12-14T08:11:43.656+00:00 127.0.0.1 at org.apache.cassandra.db.RowMutation.makeRowMutationMessage(RowMutation.java:216)
2010-12-14T08:11:43.657+00:00 127.0.0.1 at org.apache.cassandra.service.StorageProxy.mutate(StorageProxy.java:136)
2010-12-14T08:11:43.657+00:00 127.0.0.1 at org.apache.cassandra.thrift.CassandraServer.doInsert(CassandraServer.java:442)
2010-12-14T08:11:43.657+00:00 127.0.0.1 at org.apache.cassandra.thrift.CassandraServer.insert(CassandraServer.java:379)
2010-12-14T08:11:43.657+00:00 127.0.0.1 at org.apache.cassandra.thrift.Cassandra$Processor$insert.process(Cassandra.java:2952)
2010-12-14T08:11:43.657+00:00 127.0.0.1 at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2555)
2010-12-14T08:11:43.657+00:00 127.0.0.1 at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)

2010-12-14T08:11:43.657+00:00 127.0.0.1 at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
2010-12-14T08:11:43.657+00:00 127.0.0.1 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
2010-12-14T08:11:43.657+00:00 127.0.0.1 at java.lang.Thread.run(Thread.java:662)

That assertion failure is:

        assert column.name().remaining() > 0;

While I have not absolutely confirmed that we are not accidentally sending empty column names, I don't believe that is the case and regardless Cassandra does try to validate RowMutations and part of that validation is that column names are non-empty (ThriftValidation.validateColumns()).

Some more information:

This is on a three-node test cluster. We have some low number of processes doing nothing but insertions using insert() (via pycassa). As far as I can tell the column validation code is triggering regardless of whether you use batch_mutate() or insert() (pycassa optimizes the single-column case by using insert()).

The interesting thing is that we have three nodes in the cluster and they trigger different issues. On the two remaining nodes we had already observed two different stack traces:


org.apache.cassandra.db.UnserializableColumnFamilyException: Couldn't find cfId=798579
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:117)
        at org.apache.cassandra.db.RowMutationSerializer.defreezeTheMaps(RowMutation.java:388)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:398)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:356)
        at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:52)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:63)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

And the other one:


java.io.EOFException
        at java.io.DataInputStream.readFully(DataInputStream.java:180)
        at org.apache.cassandra.utils.FBUtilities.readByteArray(FBUtilities.java:265)
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:95)
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:35)
        at org.apache.cassandra.db.ColumnFamilySerializer.deserializeColumns(ColumnFamilySerializer.java:129)
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:120)
        at org.apache.cassandra.db.RowMutationSerializer.defreezeTheMaps(RowMutation.java:388)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:398)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:356)
        at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:52)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:63)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

Originally I was only looking at these later two wondering whether it might have been a result of a schema mismatch due to a bootstrapping mistake w.r.t. seed nodes joining the cluster. However, the schema looks matching on all nodes in the cluster and we keep seeing those exceptions even after a full repair/compact on the entire cluster (and those don't fail; so all data is read without problems) in addition to removing of the hints sstables to remove any lingering mutations.

Today I noticed that the third node, that does NOT see the above two issues, reports quite a number of exceptions of the assertion failure kind. I am not sure why the behavior does not seem to be consistent; we have three hosts running the python/pycassa clients spread over several processes. tcpdump indicates thrift RPC calls are definitely flowing to hosts other than the third one (that sees the assertion failure). They are all running the same version of Cassandra (same debian package, definitely restarted since the latest upgrade - I know that since I restarted nodes when testing the schema change issue I have filed separately).

At this moment I don't have better information than the above and I have not reproduced it on e.g. an independent cluster.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20346,,,Wed Dec 15 09:43:19 UTC 2010,,,,,,,,,,"0|i0g7un:",92702,,,,,Normal,,,,,,,,,,,,,,,,,"14/Dec/10 17:58;scode;(Sorry original title is misleading - edited. I started filling in the form incrementally prior to being done. It's not in the row mutation stage.);;;","14/Dec/10 22:03;tjake;Was CASSANDRA-1847 committed in your tree at the time of your tests?

This may have been the problem.;;;","14/Dec/10 22:32;scode;No :( Apparently my sense of time is completely FUBAR. I recognize CASSANDRA-1847  but I thought there had been at least a few days since the last ByteBuffer related JIRA traffic. Obviously not.

I'm merging and rebuilding. I will report back, but it may be a while because this is not something we trivially triggered immediately. Will probably leave it running at least overnight.;;;","15/Dec/10 17:43;scode;Nothing triggered over night. Resolving as duplicate of CASSANDRA-1847.

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming broken on windows (FileStreamTask.CHUNK_SIZE is too big).,CASSANDRA-795,12456333,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,gdusbabek,gdusbabek,gdusbabek,15/Feb/10 23:32,16/Apr/19 17:33,22/Mar/23 14:57,17/Feb/10 06:57,0.5,,,,0,,,,,,Setting chunk size smaller addresses the problem.  We should explore setting SO_SNDBUF higher to see if that fixes the problem.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Feb/10 03:26;gdusbabek;0001-set-SO_SNDBUF-bigger.patch;https://issues.apache.org/jira/secure/attachment/12435896/0001-set-SO_SNDBUF-bigger.patch","16/Feb/10 03:26;gdusbabek;0002-unit-test-for-streaming-a-big-file.patch;https://issues.apache.org/jira/secure/attachment/12435895/0002-unit-test-for-streaming-a-big-file.patch",,,,,,,,,,,,,2.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19866,,,Tue Feb 16 22:57:18 UTC 2010,,,,,,,,,,"0|i0g14f:",91612,,,,,Low,,,,,,,,,,,,,,,,,"15/Feb/10 23:36;jbellis;(Closed CASSANDRA-793 as dupe of this.);;;","15/Feb/10 23:38;jbellis;If you add a unit test w/ a 64MB file I can test on windows.  Writing a row w/ a single column of 64MB ought to do it.;;;","16/Feb/10 03:26;gdusbabek;Test case attached (it takes a while to complete).;;;","16/Feb/10 03:56;jbellis;it would probably be a lot faster if you put the bulk of the volume in a column value[] so it doesn't have to do a whole bunch of String serialization in the setup.

Test passes for me (64 bit windows 7) w/o patch 01 so I don't think I can usefully report on that. :);;;","16/Feb/10 20:25;tantra;On WinXp this patch doesn't work. Help only reduce CHUNK_SIZE

java.lang.RuntimeException: java.io.IOException: Невозможно выполнить операцию на сокете, т.к. буфер слишком мал или очередь переполнена
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.IOException: Невозможно выполнить операцию на сокете, т.к. буфер слишком мал или очередь переполнена
	at sun.nio.ch.SocketDispatcher.write0(Native Method)
	at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:33)
	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:104)
	at sun.nio.ch.IOUtil.write(IOUtil.java:60)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:334)
	at sun.nio.ch.FileChannelImpl.transferToTrustedChannel(FileChannelImpl.java:449)
	at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:520)
	at org.apache.cassandra.net.FileStreamTask.stream(FileStreamTask.java:96)
	at org.apache.cassandra.net.FileStreamTask.runMayThrow(FileStreamTask.java:64)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more
ERROR [MESSAGE-STREAMING-POOL:1] 2010-02-16 15:21:34,234 CassandraDaemon.java (line 78) Fatal exception in thread Thread[MESSAGE-STREAMING-POOL:1,5,main]
java.lang.RuntimeException: java.io.IOException: Невозможно выполнить операцию на сокете, т.к. буфер слишком мал или очередь переполнена
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.IOException: Невозможно выполнить операцию на сокете, т.к. буфер слишком мал или очередь переполнена
	at sun.nio.ch.SocketDispatcher.write0(Native Method)
	at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:33)
	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:104)
	at sun.nio.ch.IOUtil.write(IOUtil.java:60)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:334)
	at sun.nio.ch.FileChannelImpl.transferToTrustedChannel(FileChannelImpl.java:449)
	at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:520)
	at org.apache.cassandra.net.FileStreamTask.stream(FileStreamTask.java:96)
	at org.apache.cassandra.net.FileStreamTask.runMayThrow(FileStreamTask.java:64)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more
;;;","17/Feb/10 06:09;gdusbabek;I was able to duplicate this in windows XP by throwing roughly 200MB at a socket in short order. SO_RCVBUF and SO_SNDBUF both default to 8k and setting them up to 256k didn't address the problem. Since the default values were so low, I didn't think that setting them in the multi-MB range was going to prove fruitful.

Google wasn't very helpful explaining the underlying cause of the error (stems from winsock err code WSAENOBUFS) except for stating the obvious (there isn't enough allocated memory somewhere). Taking it all in, I get the sense that when you chunk data up to send over a socket, those chunks end up in kernel memory (paged), not user memory, and if you use too much of that (varies across windows versions), a WSAENOBUFS ensues.

Setting FileStreamTask.CHUNK_SIZE to 32MB solves the problem though. Unless there are any objections, I think we'll stick with that solution.;;;","17/Feb/10 06:15;jbellis;+1 set to 32MB;;;","17/Feb/10 06:57;gdusbabek;r910738 (0.5), 910744 (trunk);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Path not found under Windows 7,CASSANDRA-1270,12469062,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,szogun1987,szogun1987,12/Jul/10 20:57,16/Apr/19 17:33,22/Mar/23 14:57,28/Dec/10 23:41,,,,,0,,,,,,"I'm not sure that this is bug maybe it is my fault but when I try to run Cassandra using bin\cassandra -f my system returns ""Path not found message"". When i comment ECHO OFF from cassandra.bat I have seen that last line of output contains "".8.jar"";""D:\Cassandra\bin\..\lib\slf4j-log4j12-1.5.8.jar"";""D:\Cassandra\bin\..\build\classes"" ""org.apache.cassandra.thrift.CassandraDaemon""""
D:\Cassandra is my cassandra root directory. Directory ""D:\Casandra\build\classes\org\apache\cassandra\thrift"" contains CassandraDaemon.class, CassandraDaemon$1.class, CassandraDaemon$2.class files.

Apologize for my Vocabulary and Grammar.
","Windows 7 Professional, JRE 1.6",szogun1987,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20054,,,Tue Dec 28 15:42:08 UTC 2010,,,,,,,,,,"0|i0g41b:",92084,,,,,Low,,,,,,,,,,,,,,,,,"28/Dec/10 23:42;jbellis;I believe this is fixed in recent 0.6 releases;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get_key_range timeout and exception,CASSANDRA-153,12424856,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,nk11,nk11,08/May/09 04:20,16/Apr/19 17:33,22/Mar/23 14:57,12/May/09 07:57,,,,,0,,,,,,"My test code:

		int max = 5000;		
		for (int a = 0; a < max; a++) {
			System.out.println(a);
			client.insert(""Table1"", ""k1:"" + a, ""Super1:x:x"", new byte[] { (byte) 1 }, 0, false);
		}		

		client.get_key_range(""Table1"", ""k1:0"", ""k1:1000"", 1000);

Produces in the logs:

ERROR [ROW-READ-STAGE:9] 2009-05-07 23:04:56,609 CassandraDaemon.java (line 61) Fatal exception in thread Thread[ROW-READ-STAGE:9,5,main]
java.lang.RuntimeException: corrupt sstable
	at org.apache.cassandra.db.FileStruct.seekTo(FileStruct.java:107)
	at org.apache.cassandra.db.Table.getKeyRange(Table.java:905)
	at org.apache.cassandra.service.RangeVerbHandler.doVerb(RangeVerbHandler.java:23)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:46)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.io.EOFException
	at java.io.RandomAccessFile.readUnsignedShort(Unknown Source)
	at java.io.DataInputStream.readUTF(Unknown Source)
	at java.io.RandomAccessFile.readUTF(Unknown Source)
	at org.apache.cassandra.io.SequenceFile$AbstractReader.getPositionFromBlockIndex(SequenceFile.java:562)
	at org.apache.cassandra.db.FileStruct.seekTo(FileStruct.java:86)
	... 6 more
ERROR [pool-1-thread-2] 2009-05-07 23:05:01,593 Cassandra.java (line 1187) Internal error processing get_key_range
java.lang.RuntimeException: error reading keyrange RangeCommand(table='Table1', startWith='k1:0', stopAt='k1:1000', maxResults=1000)
	at org.apache.cassandra.service.StorageProxy.getKeyRange(StorageProxy.java:682)
	at org.apache.cassandra.service.CassandraServer.get_key_range(CassandraServer.java:511)
	at org.apache.cassandra.service.Cassandra$Processor$get_key_range.process(Cassandra.java:1183)
	at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:805)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:252)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.util.concurrent.TimeoutException: Operation timed out.
	at org.apache.cassandra.net.AsyncResult.get(AsyncResult.java:95)
	at org.apache.cassandra.service.StorageProxy.getKeyRange(StorageProxy.java:677)
	... 7 more

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/May/09 00:18;jbellis;153-02.patch;https://issues.apache.org/jira/secure/attachment/12407631/153-02.patch","08/May/09 06:04;jbellis;153.patch;https://issues.apache.org/jira/secure/attachment/12407580/153.patch","11/May/09 22:28;jbellis;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-153-unit-test-to-expose-bug-system-test-is-r.txt;https://issues.apache.org/jira/secure/attachment/12407781/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-153-unit-test-to-expose-bug-system-test-is-r.txt","11/May/09 22:28;jbellis;ASF.LICENSE.NOT.GRANTED--0002-cannonicalize-all-accesses-to-indexMeatdataMap.txt;https://issues.apache.org/jira/secure/attachment/12407782/ASF.LICENSE.NOT.GRANTED--0002-cannonicalize-all-accesses-to-indexMeatdataMap.txt","11/May/09 22:28;jbellis;ASF.LICENSE.NOT.GRANTED--0003-check-for-at-end-of-data-in-iterator-init.txt;https://issues.apache.org/jira/secure/attachment/12407783/ASF.LICENSE.NOT.GRANTED--0003-check-for-at-end-of-data-in-iterator-init.txt",,,,,,,,,,5.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19571,,,Wed May 13 09:26:39 UTC 2009,,,,,,,,,,"0|i0fx73:",90976,,,,,Normal,,,,,,,,,,,,,,,,,"08/May/09 04:21;nk11;1 node

Default config except:

 <MemtableSizeInMB>1</MemtableSizeInMB>
 <MemtableObjectCountInMillions>0.001</MemtableObjectCountInMillions>
;;;","08/May/09 06:04;jbellis;try it with this patch.  if that doesn't work, rm -r /var/cassandra/* to reset things and try again; it's possible that you have corrupt data from a bug that is fixed now.;;;","08/May/09 06:30;nk11;patched, rebuilded, removed var/cassandra, still failing with the same exception;;;","08/May/09 14:32;nk11;If it helps:

The exception is thrown in SequenceFile.java at line 562 in getPositionFromBlockIndex() method.

     String blockIndexKey = file_.readUTF();

The key is ""k1:0"" and blockIndexPosition=102016	

If I do a file_.read() there I got -1 so the end of the file is reached.;;;","09/May/09 00:18;jbellis;committed first patch.

i can reproduce the exception now using a different test after one more patch to fix another bug first (attached).;;;","09/May/09 00:18;jbellis;fix for CME during getKeyRange;;;","09/May/09 01:22;nk11;it works;;;","09/May/09 05:02;nk11;I spoke to soon. For 20000 keys it reproduces again...;;;","09/May/09 09:41;jbellis;committed second patch.  still working on reading-past-EOF bug.  I think it's compaction-related.;;;","11/May/09 22:31;jbellis;fixed two more bugs.  the first is what you were running into; SequenceFile and FileStruct both assume when seeking that the block index exists at the end of the Coordinate computed by SSTable.  But we weren't always consistent in specifying the index filename and instead of raising an error Coordinate would just return something bogus.  these are both fixed in 02.

03 fixes a bug in FS iteration when no keys in the range specified exist in a given SSTable.;;;","12/May/09 04:53;junrao;The fix in patch 0003 seems redundant. The same code is already called in advance(). The correct fix seems to be getting rid of  line
           saved = key;
in FileStructIterator().
;;;","12/May/09 05:18;jbellis;no, because then we will always skip the current key, which in the Range context is always going to be a key we are interested in (since seekTo skips all the keys we do not want, stopping when it comes to the first interesting one).

saved = key

in the constructor is what allows next() to return that current key.

but, if seekTo actually skipped _everything_ (and current key is block index) then we don't want to include that.  that's what 0003 fixes.

(I checked in case my intuition was wrong and the test_range_collation system test fails with your alternative, so they are indeed not equivalent.);;;","12/May/09 06:39;junrao;Ok, I understood this now. You are right and the patch looks fine. 

What confused me is that the iterator over FileStruct relies on a seekTo call first, which is not how a typical iterator works.

I got a CME execption on testCompactions. I guess that's related to another issue you just opened.
;;;","12/May/09 07:57;jbellis;committed;;;","13/May/09 17:26;hudson;Integrated in Cassandra #73 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/73/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multi-get slice failing Nullpointer Exception,CASSANDRA-689,12445261,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,lenn0x,lenn0x,12/Jan/10 14:36,16/Apr/19 17:33,22/Mar/23 14:57,14/Jan/10 03:11,0.6,,,,0,,,,,,"Noticed this in trunk

ERROR [pool-1-thread-40] 2010-01-11 22:13:55,333 Cassandra.java (line 960) Internal error processing multiget_slice
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NullPointerException
        at org.apache.cassandra.service.StorageProxy.weakReadLocal(StorageProxy.java:510)
        at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:375)
        at org.apache.cassandra.service.CassandraServer.readColumnFamily(CassandraServer.java:81)
        at org.apache.cassandra.service.CassandraServer.getSlice(CassandraServer.java:164)
        at org.apache.cassandra.service.CassandraServer.multigetSliceInternal(CassandraServer.java:237)
        at org.apache.cassandra.service.CassandraServer.multiget_slice(CassandraServer.java:209)
        at org.apache.cassandra.service.Cassandra$Processor$multiget_slice.process(Cassandra.java:952)
        at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:842)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.service.StorageProxy.weakReadLocal(StorageProxy.java:506)
        ... 11 more
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.filter.SliceQueryFilter.filterSuperColumn(SliceQueryFilter.java:70)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:809)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:750)
        at org.apache.cassandra.db.Table.getRow(Table.java:398)
        at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:59)
        at org.apache.cassandra.service.StorageProxy$weakReadLocalCallable.call(StorageProxy.java:691)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        ... 3 more
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Jan/10 14:28;jbellis;ASF.LICENSE.NOT.GRANTED--0001-fix-enabling-disabling-row-cache.txt;https://issues.apache.org/jira/secure/attachment/12430101/ASF.LICENSE.NOT.GRANTED--0001-fix-enabling-disabling-row-cache.txt","13/Jan/10 14:28;jbellis;ASF.LICENSE.NOT.GRANTED--0002-add-missing-gcBefore-parameter-to-removeDeleted-clone-.txt;https://issues.apache.org/jira/secure/attachment/12430102/ASF.LICENSE.NOT.GRANTED--0002-add-missing-gcBefore-parameter-to-removeDeleted-clone-.txt","13/Jan/10 14:28;jbellis;ASF.LICENSE.NOT.GRANTED--0003-fix-missing-update-of-local-deletion-time-from-a-while.txt;https://issues.apache.org/jira/secure/attachment/12430103/ASF.LICENSE.NOT.GRANTED--0003-fix-missing-update-of-local-deletion-time-from-a-while.txt",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19819,,,Thu Jan 14 12:35:00 UTC 2010,,,,,,,,,,"0|i0g0h3:",91507,,,,,Normal,,,,,,,,,,,,,,,,,"12/Jan/10 21:22;jbellis;looks like a bug I introduced w/ the mmap code.  I don't suppose you have a query that can reproduce it?;;;","13/Jan/10 06:41;jbellis;I can reproduce the problem; it's definitely from the cache patches (not mmap);;;","13/Jan/10 14:29;jbellis;03
    fix missing update of local deletion time from a while ago

02
    add missing gcBefore parameter to removeDeleted; clone SC from cache before modifying

01
    fix enabling/disabling row cache (which exposes rowcache to tests) and fix obvious NPE bugs

;;;","13/Jan/10 15:28;lenn0x;Applied to our unstable cluster, will update ticket when further testing is complete.;;;","14/Jan/10 01:36;lenn0x;+1;;;","14/Jan/10 03:11;jbellis;committed;;;","14/Jan/10 20:35;hudson;Integrated in Cassandra #323 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/323/])
    fix missing update of local deletion time introduced in #658.
patch by jbellis; reviewed by goffinet for 
add missing gcBefore parameter to removeDeleted; clone SC from cache before modifying
patch by jbellis; reviewed by goffinet for 
fix enabling/disabling row cache.
patch by jbellis; reviewed by goffinet for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in AntiEntropyService.getNeighbors,CASSANDRA-1028,12463120,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,stuhood,jbellis,jbellis,27/Apr/10 23:25,16/Apr/19 17:33,22/Mar/23 14:57,24/Jul/10 02:03,0.7 beta 1,,,,0,,,,,,"Sometimes, but not always, I see this during a test run:

    [junit] Testsuite: org.apache.cassandra.service.AntiEntropyServiceTest
    [junit] Tests run: 10, Failures: 0, Errors: 0, Time elapsed: 3.189 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] ERROR 10:19:09,743 Error in executor futuretask
    [junit] java.util.concurrent.ExecutionException: java.lang.NullPointerException
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
    [junit] 	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
    [junit] 	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:87)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:637)
    [junit] Caused by: java.lang.NullPointerException
    [junit] 	at java.util.AbstractCollection.addAll(AbstractCollection.java:303)
    [junit] 	at org.apache.cassandra.service.AntiEntropyService.getNeighbors(AntiEntropyService.java:151)
    [junit] 	at org.apache.cassandra.service.AntiEntropyService.rendezvous(AntiEntropyService.java:176)
    [junit] 	at org.apache.cassandra.service.AntiEntropyService.access$100(AntiEntropyService.java:86)
    [junit] 	at org.apache.cassandra.service.AntiEntropyService$Validator.call(AntiEntropyService.java:487)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	... 2 more
    [junit] ------------- ---------------- ---------------

Ideally it would be nice if this could cause an actual test failure when it happens.  Not sure how feasible that is.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/May/10 02:32;stuhood;0001-Clear-AES-before-begininning-the-next-test.patch;https://issues.apache.org/jira/secure/attachment/12444580/0001-Clear-AES-before-begininning-the-next-test.patch","23/Jul/10 05:15;stuhood;0001-Die-bug-die.patch;https://issues.apache.org/jira/secure/attachment/12450217/0001-Die-bug-die.patch",,,,,,,,,,,,,2.0,stuhood,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19964,,,Tue Jul 27 13:41:02 UTC 2010,,,,,,,,,,"0|i0g2k7:",91845,,,,,Normal,,,,,,,,,,,,,,,,,"16/May/10 02:32;stuhood;Since nothing we do with the TokenMetadata is multithreaded, I'm fairly certain this is caused by background tasks left in the AES stage after tests. This patch clears the stage during teardown. 100ish runs of the test seem to confirm that this fixes the problem.;;;","17/May/10 22:50;jbellis;committed, thanks;;;","18/May/10 21:31;hudson;Integrated in Cassandra #439 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/439/])
    block for AES to clear before we teardown the token metadata for the next test.  patch by Stu Hood; reviewed by jbellis for CASSANDRA-1028
;;;","06/Jul/10 12:01;jbellis;Getting this again / still in latest trunk:

    [junit] Exception in thread ""AE-SERVICE-STAGE:1"" java.lang.NullPointerException
    [junit] 	at java.util.AbstractCollection.addAll(AbstractCollection.java:303)
    [junit] 	at org.apache.cassandra.service.AntiEntropyService.getNeighbors(AntiEntropyService.java:164)
    [junit] 	at org.apache.cassandra.service.AntiEntropyService.rendezvous(AntiEntropyService.java:184)
    [junit] 	at org.apache.cassandra.service.AntiEntropyService.access$100(AntiEntropyService.java:84)
    [junit] 	at org.apache.cassandra.service.AntiEntropyService$TreeResponseVerbHandler.doVerb(AntiEntropyService.java:684)
    [junit] 	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:41)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:637)
;;;","23/Jul/10 05:15;stuhood;Double flush AE_SERVICE_STAGE to ensure that tasks triggered by existing tasks are cleared.;;;","24/Jul/10 02:03;jbellis;committed;;;","27/Jul/10 21:41;hudson;Integrated in Cassandra #501 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/501/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.lang.ArrayIndexOutOfBoundsException while executing repair on a freshly added node (0.7.0),CASSANDRA-1959,12495158,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,tbritz,tbritz,11/Jan/11 03:03,16/Apr/19 17:33,22/Mar/23 14:57,12/Jan/11 07:41,0.6.10,0.7.1,,,0,,,,,,"Hi,

I added a node to the cluster (20 nodes in total) and ran repair on it after a while.

The repair still runs, but there are errors in the log file (see below).

Some of the data in the clsuter has been filled with rc-4. The cluster runs on version 0.7.0 (the release linked on the main cassandra web site).

Any ideas what might cause this?
(PS. 0.7.0 turns up as unreleased version in Affects Version/s:) 


INFO [CompactionExecutor:1] 2011-01-10 19:55:20,684 SSTableReader.java (line 158) Opening /hd2/cassandra_md5/data/table_x/table_x-e-4
 INFO [CompactionExecutor:1] 2011-01-10 19:55:20,775 SSTableReader.java (line 158) Opening /hd2/cassandra_md5/data/table_x/table_x_meta-e-14
ERROR [RequestResponseStage:3] 2011-01-10 19:55:20,856 DebuggableThreadPoolExecutor.java (line 103) Error in ThreadPoolExecutor
java.lang.ArrayIndexOutOfBoundsException: -1
        at java.util.ArrayList.fastRemove(ArrayList.java:441)
        at java.util.ArrayList.remove(ArrayList.java:424)
        at com.google.common.collect.AbstractMultimap.remove(AbstractMultimap.java:219)
        at com.google.common.collect.ArrayListMultimap.remove(ArrayListMultimap.java:60)
        at org.apache.cassandra.net.MessagingService.responseReceivedFrom(MessagingService.java:436)
        at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:40)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:63)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
ERROR [RequestResponseStage:3] 2011-01-10 19:55:20,856 AbstractCassandraDaemon.java (line 91) Fatal exception in thread Thread[RequestResponseStage:3,5,main]
java.lang.ArrayIndexOutOfBoundsException: -1
        at java.util.ArrayList.fastRemove(ArrayList.java:441)
        at java.util.ArrayList.remove(ArrayList.java:424)
        at com.google.common.collect.AbstractMultimap.remove(AbstractMultimap.java:219)
        at com.google.common.collect.ArrayListMultimap.remove(ArrayListMultimap.java:60)
        at org.apache.cassandra.net.MessagingService.responseReceivedFrom(MessagingService.java:436)
        at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:40)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:63)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
",,,,,,,,,,,,,,,,,,,,,,,14400,14400,,0%,14400,14400,,,,,,,,,,,,,,,,,,,,"12/Jan/11 01:24;jbellis;1959-v2.txt;https://issues.apache.org/jira/secure/attachment/12468027/1959-v2.txt","12/Jan/11 05:03;messi;post-1959-v2-cleanup.patch.txt;https://issues.apache.org/jira/secure/attachment/12468057/post-1959-v2-cleanup.patch.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20382,,,Wed Jan 12 00:38:08 UTC 2011,,,,,,,,,,"0|i0g8g7:",92799,,messi,,messi,Normal,,,,,,,,,,,,,,,,,"11/Jan/11 16:21;messi;Looks like MessagingService.targets needs to be synchronized.;;;","12/Jan/11 00:46;nickmbailey;Looks like this was caused by CASSANDRA-1905. Or maybe just exposed by it.

I think this may be particularly nasty due to the fact that droped responses will lead to inflated timings reported to the DynamicEndpointSnitch.  I'm not sure but I think this may be contributing to flapping in another case where I've seen this error.;;;","12/Jan/11 01:24;jbellis;Right, this is from 1905 and there's definitely a concurrency bug here.

I don't want to use the synchronized multimap option though since *every* message hits this so we need better concurrency.  v2 is a patch that replaces multimap with a manually constructed Map of Sets.;;;","12/Jan/11 05:03;messi;I saw that you added putTarget() as instance method although targets itself is a static field. This patch cleans up these inconsistencies and makes MessagingService a true singleton.;;;","12/Jan/11 08:37;hudson;Integrated in Cassandra-0.7 #152 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/152/])
    convert MessagingService into a true singleton
patch by Folke Behrens; reviewed by jbellis for CASSANDRA-1959
fix race condition in MessagingService.targets
patch by jbellis; reviewed by Folke Behrens for CASSANDRA-1959
;;;","12/Jan/11 08:38;hudson;Integrated in Cassandra-0.6 #49 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/49/])
    backport CASSANDRA-1959 from 0.7
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE during read repair,CASSANDRA-478,12437607,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,08/Oct/09 23:44,16/Apr/19 17:33,22/Mar/23 14:57,13/Oct/09 03:57,0.4,,,,0,,,,,,"From Teodor Sigaev:

ERROR [RESPONSE-STAGE:1] 2009-10-08 17:05:25,864 DebuggableThreadPoolExecutor.java (line 110) Error in ThreadPoolExecutor
java.lang.NullPointerException
       at org.apache.cassandra.service.ConsistencyManager$DigestResponseHandler.handleDigestResponses(ConsistencyManager.java:68)
       at org.apache.cassandra.service.ConsistencyManager$DigestResponseHandler.response(ConsistencyManager.java:55)
       at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:35)
       at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:39)
       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
       at java.lang.Thread.run(Thread.java:636)
ERROR [RESPONSE-STAGE:1] 2009-10-08 17:05:25,916 CassandraDaemon.java (line 71) Fatal exception in thread Thread[RESPONSE-STAGE:1,5,main]
java.lang.NullPointerException
       at org.apache.cassandra.service.ConsistencyManager$DigestResponseHandler.handleDigestResponses(ConsistencyManager.java:68)
       at org.apache.cassandra.service.ConsistencyManager$DigestResponseHandler.response(ConsistencyManager.java:55)
       at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:35)
       at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:39)
       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Oct/09 02:12;jbellis;478-asserts.patch;https://issues.apache.org/jira/secure/attachment/12421888/478-asserts.patch","13/Oct/09 02:55;jbellis;478-fix.patch;https://issues.apache.org/jira/secure/attachment/12421893/478-fix.patch","08/Oct/09 23:45;jbellis;478-logging.patch;https://issues.apache.org/jira/secure/attachment/12421630/478-logging.patch",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19711,,,Wed Oct 14 07:55:18 UTC 2009,,,,,,,,,,"0|i0fz6f:",91297,,,,,Normal,,,,,,,,,,,,,,,,,"08/Oct/09 23:45;jbellis;Patch to log the Row that encounters this error;;;","13/Oct/09 01:26;teodor;Got it again:
ERROR [RESPONSE-STAGE:2] 2009-10-12 21:17:44,256 DebuggableThreadPoolExecutor.java (line 110) Error in ThreadPoolExecutor
java.lang.RuntimeException: Error handling responses for Row(00000000000176799500 [)]
        at org.apache.cassandra.service.ConsistencyManager$DigestResponseHandler.handleDigestResponses(ConsistencyManager.java:82)
        at org.apache.cassandra.service.ConsistencyManager$DigestResponseHandler.response(ConsistencyManager.java:55)
        at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:35)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:39)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.service.ConsistencyManager$DigestResponseHandler.handleDigestResponses(ConsistencyManager.java:70)
        ... 6 more
ERROR [RESPONSE-STAGE:2] 2009-10-12 21:17:44,256 CassandraDaemon.java (line 71) Fatal exception in thread Thread[RESPONSE-STAGE:2,5,main]
java.lang.RuntimeException: Error handling responses for Row(00000000000176799500 [)]
        at org.apache.cassandra.service.ConsistencyManager$DigestResponseHandler.handleDigestResponses(ConsistencyManager.java:82)
        at org.apache.cassandra.service.ConsistencyManager$DigestResponseHandler.response(ConsistencyManager.java:55)
        at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:35)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:39)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.service.ConsistencyManager$DigestResponseHandler.handleDigestResponses(ConsistencyManager.java:70)
        ... 6 more
;;;","13/Oct/09 01:51;jbellis;is it reproducible when you re-run the query on that key? (00000000000176799500);;;","13/Oct/09 02:10;teodor;No, it works ;;;","13/Oct/09 02:12;jbellis;asserts to find where the null Message body is coming from;;;","13/Oct/09 02:16;teodor;Ok, will apply;;;","13/Oct/09 02:55;jbellis;aha, it's a race condition :)

here's a fix.;;;","13/Oct/09 02:58;jbellis;(there's no way for a null Message to be passed to response() -- see ResponseVerbHandler for details.  but it could see a null in the List if there is another thread modifying it concurrently.);;;","13/Oct/09 03:02;urandom;Nice. :) +1;;;","13/Oct/09 03:57;jbellis;committed to 0.4 and trunk;;;","13/Oct/09 20:35;hudson;Integrated in Cassandra #226 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/226/])
    move callback synchronization into ResponseVerbHandler where it's less likely to be a maintenance problem
patch by jbellis for 
r/m unused attachContext.
patch by jbellis for 
cleanup Message; add asserts
patch by jbellis for 
logging & cleanup in ConsistencyManager
patch by jbellis for 
;;;","14/Oct/09 15:55;teodor;Thank you a lot!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Server doesn't seem to close SSTables correctly, and ends up with too many file descriptors open",CASSANDRA-1495,12473885,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,brandon.williams,blanquer,blanquer,11/Sep/10 08:45,16/Apr/19 17:33,22/Mar/23 14:57,13/Sep/10 08:55,0.7 beta 2,,,,0,,,,,,"Cassandra server accumulates many open file descriptors as the time progresses. Obviously when it hits whatever limit the system allows, the service stops accepting messages.

The exact trace is:
WARN 05:41:47,929 Transport error occurred during acceptance of message.
org.apache.thrift.transport.TTransportException: java.net.SocketException: Too many open files
        at org.apache.thrift.transport.TServerSocket.acceptImpl(TServerSocket.java:124)
        at org.apache.thrift.transport.TServerSocket.acceptImpl(TServerSocket.java:35)
        at org.apache.thrift.transport.TServerTransport.accept(TServerTransport.java:31)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer.serve(CustomTThreadPoolServer.java:98)
        at org.apache.cassandra.thrift.CassandraDaemon.start(CassandraDaemon.java:210)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.commons.daemon.support.DaemonLoader.start(DaemonLoader.java:177)
Caused by: java.net.SocketException: Too many open files
        at java.net.PlainSocketImpl.socketAccept(Native Method)
        at java.net.PlainSocketImpl.accept(PlainSocketImpl.java:390)
        at java.net.ServerSocket.implAccept(ServerSocket.java:453)
        at java.net.ServerSocket.accept(ServerSocket.java:421)
        at org.apache.thrift.transport.TServerSocket.acceptImpl(TServerSocket.java:119)
        ... 9 more

I've increased the ulimit to 8K from the standard 1K ... but still gets hit. So it seems that basically there are many fd's that never get closed.
lsof shows that there are many fd's hanging on to SSTables...albeit it's only a small subset of unique files. In my case there were only about 4 distinct SSTable files...but kept opened hundreds of time each. 
All of these files seem to be ""Data"" files....no Filter or Index ones (as far as I remember)

In case it matters:  DiskAccessMode 'auto' determined to be standard, indexAccessMode is standard

The service doesn't have a high rate of writes at the moment...I have 3 nodes, 1 being receiving mostly the thrift entry point. I see the problem being more acute on the 2 nodes that instead of being contacted by the clients, they just get the writes from the coordinator...(I have RF=2). 
",0.7 beta1. Built from: http://www.apache.org/dyn/closer.cgi?path=/cassandra/0.7.0/apache-cassandra-0.7.0-beta1-src.tar.gz using dpkg-buildpackage from it. ,brandon.williams,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20167,,,Mon Sep 13 00:55:23 UTC 2010,,,,,,,,,,"0|i0g5ev:",92307,,,,,Normal,,,,,,,,,,,,,,,,,"12/Sep/10 01:24;brandon.williams;Can you reproduce on trunk?  This sounds like a duplicate of CASSANDRA-1410.;;;","12/Sep/10 01:53;blanquer;1410? Is that the number you meant to refer to? I cannot see any resemblance...you probably know better though.

I'm not sure if this reproduces on trunk, this is a running and loaded system and I cannot switch to trunk easily at all.;;;","12/Sep/10 03:53;brandon.williams;Hah, oops, I meant CASSANDRA-1416;;;","13/Sep/10 06:31;blanquer;Ah, yes, it smells quite similar. There's not much text/description in it, I think that's why I couldn't find it when I searched around before opening the ticket.
So, I've done some in-place patching on some of the machines, and I believe it is exactly the same problem (they don't seem to leak anymore).
..so that 'solves' it for now I guess. This ticket should be closed or marked as duplicate?...not sure how you guys do it in here, so I'll let you proceed with it if you don't mind. :)
Thanks for the pointer Brandon.;;;","13/Sep/10 08:55;brandon.williams;Closed as duplicate.  By the way, I did not search jira to find the issue, I knew that we'd already fixed an FD leak since b1, so I looked at CHANGES.txt in trunk to find it.  Incidentally, 1410 was right after it, which is why I linked the wrong ticket the first time. :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Cannot get_slice from CF defined with CompareWith=""TimeUUIDType""",CASSANDRA-377,12433405,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,eweaver,eweaver,19/Aug/09 09:53,16/Apr/19 17:33,22/Mar/23 14:57,20/Aug/09 00:28,,,,,0,,,,,,"      <Keyspace Name=""Multiblog"">      
        <KeysCachedFraction>0.01</KeysCachedFraction>
        <ColumnFamily CompareWith=""TimeUUIDType"" Name=""Blogs""/>
        <ColumnFamily CompareWith=""TimeUUIDType"" Name=""Comments""/>
      </Keyspace>

>> multiblog.insert(:Comments, ""test"", {UUID.new => 'I like this cat'})
=> nil
>> multiblog.get(:Comments, ""test"")
Thrift::ApplicationException: Thrift::ApplicationException

Server said:

DEBUG - get_slice_from
ERROR - Internal error processing get_slice
org.apache.cassandra.db.marshal.MarshalException: UUIDs must be exactly 16 bytes
	at org.apache.cassandra.db.marshal.TimeUUIDType.getString(TimeUUIDType.java:48)
	at org.apache.cassandra.db.SliceFromReadCommand.toString(SliceFromReadCommand.java:71)
	at java.lang.String.valueOf(String.java:2827)
	at java.lang.StringBuilder.append(StringBuilder.java:115)
	at org.apache.cassandra.service.StorageProxy.weakReadLocal(StorageProxy.java:602)
	at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:320)
	at org.apache.cassandra.service.CassandraServer.readColumnFamily(CassandraServer.java:92)
	at org.apache.cassandra.service.CassandraServer.getSlice(CassandraServer.java:173)
	at org.apache.cassandra.service.CassandraServer.get_slice(CassandraServer.java:213)
	at org.apache.cassandra.service.Cassandra$Processor$get_slice.process(Cassandra.java:572)
	at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:560)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:252)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:637)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Aug/09 23:06;eweaver;CASSANDRA-377.diff;https://issues.apache.org/jira/secure/attachment/12417025/CASSANDRA-377.diff",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19659,,,Thu Aug 20 13:05:50 UTC 2009,,,,,,,,,,"0|i0fykf:",91198,,,,,Normal,,,,,,,,,,,,,,,,,"19/Aug/09 09:55;eweaver;Maybe start/finish aren't allowed to be empty strings anymore?;;;","19/Aug/09 09:59;eweaver;Yeah...let me know if

get_slice(""Multiblog"", ""test"", <CassandraThrift::ColumnParent column_family:""Comments"">, <CassandraThrift::SlicePredicate slice_range:<CassandraThrift::SliceRange start:"""", finish:"""", reversed:false, count:100>>, 1)

is supposed to work;;;","19/Aug/09 10:05;eweaver;if I send nil:

<CassandraThrift::SlicePredicate slice_range:<CassandraThrift::SliceRange start:nil, finish:nil, reversed:false, count:100>>

I get:

ERROR - Internal error processing get_slice
java.lang.NullPointerException
	at org.apache.cassandra.db.marshal.TimeUUIDType.getString(TimeUUIDType.java:46)
	at org.apache.cassandra.db.SliceFromReadCommand.toString(SliceFromReadCommand.java:71)
	at java.lang.String.valueOf(String.java:2827)
	at java.lang.StringBuilder.append(StringBuilder.java:115)
;;;","19/Aug/09 11:02;jbellis;looks like setting logger to INFO will work around this so it doesn't try to tostring an empty start;;;","19/Aug/09 11:33;eweaver;oh, it's the logger killing it? easy fix i guess. will try.;;;","20/Aug/09 00:28;jbellis;Applied, with similar fixes for LexicalUUID and Long types.  Also fixed the Command toStrings to use the subcolumn comparators when toStringing slices of subcolumns (that is why the test suite didn't hit this bug before).;;;","20/Aug/09 21:05;hudson;Integrated in Cassandra #173 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/173/])
    use the subcomparator when toStringing slice commands on subcolumns.  this exposes a couple bugs: fix getString in non-string types to accept byte[0], and fix a test to send a long to a LongType subcolumn.  patch by jbellis and Evan Weaver for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Blocking insert may have fewer responses than replication factor,CASSANDRA-180,12425493,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,junrao,junrao,junrao,15/May/09 02:42,16/Apr/19 17:33,22/Mar/23 14:57,16/May/09 05:37,,,,,0,,,,,,"Currently, block_insert always assumes the number of responses equals the replication factor. However, for a small cluster (e,g, 1 node) and/or when failure occurs, the number of responses could be fewer than the replication factor.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/May/09 23:17;jbellis;180-v2.patch;https://issues.apache.org/jira/secure/attachment/12408253/180-v2.patch","15/May/09 23:20;jbellis;180-v3.patch;https://issues.apache.org/jira/secure/attachment/12408254/180-v3.patch","15/May/09 02:45;junrao;issue180.patchv1;https://issues.apache.org/jira/secure/attachment/12408161/issue180.patchv1",,,,,,,,,,,,3.0,junrao,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19585,,,Sat May 16 13:04:11 UTC 2009,,,,,,,,,,"0|i0fxcv:",91002,,,,,Low,,,,,,,,,,,,,,,,,"15/May/09 02:45;junrao;Attach a fix. The expected number of responses is set based on the number of live nodes identified.;;;","15/May/09 23:17;jbellis;Reducing the responseCount is going to break things, since that's used for determining when a successful quorum has been reached.

Say you have a 5 node cluster and a replication factor of 3.  But there is a network split and the node a client is talking to can only see itself.  With your patch it would start up a QRH with a RC of 1, get the ack, and report that the write was successful.  But we've just sliently violated our promise of quorum consistency (at least 2 nodes).

The existing code is optimal for when a write succeeeds -- as soon as a quorum is reached it returns, w/o waiting for any more responses that may or may not come.  The only problem is that it will wait for timeout when it is impossible for a write to reach quorum b/c there are not enough nodes.  I've attached a patch that addresses that problem.  What do you think?

(Note that we don't need to try to solve the problem of ""what if at the beginning of a write there are enough nodes to reach quorum, but partway through we get a nack from a node making it impossible"" b/c nodes only ack success, they don't nack failure.  And making them do so adds more complication than it is worth for such an uncommon case.);;;","15/May/09 23:20;jbellis;v3 checks for the right quorum count.;;;","15/May/09 23:31;sandeep_tata;+1 for v3

We can now get rid of ""// TODO: throw a thrift exception if we do not have N nodes"";;;","16/May/09 05:20;junrao;v3 looks good to me too.
;;;","16/May/09 05:37;jbellis;committed v3, and r/m'd the TODO line.;;;","16/May/09 21:04;hudson;Integrated in Cassandra #78 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/78/])
    check for enough endpoints before starting a quorum wait.
patch by jbellis; reviewed by Jun Rao and Sandeep Tata for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Major compaction still leaves large set of files,CASSANDRA-473,12437407,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,lenn0x,lenn0x,06/Oct/09 23:46,16/Apr/19 17:33,22/Mar/23 14:57,08/Oct/09 03:59,0.4,,,,0,,,,,,"We did a major compaction on roughly 1000-2000 files. The disk drive had a capacity of 1.6TB. The disk usage with Cassandra was 1.1TB. I saw this error, maybe this is why compaction did not finish? Attaching system.log",,hammer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Oct/09 01:46;jbellis;473.patch;https://issues.apache.org/jira/secure/attachment/12421447/473.patch","06/Oct/09 23:47;lenn0x;system.log;https://issues.apache.org/jira/secure/attachment/12421434/system.log",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19708,,,Thu Oct 08 12:35:14 UTC 2009,,,,,,,,,,"0|i0fz5j:",91293,,,,,Normal,,,,,,,,,,,,,,,,,"07/Oct/09 01:46;jbellis;okay, so here's what is going on.  there is a feature involved, and a bug :)

the feature is that you will end up with multiple files if you try to major compact but don't have enough room.  cassandra can't r/m the old files until the new ones are finished (in the worst case), so it will cut down the compaction set to something it knows will fit in the remaining space.

the bug is that using subList in doCompaction is causing the java.util.Collections$UnmodifiableCollection.remove error, since like arrays.asList, the list objects returned by subList don't support remove().;;;","08/Oct/09 03:47;lenn0x;This patch worked. It removed the old files, 200+ down to 32. (technically 8 data files)

+1;;;","08/Oct/09 03:59;jbellis;committed to 0.4 and trunk;;;","08/Oct/09 20:35;hudson;Integrated in Cassandra #221 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/221/])
    create new collection when reducing the number of sstables compacted; the lists returned by subList are unmodifiable
patch by jbellis; reviewed by goffinet for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
batch insert failing with TokenMetadata AssertionError,CASSANDRA-722,12445995,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jaakko,dispalt,dispalt,20/Jan/10 12:38,16/Apr/19 17:33,22/Mar/23 14:57,22/Jan/10 08:01,0.5,,,,0,,,,,,"I get this during the course of an insert.

ERROR [pool-1-thread-17] 2010-01-20 03:50:40,517 Cassandra.java (line 1096) Internal error processing batch_insert
java.lang.AssertionError
        at org.apache.cassandra.locator.TokenMetadata.getToken(TokenMetadata.java:212)
        at org.apache.cassandra.locator.AbstractReplicationStrategy.getHintedMapForEndpoints(AbstractReplicationStrategy.java:129)
        at org.apache.cassandra.locator.AbstractReplicationStrategy.getHintedEndpoints(AbstractReplicationStrategy.java:76)
        at org.apache.cassandra.service.StorageService.getHintedEndpointMap(StorageService.java:1183)
        at org.apache.cassandra.service.StorageProxy.insert(StorageProxy.java:101)
        at org.apache.cassandra.service.CassandraServer.doInsert(CassandraServer.java:470)
        at org.apache.cassandra.service.CassandraServer.batch_insert(CassandraServer.java:445)
        at org.apache.cassandra.service.Cassandra$Processor$batch_insert.process(Cassandra.java:1088)
        at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:817)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)","r900058  4 node cluster, 1 more bootstrapping",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jan/10 09:57;jaakko;722.patch;https://issues.apache.org/jira/secure/attachment/12430977/722.patch",,,,,,,,,,,,,,1.0,jaakko,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19837,,,Fri Jan 22 00:01:08 UTC 2010,,,,,,,,,,"0|i0g0o7:",91539,,,,,Normal,,,,,,,,,,,,,,,,,"20/Jan/10 12:42;jbellis;have you done any node movement in this cluster?;;;","20/Jan/10 12:57;dispalt;I have used loadbalance quite a few times.  I have never done a decommision or a move command explicitly.

A machine does seem to be stalled in the middle of bootstrapping.  Trying to get intellij setup, to dig deeper on to what that machine is doing.;;;","20/Jan/10 13:16;jaakko;Seems there's a bug in getHintedMapForEndpoints. 

That bootsrapping node is probably considered dead by other nodes

This is what's happening:
(1) Node bootstraps -> it has pending ranges, but it is not member of token ring
(2) During write, pending ranges match -> node is added to write endpoints
(3) Node is down -> a hinted target will be searched for
(4) getToken explodes because endpoint is not member

There are two things that need to be considered:
(1) We probably should ignore endpoints that are not members when looking for hinted targets
(2) That assert in getToken is a bit outdated I think. Nodes may come and go during the time ARS and friends are looking for write targets. It might very well happen that a node was removed between getting natural endpoints and coming back to get hinted targets. There's a comment saying ""don't want to return nulls"", but perhaps we'll need to reconsider this?
;;;","20/Jan/10 13:41;jbellis;(1) definitely
(2) Under what conditions does it make sense to ask for Token of a node that is not a ring member?;;;","20/Jan/10 14:07;jaakko;(2) it does not make sense, but it might happen. Suppose a node is natural endpoint of a write. StorageProxy gets these in mutate, and then passes the same list to getHintedEndpointMap. If one of these natural endpoints is removed in between getNaturalEndpoints and finding hinted targets, there will be a call to getToken for non-existing endpoint. It does not help even if we insert isMember check right before getToken, as there is still small window that could cause this.

Basically the same can happen wherever we use getToken;;;","20/Jan/10 14:14;jbellis;We could fix that by picking a random node to start our scan of potential hint destinations; there's nothing magical about starting w/ endpoint+1.  In fact picking random is better, since currently having a node down degrades performance more than it should since all the hints go to the same node.;;;","20/Jan/10 22:20;jaakko;Is it really OK to send hints to random targets? That will scatter hinted data around the cluster as each hinted write may go to different location. Might not be problematic, but feels like we're going around the problem instead of fixing the cause. Something would certainly be gained by having the load spread amongst nodes, but isn't something also lost if (potentially) all nodes in the cluster stream hinted data instead of one node?
;;;","20/Jan/10 22:31;jbellis;no, we don't rely on hint location.

but actually using sortByProximity is probably best of all.;;;","20/Jan/10 23:12;jaakko;yeah, proximity seems good.

Attached patch ignores dead bootstrappers and looks for hinted locations based on proximity (list of addresses is from gossiper.liveMembers).

This couples ARS with Gossiper, but don't know if that matters so much as it used FD already before.

Edit: this patch has not been tested much, have to do some more testing tomorrow.;;;","20/Jan/10 23:28;jbellis;+1;;;","21/Jan/10 05:01;dispalt;Is there a way to remove the failed bootstrapped node from the cluster, decommission obviously doesn't work...

Or should this patch fix things?;;;","21/Jan/10 05:30;jbellis;this will fix your batch_insert problem;;;","21/Jan/10 05:49;dispalt;Should I add another bug for the failed bootstrapping problem?  

The node seems to be getting data; like it thinks its part of the cluster and datagrowth is that of a normal working node.  However, it doesn't have all the data (should be at least 50g and it 3 gb) and if I try to nodeprobe ring it doesn't show up.;;;","21/Jan/10 09:36;jaakko;new version. skip non-members also when considering hinted location.
;;;","21/Jan/10 09:39;jaakko;> Is there a way to remove the failed bootstrapped node from the cluster, decommission obviously doesn't work...

Actually there is an ""undocumented"" feature in removetoken command that it will clear the token from bootstrapping as well. If you know the token, remove it and that should take care of it.

Probably need to provide better tools to investigate/poke bootstrapping and leaving tokens.
;;;","21/Jan/10 09:47;dispalt;I think jonathan mentioned that I would need the patch rom 644 is that correct?;;;","21/Jan/10 09:57;jaakko;accidentally submitted a japanese version, this should be better.
;;;","21/Jan/10 09:59;jaakko;yeah, 644 is needed to remove it from gossip, but it will be removed from token metadata even without 644, which is what matters in this case.
;;;","22/Jan/10 08:01;jaakko;committed to 0.5.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Untrapped exceptions in ThreadPool have a variety of ill effects,CASSANDRA-1776,12480885,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,appodictic,appodictic,25/Nov/10 11:32,16/Apr/19 17:33,22/Mar/23 14:57,16/May/12 01:48,,,,,0,,,,,,"I have seen a variety of conditions that keep the Cassandra process running even though it mostly failed. At times the node stays up sending gossip messages so other nodes think the node is up. In the worst case condition a node gets in a tight loop fully utilizing 16 cores of a system and sending gossip messages that cause cascading issues across the cluster. 

I have seen untrapped OOM errors.  The interesting part of the attached log is that we are not using super columns. I also have machines that come up out of a 40 second garbage collect, (I assume they gossip themselves as UP)  messages then go back into a garbage collect to repeat again.",,gdusbabek,jbellis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Nov/10 11:33;appodictic;logs;https://issues.apache.org/jira/secure/attachment/12460428/logs",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20308,,,Tue May 15 17:48:06 UTC 2012,,,,,,,,,,"0|i0g7br:",92617,,,,,Normal,,,,,,,,,,,,,,,,,"30/Nov/10 01:04;jbellis;the errors in the attached file are not fatal.  looks like things are working as designed, modulo the NPE in the first place.

a real OOM should take down the server, this works very well in my experience.  if you have a log file post 0.6.3 where it does not then we can fix that.

yes, a ""GC storm"" will give you a scenario where the local Cassandra believes it is fine but really it is not.  Dynamic snitch is designed to mitigate this but really it needs to be solved through monitoring and tuning.;;;","30/Nov/10 01:18;appodictic;I may have explained poorly. On two occasions ~20 minutes after I see this in the logs Cassandra on this node is at 100% user+system on all cores. The entire cluster quickly degrades. Many pending messages in the Gossip stage and the entire cluster is 100% CPU on all cores. The only course of action is to bring down the entire cluster, or if you catch the problem early enough bring down multiple nodes at a time.;;;","30/Nov/10 03:35;jbellis;Get a stack trace when it's doing that (e.g. w/ jstack), then use the steps at http://publib.boulder.ibm.com/infocenter/javasdk/tools/index.jsp?topic=/com.ibm.java.doc.igaa/_1vg0001475cb4a-1190e2e0f74-8000_1007.html to find out which Cassandra/JVM threads are taking up all that CPU so we can figure out what is going on.;;;","16/May/12 01:48;jbellis;uncaught OOMs were fixed in CASSANDRA-3201;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broken keyspace strategy_option with zero replicas,CASSANDRA-1924,12494336,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,thorcarpenter,thorcarpenter,31/Dec/10 06:59,16/Apr/19 17:33,22/Mar/23 14:57,12/Mar/11 02:18,0.7.4,,,,0,,,,,,"When a keyspace is defined that has strategy options specifying zero replicas should be place in a datacenter (e.g. DCX:0), an assert is violated for any insert and LOCAL_QUORUM reads fail.  I'm not sure if the issue is that there are no nodes in DCX or that I'm saying DCX shouldn't get any replicas, or a combination of the two.

The broken keyspace:

create keyspace KeyspaceDC1 with
  replication_factor = 1 and
  placement_strategy = 'org.apache.cassandra.locator.NetworkTopologyStrategy' and
  strategy_options = [{DC1:1, DC2:0}];

The fixed keyspace:

create keyspace KeyspaceDC1 with
  replication_factor = 1 and
  placement_strategy = 'org.apache.cassandra.locator.NetworkTopologyStrategy' and
  strategy_options = [{DC1:1}];


To reproduce:

* Install the 0.7rc3 rpm on a single node in ""DC1"".
* In cassandra.yaml set initial_token = 1 and specify PropertyFileSnitch.
* cassandra-topology.properties:

10.5.64.26=DC1:R1
default=DC2:R1

* Schema loaded via cassandra-cli:

create keyspace KeyspaceDC1 with
  replication_factor = 1 and
  placement_strategy = 'org.apache.cassandra.locator.NetworkTopologyStrategy' and
  strategy_options = [{DC1:1, DC2:0}];

use KeyspaceDC1;

create column family TestCF with
  column_type = 'Standard' and
  comparator = 'BytesType' and
  keys_cached = 200000 and
  rows_cached = 2000 and
  gc_grace = 0 and
  read_repair_chance = 0.0;

* In cassandra-cli execute the following:

[default@unknown] use KeyspaceDC1;
Authenticated to keyspace: KeyspaceDC1
[default@KeyspaceDC1] set TestCF['some key']['some col'] = 'some value';
Internal error processing insert

* If you have asserts enabled, check system.log where you should find the assertion error: 

DEBUG [pool-1-thread-3] 2010-12-29 12:10:38,897 CassandraServer.java (line 362) insert
ERROR [pool-1-thread-3] 2010-12-29 12:10:38,906 Cassandra.java (line 2960) Internal error processing insert
java.lang.AssertionError
        at org.apache.cassandra.locator.TokenMetadata.firstTokenIndex(TokenMetadata.java:392) 
        at org.apache.cassandra.locator.TokenMetadata.ringIterator(TokenMetadata.java:417)
        at org.apache.cassandra.locator.NetworkTopologyStrategy.calculateNaturalEndpoints(NetworkTopologyStrategy.java:95)
        at org.apache.cassandra.locator.AbstractReplicationStrategy.getNaturalEndpoints(AbstractReplicationStrategy.java:99)
        at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:1411)
        at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:1394)
        at org.apache.cassandra.service.StorageProxy.mutate(StorageProxy.java:109)
        at org.apache.cassandra.thrift.CassandraServer.doInsert(CassandraServer.java:442)
        at org.apache.cassandra.thrift.CassandraServer.insert(CassandraServer.java:379)
        at org.apache.cassandra.thrift.Cassandra$Processor$insert.process(Cassandra.java:2952)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2555)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) 
        at java.lang.Thread.run(Thread.java:619)

* If you don't have asserts enabled, you should find that no errors are logged but LOCAL_QUORUM reads cause TimedOutExceptions on the client.
",,thorcarpenter,tommysdk,,,,,,,,,,,,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,,,,,"11/Mar/11 22:54;slebresne;0001-Ring-iterator-empty-list-test.patch;https://issues.apache.org/jira/secure/attachment/12473396/0001-Ring-iterator-empty-list-test.patch","11/Mar/11 21:43;slebresne;0002-NetworkTopologyStrategy-test-and-small-optim.patch;https://issues.apache.org/jira/secure/attachment/12473392/0002-NetworkTopologyStrategy-test-and-small-optim.patch",,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20372,,,Fri Mar 11 23:07:03 UTC 2011,,,,,,,,,,"0|i0g88v:",92766,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"10/Mar/11 17:11;tommysdk;So the AssertionError occurs in TokenMetadata.firstTokenIndex where the token ring is expected to have a size greater than 0. A discovery I made is that the TokenMetadata.ringIterator method which makes the call to firstTokenIndex, also expects the ring to be non-empty if the includeMin parameter is true:
final boolean insertMin = (includeMin && !ring.get(0).equals(StorageService.getPartitioner().getMinimumToken())) ? true : false;

If the includeMin parameter is true and the ring is empty, it would raise a java.lang.IndexOutOfBoundsException: Index: 0, Size: 0. However, the method only seems to be called with includeMin == true from StorageProxy.getRestrictedRanges. As in the case of this issue, it is called from NetworkTopologyStrategy with a includeMin == false, thus failing at the assertion in TokenMetadata.firstTokenIndex since there are no TokenMetadata tokens present.

;;;","11/Mar/11 21:43;slebresne;Indeed ringIterator() shouldn't fail when the argument token list is empty.

Attaching patch to fix this with a unit test.

The second patch attached adds a more high level unit test that tests the problem at the level of NetworkTopologyStrategy (while the first patch has a ringIterator test). It also include a tiny optimisation for NetworkTopologyStrategy that skip unnecessary steps when the number of replicas in the DC is 0. This optimisation would hide the actual problem so that's why it's attached separately.;;;","11/Mar/11 22:18;jbellis;would it be simpler to say something like ""if ring.isEmpty() return Iterators.emptyIterator()""?;;;","11/Mar/11 22:54;slebresne;You are right, it's a bit cleaner. Attaching new version of first patch using Iterators functions.;;;","12/Mar/11 02:18;jbellis;committed w/o the NTS optimization.

(open to being convinced otherwise but my reasoning is that performance is not critical--since the result of calculateNE is cached by ARS.getNE--so we should not add special cases there that can hide bugs.);;;","12/Mar/11 07:07;hudson;Integrated in Cassandra-0.7 #375 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/375/])
    allow zero replicas in a NTSdatacenter
patch by slebresne; reviewed by jbellis for CASSANDRA-1924
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
describe_ring() throws on single node clusters and/or probably clusters without a replication factor,CASSANDRA-1111,12465029,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,gdusbabek,dccwilliams,dccwilliams,20/May/10 22:12,16/Apr/19 17:33,22/Mar/23 14:57,29/May/10 03:28,0.6.3,0.7 beta 1,,,0,describe_ring,exception,replication,,,"You use Thrift to call describe_ring() on a cluster with only a single node. The Thrift connection is broken, and the system.log shows the exception that has been thrown:

ERROR [pool-1-thread-15] 2010-05-20 13:15:24,753 TThreadPoolServer.java (line 259) Error occurred during processing of message.
java.lang.RuntimeException: No replica strategy configured for L1AbuseReports
        at org.apache.cassandra.service.StorageService.getReplicationStrategy(StorageService.java:246)
        at org.apache.cassandra.service.StorageService.constructRangeToEndPointMap(StorageService.java:457)
        at org.apache.cassandra.service.StorageService.getRangeToAddressMap(StorageService.java:443)
        at org.apache.cassandra.service.StorageService.getRangeToEndPointMap(StorageService.java:433)
        at org.apache.cassandra.thrift.CassandraServer.describe_ring(CassandraServer.java:628)
        at org.apache.cassandra.thrift.Cassandra$Processor$describe_ring.process(Cassandra.java:1781)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:1125)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:637)",Mac OS X Snow Leopard,,,,,,,,,,,,,,,,,,,,,,2700,2700,,0%,2700,2700,,,,,,,,,,,,,,,,,,,,"29/May/10 02:28;gdusbabek;1111-0.6.txt;https://issues.apache.org/jira/secure/attachment/12445800/1111-0.6.txt",,,,,,,,,,,,,,1.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,687,,,Mon May 31 09:59:46 UTC 2010,,,,,,,,,,"0|i0g32f:",91927,,,,,Normal,,,,,,,,,,,,,,,,,"22/May/10 00:27;gdusbabek;I'm having difficulty reproducing this problem in branches/cassandra-0.6.  What I comment out <ReplicaPlacementStrategy>, I get the following error on startup:

ERROR 11:26:08,593 Fatal error: Missing replicaplacementstrategy directive for Keyspace1
Bad configuration; unable to start server

I get a similar error when I configure a replicaplacementstrategy, but leave the replicationfactor out.

Is it possible to work up a short test that duplicates this problem?;;;","24/May/10 22:31;gdusbabek;Closing until we better understand how to duplicate the problem.;;;","27/May/10 12:48;tholzer;I can reproduce this with the following:

Linux & Python 2.6.4

{noformat}
from thrift.transport.TSocket import TSocket
from thrift.protocol.TBinaryProtocol import TBinaryProtocol
from cassandra.Cassandra import Client

if __name__ == ""__main__"":
    tsocket = TSocket('localhost', 9160)
    tsocket.open()
    tprotocol = TBinaryProtocol(tsocket)
    client = Client(tprotocol)
    keyspaces = client.describe_keyspaces()
    for keyspace in keyspaces:
        print ""%s"" % client.describe_ring(keyspace)
{noformat}

Output:
{noformat}
[TokenRange(end_token='107294900513650794844962875501795914878', start_token='107294900513650794844962875501795914878', endpoints=['127.0.0.1'])]
Traceback (most recent call last):
  File ""test/t1.py"", line 13, in <module>
    print ""%s"" % client.describe_ring(keyspace)
  File ""cassandra/Cassandra.py"", line 964, in describe_ring
    return self.recv_describe_ring()
  File ""cassandra/Cassandra.py"", line 975, in recv_describe_ring
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
  File ""thrift/protocol/TBinaryProtocol.py"", line 126, in readMessageBegin
    sz = self.readI32()
  File ""thrift/protocol/TBinaryProtocol.py"", line 203, in readI32
    buff = self.trans.readAll(4)
  File ""thrift/transport/TTransport.py"", line 58, in readAll
    chunk = self.read(sz-have)
  File ""thrift/transport/TSocket.py"", line 94, in read
    raise TTransportException(type=TTransportException.END_OF_FILE, message='TSocket read 0 bytes')
thrift.transport.TTransport.TTransportException: TSocket read 0 bytes
{noformat}

Server log:
{noformat}
ERROR 16:40:55,288 Error occurred during processing of message.
java.lang.RuntimeException: No replica strategy configured for system
        at org.apache.cassandra.service.StorageService.getReplicationStrategy(StorageService.java:246)
        at org.apache.cassandra.service.StorageService.constructRangeToEndPointMap(StorageService.java:457)
        at org.apache.cassandra.service.StorageService.getRangeToAddressMap(StorageService.java:443)
        at org.apache.cassandra.service.StorageService.getRangeToEndPointMap(StorageService.java:433)
        at org.apache.cassandra.thrift.CassandraServer.describe_ring(CassandraServer.java:628)
        at org.apache.cassandra.thrift.Cassandra$Processor$describe_ring.process(Cassandra.java:1781)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:1125)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
{noformat}
;;;","27/May/10 20:48;gdusbabek;Will re-examine using supplied steps.;;;","29/May/10 02:29;gdusbabek;describe_ring(""system"") should be treated as an invalid request since that keyspace has no ring.;;;","29/May/10 02:47;jbellis;+1;;;","31/May/10 17:59;dccwilliams;Hi, fwiw was planning to use describe_ring() in new Pelops Java client library to discover the nodes in a cluster i.e. connection pool starts with a few known nodes then discovers all.

Problem occurred on single node setup on my laptop but I'm guessing that without this working there would be no way to ""know"" there is only one node.

In such cases TokenRange will always map to 1/the same node, but that's still useful information, at least for Pelops;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError in MappedFileDataInput.skipBytes when slicing a large number of keys,CASSANDRA-857,12458371,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,edmonds,edmonds,07/Mar/10 12:08,16/Apr/19 17:33,22/Mar/23 14:57,20/Mar/10 09:15,0.6,,,,0,,,,,,"i'm getting the following error when performing a range query that is supposed to return a large number of keys:

ERROR [ROW-READ-STAGE:9] 2010-03-07 03:54:49,672 CassandraDaemon.java (line 78) Fatal exception in thread Thread[ROW-READ-STAGE:9,5,main]
java.lang.AssertionError
	at org.apache.cassandra.io.util.MappedFileDataInput.skipBytes(MappedFileDataInput.java:104)
	at org.apache.cassandra.io.SSTableReader.getPosition(SSTableReader.java:382)
	at org.apache.cassandra.io.SSTableReader.getFileDataInput(SSTableReader.java:481)
	at org.apache.cassandra.db.filter.SSTableSliceIterator.<init>(SSTableSliceIterator.java:54)
	at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:63)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:851)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:771)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:740)
	at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1040)
	at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:41)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:40)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)","debian, amd64, sun java 1.6.0_12",johanoskarsson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Mar/10 05:08;jbellis;857.txt;https://issues.apache.org/jira/secure/attachment/12439328/857.txt","20/Mar/10 02:10;jbellis;857.txt;https://issues.apache.org/jira/secure/attachment/12439306/857.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19892,,,Mon Mar 22 20:04:38 UTC 2010,,,,,,,,,,"0|i0g1i7:",91674,,,,,Normal,,,,,,,,,,,,,,,,,"19/Mar/10 08:53;jbellis;There are two bugs w/ index positions crossing mmap 2GB segment boundaries.

Working on fix.

--- src/java/org/apache/cassandra/io/SSTableReader.java	(revision 925055)
+++ src/java/org/apache/cassandra/io/SSTableReader.java	(working copy)
@@ -363,6 +363,7 @@
             do
             {
                 DecoratedKey indexDecoratedKey;
+                // bug: EOFing may mean ""try next mmap segment,"" rather than ""we're done""
                 try
                 {
                     indexDecoratedKey = partitioner.convertFromDiskFormat(input.readUTF());
@@ -375,6 +376,7 @@
                 int v = indexDecoratedKey.compareTo(decoratedKey);
                 if (v == 0)
                 {
+                    // bug: ""get next info"" (to find data length) can cross mmap boundary & error out
                     PositionSize info;
                     if (!input.isEOF())
                     {
;;;","20/Mar/10 02:10;jbellis;patch against 0.6 attached.  (will not apply cleanly to trunk, I will have to merge that separately.);;;","20/Mar/10 02:14;jbellis;patch moves indexPositions and spannedIndexDataPositions from SSTR into IndexSummary class, and adds spannedIndexPositions to map  indexPosition -> KeyPosition.  This lets us tell when we reach the last index entry we can safely read from a mmap segment, without having to read it.  (Relying on reading -> erroring out to tell us when this was the case is not as good, since we lose the ability to tell the difference b/t a corrupt file and a mmap buffer boundary.);;;","20/Mar/10 05:08;jbellis;rebased;;;","20/Mar/10 06:14;stuhood;+1 Looks good.;;;","20/Mar/10 09:15;jbellis;committed to 0.6; will merge to trunk;;;","23/Mar/10 04:04;jbellis;(this fix will be in 0.6 RC1, it is not in beta 3.);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL: NPE running SELECT with an IN clause,CASSANDRA-2538,12504935,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,urandom,cdaw,cdaw,22/Apr/11 07:41,16/Apr/19 17:33,22/Mar/23 14:57,27/Apr/11 06:12,0.8.0 beta 2,,,,0,cql,,,,,"*Test Case to Run*
{noformat}
cqlsh> select * from users where key in ('user2', 'user3');
Internal application error
{noformat}


*Test Setup*
{noformat}
CREATE COLUMNFAMILY users (
  KEY varchar PRIMARY KEY,
  password varchar);

INSERT INTO users (KEY, password) VALUES ('user1', 'ch@ngem3a');
{noformat}


*Log Files*
{noformat}
ERROR [RequestResponseStage:17] 2011-04-21 23:36:41,600 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[RequestResponseStage:17,5,main]
java.lang.AssertionError
	at org.apache.cassandra.service.ReadCallback.response(ReadCallback.java:127)
	at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:49)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
ERROR [RequestResponseStage:17] 2011-04-21 23:36:41,600 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[RequestResponseStage:17,5,main]
java.lang.AssertionError
	at org.apache.cassandra.service.ReadCallback.response(ReadCallback.java:127)
	at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:49)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
ERROR [pool-2-thread-5] 2011-04-21 23:37:12,026 Cassandra.java (line 4082) Internal error processing execute_cql_query
java.lang.NullPointerException
	at org.apache.cassandra.cql.WhereClause.and(WhereClause.java:59)
	at org.apache.cassandra.cql.WhereClause.<init>(WhereClause.java:44)
	at org.apache.cassandra.cql.CqlParser.whereClause(CqlParser.java:816)
	at org.apache.cassandra.cql.CqlParser.selectStatement(CqlParser.java:502)
	at org.apache.cassandra.cql.CqlParser.query(CqlParser.java:191)
	at org.apache.cassandra.cql.QueryProcessor.getStatement(QueryProcessor.java:834)
	at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:463)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1134)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.process(Cassandra.java:4072)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Apr/11 10:33;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2538-NPE-running-SELECT-with-an-IN-clause.txt;https://issues.apache.org/jira/secure/attachment/12477069/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2538-NPE-running-SELECT-with-an-IN-clause.txt",,,,,,,,,,,,,,1.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20683,,,Tue Apr 26 22:56:43 UTC 2011,,,,,,,,,,"0|i0gbyf:",93367,,cdaw,,cdaw,Low,,,,,,,,,,,,,,,,,"22/Apr/11 09:42;jbellis;Eric, can you make it so CQL is more clear about not supporting IN, without internal errors?;;;","22/Apr/11 10:36;urandom;This should result in a syntax error like any other mis-statement, but the NPE was beating it to the punch.  Patch attached.;;;","23/Apr/11 06:20;cdaw;This is just a consistency thing, but the documentation is misleading.

The DELETE documentation shows you can use an 'IN' clause:
DELETE [COLUMNS] FROM <COLUMN FAMILY> [USING <CONSISTENCY>] WHERE KEY IN (keyname1, keyname2);

The example for DELETE uses UPDATE, so I assumed 'IN' would work for DELETE/UPDATE/SELECT.
UPDATE ... WHERE KEY IN (keyname1, keyname2)
;;;","23/Apr/11 08:17;jbellis;Totally didn't know we support IN for DELETE. :)

Would prefer to fix by offering IN for for the other statements as well. The Antlr doesn't look too daunting w/ DELETE as an example (knock on wood). Can take a stab at that tomorrow.;;;","23/Apr/11 11:12;urandom;{quote}
Totally didn't know we support IN for DELETE. 

Would prefer to fix by offering IN for for the other statements as well. The Antlr doesn't look too daunting w/ DELETE as an example (knock on wood). Can take a stab at that tomorrow.
{quote}

This is {{multiget_slice()}} / {{multiget_count()}} and was intentionally omitted (remember?).  Not saying this is decision that can't be revisited, just reminding.

Oh, and if we do move forward, it's post-0.8, we're in a feature-freeze. :);;;","25/Apr/11 21:43;jbellis;Okay, created CASSANDRA-2553 for adding IN support to 0.8.1.

+1 on the NPE fix.;;;","27/Apr/11 05:46;cdaw;The patch generates this message:
{code}
cqlsh> select * from users where KEY in ('cathy', 'ed');
Bad Request: line 1:30 mismatched input 'in' expecting set null
{code}
;;;","27/Apr/11 06:10;urandom;This says that it's a bad request, a problem at line 1, column 30, where it encountered mismatched input in the form of the word IN.  This is because `KEY IN ...' is not supported.;;;","27/Apr/11 06:12;urandom;committed.;;;","27/Apr/11 06:17;cdaw;Sorry if I wasn't clear. The patch passed in testing because it no longer generated an NPE and gave an ERROR message. I included the message just for info.;;;","27/Apr/11 06:56;hudson;Integrated in Cassandra-0.8 #43 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/43/])
    NPE running SELECT with an IN clause

Patch by eevans; reviewed by jbellis for CASSANDRA-2538
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DC Quorum broken @ trunk,CASSANDRA-952,12461108,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,04/Apr/10 13:43,16/Apr/19 17:33,22/Mar/23 14:57,19/May/10 01:16,0.7 beta 1,,,,0,,,,,,"Currently DCQuorum is broken in trunk, Suggesting the following fix... 

Write to DC's
1) Move determineBlockFor(int expandedTargets, ConsistencyLevel consistency_level) to AbstractEndpointSnitch
2) Add the same to support DC Quorum in DatacenterShardStategy

Read to DC's
1) find suitable nodes was a list which was returning a list of local DC's earlier but now it is just one node and MD is been sent by other nodes. Need to have an option to even avoid MD from other DC's?","Linux, Cassandra",jeromatron,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/May/10 07:55;vijay2win@yahoo.com;0002-dcquorum-fixes-v2.txt;https://issues.apache.org/jira/secure/attachment/12444546/0002-dcquorum-fixes-v2.txt","09/Apr/10 13:31;vijay2win@yahoo.com;952-Canges_To_Stategy_V002.txt;https://issues.apache.org/jira/secure/attachment/12441264/952-Canges_To_Stategy_V002.txt","06/Apr/10 01:07;vijay2win@yahoo.com;952-Change_BlockFor.txt;https://issues.apache.org/jira/secure/attachment/12440777/952-Change_BlockFor.txt","09/Apr/10 13:31;vijay2win@yahoo.com;952-Changes_To_ResponseHandler_v002.txt;https://issues.apache.org/jira/secure/attachment/12441265/952-Changes_To_ResponseHandler_v002.txt","06/Apr/10 09:16;vijay2win@yahoo.com;952-Fix_Refactor_DCStatergy.txt;https://issues.apache.org/jira/secure/attachment/12440820/952-Fix_Refactor_DCStatergy.txt","17/May/10 22:29;jbellis;952-v3.txt;https://issues.apache.org/jira/secure/attachment/12444683/952-v3.txt","28/Apr/10 14:31;vijay2win@yahoo.com;952-v3.txt;https://issues.apache.org/jira/secure/attachment/12443042/952-v3.txt","18/May/10 02:49;vijay2win@yahoo.com;952-v4.txt;https://issues.apache.org/jira/secure/attachment/12444724/952-v4.txt","30/Apr/10 01:42;vijay2win@yahoo.com;952-v4.txt;https://issues.apache.org/jira/secure/attachment/12443208/952-v4.txt","02/May/10 04:55;vijay2win@yahoo.com;952-v5.txt;https://issues.apache.org/jira/secure/attachment/12443389/952-v5.txt","13/May/10 05:48;jbellis;ASF.LICENSE.NOT.GRANTED--0001-clean-out-callback-purging-from-truncate-writerh.txt;https://issues.apache.org/jira/secure/attachment/12444346/ASF.LICENSE.NOT.GRANTED--0001-clean-out-callback-purging-from-truncate-writerh.txt","13/May/10 05:48;jbellis;ASF.LICENSE.NOT.GRANTED--0002-dcquorum-fixes.txt;https://issues.apache.org/jira/secure/attachment/12444347/ASF.LICENSE.NOT.GRANTED--0002-dcquorum-fixes.txt","02/May/10 04:55;vijay2win@yahoo.com;DC-Config.xml;https://issues.apache.org/jira/secure/attachment/12443390/DC-Config.xml",,13.0,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19935,,,Tue May 18 17:16:20 UTC 2010,,,,,,,,,,"0|i0g23b:",91769,,,,,Low,,,,,,,,,,,,,,,,,"06/Apr/10 22:47;jbellis;The fundamental problem here is that an int count of replicas to block for is not sufficient to represent the DC strategy wants: we want to SEND to all replicas, but only count replicas in the CURRENT DC towards success.  Allowing other DC replicas will mostly work, since they will typically be slower to arrive than local ones, but if there are failure conditions locally then it could succeed where it should not, which would violate its contract to readers.

I think you will need to push the concept of what to block for into the WriteResponseHandler, and similarly for reads.

;;;","09/Apr/10 13:31;vijay2win@yahoo.com;Made Changes as suggested;;;","28/Apr/10 02:49;jbellis;Finally ready to get this in (now that CASSANDRA-994 is done).  Can you rebase?  Sorry, it's going to be a bit messy.;;;","28/Apr/10 14:31;vijay2win@yahoo.com;this patch works in my environment, and i think we need couple of methods to be added to RackInferringSnitch which is getReplicationFactor and getDatacenters to make it work with the DCSS. Let me know if you dont agree with the changes, i have renamed the DCEPS to XMLFileSnitch. there is a lot of changes in these classes.... ;;;","28/Apr/10 21:50;jbellis;We're not adding DCEPS/XMLFileSnitch back, and we shouldn't need to make any changes to the Snitch classes.  It should be able to run against any RackAwareSnitch, since it gets the per-DC settings from DCSS/datacenters.properties now.;;;","30/Apr/10 01:42;vijay2win@yahoo.com;Attached is the modified version, please note that XMLFileSnitch included in this patch, uses a different logic than PFS to read the hosts, if you choose not to include that thats fine too.;;;","01/May/10 01:11;jbellis;It looks like this is mixing code from CASSANDRA-967 in.  Let's do these one at a time, or we're more likely to introduce bugs.;;;","02/May/10 04:55;vijay2win@yahoo.com;I have attached the changes, Please note that we have to change a bit of logic in rangeslice in order to remove blockFor for from the SP, IF you think it is unnecessary, i put blockFor for now and add a depreciated annotation on it (by doing this i can leave the rangeslice untouched).;;;","13/May/10 05:55;jbellis;patch 01 cleans out callback purging from WriteResponseHandler (leaving ExpiringMap to clean it out automatically).

02 adds the dcquorum changes, extracting AbstractWRH so the DQSyncRH doesn't have to subclass WRH which is a poor fit, and cleans up the WRH heirarchy.;;;","14/May/10 00:49;jeromatron;Reviewing part of it and I know this wasn't part of the change - but in RackAwareStrategy couldn't we save some instruction execution by moving the check for whether we already have found another datacenter/rack outside of the checks?  Like so:

{code}
Token t = iter.next();

// First try to find one in a different data center
// If we have already found something in a diff datacenter no need to find another
if (!bDataCenter)
{
    if (!snitch.getDatacenter(metadata.getEndpoint(primaryToken)).equals(snitch.getDatacenter(metadata.getEndpoint(t))))
    {
        endpoints.add(metadata.getEndpoint(t));
        bDataCenter = true;
        continue;
    }
}

// Now  try to find one on a different rack
// If we have already found something in a diff rack no need to find another
if (!bOtherRack)
{
    if (!snitch.getRack(metadata.getEndpoint(primaryToken)).equals(snitch.getRack(metadata.getEndpoint(t))) &&
        snitch.getDatacenter(metadata.getEndpoint(primaryToken)).equals(snitch.getDatacenter(metadata.getEndpoint(t))))
    {
        endpoints.add(metadata.getEndpoint(t));
        bOtherRack = true;
        continue;
    }
}
{code}

not that big of a deal, but since it's done on every write, might be helpful.  Maybe I'm missing something there though...;;;","14/May/10 01:01;vijay2win@yahoo.com;Jeremy, Thanks... different datacenter is physically a different rack.... I dont think we need additional check for that....;;;","14/May/10 01:36;jeromatron;Vijay - okay - I wasn't thinking of doing an additional check, I was just thinking of bringing the check for bDatacenter and bOtherRack outside of the block they were in, in RackAwareStrategy.;;;","14/May/10 01:38;jeromatron;Jonathan - in your 0002 patch, the DatacenterShardStrategy changes - you refactored forLoopReturn into endpoints.  I'm not sure that you want to do that.  forLoopReturn is re-initialized on every loop iteration to include all of the replicas needed for that particular datacenter.  Endpoints includes all replicas regardless of datacenter.;;;","14/May/10 04:05;jeromatron;Jonathan - Also it looks like you're doing the datacenter replication factor per keyspace now, which is what we're hoping to have for multi-tenants.  However, in StorageService, I was under the impression that the replication strategy is handled as though it was per keyspace but in reality it is a quasi singleton.  So all keyspaces will share the same RS of that type.  Is that not true now?  I didn't see any change to that in your patches.  Just wanted to make sure that doesn't cause odd problems - if all of the keyspaces are sharing the same DatacenterShardStrategy instance which is only meant for one of those keyspaces.;;;","15/May/10 01:59;vijay2win@yahoo.com;Jeremy, 
For 002 i am submitting the update for the patch... Thanks!
replication strategy - Yes currently thats the case, but DSS can handle all the required levels it will just forward the requests to AbstractReplicationStrategy if other than DCQuorum or DCQuorumSync.;;;","15/May/10 03:59;jeromatron;Cool - yeah - looking at it again, DSS contains DC RF information for all keyspaces, so it really wouldn't matter if it was a singleton for all keyspaces.

Btw, I was mostly just reviewing the getNaturalEndpointsInternal method of DSS - Jonathan had asked me to take a look at it in IRC.  Just so you know I wasn't reviewing the entire patch.  I don't know if that matters too much, but thought I would clarify.;;;","15/May/10 07:55;vijay2win@yahoo.com;Attached is the fix and along with it i have a testcase, so we dont miss this in future.

Thanks
Vijay;;;","17/May/10 22:29;jbellis;v3 attached is the same as v2 w/ (unintentional?) breakages to TruncateResponseHandler reverted and formatting fixed.

the new test needs to work with normal propertyfilesnitch, XMLFS is not going back in.;;;","18/May/10 02:49;vijay2win@yahoo.com;Yes that was unintentional... This has updates to the test and also fix to the PEPS. And Hope it works... ;;;","18/May/10 21:31;hudson;Integrated in Cassandra #439 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/439/])
    make DCQUORUM CLs actually wait for the nodes per DC rather than attempting a purely count-based approach that didn't really work as advertised.  patch by Vijay Parthasarathy and jbellis; reviewed by Jeremy Hanna for CASSANDRA-952
;;;","19/May/10 01:16;jbellis;committed v4.

there is still a minor issue w/ replica placement but I have opened CASSANDRA-1103 instead of round-tripping this again.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tombstone records in Cassandra are not being deleted,CASSANDRA-507,12438736,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,rrabah,rrabah,22/Oct/09 03:29,16/Apr/19 17:33,22/Mar/23 14:57,22/Oct/09 23:41,0.5,,,,0,,,,,,"I am running into problems with get_key_range.
My command is client.get_key_range(""Keyspace1"", ""DatastoreDeletionSchedule"",
                   """", """", 25, ConsistencyLevel.ONE);

After a lot of deletes on the datastore, I am getting 

ERROR [pool-1-thread-36] 2009-10-19 17:24:28,223 Cassandra.java (line
770) Internal error processing get_key_range
java.lang.RuntimeException: java.util.concurrent.TimeoutException:
Operation timed out.
       at org.apache.cassandra.service.StorageProxy.getKeyRange(StorageProxy.java:560)
       at org.apache.cassandra.service.CassandraServer.get_key_range(CassandraServer.java:595)
       at org.apache.cassandra.service.Cassandra$Processor$get_key_range.process(Cassandra.java:766)
       at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:609)
       at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
       at java.lang.Thread.run(Thread.java:619)
Caused by: java.util.concurrent.TimeoutException: Operation timed out.
       at org.apache.cassandra.net.AsyncResult.get(AsyncResult.java:97)
       at org.apache.cassandra.service.StorageProxy.getKeyRange(StorageProxy.java:556)
       ... 7 more

Turns out that the compaction code removes tombstones, and it runs whenever you have
enough sstable fragments. As an optimization, if there is
only one version of a row it will just copy it to the new sstable.
This means it won't clean out tombstones, which is causing this problem.


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Oct/09 04:01;jbellis;507.patch;https://issues.apache.org/jira/secure/attachment/12422843/507.patch",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19727,,,Fri Oct 23 12:34:41 UTC 2009,,,,,,,,,,"0|i0fzcv:",91326,,,,,Normal,,,,,,,,,,,,,,,,,"22/Oct/09 04:01;jbellis;this patch fixes the bug in trunk.

unfortunately the risk/benefit of backporting this to the 0.4 branch is past my threshold of comfort.;;;","22/Oct/09 11:23;junrao;patch looks good to me. Can you add a test case for this?;;;","22/Oct/09 23:41;jbellis;committed with test;;;","23/Oct/09 20:34;hudson;Integrated in Cassandra #236 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/236/])
    all rows go through deserialize/removeDeleted so we can GC tombstones.
patch by jbellis; reviewed by junrao for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sending random data crashes thrift service,CASSANDRA-475,12437517,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,zznate,urandom,urandom,07/Oct/09 22:44,16/Apr/19 17:33,22/Mar/23 14:57,22/Jul/10 00:27,0.7 beta 1,,,,1,,,,,,"Use dd if=/dev/urandom count=1 | nc $host 9160 as a handy recipe for shutting a cassandra instance down. 

Thrift has spoken (see THRIFT-601), but ""Don't Do That"" is probably an insufficient answer for our users. ",,bendiken,brandon.williams,jigneshdhruv,paixaop,tcn,tzz,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-1266,,,,,,,,,,,,,,,,,"11/Jul/10 09:05;zznate;trunk-475-config.txt;https://issues.apache.org/jira/secure/attachment/12449181/trunk-475-config.txt","14/Jul/10 14:17;zznate;trunk-475-src-3.txt;https://issues.apache.org/jira/secure/attachment/12449424/trunk-475-src-3.txt","21/Jul/10 02:52;zznate;trunk-475-src-4.txt;https://issues.apache.org/jira/secure/attachment/12449957/trunk-475-src-4.txt",,,,,,,,,,,,3.0,zznate,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19710,,,Wed Jul 21 16:27:41 UTC 2010,,,,,,,,,,"0|i0eoqn:",83770,,,,,Normal,,,,,,,,,,,,,,,,,"11/Mar/10 03:42;jbellis;We don't have the resources to devote to properly fixing THRIFT-601, so we'll have to close this as wontfix until or unless something changes there.

If you need to expose Cassandra to untrusted sources, I suggest helping with Avro integration.  (Ask Eric for how, he is the closest to that.);;;","10/Jun/10 23:24;jbellis;THRIFT-601 has reportedly been fixed.  Not sure how painful their fix is for us, we should have a look.;;;","02/Jul/10 22:49;paixaop;Do you know if their patch has been committed to to cassandra's trunk?;;;","02/Jul/10 23:02;jbellis;this ticket is open precisely to incorporate a newer thrift and make any other necessary changes to fix the problem;;;","09/Jul/10 13:25;jbellis;looks like we need to set a transport max length and a protocol read length (besides upgrading to a new thrift jar).

unclear if these would prohibit using large binary column values.  may need to make them configurable.  (would transport length be read length + some overhead?  or is transport length just a buffer size that we can leave at some reasonably safe value and is transparent to the rest?);;;","10/Jul/10 09:06;zznate;TFramedTransport maxLength is just a frame buffer size. It defaults to 2^31-1 in the latest thrift. Im not sure tweaking that buys us much given the following. 

TBinaryProtocol.readLength is what needs to be set. *NOTE*: The value provided here will effectively be the max column size (there is no overhead due to thrift's delimitation granularity). 

This should be configurable with a sane default. How about thrift_max_message_length with a default of 1 (in MB). (This is the default max_packet_size in mysql, so might be easier to grok if we match that). 


;;;","10/Jul/10 11:02;jbellis;so readLength is the max size of a single field, and maxLength the max size of the entire rpc call?

is maxLength actually controlling a byte[] size somewhere, or not?  if it is then we do need to set it, or if the first 4 bytes correspond to a large int we still OOM.;;;","10/Jul/10 13:33;zznate;Yes, maxLength does directly control the size of the input byte[] on TFramedTransport, so it is succeptible to the same issue. 

I would like to change thrift_framed_transport to thrift_framed_transport_size with 0 (in MB as well) as the default indicating TSocket over TFramedTransport;;;","10/Jul/10 14:30;zznate;Looks like the thrift folks have changed the parameterization of TBase: THRIFT-759

Unfortunately, this beat our patch for THRIFT-601 by a couple of days. Thoughts?;;;","10/Jul/10 22:18;jbellis;If you're asking ""is this worth having to rewrite a bunch of our code"" then the answer is ""yes, but this is why we upgrade thrift version so infrequently."";;;","11/Jul/10 00:50;zznate;We are agreed on the importance of this. The changes on the surface seem trivial, since Comparator is widely used already. My concerns here are strictly schedule related given the process overhead involved in touching so many files in active development. 

I'm going to have to put this down temporarily, until I can focus on this and make the change a one-shot deal (or as close to that as possible). 

EDIT: I just added CASSANDRA-1266 to track the jar upgrade separately so we don't muddy the waters on this issue.;;;","11/Jul/10 01:05;zznate;I can continue with the config modifications required if the changes mentioned above are cool;;;","11/Jul/10 09:05;zznate;- trunk-475-config.txt updates the yaml config
- trunk-475-src.txt updates java src

EDIT: I dropped in a default setting of 256mb in the Converter when someone had framed transport configured in storage-conf. Was not sure what all to do there except add a sane default and a warning message. ;;;","14/Jul/10 10:22;jbellis;do we need a sanity check that frame size must be < message length?;;;","14/Jul/10 11:42;zznate;I thought about that, but it's pretty clear exception wise when you go over the frame size that it is a transport issue;;;","14/Jul/10 11:54;zznate;Come to think, nice to have completeness-wise and was easy enough to add. 

trunk-475-src-2.txt superseeds trunk-475-src.txt ;;;","14/Jul/10 13:18;jbellis;hmm, why isn't that check firing with the default frame of 256 and length of 1?;;;","14/Jul/10 14:17;zznate;I'm gonna blame that one on your central-TX weather melting my brain. 

trunk-475-src-3.txt replaces trunk-475-src-2.txt, fixes comparisson induced by mild heat stroke. ;;;","14/Jul/10 22:01;jbellis;is thrift smart enough to re-use those byte[] buffers for multiple requests?  allocating byte[] is expensive in java.;;;","15/Jul/10 06:38;zznate;Both TBinaryProtocol and TFramedTransport use the limits in checks only. The size used to construct the byte[] is pulled from the first couple bytes of any thrift message. When junk was sent, this size was getting intepreted as extremely large values.;;;","15/Jul/10 09:22;jbellis;That makes sense.

That would be an obvious optimization for them to make, then -- allocate a buffer at the max size, and re-use it.;;;","15/Jul/10 09:22;jbellis;committed;;;","15/Jul/10 21:42;brandon.williams;I'm occasionally getting this error with stress.py after doing a few million inserts:

ERROR 13:30:12,158 Thrift error occurred during processing of message.
org.apache.thrift.TException: Message length exceeded: 32
        at org.apache.thrift.protocol.TBinaryProtocol.checkReadLength(TBinaryProtocol.java:384)
        at org.apache.thrift.protocol.TBinaryProtocol.readBinary(TBinaryProtocol.java:361)
        at org.apache.cassandra.thrift.Column.read(Column.java:498)
        at org.apache.cassandra.thrift.ColumnOrSuperColumn.read(ColumnOrSuperColumn.java:351)
        at org.apache.cassandra.thrift.Mutation.read(Mutation.java:346)
        at org.apache.cassandra.thrift.Cassandra$batch_mutate_args.read(Cassandra.java:16780)
        at org.apache.cassandra.thrift.Cassandra$Processor$batch_mutate.process(Cassandra.java:3041)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2531)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619);;;","16/Jul/10 03:04;jhermes;Hudson also found -this- an error in build 492.
I assume the hadoop test that failed found it by doing a lot of inserts as above.;;;","16/Jul/10 04:34;zznate;checkReadLength in TBinaryProtocol looks like it is treating the readLength attribute as an instance variable and is therefore slowly getting decremented on each call!

I'm verifying my findings w/ thrift folks now;;;","16/Jul/10 04:46;zznate;THRIFT-820 created and patch submitted. Waiting on their review and feedback.;;;","16/Jul/10 21:02;hudson;Integrated in Cassandra #493 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/493/])
    ;;;","21/Jul/10 02:52;zznate;Resets the input and output protocol on each after each successful call to process in CustomTThreadPoolServer

passes nosetests and stress.py with 5 million rows;;;","21/Jul/10 04:30;brandon.williams;Verified this solves the issue and checked for any adverse performance effects.  Committed.;;;","21/Jul/10 04:59;jbellis;is this going to hurt performance?  (not sure how heavyweight getProtocol is);;;","21/Jul/10 05:05;brandon.williams;I checked the performance and couldn't see any noticeable difference.  The extra garbage might minorly exacerbate CASSANDRA-1014.;;;","21/Jul/10 20:50;hudson;Integrated in Cassandra #496 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/496/])
    Reset the input and output protocol on each after each successful call.  Patch by Nate McCall, reviewed by brandonwilliams for CASSANDRA-475
;;;","22/Jul/10 00:15;jigneshdhruv;Hello,

I am using the latest source code from trunk. SVN 961952

After few hundred thousand inserts cassandra crashes and is throwing 2 different types of exceptions:
The first one being:
org.apache.thrift.transport.TTransportException
at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
at org.apache.thrift.transport.TFramedTransport.readFrame(TFramedTransport.java:129)
at org.apache.thrift.transport.TFramedTransport.read(TFramedTransport.java:101)
at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:369)
at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:295)
at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:202)
at org.apache.cassandra.thrift.Cassandra$Client.recv_batch_mutate(Cassandra.java:960)
at org.apache.cassandra.thrift.Cassandra$Client.batch_mutate(Cassandra.java:944)
at com.cbsi.pi.rtss.data.cassandra.CassandraDataManager.insert(CassandraDataManager.java:107)
at com.cbsi.pi.rtss.service.bulk.BulkThread.run(BulkThread.java:59)
at java.lang.Thread.run(Unknown Source)

and the second one being:
org.apache.thrift.transport.TTransportException: java.net.SocketException: Software caused connection abort: socket write error
at org.apache.thrift.transport.TIOStreamTransport.write(TIOStreamTransport.java:147)
at org.apache.thrift.transport.TFramedTransport.flush(TFramedTransport.java:156)
at org.apache.cassandra.thrift.Cassandra$Client.send_set_keyspace(Cassandra.java:441)
at org.apache.cassandra.thrift.Cassandra$Client.set_keyspace(Cassandra.java:430)
at com.cbsi.pi.rtss.data.cassandra.CassandraDataManager.insert(CassandraDataManager.java:106)
at com.cbsi.pi.rtss.service.bulk.BulkThread.run(BulkThread.java:59)
at java.lang.Thread.run(Unknown Source)
Caused by: java.net.SocketException: Software caused connection abort: socket write error
at java.net.SocketOutputStream.socketWrite0(Native Method)
at java.net.SocketOutputStream.socketWrite(Unknown Source)
at java.net.SocketOutputStream.write(Unknown Source)
at java.io.BufferedOutputStream.flushBuffer(Unknown Source)
at java.io.BufferedOutputStream.write(Unknown Source)
at org.apache.thrift.transport.TIOStreamTransport.write(TIOStreamTransport.java:145)
... 6 more

I was not getting this error yesterday but this morning when I updated my svn trunk I got update for following 2 files:
U src/java/org/apache/cassandra/thrift/CustomTThreadPoolServer.java
U src/java/org/apache/cassandra/scheduler/RoundRobinScheduler.java

One more thing.

Before I did a SVN update this morning, I checkout thrift and applied the patch suggested by Nate in bug THRIFT-820 https://issues.apache.org/jira/browse/THRIFT-820. When I rebuild thrift jar file and used it, I had no crashes or exceptions yesterday.

But today with/without thrift patch, I am getting exceptions mentioned above.

Thanks,
Jignesh

and that is causing the problem.;;;","22/Jul/10 00:27;jigneshdhruv;Apologize for the confusion. I did another svn update and picked bunch of new files. I am not getting any cassandra crashes or exceptions any more.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Endpoint cache for a token should be part of AbstractReplicationStrategy and not Snitch,CASSANDRA-1643,12478014,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,khichrishi,khichrishi,22/Oct/10 02:31,16/Apr/19 17:33,22/Mar/23 14:57,22/Oct/10 04:55,0.7 beta 3,,,,0,,,,,,"There is a single DynamicEndpointSnitch object for all ReplicationStrategy objects. This DynamicEndpointSnitch object contains a single IEndpointSnitch subsnitch object. This subsnitch object contains the Endpoint cache for a token. Thus there is a single endpoint cache for all ReplicationStrategy objects. This implies that replica nodes for a Token as returned by the Cache would be same irrespective of the ReplicationStrategy object. This is a bug, the Endpoint cache should be a part of ""AbstractReplicationStrategy"" object rather than the IEndpointSnitch object.
",,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,"22/Oct/10 04:37;jbellis;1643-v2.txt;https://issues.apache.org/jira/secure/attachment/12457780/1643-v2.txt","22/Oct/10 03:28;jbellis;1643.txt;https://issues.apache.org/jira/secure/attachment/12457775/1643.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20234,,,Fri Oct 22 12:52:29 UTC 2010,,,,,,,,,,"0|i0g6hr:",92482,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"22/Oct/10 04:19;brandon.williams;SimpleStrategyTest is broken:

    [junit] Testsuite: org.apache.cassandra.locator.SimpleStrategyTest
    [junit] Tests run: 4, Failures: 1, Errors: 0, Time elapsed: 0.591 sec
    [junit]
    [junit] Testcase: tryBogusTable(org.apache.cassandra.locator.SimpleStrategyTest):   FAILED
    [junit] No replica strategy configured for SomeBogusTableThatDoesntExist
    [junit] junit.framework.AssertionFailedError: No replica strategy configured for SomeBogusTableThatDoesntExist
    [junit]     at org.apache.cassandra.service.StorageService.getReplicationStrategy(StorageService.java:254)
    [junit]     at org.apache.cassandra.locator.SimpleStrategyTest.tryBogusTable(SimpleStrategyTest.java:48)
    [junit]
    [junit]
    [junit] Test org.apache.cassandra.locator.SimpleStrategyTest FAILED
;;;","22/Oct/10 04:37;jbellis;fixed test;;;","22/Oct/10 04:44;brandon.williams;+1;;;","22/Oct/10 04:55;jbellis;committed;;;","22/Oct/10 20:52;hudson;Integrated in Cassandra #573 (See [https://hudson.apache.org/hudson/job/Cassandra/573/])
    move endpoint cache from snitch to strategy
patch by jbellis; reviewed by brandonwilliams for CASSANDRA-1643
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
heisenbug in RoundRobinSchedulerTest,CASSANDRA-1279,12469264,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,rnirmal,jbellis,jbellis,14/Jul/10 23:58,16/Apr/19 17:33,22/Mar/23 14:57,04/Aug/10 01:27,0.7 beta 1,,,,0,,,,,,"Occasionally I see this error in the test suite:

    [junit] Testcase: testScheduling(org.apache.cassandra.scheduler.RoundRobinSchedulerTest):	FAILED
    [junit] 
    [junit] junit.framework.AssertionFailedError: 
    [junit] 	at org.apache.cassandra.scheduler.RoundRobinSchedulerTest.testScheduling(RoundRobinSchedulerTest.java:90)
    [junit] 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Aug/10 01:08;rnirmal;1279-v3.patch;https://issues.apache.org/jira/secure/attachment/12451135/1279-v3.patch","21/Jul/10 05:51;rnirmal;Cassandra-1279-v2.patch;https://issues.apache.org/jira/secure/attachment/12449982/Cassandra-1279-v2.patch","21/Jul/10 01:02;rnirmal;Cassandra-1279.patch;https://issues.apache.org/jira/secure/attachment/12449945/Cassandra-1279.patch",,,,,,,,,,,,3.0,rnirmal,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20056,,,Wed Aug 04 13:25:29 UTC 2010,,,,,,,,,,"0|i0g43b:",92093,,,,,Normal,,,,,,,,,,,,,,,,,"21/Jul/10 01:02;rnirmal;Fixed the problem, it's due to the unpredictable thread scheduling. The fixe loads the requests and then lets the scheduler proceed, that way the tests could be somewhat predictable and realize the expected behavior.;;;","21/Jul/10 04:36;jbellis;can you add a comment as to what is going on?

also, adding a package-private method for the test to use would be cleaner than poking through reflection;;;","21/Jul/10 05:51;rnirmal;The test is done with 15 simulated connections, 10 for K1(keyspace), 2 for K2 and 3 for k3. Requests came in the order of K1(1 thru 10), K2(11 thru 12), K3(13 thru15) and the test checked if K2 and K3 requests ran earlier then their request order. With the scheduler starting simultaneously, the requests were pretty much routed in order +/- the jvm thread scheduling order, hence the cause for the bug. 

Now the scheduler is paused still all the requests arrive and placed in their respective queues. When the scheduler is resumed, each pass retrieves one request from each keyspace queue, hence since K2 & K3 have only 2 and 3 requests, they get serviced faster than the order in which they arrived. This test just validates that the requests are RoundRobin and that's what we want to unit test.;;;","21/Jul/10 06:03;jbellis;committed;;;","21/Jul/10 20:50;hudson;Integrated in Cassandra #496 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/496/])
    fix RoundRobinSchedulerTest heisenbug.  patch by Nirmal Ranganathan; reviewed by jbellis for CASSANDRA-1279
;;;","03/Aug/10 23:14;jbellis;looks like this is only mostly fixed.  still getting

    [junit] junit.framework.AssertionFailedError: 
    [junit] 	at org.apache.cassandra.scheduler.RoundRobinSchedulerTest.testScheduling(RoundRobinSchedulerTest.java:93)
    ;;;","03/Aug/10 23:24;rnirmal;Hmm.... looks like I'll have to change the way we test it. ;;;","04/Aug/10 01:08;rnirmal;The scheduler will only have 1 token at anytime and the run/release of each thread is synchronized, effectively running only one thread at a time. So hopefully no threading inconsistencies occur.;;;","04/Aug/10 01:10;jbellis;Is this the point at which we say that ""this is different enough from 'live' code that it's not really a useful test anymore?""  Because I'm okay with that.;;;","04/Aug/10 01:23;rnirmal;Yes I'd say that, because it doesn't reflect the actual concurrency that will take place. So if it's ok we could remove it.;;;","04/Aug/10 01:27;jbellis;removed;;;","04/Aug/10 21:25;hudson;Integrated in Cassandra #509 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/509/])
    r/m RoundRobinSchedulerTest for CASSANDRA-1279; it doesn't appear possible to test concurrency usefully
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flush and Compaction Unnecessarily Allocate 256MB Contiguous Buffers,CASSANDRA-2463,12504105,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,cscotta,cscotta,cscotta,13/Apr/11 05:23,16/Apr/19 17:33,22/Mar/23 14:57,13/Apr/11 13:34,0.7.5,,,,0,,,,,,"Currently, Cassandra 0.7.x allocates a 256MB contiguous byte array at the beginning of a memtable flush or compaction (presently hard-coded as Config.in_memory_compaction_limit_in_mb). When several memtable flushes are triggered at once (as by `nodetool flush` or `nodetool snapshot`), the tenured generation will typically experience extreme pressure as it attempts to locate [n] contiguous 256mb chunks of heap to allocate. This will often trigger a promotion failure, resulting in a stop-the-world GC until the allocation can be made. (Note that in the case of the ""release valve"" being triggered, the problem is even further exacerbated; the release valve will ironically trigger two contiguous 256MB allocations when attempting to flush the two largest memtables).

This patch sets the buffer to be used by BufferedRandomAccessFile to Math.min(bytesToWrite, BufferedRandomAccessFile.DEFAULT_BUFFER_SIZE) rather than a hard-coded 256MB. The typical resulting buffer size is 64kb.

I've taken some time to measure the impact of this change on the base 0.7.4 release and with this patch applied. This test involved launching Cassandra, performing four million writes across three column families from three clients, and monitoring heap usage and garbage collections. Cassandra was launched with 2GB of heap and the default JVM options shipped with the project. This configuration has 7 column families with a total of 15GB of data.

Here's the base 0.7.4 release:
http://cl.ly/413g2K06121z252e2t10

Note that on launch, we see a flush + compaction triggered almost immediately, resulting in at least 7x very quick 256MB allocations maxing out the heap, resulting in a promotion failure and a full GC. As flushes proceeed, we see that most of these have a corresponding CMS, consistent with the pattern of a large allocation and immediate collection. We see a second promotion failure and full GC at the 75% mark as the allocations cannot be satisfied without a collection, along with several CMSs in between. In the failure cases, the allocation requests occur so quickly that a standard CMS phase cannot completed before a ParNew attempts to promote the surviving byte array into the tenured generation. The heap usage and GC profile of this graph is very unhealthy.

Here's the 0.7.4 release with this patch applied:
http://cl.ly/050I1g26401B1X0w3s1f

This graph is very different. At launch, rather than a immediate spike to full allocation and a promotion failure, we see a slow allocation slope reaching only 1/8th of total heap size. As writes begin, we see several flushes and compactions, but none result in immediate, large allocations. The ParNew collector keeps up with collections far more ably, resulting in only one healthy CMS collection with no promotion failure. Unlike the unhealthy rapid allocation and massive collection pattern we see in the first graph, this graph depicts a healthy sawtooth pattern of ParNews and an occasional effective CMS with no danger of heap fragmentation resulting in a promotion failure.

The bottom line is that there's no need to allocate a hard-coded 256MB write buffer for flushing memtables and compactions to disk. Doing so results in unhealthy rapid allocation patterns and increases the probability of triggering promotion failures and full stop-the-world GCs which can cause nodes to become unresponsive and shunned from the ring during flushes and compactions.",Any,cburroughs,cscotta,eonnen,stuhood,,,,,,,,,,,,,,,,,,259200,259200,,0%,259200,259200,,,,,,,,,,,,,,,,,,,,"13/Apr/11 06:07;jbellis;2463-v2.txt;https://issues.apache.org/jira/secure/attachment/12476182/2463-v2.txt","13/Apr/11 05:28;cscotta;patch.diff;https://issues.apache.org/jira/secure/attachment/12476177/patch.diff",,,,,,,,,,,,,2.0,cscotta,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20638,,,Wed Apr 13 05:34:50 UTC 2011,,,,,,,,,,"0|i0gbiv:",93297,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"13/Apr/11 05:26;cscotta;[ Patch attached ];;;","13/Apr/11 05:28;cscotta;Patch attached. Applies cleanly to tag 'cassandra-0.7.4'. All tests pass.;;;","13/Apr/11 06:07;jbellis;I started making it more complicated:

{code}
        // the gymnastics here are because
        //  - we want the buffer large enough that we're not re-buffering when we have to seek back to the
        //    start of a row to write the data size.  Here, ""10% larger than the average row"" is ""large enough,""
        //    meaning we expect to seek and rebuffer about 1/10 of the time.
        //  - but we don't want to allocate a huge buffer unnecessarily for a small amount of data
        //  - and on the low end, we don't want to be absurdly stingy with the buffer size for small rows
        assert estimatedSize > 0;
        long maxBufferSize = Math.min(DatabaseDescriptor.getInMemoryCompactionLimit(), 1024 * 1024);
        int bufferSize;
        if (estimatedSize < 64 * 1024)
        {
            bufferSize = (int) estimatedSize;
        }
        else
        {
            long estimatedRowSize = estimatedSize / keyCount;
            bufferSize = (int) Math.min(Math.max(1.1 * estimatedRowSize, 64 * 1024), maxBufferSize);
        }
{code}

...  but the larger our buffer is, the larger the penalty for guessing wrong when we have to seek back and rebuffer.

Then I went through and added size estimation to the CompactionManager, until I thought ""it's kind of ridiculous to be worrying about saving a few bytes less than 64KB, especially when we expect most memtables to have more data in them than 64K when flushed.""

Thus, I arrived at the patch Antoine de Saint-Exupery would have written, attached as v2.;;;","13/Apr/11 06:44;scode;A noteworthy factor here is that unless an fsync()+fadvise()/madvise() have evicted data, in the normal case this stuff should still be in page cache for any reasonably sized row. For truly huge rows, the penalty of seeking back should be insignificant anyway.

Total +1 on avoiding huge allocations. I was surprised to realize, when this ticket came along, that this was happening ;)

I have been suspecting that the bloom filters are a major concern too with respect to triggering promotion failures (but I haven't done testing to confirm this). Are there other cases than this and the bloom filters where we know that we're doing large allocations?;;;","13/Apr/11 07:58;jbellis;(I wonder if this is the cause of the intermittent load-spikes-after-upgrade-to-0.7 reports we've seen.);;;","13/Apr/11 08:56;eonnen;As a data point to that question, we hardly ever had CMS collections on 0.6.8 and maybe one full GC ever that I can think of for what was years of cumulative uptime. It surely differs for workloads, but in our case 0.7 got much worse along the CMS dimension.;;;","13/Apr/11 11:45;scode;Filed CASSANDRA-2466 for the bloom filter case.
;;;","13/Apr/11 13:34;jbellis;First time I got a +1 via Twitter: http://twitter.com/#!/cscotta/status/58031493565513728

committed.;;;","13/Apr/11 13:34;jbellis;Thanks for tracking this down, Scott and Erik!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
user created with debian packaging is unable to increase memlock,CASSANDRA-2169,12498679,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jeromatron,jeromatron,jeromatron,16/Feb/11 03:36,16/Apr/19 17:33,22/Mar/23 14:57,20/Feb/11 06:54,0.7.3,,Packaging,,0,,,,,,"To reproduce:
- Install a fresh copy of ubuntu 10.04.
- Install sun's java6 jdk.
- Install libjna-java 3.2.7 into /usr/share/java.
- Install cassandra 0.7.0 from the apache debian packages.
- Start cassandra using /etc/init.d/cassandra
In the output.log there will be the following error:
{quote}
Unable to lock JVM memory (ENOMEM). This can result in part of the JVM being swapped out, especially with mmapped I/O enabled. Increase RLIMIT_MEMLOCK or run Cassandra as root.
{quote}
This shouldn't be as the debian package creates /etc/security/limits.d/cassandra.conf and sets the cassandra user's memlock limit to 'unlimited'.

I tried a variety of things including making the memlock unlimited for all users in /etc/security/limits.conf.  I was able to run cassandra using root with jna symbolically linked into /usr/share/cassandra from /usr/share/java, but I could never get the init.d script to work and get beyond that error.

Based on all the trial and error, I think it might have to do with the cassandra user itself, but my debian/ubuntu fu isn't as good as others'.",,brandon.williams,thepaul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Feb/11 05:14;jeromatron;2169-0_7.txt;https://issues.apache.org/jira/secure/attachment/12471439/2169-0_7.txt",,,,,,,,,,,,,,1.0,jeromatron,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20478,,,Sat Feb 19 23:16:39 UTC 2011,,,,,,,,,,"0|i0g9qn:",93008,,urandom,,urandom,Low,,,,,,,,,,,,,,,,,"16/Feb/11 03:46;jeromatron;Upgraded to 0.7.1 and still appears to have the same issue.;;;","17/Feb/11 02:39;brandon.williams;What happens if you su to the cassandra user and check ulimit?;;;","17/Feb/11 02:50;jeromatron;""ulimit -l"" always returns 64.

I gave the cassandra user a shell so I can su to that user.  When I do, the memlock doesn't ever take - whether it's defined in /etc/security/limits.conf for the cassandra user, for all users '*', or in the /etc/security/limits.d/cassandra.conf.;;;","17/Feb/11 02:53;brandon.williams;Just to be certain limits.conf is taking effect, have you tried rebooting the box since changing it?;;;","17/Feb/11 02:56;jeromatron;I made sure it took effect for the user I originally logged in as - ubuntu for my ec2 instance and jeremy for my local vm.  It took effect for those users whenever I changed it.  I also tried rebooting.  That was the way it took effect for the ec2 server's ubuntu user iirc.  But it didn't take effect for the cassandra user.;;;","17/Feb/11 05:17;jeromatron;Based on a suggestion from jake and brandon, I added {quote}session required pam_limits.so{quote} to /etc/pam.d/common-session and rebooted.  That made it so the memlock value was set correctly.  However, for some reason, using /etc/init.d/cassandra still gives the memory locking error.  I su to the cassandra user and run {quote}/usr/sbin/cassandra -f{quote} and it is able to lock the memory.

For the pam setting, see http://posidev.com/blog/2009/06/04/set-ulimit-parameters-on-ubuntu/;;;","18/Feb/11 13:10;thepaul;I grepped through jsvc's source, and I don't see any references to pam at all. So unless the jre has some special ""switch users and set up a pam session"" functionality I don't know about, jsvc isn't setting up a pam session when switching users.

This means limits.conf (and limits.d/cassandra.conf) are useless, except in what they define for root's resource limits.

If we want to use both limits.conf and jsvc, then hrm. Maybe we could switch users in the initscript using /bin/su, but afaik default debian and ubuntu systems all comment out pam_limits.so from /etc/pam.d/su , so that wouldn't work without monkeying with users' conffiles.

We could switch users in the initscript with sudo, but it's pretty hard to be sure the user hasn't done something funky with their sudoers file which would break our startup.

I can only come up with 2 halfway-decent options: both involve ditching limits.d/cassandra.conf.

1: just /bin/su in the initscript and do a 'ulimit -l unlimited' in the child before exec'ing jsvc.

2: implement limit setting and user switching in cassandra itself. is there any good way to do setrlimit() and setuid() in java?;;;","18/Feb/11 13:19;jbellis;bq. 2: implement limit setting and user switching in cassandra itself. is there any good way to do setrlimit() and setuid() in java?

No, you'd have to write a JNI wrapper or use JNA.  (Which I guess is technically feasible since JNA is packaged for both deb and rpm but eww. :);;;","18/Feb/11 13:23;thepaul;Actually, durr, you could just do 'ulimit -l unlimited' in sh before switching users. It should stick. Sorry, I'm slow tonight.

Still might be a good idea to keep limits.d/cassandra.conf around, in case people want to su or sudo to the cassandra user for testing stuff.;;;","19/Feb/11 00:21;jeromatron;Paul: so is that something that would go in the /debian configuration as a change?  Would you like me to try that in my test server and see if that resolves the problem?;;;","19/Feb/11 02:49;jeromatron;so adding that manually to the /etc/init.d/cassandra script does the trick.  I will submit a patch that adds that.  Thanks guys!;;;","19/Feb/11 05:14;jeromatron;One line patch to fix the ulimit stuff.;;;","20/Feb/11 06:54;urandom;committed; thanks!;;;","20/Feb/11 07:16;hudson;Integrated in Cassandra-0.7 #299 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/299/])
    increase memlock at daemon startup

Patch by Jeremy Hanna; reviewed by eevans for CASSANDRA-2169
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Null pointer exception in doIndexing(ColumnIndexer.java:142),CASSANDRA-458,12436600,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,teodor,teodor,25/Sep/09 21:29,16/Apr/19 17:33,22/Mar/23 14:57,26/Sep/09 20:54,0.4,0.5,,,0,,,,,,"INFO - Saved Token not found. Using 17570558338530880605478324248305304996
INFO - Cassandra starting up...
INFO - Standard1 has reached its threshold; switching in a fresh Memtable
INFO - Enqueuing flush of Memtable(Standard1)@15830327
INFO - Sorting Memtable(Standard1)@15830327
INFO - Writing Memtable(Standard1)@15830327
INFO - Completed flushing /spool/cassandra/data/Keyspace1/Standard1-1-Data.db
INFO - Standard1 has reached its threshold; switching in a fresh Memtable
INFO - Enqueuing flush of Memtable(Standard1)@22655307
INFO - Sorting Memtable(Standard1)@22655307
INFO - Writing Memtable(Standard1)@22655307
ERROR - Error in executor futuretask
java.util.concurrent.ExecutionException: java.lang.AssertionError
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.logFutur
eExceptions(DebuggableThreadPoolExecutor.java:95)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExe
cute(DebuggableThreadPoolExecutor.java:82)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExec
utor.java:887)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor
.java:907)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.AssertionError
        at org.apache.cassandra.db.ColumnIndexer.doIndexing(ColumnIndexer.java:1
07)
        at org.apache.cassandra.db.ColumnIndexer.serialize(ColumnIndexer.java:62
)
        at org.apache.cassandra.db.ColumnFamilySerializer.serializeWithIndexes(C
olumnFamilySerializer.java:78)
        at org.apache.cassandra.db.Memtable.writeSortedContents(Memtable.java:22
2)
        at org.apache.cassandra.db.ColumnFamilyStore$2$1.run(ColumnFamilyStore.j
ava:934)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:44
1)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExec
utor.java:885)
        ... 2 more
INFO - Standard1 has reached its threshold; switching in a fresh Memtable
INFO - Enqueuing flush of Memtable(Standard1)@14600171
INFO - Sorting Memtable(Standard1)@14600171
INFO - Writing Memtable(Standard1)@14600171
INFO - Completed flushing /spool/cassandra/data/Keyspace1/Standard1-3-Data.db 

How to reproduce: Run perl script pointed below, three at once.  In short, script just inserts a row and immediately removes it.
#!/usr/local/bin/perl
use lib qw(/usr/local/cassandra/interface/gen-perl/Cassandra /usr/local/cassandra/interface/gen-perl);
use strict;

use Cassandra;

use Thrift::Socket;
use Thrift::BinaryProtocol;
use Thrift::FramedTransport;
use Thrift::BufferedTransport;

use Data::Dumper;
use Time::HiRes qw( gettimeofday tv_interval );
use Getopt::Std;
my %opt;
getopts('iu:t:rn:', \%opt);

my $socket = Thrift::Socket->new('localhost', 9160);
   $socket->setSendTimeout(1000);
   $socket->setRecvTimeout(5000);
my $transport =  Thrift::BufferedTransport->new($socket, 1024, 1024);
my $protocol = Thrift::BinaryProtocol->new($transport);
my $client = Cassandra::CassandraClient->new($protocol);

$transport->open();


eval {
    my $id=0;
    for(;;) {
        $id++;
        my $PID = sprintf(""%040lld"", int(1000000 * rand()));
        $client->batch_insert(
            'Keyspace1',
            $PID,
            {
                'Standard1' => _makeColumnList ({
                    map {
                        $_=>'0'x(int(1 + 100 * rand()))
                    } (0..int(1+10*rand()))
                })
            },
            Cassandra::ConsistencyLevel::ONE
        );
 
        $client->remove(
            'Keyspace1',
            $PID,
            Cassandra::ColumnPath->new({
                column_family=>'Standard1',
            }),
            time(),
            Cassandra::ConsistencyLevel::ONE
        );
        print ""$id\n"" if ($id%100 == 0);
    }
};
 
die Dumper($@) if ($@);
 
$transport->close();
sub _makeColumnList($$) {
    my ($row) = @_;
 
    my @cfmap;
 
    foreach my $k (keys %$row) {
        push @cfmap, Cassandra::ColumnOrSuperColumn->new({
            column=>Cassandra::Column->new({
                name=>$k,
                value=>$row->{$k},
                timestamp=>time(),
            })
        });
    }
    die if $#cfmap < 0;
    return \@cfmap;
}

","FreeBSD 7.2, diablo-jdk-1.6.0.07.02_5, snapshot cassandra-0.4.0-final at 24/09/2009 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Sep/09 02:11;jbellis;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-458.txt;https://issues.apache.org/jira/secure/attachment/12420585/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-458.txt","26/Sep/09 02:11;jbellis;ASF.LICENSE.NOT.GRANTED--0002-r-m-unused-byte-tracking-code.txt;https://issues.apache.org/jira/secure/attachment/12420586/ASF.LICENSE.NOT.GRANTED--0002-r-m-unused-byte-tracking-code.txt","26/Sep/09 02:11;jbellis;ASF.LICENSE.NOT.GRANTED--0003-columns-may-be-empty-if-the-only-pre-flush-op-was-a-CF.txt;https://issues.apache.org/jira/secure/attachment/12420587/ASF.LICENSE.NOT.GRANTED--0003-columns-may-be-empty-if-the-only-pre-flush-op-was-a-CF.txt","25/Sep/09 21:31;teodor;test.pl;https://issues.apache.org/jira/secure/attachment/12420556/test.pl",,,,,,,,,,,4.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19697,,,Sun Sep 27 12:34:22 UTC 2009,,,,,,,,,,"0|i0fz27:",91278,,,,,Normal,,,,,,,,,,,,,,,,,"25/Sep/09 21:31;teodor;Script to reproduce a problem;;;","25/Sep/09 22:54;jbellis;Sorry, I'm not very familiar with perl.  Any ideas how to solve this?

$ perl test.pl
Can't locate Bit/Vector.pm in @INC
;;;","25/Sep/09 23:04;teodor;From root:
# perl -MCPAN -e shell 
and then
cpan[1]> install Bit::Vector

or use your  packet manager. BTW, Bit::Vector is a prerequisite of thrift;;;","25/Sep/09 23:06;urandom;aptitude install libbit-vector-perl;;;","25/Sep/09 23:07;jbellis;ok.  i have 3 copies running.  they are each past 10000.  how long should this take?;;;","25/Sep/09 23:13;teodor;Several minutes on my notebook, near 20000-50000 records usually.;;;","25/Sep/09 23:20;jbellis;ok, reproduces for me.  thanks!;;;","26/Sep/09 02:20;jbellis;03
    columns may be empty if the only pre-flush op was a CF-level remove op.  fix, with test.

02
    r/m unused byte-tracking code

01
    r/m unnecessary SortedFlushable class.  Keeping a 2nd reference to the Flushable in question,
    when it is only used to pass back to that Flushable for the write phase, was unnecessary & confusing.

Patch 03 is the bug fix against 0.4 and 0.5.  Can you verify that this fixes the problem for you?

Patches 02 and 01 are just for 0.5.;;;","26/Sep/09 02:55;teodor;Thank you a lot, it's working for me. Although patch looks too obvious and simple :);;;","26/Sep/09 13:34;teodor;After full night run all is good. But I saw discussion named ""commit logs are not deleted"" and yesterday I updated svn to include http://svn.apache.org/viewvc?view=rev&revision=819004 and rebuilded cassandra with  0003-columns-may-be-empty-if-the-only-pre-flush-op-was-a-CF.txt patch
And I  face to the same issue with commitlog directory: files never deleted and its modification time is not in the past:
% ls -l /spool/cassandra/commitlog 
/spool/cassandra/commitlog:
total 4460240
-rw-r--r--  1 teodor  wheel  134217855 Sep 26 09:20 CommitLog-1253912758033.log
-rw-r--r--  1 teodor  wheel  134218092 Sep 26 09:20 CommitLog-1253913622766.log
-rw-r--r--  1 teodor  wheel  134217873 Sep 26 09:20 CommitLog-1253914489217.log
-rw-r--r--  1 teodor  wheel  134217828 Sep 26 09:20 CommitLog-1253915364409.log
-rw-r--r--  1 teodor  wheel  134218028 Sep 26 09:20 CommitLog-1253916234925.log
-rw-r--r--  1 teodor  wheel  134218470 Sep 26 09:20 CommitLog-1253917113214.log
-rw-r--r--  1 teodor  wheel  134218152 Sep 26 09:20 CommitLog-1253917968814.log
-rw-r--r--  1 teodor  wheel  134217926 Sep 26 09:20 CommitLog-1253918815999.log
-rw-r--r--  1 teodor  wheel  134218135 Sep 26 09:20 CommitLog-1253919704092.log
-rw-r--r--  1 teodor  wheel  134218195 Sep 26 09:20 CommitLog-1253920594414.log
-rw-r--r--  1 teodor  wheel  134218081 Sep 26 09:20 CommitLog-1253921493770.log
-rw-r--r--  1 teodor  wheel  134218658 Sep 26 09:20 CommitLog-1253922402945.log
-rw-r--r--  1 teodor  wheel  134217814 Sep 26 09:20 CommitLog-1253923245407.log
-rw-r--r--  1 teodor  wheel  134218227 Sep 26 09:20 CommitLog-1253924129426.log
-rw-r--r--  1 teodor  wheel  134218131 Sep 26 09:20 CommitLog-1253925014538.log
-rw-r--r--  1 teodor  wheel  134218027 Sep 26 09:20 CommitLog-1253925890662.log
-rw-r--r--  1 teodor  wheel  134218069 Sep 26 09:20 CommitLog-1253926743631.log
-rw-r--r--  1 teodor  wheel  134218421 Sep 26 09:20 CommitLog-1253927613273.log
-rw-r--r--  1 teodor  wheel  134218410 Sep 26 09:20 CommitLog-1253928473213.log
-rw-r--r--  1 teodor  wheel  134217909 Sep 26 09:20 CommitLog-1253929324973.log
-rw-r--r--  1 teodor  wheel  134217928 Sep 26 09:20 CommitLog-1253930275986.log
-rw-r--r--  1 teodor  wheel  134217996 Sep 26 09:20 CommitLog-1253931167174.log
-rw-r--r--  1 teodor  wheel  134217760 Sep 26 09:20 CommitLog-1253932029445.log
-rw-r--r--  1 teodor  wheel  134218328 Sep 26 09:20 CommitLog-1253932891947.log
-rw-r--r--  1 teodor  wheel  134217793 Sep 26 09:20 CommitLog-1253933740244.log
-rw-r--r--  1 teodor  wheel  134217834 Sep 26 09:20 CommitLog-1253934614337.log
-rw-r--r--  1 teodor  wheel  134217758 Sep 26 09:20 CommitLog-1253935459196.log
-rw-r--r--  1 teodor  wheel  134217875 Sep 26 09:20 CommitLog-1253936347082.log
-rw-r--r--  1 teodor  wheel  134218323 Sep 26 09:20 CommitLog-1253937268917.log
-rw-r--r--  1 teodor  wheel  134217908 Sep 26 09:20 CommitLog-1253938116400.log
-rw-r--r--  1 teodor  wheel  134218158 Sep 26 09:20 CommitLog-1253938991211.log
-rw-r--r--  1 teodor  wheel  134217778 Sep 26 09:20 CommitLog-1253939833974.log
-rw-r--r--  1 teodor  wheel  134217822 Sep 26 09:20 CommitLog-1253940711231.log
-rw-r--r--  1 teodor  wheel  134218030 Sep 26 09:20 CommitLog-1253941558463.log
-rw-r--r--  1 teodor  wheel     524288 Sep 26 09:20 CommitLog-1253942415880.log

I saw discussion named ""commit logs are not deleted"" and yesterday I updated svn to  revision 819092 and rebuild cassandra with  	0003-columns-may-be-empty-if-the-only-pre-flush-op-was-a-CF.txt patch;;;","26/Sep/09 20:54;jbellis;committed patches from this ticket.

commitlog removal is a separate issue.;;;","27/Sep/09 20:34;hudson;Integrated in Cassandra #210 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/210/])
    r/m unused byte-tracking code
patch by jbellis; tested for  by Teodor Sigaev
r/m unnecessary SortedFlushable class.  Keeping a 2nd reference to the Flushable in question,
when it is only used to pass back to that Flushable for the write phase, was unnecessary & confusing.
patch by jbellis; tested for  by Teodor Sigaev
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Startup fails due to cassandra trying to delete nonexisting file,CASSANDRA-2206,12499221,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,tbritz,tbritz,21/Feb/11 19:57,16/Apr/19 17:33,22/Mar/23 14:57,24/Feb/11 23:14,0.7.3,,,,0,,,,,,"Hi,

On one of our nodes, startup fails due to cassandra trying to delete a nonexistant ""Data"" file (see below).

Why that file is missing is another mistery... The log file entries don't show any ERROR messages before cassandra restarted (for reasons I don't know) and this error occured.

Directory listing:

total 109M
-rw-r--r-- 1 root root  51M 2011-02-21 05:25 table_task-f-1666-Data.db
-rw-r--r-- 1 root root 243K 2011-02-21 05:25 table_task-f-1666-Filter.db
-rw-r--r-- 1 root root 6.1M 2011-02-21 05:25 table_task-f-1666-Index.db
-rw-r--r-- 1 root root 4.2K 2011-02-21 05:25 table_task-f-1666-Statistics.db
-rw-r--r-- 1 root root 9.8M 2011-02-21 11:36 table_task-f-1703-Data.db
-rw-r--r-- 1 root root  57K 2011-02-21 11:36 table_task-f-1703-Filter.db
-rw-r--r-- 1 root root 1.3M 2011-02-21 11:36 table_task-f-1703-Index.db
-rw-r--r-- 1 root root 4.2K 2011-02-21 11:36 table_task-f-1703-Statistics.db
-rw-r--r-- 1 root root 292K 2011-02-21 11:42 table_task-f-1704-Data.db
-rw-r--r-- 1 root root 1.7K 2011-02-21 11:42 table_task-f-1704-Filter.db
-rw-r--r-- 1 root root  42K 2011-02-21 11:42 table_task-f-1704-Index.db
-rw-r--r-- 1 root root 4.2K 2011-02-21 11:42 table_task-f-1704-Statistics.db
-rw-r--r-- 1 root root 364K 2011-02-21 11:52 table_task-f-1705-Data.db
-rw-r--r-- 1 root root 2.0K 2011-02-21 11:52 table_task-f-1705-Filter.db
-rw-r--r-- 1 root root  50K 2011-02-21 11:52 table_task-f-1705-Index.db
-rw-r--r-- 1 root root 4.2K 2011-02-21 11:52 table_task-f-1705-Statistics.db
-rw-r--r-- 1 root root 535K 2011-02-21 12:10 table_task-f-1706-Data.db
-rw-r--r-- 1 root root 2.8K 2011-02-21 12:10 table_task-f-1706-Filter.db
-rw-r--r-- 1 root root  70K 2011-02-21 12:10 table_task-f-1706-Index.db
-rw-r--r-- 1 root root 4.2K 2011-02-21 12:10 table_task-f-1706-Statistics.db
-rw-r--r-- 1 root root  11M 2011-02-21 12:11 table_task-f-1707-Data.db
-rw-r--r-- 1 root root  18M 2011-02-21 09:47 table_task_meta-f-417-Data.db
-rw-r--r-- 1 root root 271K 2011-02-21 09:47 table_task_meta-f-417-Filter.db
-rw-r--r-- 1 root root 6.7M 2011-02-21 09:47 table_task_meta-f-417-Index.db
-rw-r--r-- 1 root root 4.2K 2011-02-21 09:47 table_task_meta-f-417-Statistics.db
-rw-r--r-- 1 root root 1.2M 2011-02-21 10:47 table_task_meta-f-418-Data.db
-rw-r--r-- 1 root root  18K 2011-02-21 10:47 table_task_meta-f-418-Filter.db
-rw-r--r-- 1 root root 460K 2011-02-21 10:47 table_task_meta-f-418-Index.db
-rw-r--r-- 1 root root 4.2K 2011-02-21 10:47 table_task_meta-f-418-Statistics.db
-rw-r--r-- 1 root root 791K 2011-02-21 11:47 table_task_meta-f-419-Data.db
-rw-r--r-- 1 root root  13K 2011-02-21 11:47 table_task_meta-f-419-Filter.db
-rw-r--r-- 1 root root 311K 2011-02-21 11:47 table_task_meta-f-419-Index.db
-rw-r--r-- 1 root root 4.2K 2011-02-21 11:47 table_task_meta-f-419-Statistics.db
-rw-r--r-- 1 root root  57K 2011-02-21 12:11 table_task-tmp-f-1707-Filter.db
-rw-r--r-- 1 root root 1.4M 2011-02-21 12:11 table_task-tmp-f-1707-Index.db
-rw-r--r-- 1 root root 4.2K 2011-02-21 12:11 table_task-tmp-f-1707-Statistics.db


Cassandra log:

/software/cassandra/bin/cassandra
rm: cannot remove `/software/cassandra/lib/jna.jar': No such file or directory
root@intr1n3:/cassandra/data/table_task#  INFO 12:47:29,020 Logging initialized
 INFO 12:47:29,030 Heap size: 2614493184/2614493184
 INFO 12:47:29,031 JNA not found. Native methods will be disabled.
 INFO 12:47:29,038 Loading settings from file:/software/cassandra/conf/cassandra.yaml
 INFO 12:47:29,320 DiskAccessMode is standard, indexAccessMode is mmap
 INFO 12:47:29,332 Creating new commitlog segment /hd1/cassandra_md5/commitlog/CommitLog-1298288849332.log
 INFO 12:47:29,422 Opening /cassandra/data/system/Schema-f-244
 INFO 12:47:29,434 Opening /cassandra/data/system/Migrations-f-244
 INFO 12:47:29,437 Opening /cassandra/data/system/LocationInfo-f-137
 INFO 12:47:29,440 Opening /cassandra/data/system/HintsColumnFamily-f-352
 INFO 12:47:29,441 Opening /cassandra/data/system/HintsColumnFamily-f-353
 INFO 12:47:29,465 Loading schema version 54bc134e-2229-11e0-9159-fdf0b6b4b562
 WARN 12:47:29,623 Schema definitions were defined both locally and in cassandra.yaml. Definitions in cassandra.yaml were ignored.
ERROR 12:47:29,638 Exception encountered during startup.
java.io.IOError: java.io.IOException: Failed to delete /cassandra/data/table_task/table_task-tmp-f-1707-Data.db
        at org.apache.cassandra.io.sstable.SSTable.delete(SSTable.java:145)
        at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:468)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:153)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
Caused by: java.io.IOException: Failed to delete /cassandra/data/table_task/table_task-tmp-f-1707-Data.db
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:51)
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:41)
        at org.apache.cassandra.io.sstable.SSTable.delete(SSTable.java:133)
        ... 4 more
Exception encountered during startup.
java.io.IOError: java.io.IOException: Failed to delete /cassandra/data/table_task/table_task-tmp-f-1707-Data.db
        at org.apache.cassandra.io.sstable.SSTable.delete(SSTable.java:145)
        at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:468)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:153)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
Caused by: java.io.IOException: Failed to delete /cassandra/data/table_task/table_task-tmp-f-1707-Data.db
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:51)
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:41)
        at org.apache.cassandra.io.sstable.SSTable.delete(SSTable.java:133)
        ... 4 more
",linux,mdennis,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,"24/Feb/11 07:44;jbellis;2206-v2.txt;https://issues.apache.org/jira/secure/attachment/12471786/2206-v2.txt","21/Feb/11 22:52;jbellis;2206.txt;https://issues.apache.org/jira/secure/attachment/12471565/2206.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20503,,,Thu Feb 24 15:39:39 UTC 2011,,,,,,,,,,"0|i0g9yv:",93045,,gdusbabek,,gdusbabek,Normal,,,,,,,,,,,,,,,,,"21/Feb/11 22:37;jbellis;You can manually fix this by removing the ""tmp"" from all the tmp-f-1707 files.;;;","21/Feb/11 22:51;jbellis;Patch to fix this for new files by making sure we rename -Data last.;;;","23/Feb/11 15:57;mdennis;renaming -data files last doesn't actually fix the problem.

You can test this by running

{noformat}
cd /var/lib/cassandra/data/system
touch Schema-tmp-f-763-Data.db  Schema-f-763-Filter.db
{noformat}

before starting cassandra.

This would simulate a crash between renaming all the the components besides -data and renaming -data (though really a crash any time between when the first component is renamed and before -data is renamed will exhibit the issue).
;;;","24/Feb/11 07:44;jbellis;v2 also fixes a bug where tmp and non-tmp Descriptors were considered equal, which is what confused the file scanning for scrub in your example;;;","24/Feb/11 23:12;gdusbabek;+1;;;","24/Feb/11 23:14;jbellis;committed;;;","24/Feb/11 23:39;hudson;Integrated in Cassandra-0.7 #316 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/316/])
    improve detection and cleanup ofpartially-written sstables
patch by jbellis; reviewed by gdusbabek for CASSANDRA-2206
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-topology.properties cannot reside inside jar file,CASSANDRA-2036,12496455,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,mck,mck,mck,24/Jan/11 04:19,16/Apr/19 17:33,22/Mar/23 14:57,24/Jan/11 23:59,0.7.1,,,,0,,,,,,"PropertyFileSnitch cannot load the cassandra-topology.properties if it is located inside a jar file.

At startup cassandra will print and exit
[ERROR] 20:50:01  Fatal error: Unable to read cassandra-topology.properties
Bad configuration; unable to start server

It seems FBUtilities.resourceToFIle(..) can only be used for loading plain files.

The attached patch solves the problem. It uses the standard java approach for loading a resource stream...
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jan/11 15:45;mck;CASSANDRA-2036.patch;https://issues.apache.org/jira/secure/attachment/12469125/CASSANDRA-2036.patch","24/Jan/11 04:33;mck;CASSANDRA-2036.patch;https://issues.apache.org/jira/secure/attachment/12469101/CASSANDRA-2036.patch",,,,,,,,,,,,,2.0,mck,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20408,,,Mon Jan 24 16:38:54 UTC 2011,,,,,,,,,,"0|i0g8xj:",92877,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"24/Jan/11 13:40;jbellis;Should we also allow loading from a URL the way we do cassandra.yaml in DatabaseDescriptor?;;;","24/Jan/11 14:52;mck;There currently is no way of specifying the path/url to the cassandra-topology.properties, it is just presumed to be at the root of your classpath.

But the one cassandra-topology.properties file can serve not just every node in the cluster, but all nodes in all clusters (useful when test environments run multiple nodes/clusters per server, so it does make sense to allow loading from a URL. 

I can make a new patch for this (although i think this issue becomes an improvement then).;;;","24/Jan/11 15:45;mck;second revision. with improvements so ""-Dcassandra.propertyFileSnitch=<url>"" can be specified on the command line (in a similar manner to ""-Dcassadra.config"".;;;","24/Jan/11 23:59;jbellis;committed the first patch, and created CASSANDRA-2040 for a more general solution to the URL problem in 0.8;;;","25/Jan/11 00:38;hudson;Integrated in Cassandra-0.7 #194 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/194/])
    load PFS properties with ResourceAsStream
patch by Michael SembWever; reviewed by jbellis for CASSANDRA-2036
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
gen-thrift-py can fail but claims success,CASSANDRA-1692,12478832,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,mdennis,mdennis,mdennis,02/Nov/10 03:53,16/Apr/19 17:33,22/Mar/23 14:57,02/Nov/10 06:44,0.7.0 rc 1,,Legacy/Tools,,0,,,,,,"{code}
Buildfile: /home/mdennis/mdev/cassandra-0.7.0-beta2/build.xml

gen-thrift-py:
     [echo] Generating Thrift Python code from /home/mdennis/mdev/cassandra-0.7.0-beta2/interface/cassandra.thrift ....
     [exec] 
     [exec] [FAILURE:/home/mdennis/mdev/cassandra-0.7.0-beta2/interface/cassandra.thrift:374] error: identifier ONE is unqualified!
     [exec] Result: 1

BUILD SUCCESSFUL
Total time: 1 second
{code}

""BUILD SUCCESSFUL"" is not the phrase I would use to describe the outcome of the command in this case :P",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Nov/10 05:02;mdennis;1692-trunk.txt;https://issues.apache.org/jira/secure/attachment/12458582/1692-trunk.txt",,,,,,,,,,,,,,1.0,mdennis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20261,,,Mon Nov 01 22:44:13 UTC 2010,,,,,,,,,,"0|i0g6sv:",92532,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"02/Nov/10 03:54;mdennis;To be clear, I know the problem that causes it to fail; the complaint is that it claimed success;;;","02/Nov/10 04:51;jbellis;Do you know how to fix it?

""ant gen-thrift-py"" is a convenience for developers only, I'm not going to lose much sleep over it not knowing how to handle an out of date thrift compiler.;;;","02/Nov/10 05:02;mdennis;yes, I know how to fix it (patch attached);;;","02/Nov/10 06:44;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming occasionally makes gossip back up,CASSANDRA-2073,12497092,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,brandon.williams,brandon.williams,29/Jan/11 04:47,16/Apr/19 17:33,22/Mar/23 14:57,01/Feb/11 03:04,0.7.1,,,,0,,,,,,"Streaming occasionally makes gossip back up, causing nodes to mark each other as down even though the network is ok.  This appears to happen just after streaming has finished.  I noticed this in the course of working on CASSANDRA-2072, so decommission is one way to reproduce.  It seems to happen maybe one of fifteen or twenty tries, so it's fairly rare.",,gdusbabek,,,,,,,,,,,,,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,,,,,"29/Jan/11 08:55;jbellis;2073.txt;https://issues.apache.org/jira/secure/attachment/12469720/2073.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20428,,,Mon Jan 31 19:04:16 UTC 2011,,,,,,,,,,"0|i0g95r:",92914,,gdusbabek,,gdusbabek,Low,,,,,,,,,,,,,,,,,"29/Jan/11 05:21;gdusbabek;I see this regularly during repair operations.;;;","29/Jan/11 05:24;brandon.williams;I'm fairly certain this is the true cause of CASSANDRA-1730 and other ""gossip took longer than RING_DELAY"" bugs we've had.;;;","29/Jan/11 05:28;jbellis;bq. Streaming occasionally makes gossip back up

On the StreamOut [source] side, the StreamIn [receiver] side, or both?;;;","29/Jan/11 05:40;brandon.williams;Appears to be the receiver.  I just repro'd it with decom.  Nodes A, B, and C.  Decom B, streams to A and C complete, and afterwards A and C cannot gossip to each other for approximately 40s or so.  B did get the usual exception:

{noformat}
ERROR [Thread-6] 2011-01-28 21:23:08,720 AbstractCassandraDaemon.java (line 119) Fatal exception in thread Thread[Thread-6,5,main]
java.util.concurrent.RejectedExecutionException: ThreadPoolExecutor has shut down
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1.rejectedExecution(DebuggableThreadPoolExecutor.java:62)
        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:767)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:658)
        at org.apache.cassandra.net.MessagingService.receive(MessagingService.java:387)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:91)
{noformat}

(I had CASSANDRA-2072 applied to avoid other problems) and I don't see any message about streaming completing on it, though A and C show StreamInSessions finishing right before the gossip outage.;;;","29/Jan/11 05:41;brandon.williams;tpstats on both nodes showed one active gossip task and a bunch pending.;;;","29/Jan/11 05:51;jbellis;i wonder if we have a Thread.sleep(RING_DELAY) in code that gets called on the gossip path.

jstack during the pause?;;;","29/Jan/11 06:09;brandon.williams;It looks like in my case, both nodes are stuck in the same spot:

{noformat}

""GossipStage:1"" prio=10 tid=0x0000000041644000 nid=0xece waiting on condition [0x00007fdbe861f000]
   java.lang.Thread.State: WAITING (parking)
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  <0x00007fdc5ecf8828> (a java.util.concurrent.FutureTask$Sync)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:969)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1281)
    at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:218)
    at java.util.concurrent.FutureTask.get(FutureTask.java:83)
    at org.apache.cassandra.db.HintedHandOffManager.deleteHintsForEndPoint(HintedHandOffManager.java:156)
    at org.apache.cassandra.service.StorageService.excise(StorageService.java:831)
    at org.apache.cassandra.service.StorageService.handleStateLeft(StorageService.java:787)
    at org.apache.cassandra.service.StorageService.onChange(StorageService.java:645)
    at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:742)
    at org.apache.cassandra.gms.Gossiper.applyApplicationStateLocally(Gossiper.java:732)
    at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:649)
    at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:68)
    at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:70)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
{noformat}

They are waiting on deleting hints, and then likely for the hints CF to compact.;;;","29/Jan/11 06:15;jbellis;Bingo.  I don't see any reason why we'd want that to be blocking, do you?

If not we can just remove the .get().;;;","29/Jan/11 06:16;jbellis;... however, we DO want the flush before compaction to be blocking.

possibly kicking the whole operation out to the StorageService task queue is the best move.

(in which case the .get() is a no-op and can still be removed.);;;","29/Jan/11 07:56;brandon.williams;I don't see any reason why it should block, no.  Also I can't repro with repair on a real cluster, so I think this is the only bug.;;;","29/Jan/11 08:55;jbellis;attached;;;","31/Jan/11 23:26;gdusbabek;+1;;;","01/Feb/11 03:04;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Expected both token and generation columns""",CASSANDRA-1146,12465830,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,gdusbabek,jbellis,jbellis,01/Jun/10 14:09,16/Apr/19 17:33,22/Mar/23 14:57,07/Jun/10 21:46,0.6.3,0.7 beta 1,,,0,,,,,,"From the mailing list:

{code}
ERROR 16:14:35,975 Exception encountered during startup.
java.lang.RuntimeException: Expected both token and generation columns; found ColumnFamily(LocationInfo [Generation:false:4@4,])
    at org.apache.cassandra.db.SystemTable.initMetadata(SystemTable.java:159)
    at org.apache.cassandra.service.StorageService.initServer(StorageService.java:305)
    at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:99)
    at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:177)
Exception encountered during startup.
{code}

Separately, the same user wrote: ""I added a server to my cluster. It had some junk in the system/LocationInfo files from previous, unsuccessful attempts to add the server to the cluster. (They were unsuccessful because I hadn't opened the port on that computer.)""

Perhaps that is why it was able to create the Generation column but not the Token?
",,schubertzhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-521,,,,,,,,,"04/Jun/10 00:48;gdusbabek;0001-detect-partitioner-changes-and-fail-fast.patch;https://issues.apache.org/jira/secure/attachment/12446264/0001-detect-partitioner-changes-and-fail-fast.patch","05/Jun/10 12:59;gdusbabek;v2-0001-detect-partitioner-changes-and-fail-fast.patch;https://issues.apache.org/jira/secure/attachment/12446399/v2-0001-detect-partitioner-changes-and-fail-fast.patch",,,,,,,,,,,,,2.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20007,,,Sun Jun 06 02:20:30 UTC 2010,,,,,,,,,,"0|i0g39z:",91961,,,,,Normal,,,,,,,,,,,,,,,,,"02/Jun/10 22:46;gdusbabek;This is the error you get these days when you go from RP to OPP and back to RP.;;;","02/Jun/10 22:53;gdusbabek;Increasing priority to major since startup doesn't fail after changing partitioners and doing so corrupts your system table.;;;","03/Jun/10 01:24;jbellis;Isn't this the same as that one issue for making system table always OPP, then?

IIRC we decided to wait until we had sstable versioning done, so we could grandfather in old system tables appropriately.;;;","03/Jun/10 01:25;jbellis;Ah, CASSANDRA-521.  I see you're ahead of me again.;;;","03/Jun/10 01:40;gdusbabek;We decided to wait because we promised we wouldn't change the sstable disk format for 0.5

I guess we need to ask that question again for 0.7.

I still think a header is the right way to implement this.;;;","04/Jun/10 00:51;gdusbabek;patch is for trunk, but putting this back to 0.6 shouldn't pose a stability problem or interfere with upgrades.

Biggest change is that partitioner name is stored in system table and ST.initMetadata() flushes system tables after writing.

btw, this patch doesn't interfere with the sstable format.;;;","04/Jun/10 12:19;jbellis;why the change to flushing system tables?  it's in the commitlog, so it will get replayed if necessary.;;;","04/Jun/10 12:24;gdusbabek;The flush is for the case where a new node halts before the initial system CFs can be written to sstables.;;;","04/Jun/10 20:29;jbellis;so it's covered by the commitlog.  (which CD replays before ST ever gets involved on the next restart);;;","04/Jun/10 21:04;gdusbabek;Right, but you don't want to replay the commit log if someone has changed the partitioner, do you?  That would be Bad.  The point of this patch is to detect a partitioner mismatch at the earliest possible moment.  Flushing the CL after updating the system table means that we have to wait for one less restart for the the system sstable files to appear so that we can rely on the mismatch detection code to work.;;;","04/Jun/10 21:44;jbellis;ah, right.  i get it now.

in that case isn't CFS.forceBlockingFlush on STATUS_CF adequate?;;;","04/Jun/10 21:53;gdusbabek;Yes.  I'll make that change.;;;","05/Jun/10 12:59;gdusbabek;New patch only flushes STATUS_CF.;;;","06/Jun/10 10:20;jbellis;+1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"sstable2json generates invalid json for ""paged"" rows",CASSANDRA-2188,12498989,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,skamio,skamio,18/Feb/11 10:43,16/Apr/19 17:33,22/Mar/23 14:57,23/Feb/11 01:38,0.7.3,0.8 beta 1,Legacy/Tools,,0,,,,,,"I have a json file created with sstable2json for a column family of super column type. But json2sstable failed to create sstable from the file. It's because file format is wrong. 

 WARN 11:41:55,141 Schema definitions were defined both locally and in cassandra.yaml. Definitions in cassandra.yaml were ignored.
org.codehaus.jackson.JsonParseException: Unexpected character ('""' (code 34)): was expecting comma to separate OBJECT entries
 at [Source: dump.json; line: 2, column: 739439661]
        at org.codehaus.jackson.JsonParser._constructError(JsonParser.java:929)
        at org.codehaus.jackson.impl.JsonParserBase._reportError(JsonParserBase.java:632)
        at org.codehaus.jackson.impl.JsonParserBase._reportUnexpectedChar(JsonParserBase.java:565)
        at org.codehaus.jackson.impl.Utf8StreamParser.nextToken(Utf8StreamParser.java:128)
        at org.codehaus.jackson.map.deser.UntypedObjectDeserializer.mapObject(UntypedObjectDeserializer.java:93)
        at org.codehaus.jackson.map.deser.UntypedObjectDeserializer.deserialize(UntypedObjectDeserializer.java:65)
        at org.codehaus.jackson.map.deser.MapDeserializer._readAndBind(MapDeserializer.java:197)
        at org.codehaus.jackson.map.deser.MapDeserializer.deserialize(MapDeserializer.java:145)
        at org.codehaus.jackson.map.deser.MapDeserializer.deserialize(MapDeserializer.java:23)
        at org.codehaus.jackson.map.ObjectMapper._readValue(ObjectMapper.java:1261)
        at org.codehaus.jackson.map.ObjectMapper.readValue(ObjectMapper.java:517)
        at org.codehaus.jackson.JsonParser.readValueAs(JsonParser.java:897)
        at org.apache.cassandra.tools.SSTableImport.importUnsorted(SSTableImport.java:208)
        at org.apache.cassandra.tools.SSTableImport.importJson(SSTableImport.java:197)
        at org.apache.cassandra.tools.SSTableImport.main(SSTableImport.java:421)
ERROR: Unexpected character ('""' (code 34)): was expecting comma to separate OBJECT entries
 at [Source: dump.json; line: 2, column: 739439661]

When I looked at the file, I found that a comma is missing between super columns. The part of data is like this: 

[""756e697473"",
 ""32"",
 1297926692097000, false]]}""32303036303830373135303030302f313030303030303030302d32303036313030322d303030303030303639382d612f30"": {
""deletedAt"": -9223372036854775808,
 ""subColumns"": [[""5f64656c"",
 """",
 1297926692097000,
 false],

You'll see no comma between } and "". 

",linux,,,,,,,,,,,,,,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,,,,,"22/Feb/11 20:14;xedin;CASSANDRA-2188.patch;https://issues.apache.org/jira/secure/attachment/12471607/CASSANDRA-2188.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20490,,,Tue Feb 22 18:57:54 UTC 2011,,,,,,,,,,"0|i0g9uv:",93027,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"18/Feb/11 18:03;xedin;This is a problem when sstable2json then, can you please tell me the version of the cassandra from which you were running sstable2json? can you regenerate json file using the lastest version of the cassandra and check if it is corrent using for example http://www.jsonlint.com/? Because I can't reproduce a problem which broken JSON for super column families on my side and need a bit more details on this...;;;","22/Feb/11 12:47;muga_nishizawa;Hi Pavel, 

I was able to reproduce the problem above with my sstable file.  I explained the detail of how to generate problematic sstable on CASSANDRA-2212.  Please check it.  
;;;","22/Feb/11 13:01;jbellis;I'm not sure what CASSANDRA-2212 has to do with json. Do you mean we should close this ticket?;;;","22/Feb/11 20:14;xedin;Fixed problem in SSTable2JSON which was causing this error - no delimiter was set after each of the row paged part.;;;","23/Feb/11 01:38;jbellis;committed;;;","23/Feb/11 02:57;hudson;Integrated in Cassandra-0.7 #304 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/304/])
    fix sstable2json large-row pagination
patch by Pavel Yaskevich; reviewed by jbellis for CASSANDRA-2188
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
unclosed brackets and string literals cause cli exceptions,CASSANDRA-909,12459691,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Duplicate,,riffraff,riffraff,20/Mar/10 02:18,16/Apr/19 17:33,22/Mar/23 14:57,28/Dec/10 23:11,,,,,0,antlr,cli,,,,"As of r925353 (but also in 0.5.1) it seems that the AST is not complete


For example in 0.5.1
cassandra> get Messages_test.Messages['twitter.com:10731838401  
line 1:51 mismatched character '<EOF>' expecting '''
line 0:-1 mismatched input '<EOF>' expecting StringLiteral
Exception in thread ""main"" java.lang.AssertionError
        at org.apache.cassandra.cli.CliClient.executeGet(CliClient.java:279)
        at org.apache.cassandra.cli.CliClient.executeCLIStmt(CliClient.java:57)
        at org.apache.cassandra.cli.CliMain.processCLIStmt(CliMain.java:131)
        at org.apache.cassandra.cli.CliMain.main(CliMain.java:172)

A little debug added in trunk shows that ANTLR is emitting ""mismatched token"" nodes:

cassandra> get Messages_test.Conversations[f
line 1:32 mismatched input 'f' expecting StringLiteral
(NODE_THRIFT_GET <mismatched token: [@-1,0:0='<no text>',<-1>,0:-1], resync=Messages_test.Conversations[f>)
0
Exception in thread ""main"" java.lang.AssertionError
	at org.apache.cassandra.cli.CliClient.executeGet(CliClient.java:312)
	at org.apache.cassandra.cli.CliClient.executeCLIStmt(CliClient.java:60)
	at org.apache.cassandra.cli.CliMain.processCLIStmt(CliMain.java:213)
	at org.apache.cassandra.cli.CliMain.main(CliMain.java:270)

thus the assertion _is_ correct in complaining that it did not get a NODE_COLUMN_ACCESS but it should probably be handled instead of crashing. 

but sadly, I can't really understand where this happens, as I'm no expert of antlr, but I wonder if it could be just instructed to fail fast without reaching the ast walking?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19914,,,Tue Dec 28 15:11:36 UTC 2010,,,,,,,,,,"0|i0g1tr:",91726,,,,,Low,,,,,,,,,,,,,,,,,"20/Mar/10 03:28;jbellis;So this happens in 0.5 and 0.6, but not trunk?  Or also trunk?;;;","20/Mar/10 03:41;riffraff;sorry I must have mislabeled the issue: I only checked 0.5.1 and trunk. The first error appears in both, the latter only in 0.5.1.;;;","20/Mar/10 03:57;jbellis;Okay, I've marked this affecting 0.6 (I assume) and removed the part about the already-fixed one.  (We're not backporting minor fixes to 0.5 at this point.);;;","20/Mar/10 13:59;riffraff;thanks, I did not mean the fix in trunk should be backported, just that the same bug pattern had already been fixed once and maybe the changelog between trunk and branch could give a lead in how to fix the new one. ;;;","28/Dec/10 23:11;jbellis;fixed in 0.7 cli revamp;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Failed bootstrap can cause NPE in batch_mutate on every node, taking down the entire cluster",CASSANDRA-1463,12473282,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,ketralnis,ketralnis,04/Sep/10 04:50,16/Apr/19 17:33,22/Mar/23 14:57,05/Sep/10 04:34,0.6.6,0.7 beta 2,,,0,,,,,,"In adding a node to the cluster, the bootstrap failed (still investigating the cause). An hour later, the entire cluster failed, preventing any writes from being accepted. This exception started being printed to the logs:

{quote}
 INFO [Timer-0] 2010-09-03 12:23:33,282 Gossiper.java (line 402) FatClient /10.251.243.191 has been silent for 3600000ms, removing from gossip
ERROR [Timer-0] 2010-09-03 12:23:33,318 Gossiper.java (line 99) Gossip error
java.util.ConcurrentModificationException
        at java.util.Hashtable$Enumerator.next(Hashtable.java:1048)
        at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:383)
        at org.apache.cassandra.gms.Gossiper$GossipTimerTask.run(Gossiper.java:93)
        at java.util.TimerThread.mainLoop(Timer.java:534)
        at java.util.TimerThread.run(Timer.java:484)
ERROR [pool-1-thread-69153] 2010-09-03 12:23:33,857 Cassandra.java (line 1659) Internal error processing batch_mutate
java.lang.NullPointerException
        at org.apache.cassandra.gms.FailureDetector.isAlive(FailureDetector.java:135)
        at org.apache.cassandra.locator.AbstractReplicationStrategy.getHintedEndpoints(AbstractReplicationStrategy.java:85)
        at org.apache.cassandra.service.StorageProxy.mutateBlocking(StorageProxy.java:204)
        at org.apache.cassandra.thrift.CassandraServer.batch_mutate(CassandraServer.java:415)
        at org.apache.cassandra.thrift.Cassandra$Processor$batch_mutate.process(Cassandra.java:1651)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:1166)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
ERROR [pool-1-thread-69154] 2010-09-03 12:23:33,869 Cassandra.java (line 1659) Internal error processing batch_mutate
java.lang.NullPointerException
        at org.apache.cassandra.gms.FailureDetector.isAlive(FailureDetector.java:135)
        at org.apache.cassandra.locator.AbstractReplicationStrategy.getHintedEndpoints(AbstractReplicationStrategy.java:85)
        at org.apache.cassandra.service.StorageProxy.mutateBlocking(StorageProxy.java:204)
        at org.apache.cassandra.thrift.CassandraServer.batch_mutate(CassandraServer.java:415)
        at org.apache.cassandra.thrift.Cassandra$Processor$batch_mutate.process(Cassandra.java:1651)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:1166)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
{quote}

After a large number of iterations of that (at least thousands), the printed exception was shortened (this shortening is what made me mistakenly file #1462) to

{quote}
ERROR [pool-1-thread-68869] 2010-09-03 12:39:22,857 Cassandra.java (line 1659) Internal error processing batch_mutate
java.lang.NullPointerException
ERROR [pool-1-thread-68869] 2010-09-03 12:39:22,883 Cassandra.java (line 1659) Internal error processing batch_mutate
java.lang.NullPointerException
ERROR [pool-1-thread-68869] 2010-09-03 12:39:22,894 Cassandra.java (line 1659) Internal error processing batch_mutate
java.lang.NullPointerException
ERROR [pool-1-thread-68970] 2010-09-03 12:39:22,985 Cassandra.java (line 1659) Internal error processing batch_mutate
java.lang.NullPointerException
ERROR [pool-1-thread-68970] 2010-09-03 12:39:23,084 Cassandra.java (line 1659) Internal error processing batch_mutate
java.lang.NullPointerException
{quote}

Rolling a restart over the cluster fixed it, but every node had to be restarted before it started accepting writes again.",,brandon.williams,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Sep/10 12:14;jbellis;1463-v2.txt;https://issues.apache.org/jira/secure/attachment/12453859/1463-v2.txt","04/Sep/10 23:02;jbellis;1463-v3.txt;https://issues.apache.org/jira/secure/attachment/12453867/1463-v3.txt","04/Sep/10 05:10;jbellis;1463.txt;https://issues.apache.org/jira/secure/attachment/12453827/1463.txt",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20154,,,Mon Dec 27 23:30:11 UTC 2010,,,,,,,,,,"0|i0g57r:",92275,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"04/Sep/10 04:56;brandon.williams;Fixed in 0.7 by CASSANDRA-757, but the approach we took for 0.6 was CASSANDRA-1289.  My guess is so many batch_mutate errors were being logged, logging consumed all the cpu before the gossiper timer could run again, which would have solved it.  I'm not sure how to solve this in 0.6 in a less invasive way than the 0.7 approach.;;;","04/Sep/10 05:10;jbellis;the CME is a red herring, the real problem is the NPE (caused by the IP being cleared out of the gossip records as indicated in the log, but not out of the pending ranges)

attached patch should fix the NPE, looking at how much of a bitch it would be to fix the root cause (the PR orphan);;;","04/Sep/10 11:30;jbellis;v2 adds an onRemove callback to the gossiper interface, to do the remove from TokenMetadata.;;;","04/Sep/10 11:31;jbellis;patch is against 0.6, i will handle rebase to 0.7 afterwards;;;","04/Sep/10 12:01;brandon.williams;v2 fails to compile:     [javac] /srv/cassandra/src/java/org/apache/cassandra/service/StorageLoadBalancer.java:49: org.apache.cassandra.service.StorageLoadBalancer is not abstract and does not override abstract method onRemove(java.net.InetAddress) in org.apache.cassandra.gms.IEndPointStateChangeSubscriber
;;;","04/Sep/10 12:14;jbellis;corrected v2.  (is it just me or is ""ant"" w/o ""clean"" getting worse at dependency discovery as we add more classes?);;;","04/Sep/10 13:32;brandon.williams;v2 produces this:


 INFO 05:26:36,245 FatClient /10.179.65.102 has been silent for 3600000ms, removing from gossip
ERROR 05:26:36,247 Uncaught exception in thread Thread[Timer-0,5,main]
java.lang.AssertionError
        at org.apache.cassandra.locator.TokenMetadata.removeEndpoint(TokenMetadata.java:192)
        at org.apache.cassandra.service.StorageService.onRemove(StorageService.java:879)
        at org.apache.cassandra.gms.Gossiper.removeEndPoint(Gossiper.java:221)
        at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:407)
        at org.apache.cassandra.gms.Gossiper$GossipTimerTask.run(Gossiper.java:93)
        at java.util.TimerThread.mainLoop(Timer.java:534)
        at java.util.TimerThread.run(Timer.java:484)

But that is all I see, with constant insert pressure.;;;","04/Sep/10 23:02;jbellis;v3 removes obsolete assertion and adds call to calculatePendingRanges in onRemove.;;;","05/Sep/10 02:28;brandon.williams;Looks good now. +1;;;","05/Sep/10 04:34;jbellis;committed;;;","28/Dec/10 07:30;hudson;Integrated in Cassandra-0.7 #121 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/121/])
    Java-based stress util.  Patch by Pavel Yaskevich, reviewed by
brandonwilliams for CASSANDRA-1463
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot Start Cassandra Under Windows,CASSANDRA-948,12461065,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,gdusbabek,nberardi,nberardi,03/Apr/10 05:22,16/Apr/19 17:33,22/Mar/23 14:57,06/May/10 04:53,0.6.2,0.7 beta 1,,,0,windows,wont-start,,,,"I get the following error when I try to launch the RC of Cassandra from the command line:

Starting Cassandra Server
Listening for transport dt_socket at address: 8888
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/cassandra/thrift/CassandraDaemon
Caused by: java.lang.ClassNotFoundException: org.apache.cassandra.thrift.CassandraDaemon
        at java.net.URLClassLoader$1.run(Unknown Source)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(Unknown Source)
        at java.lang.ClassLoader.loadClass(Unknown Source)
        at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source)
        at java.lang.ClassLoader.loadClass(Unknown Source)
Could not find the main class: org.apache.cassandra.thrift.CassandraDaemon.  Program will exit.","Windows 7 (NT 6.1) 64-bit, Java 6 Update 19 64-bit, Cassandra 0.6 RC",astrouk,mgreene,nberardi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Apr/10 05:09;gdusbabek;cassandra-with-fixes.bat;https://issues.apache.org/jira/secure/attachment/12442349/cassandra-with-fixes.bat","03/Apr/10 05:34;nberardi;cassandra.bat;https://issues.apache.org/jira/secure/attachment/12440652/cassandra.bat","05/May/10 05:54;niarck;ccassandra.bat;https://issues.apache.org/jira/secure/attachment/12443649/ccassandra.bat","03/Apr/10 05:24;nberardi;storage-conf.xml;https://issues.apache.org/jira/secure/attachment/12440650/storage-conf.xml","21/Apr/10 05:07;gdusbabek;trunk-CASSANDRA-948.txt;https://issues.apache.org/jira/secure/attachment/12442348/trunk-CASSANDRA-948.txt",,,,,,,,,,5.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19934,,,Wed May 05 20:53:39 UTC 2010,,,,,,,,,,"0|i0g22f:",91765,,,,,Normal,,,,,,,,,,,,,,,,,"03/Apr/10 05:24;nberardi;Here is my storage-conf.xml file.;;;","03/Apr/10 05:34;nberardi;Here is a copy of my cassandra.bat file.;;;","03/Apr/10 23:19;rodrigoap;Did you run ant before? Have you asked for help on the users mailing list before creating this issue?;;;","04/Apr/10 02:06;nberardi;This was an already pre built binary downloaded from the Cassandra site, so I didn't run the ant build tool.  

This appears to be related to data storage paths I set, because if I switch the paths back to the default UNIX paths.  Everything runs fine. 

These paths that I have set worked fine in the 0.5.1 build of Cassandra.  Please see my attached config file for more details.;;;","19/Apr/10 12:44;jbellis;Mark, are we broken again on Windows?;;;","20/Apr/10 22:22;reldan;Hi guys. 
User skanga from http://www.sodeso.nl/?p=80 resolve the problem such way: 

The substitution in the batch file did not work for me as shown below.

C:\Java\servers\Apache-Cassandra-0.6.0-beta2\bin>cassandra
Invalid parameter - P:
Starting Cassandra Server
Listening for transport dt_socket at address: 8888
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/cassandra/thrift/CassandraDaemon
Caused by: java.lang.ClassNotFoundException: org.apache.cassandra.thrift.CassandraDaemon
at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
at java.security.AccessController.doPrivileged(Native Method)
at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
at java.lang.ClassLoader.loadClass(ClassLoader.java:307)
at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
at java.lang.ClassLoader.loadClass(ClassLoader.java:248)
Could not find the main class: org.apache.cassandra.thrift.CassandraDaemon. Program will exit.

C:\Java\servers\Apache-Cassandra-0.6.0-beta2\bin>cassandra.bat

I fixed it in cassandra.bat by replacing

REM Shorten lib path for old platforms
subst P: ""%CASSANDRA_HOME%\lib""
P:
set CLASSPATH=P:\

for %%i in (*.jar) do call :append %%i
goto okClasspath

:append
set CLASSPATH=%CLASSPATH%;P:\%*
goto :eof

WITH

REM For each jar in the CASSANDRA_HOME lib directory call append to build the CLASSPATH variable.
for %%i in (%CASSANDRA_HOME%\lib\*.jar) do call :append %%~fi
goto okClasspath

:append
set CLASSPATH=%CLASSPATH%;%1%2
goto :eof
;;;","21/Apr/10 00:20;nberardi;Thanks Eldar,

I know you are trying to be helpful, but as I indicated in my comments above the issue has to do with the CommitLogDirectory and DataFileDirectory when changed from a UNIX path to a Windows path.  

This issue appears to be solved. Because I took my config file from above and started it on 0.6.1 this morning.  

Nick;;;","21/Apr/10 05:07;gdusbabek;This fixes the problem in trunk and 0.6.  

I will commit the change after someone tests and gives it a +1 review.;;;","21/Apr/10 05:09;gdusbabek;Including a patched version of the batch file for the patch-averse.;;;","22/Apr/10 08:48;mgreene;Using the cassandra-with-fixes.bat I was able to start cassandra 0.6.1 on windows. However, this error did appear in the log:

log4j:ERROR Could not read configuration file [null\log4j.properties].
java.io.FileNotFoundException: null\log4j.properties (The system cannot find the path specified)
        at java.io.FileInputStream.open(Native Method)
        at java.io.FileInputStream.<init>(FileInputStream.java:106)
        at java.io.FileInputStream.<init>(FileInputStream.java:66)
        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:306)
        at org.apache.log4j.PropertyConfigurator.configure(PropertyConfigurator.java:324)
        at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:62)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:177)
log4j:ERROR Ignoring configuration file [null\log4j.properties].;;;","22/Apr/10 09:16;gdusbabek;Thanks Mark.  Can you tell me where you keep log4j.properties in relation to everything else?;;;","22/Apr/10 09:27;mgreene;@Gary Sure....for me it's C:\dev\apache-cassandra-0.6.1\conf\log4j.properties.

;;;","28/Apr/10 12:01;astrouk;Hi Gary and Mark,
I have just tried running cassandra-with-fixes.bat on fresh installation of 0.6.1 and have received an original error. 
It is Windows 2003 Server R2 32-bit and JDK 6.0.20 if it makes any sense.
Thank you.

Alex;;;","28/Apr/10 19:44;mgreene;@Alexander Hint: You may want to give the details, perhaps the stack trace or the log line that is this ""original error"" :-);;;","29/Apr/10 02:14;astrouk;Sure, Mark.  I have meant an error reported by Nick. I am trying to launch cassandra from command line and receive the following output:

C:\Program Files\apache-cassandra-0.6.1\bin>cassandra-with-fixes.bat
Starting Cassandra Server
Listening for transport dt_socket at address: 8888
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/cassandra/
thrift/CassandraDaemon
Caused by: java.lang.ClassNotFoundException: org.apache.cassandra.thrift.Cassand
raDaemon
        at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:307)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:248)
Could not find the main class: org.apache.cassandra.thrift.CassandraDaemon.  Pro
gram will exit.

This is all I have.

Thank you;;;","29/Apr/10 09:10;astrouk;I got it. CLASSPATH variable breaks at whitespace in a folder name (""Program Files"" in my case). I have just moved it to a different folder, ""Cassandra"", and it works fine.

Thank you.;;;","29/Apr/10 09:19;gdusbabek;Thanks.  I'll treat that as a +1.  I'll add some documentation in a few places about the space-in-classpath problem and then commit this.;;;","29/Apr/10 09:26;jbellis;http://lkamal.blogspot.com/2006/12/setting-java-classpath-option-with.html;;;","01/May/10 06:20;astrouk;Thank you, Gary. However log4j error described above by Mark still occurs. See below:

C:\Cassandra\apache-cassandra-0.6.1\bin>cassandra.bat
Starting Cassandra Server
Listening for transport dt_socket at address: 8888
log4j:ERROR Could not read configuration file [null\log4j.properties].
java.io.FileNotFoundException: null\log4j.properties (The system cannot find the
 path specified)
        at java.io.FileInputStream.open(Native Method)
        at java.io.FileInputStream.<init>(FileInputStream.java:106)
        at java.io.FileInputStream.<init>(FileInputStream.java:66)
        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurato
r.java:306)
        at org.apache.log4j.PropertyConfigurator.configure(PropertyConfigurator.
java:324)
        at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.jav
a:62)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java
:177);;;","03/May/10 22:49;gdusbabek;cassandra-with-fixes.bat (and all the previous versions afaict) already quote the classpath.  My windows VM is ill and I don't feel like doctoring it atm, so if there are any windows users willing to take this one up and work on it, that would be great.;;;","05/May/10 05:54;niarck;my version of the batch file without the space-in-classpath problem and the logj4 error;;;","06/May/10 04:19;akapuya;Fixed by adding CASSANDRA_HOME environment in Windows;;;","06/May/10 04:35;gdusbabek;from IRC:  n]> gdusbabek, ccassandra.bat is the one working.

I'm committing it.
;;;","06/May/10 04:53;gdusbabek;Thanks for the patch Leif!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Lack of subcomparator_type will corrupt the keyspace in Thrift system_add_keyspace(),CASSANDRA-1122,12465309,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,gdusbabek,arya,arya,25/May/10 05:27,16/Apr/19 17:33,22/Mar/23 14:57,27/May/10 03:04,0.7 beta 1,,,,0,,,,,,"I had a problem earlier where I create a Keyspace by reading the default yaml config shipped with the above cassandra package and after parsing and creating the keyspace with PHP Thrift system_add_keysapce command, I would get 'TException: Error: Internal error processing describe_keyspace ' when trying to get describe_keyspace. In cassandra-cli I get the same error.

The cassandra log shows a null pointed exception:
ERROR [pool-1-thread-39] 2010-05-24 14:17:35,204 Cassandra.java (line 1943) Internal error processing describe_keyspace
java.lang.NullPointerException
	at org.apache.cassandra.thrift.CassandraServer.describe_keyspace(CassandraServer.java:476)
	at org.apache.cassandra.thrift.Cassandra$Processor$describe_keyspace.process(Cassandra.java:1939)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:1276)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)

I traced down the problem to where I define cassandra_CfDef. When the type is Super and subcomparator_type is not set, Thrift code does not set it to any default value but blank """". The command system_add_keyspace() runs with no problem. Whatever happens in Cassandra afterwards, will create a useless keyspace.

Here is my code snippet to generate the exception:

 <?php
$GLOBALS['THRIFT_ROOT'] = '/usr/share/php/Thrift';
require_once $GLOBALS['THRIFT_ROOT'].'/packages/cassandra/Cassandra.php';
require_once $GLOBALS['THRIFT_ROOT'].'/packages/cassandra/cassandra_types.php';
require_once $GLOBALS['THRIFT_ROOT'].'/transport/TSocket.php';
require_once $GLOBALS['THRIFT_ROOT'].'/protocol/TBinaryProtocol.php';
require_once $GLOBALS['THRIFT_ROOT'].'/transport/TFramedTransport.php';
require_once $GLOBALS['THRIFT_ROOT'].'/transport/TBufferedTransport.php';

try {
  // Make a connection to the Thrift interface to Cassandra
  $socket = new TSocket('127.0.0.1', 9160);
  $transport = new TBufferedTransport($socket, 1024, 1024);
  $protocol = new TBinaryProtocolAccelerated($transport);
  $client = new CassandraClient($protocol);
  $transport->open();

  //Try creating some keyspace/column family defs
  $ks = new cassandra_KsDef();
  $ks->name = 'agoudarzi_Keyspace1';
  $ks->strategy_class = 'org.apache.cassandra.locator.RackUnawareStrategy';
  $ks->replication_factor = '1';
  
  //Now add a column family to it
  $cf = new cassandra_CfDef();
  $cf->name = 'Super3';
  $cf->table = 'agoudarzi_Keyspace1';
  $cf->column_type = 'Super';
  $cf->comparator_type = 'LongType';
  $cf->row_cache_size = '0';
  $cf->key_cache_size = '50';
  $cf->comment = 'A column family with supercolumns, whose column names are Longs (8 bytes)';
  $ks->cf_defs[] = $cf;

  $client->system_add_keyspace($ks);
  
  sleep(2);
  
  //Try to check if keyspace description is good
  $client->set_keyspace('agoudarzi_Keyspace1');
  $rs = $client->describe_keyspace('agoudarzi_Keyspace1');

  $transport->close();

} catch (TException $tx) {
   print 'TException: '.$tx->why. ' Error: '.$tx->getMessage() . ""\n"";
}
?>

I think a default subcomparator should be set by thrift or other defensive method be used to prevent this problem.

Please investigate. 

Thanks.","CentOS 5.2 - Linux 2.6.18-164.15.1.el5 #1 SMP Wed Mar 17 11:30:06 EDT 2010 x86_64 x86_64 x86_64 GNU/Linux
apache-cassandra-2010-05-21_13-27-42
Thrift 2.0 with patch https://issues.apache.org/jira/browse/THRIFT-780
PHP 5.2",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/May/10 02:33;gdusbabek;0001-CFMeta.subcolumncomparator-should-default-to-BytesTy.patch;https://issues.apache.org/jira/secure/attachment/12445574/0001-CFMeta.subcolumncomparator-should-default-to-BytesTy.patch","25/May/10 05:29;arya;test_cass.php;https://issues.apache.org/jira/secure/attachment/12445388/test_cass.php",,,,,,,,,,,,,2.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19999,,,Thu May 27 12:47:29 UTC 2010,,,,,,,,,,"0|i0g34v:",91938,,,,,Normal,,,,,,,,,,,,,,,,,"25/May/10 05:29;arya;The PHP Code to reproduce this exception. ;;;","27/May/10 02:36;gdusbabek;Before CASSANDRA-44, we assumed defaulted CF.subcolumcomparator to BytesType if it wasn't specified on a supercolumn.  We can't have a conditional default in thrift, so make it right on the server.

Includes a system test that could reproduce the error prior to the patch.;;;","27/May/10 02:36;jbellis;+1;;;","27/May/10 20:47;hudson;Integrated in Cassandra #447 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/447/])
    CFMeta.subcolumncomparator should default to BytesType if cftype==Super. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1122
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unsafe Multimap Access in MessagingService,CASSANDRA-2037,12496456,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Duplicate,,eonnen,eonnen,24/Jan/11 04:20,16/Apr/19 17:33,22/Mar/23 14:57,24/Jan/11 05:29,,,,,0,,,,,,"MessagingSerice is a system singleton with a static Multimap field targets. Multimaps are not thread safe but no attempt is made to synchronize access to that field. Multimap ultimately uses the standard java HashMap which is susceptible to a race condition where threads will get stuck during a get operation yielding multiple threads similar to the following stack:

""pool-1-thread-6451"" prio=10 tid=0x00007fa5242c9000 nid=0x10f4 runnable [0x00007fa52fde4000]
   java.lang.Thread.State: RUNNABLE
	at java.util.HashMap.get(HashMap.java:303)
	at com.google.common.collect.AbstractMultimap.getOrCreateCollection(AbstractMultimap.java:205)
	at com.google.common.collect.AbstractMultimap.put(AbstractMultimap.java:194)
	at com.google.common.collect.AbstractListMultimap.put(AbstractListMultimap.java:72)
	at com.google.common.collect.ArrayListMultimap.put(ArrayListMultimap.java:60)
	at org.apache.cassandra.net.MessagingService.sendRR(MessagingService.java:303)
	at org.apache.cassandra.service.StorageProxy.strongRead(StorageProxy.java:353)
	at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:229)
	at org.apache.cassandra.thrift.CassandraServer.readColumnFamily(CassandraServer.java:98)
	at org.apache.cassandra.thrift.CassandraServer.get(CassandraServer.java:289)
	at org.apache.cassandra.thrift.Cassandra$Processor$get.process(Cassandra.java:2655)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2555)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)",,eonnen,johanoskarsson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jan/11 01:31;tbritz;jstackerror.txt;https://issues.apache.org/jira/secure/attachment/12469302/jstackerror.txt",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20409,,,Tue Jan 25 22:00:35 UTC 2011,,,,,,,,,,"0|i0g8xr:",92878,,,,,Critical,,,,,,,,,,,,,,,,,"24/Jan/11 05:14;eonnen;Looks like this was fixed in trunk w/ 1057935;;;","24/Jan/11 05:29;jbellis;yes, fixed for CASSANDRA-1959;;;","24/Jan/11 20:41;tbritz;I tried https://hudson.apache.org/hudson/job/Cassandra-0.7/193/artifact/cassandra/build/apache-cassandra-2011-01-24_06-01-26-bin.tar.gz.

After already a few seconds of running my app, I already had one instance taking over all cpus (uptime load was > 1000).

Unfortunately, I can't output a stacktrace since jstack won't connect (also the -F function won't have any effect).

So there is still a bug somewhere...

;;;","24/Jan/11 22:23;jbellis;Thibaut, are you doing reads, writes, or both?;;;","26/Jan/11 01:29;tbritz;Both. But there might be many requests during the first few seconds when I restart our application. The cluster has size 20.

I disabled JNA, but this didn't help. I still see sudden spikes where cassandra will take up an enormous amount of cpu (uptime load > 1000).

Jstack won't work anymore:

-bash-4.1# jstack 27699 > /tmp/jstackerror
27699: Unable to open socket file: target process not responding or HotSpot VM not loaded
The -F option can be used when the target process is not responding

Also, my entire application comes to a halt as the node is still marked as up, but won't respond (cassandra is taking up all the cpu on the first node)

/software/cassandra/bin/nodetool -h localhost ring
Address         Status State   Load            Owns    Token                                                             
                                                       ffffffffffffffff                                                  
192.168.0.1     Up     Normal  3.48 GB         5.00%   0cc                                                               
192.168.0.2     Up     Normal  3.48 GB         5.00%   199                                                               
192.168.0.3     Up     Normal  3.67 GB         5.00%   266                                                               
192.168.0.4     Up     Normal  2.55 GB         5.00%   333                                                               
192.168.0.5     Up     Normal  2.58 GB         5.00%   400                                                               
192.168.0.6     Up     Normal  2.54 GB         5.00%   4cc                                                               
192.168.0.7     Up     Normal  2.59 GB         5.00%   599                                                               
192.168.0.8     Up     Normal  2.58 GB         5.00%   666                                                               
192.168.0.9     Up     Normal  2.33 GB         5.00%   733                                                               
192.168.0.10    Down   Normal  2.39 GB         5.00%   7ff                                                               
192.168.0.11    Up     Normal  2.4 GB          5.00%   8cc                                                               
192.168.0.12    Up     Normal  2.74 GB         5.00%   999                                                               
192.168.0.13    Up     Normal  3.17 GB         5.00%   a66                                                               
192.168.0.14    Up     Normal  3.25 GB         5.00%   b33                                                               
192.168.0.15    Up     Normal  3.01 GB         5.00%   c00                                                               
192.168.0.16    Up     Normal  2.48 GB         5.00%   ccc                                                               
192.168.0.17    Up     Normal  2.41 GB         5.00%   d99                                                               
192.168.0.18    Up     Normal  2.3 GB          5.00%   e66                                                               
192.168.0.19    Up     Normal  2.27 GB         5.00%   f33                                                               
192.168.0.20    Up     Normal  2.32 GB         5.00%   ffffffffffffffff  


The interesting part is that after a while (seconds or minutes), I have seen cassandra nodes return to a normal state again (without restart). I have also never seen this happen at 2 nodes at the same time in the cluster (the node where it happens differes, but there seems to be scheme for it to happen on the first node most of the times).

In the above case, I restarted node 192.168.0.10  and the first node returned to normal state. (I don't know if there is a correlation)

I attached the jstack of the node in trouble (as soon as I could access it with jstack, but I suspect this is the jstack when the node was running normal again).



;;;","26/Jan/11 01:31;tbritz;Jstack shortly after node returned to normal state;;;","26/Jan/11 01:41;jbellis;thibaut, can you create a new ticket for this?  I don't think it's related to the original multimap problem here.

(next thing to check: is the cpu maxing related to JVM GC?  uncomment the verbose GC logging from cassandra-env.sh.);;;","26/Jan/11 06:00;tbritz;Created https://issues.apache.org/jira/browse/CASSANDRA-2054;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bootstrapping new node causes RowMutationVerbHandler Couldn't find cfId,CASSANDRA-1645,12478033,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,gdusbabek,bterm,bterm,22/Oct/10 05:26,16/Apr/19 17:33,22/Mar/23 14:57,29/Oct/10 05:25,0.7.0 rc 1,,,,0,,,,,,"Existing 0.7.0-beta2 cluster adding 1 new node with no data data on it. Enable bootstrapping and start new node and received stream of Couldn't find cfId, added keyspaces via cli and errors stopped. Node did not bootstrap.


ERROR [MUTATION_STAGE:6] 2010-10-21 15:46:58,950 RowMutationVerbHandler.java (line 81) Error in row mutation
org.apache.cassandra.db.UnserializableColumnFamilyException: Couldn't find cfId=1011
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:113)
        at org.apache.cassandra.db.RowMutationSerializer.defreezeTheMaps(RowMutation.java:365)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:375)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:333)
        at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:49)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
ERROR [MUTATION_STAGE:21] 2010-10-21 15:46:58,950 RowMutationVerbHandler.java (line 81) Error in row mutation
org.apache.cassandra.db.UnserializableColumnFamilyException: Couldn't find cfId=1011
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:113)
        at org.apache.cassandra.db.RowMutationSerializer.defreezeTheMaps(RowMutation.java:365)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:375)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:333)
        at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:49)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
ERROR [MUTATION_STAGE:31] 2010-10-21 15:46:58,950 RowMutationVerbHandler.java (line 81) Error in row mutation
org.apache.cassandra.db.UnserializableColumnFamilyException: Couldn't find cfId=1016
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:113)
        at org.apache.cassandra.db.RowMutationSerializer.defreezeTheMaps(RowMutation.java:365)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:375)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:333)
        at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:49)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
ERROR [MUTATION_STAGE:4] 2010-10-21 15:46:58,950 RowMutationVerbHandler.java (line 81) Error in row mutation",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Oct/10 05:31;stuhood;1645.diff;https://issues.apache.org/jira/secure/attachment/12457873/1645.diff","29/Oct/10 05:20;gdusbabek;ASF.LICENSE.NOT.GRANTED--v2-0001-FBU.decodeToUtf8-duplicates-the-BB-so-other-threads-ca.txt;https://issues.apache.org/jira/secure/attachment/12458280/ASF.LICENSE.NOT.GRANTED--v2-0001-FBU.decodeToUtf8-duplicates-the-BB-so-other-threads-ca.txt","29/Oct/10 05:20;gdusbabek;ASF.LICENSE.NOT.GRANTED--v2-0002-BytesToken.convertByteBuffer-duplicates-the-BB-so-othe.txt;https://issues.apache.org/jira/secure/attachment/12458281/ASF.LICENSE.NOT.GRANTED--v2-0002-BytesToken.convertByteBuffer-duplicates-the-BB-so-othe.txt",,,,,,,,,,,,3.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20236,,,Fri Oct 29 15:41:25 UTC 2010,,,,,,,,,,"0|i0g6i7:",92484,,,,,Normal,,,,,,,,,,,,,,,,,"22/Oct/10 05:35;jbellis;is this reproducible for you?  if so, can you try the latest nightly build?  (https://hudson.apache.org/hudson/job/Cassandra/lastSuccessfulBuild/artifact/cassandra/build/);;;","23/Oct/10 01:41;brandon.williams;Can't reproduce against trunk, this is was likely fixed sometime after beta2.;;;","23/Oct/10 04:48;brandon.williams;Reproduced upgrading beta2 to trunk.  The underlying cause is that there is a problem deserializing the schema, but the bootstrap continues on anyway and then doesn't know about the CFs when it's done.

 INFO 20:43:00,007 Sleeping 90000 ms to wait for load information...
ERROR 20:43:00,291 Error in ThreadPoolExecutor
java.lang.ClassCastException: org.apache.avro.generic.GenericData$Record cannot be cast to org.apache.cassandra.avro.KsDef
        at org.apache.cassandra.db.migration.avro.AddKeyspace.put(AddKeyspace.java:24)
        at org.apache.avro.generic.GenericDatumReader.setField(GenericDatumReader.java:152)
        at org.apache.avro.generic.GenericDatumReader.readRecord(GenericDatumReader.java:142)
        at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:114)
        at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:118)
        at org.apache.avro.generic.GenericDatumReader.readRecord(GenericDatumReader.java:142)
        at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:114)
        at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:105)
        at org.apache.cassandra.io.SerDeUtils.deserializeWithSchema(SerDeUtils.java:98)
        at org.apache.cassandra.db.migration.Migration.deserialize(Migration.java:262)
        at org.apache.cassandra.db.DefinitionsUpdateResponseVerbHandler.doVerb(DefinitionsUpdateResponseVerbHandler.java:57)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
ERROR 20:43:00,293 Fatal exception in thread Thread[ReadStage:1,5,main]
java.lang.ClassCastException: org.apache.avro.generic.GenericData$Record cannot be cast to org.apache.cassandra.avro.KsDef
        at org.apache.cassandra.db.migration.avro.AddKeyspace.put(AddKeyspace.java:24)
        at org.apache.avro.generic.GenericDatumReader.setField(GenericDatumReader.java:152)
        at org.apache.avro.generic.GenericDatumReader.readRecord(GenericDatumReader.java:142)
        at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:114)
        at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:118)
        at org.apache.avro.generic.GenericDatumReader.readRecord(GenericDatumReader.java:142)
        at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:114)
        at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:105)
        at org.apache.cassandra.io.SerDeUtils.deserializeWithSchema(SerDeUtils.java:98)
        at org.apache.cassandra.db.migration.Migration.deserialize(Migration.java:262)
        at org.apache.cassandra.db.DefinitionsUpdateResponseVerbHandler.doVerb(DefinitionsUpdateResponseVerbHandler.java:57)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)


;;;","23/Oct/10 05:43;brandon.williams;+1, committed.;;;","23/Oct/10 20:49;hudson;Integrated in Cassandra #574 (See [https://hudson.apache.org/hudson/job/Cassandra/574/])
    Return correct SpecificDatumReader for schema records.  Patch by Stu Hood, reviewed by brandonwilliams for CASSANDRA-1645.
;;;","28/Oct/10 06:30;bterm;upgraded thrift to 0.5.0, upgraded cassandra to 0.7.0-rc1, removed all data and logs. used default configuration other than changing ip addresses. started cassandra and created keyspaces via cli and started sending data to cassandra. 


ERROR [MutationStage:19] 2010-10-27 17:21:46,739 RowMutationVerbHandler.java (line 83) Error in row mutation
org.apache.cassandra.db.UnserializableColumnFamilyException: Couldn't find cfId=1018
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:112)
        at org.apache.cassandra.db.RowMutationSerializer.defreezeTheMaps(RowMutation.java:368)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:378)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:336)
        at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:52)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
ERROR [MutationStage:25] 2010-10-27 17:21:46,739 RowMutationVerbHandler.java (line 83) Error in row mutation
org.apache.cassandra.db.UnserializableColumnFamilyException: Couldn't find cfId=1013
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:112)
        at org.apache.cassandra.db.RowMutationSerializer.defreezeTheMaps(RowMutation.java:368)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:378)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:336)
        at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:52)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
ERROR [MutationStage:27] 2010-10-27 17:21:46,739 RowMutationVerbHandler.java (line 83) Error in row mutation
org.apache.cassandra.db.UnserializableColumnFamilyException: Couldn't find cfId=1018
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:112)
        at org.apache.cassandra.db.RowMutationSerializer.defreezeTheMaps(RowMutation.java:368)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:378)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:336)
        at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:52)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
ERROR [MutationStage:18] 2010-10-27 17:21:46,739 RowMutationVerbHandler.java (line 83) Error in row mutation
org.apache.cassandra.db.UnserializableColumnFamilyException: Couldn't find cfId=1013
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:112)
        at org.apache.cassandra.db.RowMutationSerializer.defreezeTheMaps(RowMutation.java:368)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:378)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:336)
        at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:52)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
ERROR [MutationStage:14] 2010-10-27 17:21:46,739 RowMutationVerbHandler.java (line 83) Error in row mutation
org.apache.cassandra.db.UnserializableColumnFamilyException: Couldn't find cfId=1018
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:112)
        at org.apache.cassandra.db.RowMutationSerializer.defreezeTheMaps(RowMutation.java:368)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:378)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:336)
        at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:52)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619);;;","28/Oct/10 22:17;bterm;steps taken to reproduced it on latest trunk (r1028108):
Stop all running instances of cassandra. remove all files from data and log directories. build source from trunk or grab rc1. change cassandra.yaml seed list, thrift ip addresses and partitioner (using ordered). Start all nodes and watch logs to ensure they cluster. use cassandra-cli on node1 to configure keyspaces with a replication factor of 3, all column families are using comparator = bytestype. start sending data to the cluster (using N python clients). Errors start occurring for RowMutationVerbHandler.java Error in row mutation ...;;;","29/Oct/10 05:13;jbellis;+1 for duplicate fix;;;","29/Oct/10 23:41;hudson;Integrated in Cassandra #580 (See [https://hudson.apache.org/hudson/job/Cassandra/580/])
    BytesToken.convertByteBuffer duplicates the BB so other threads can trust its position. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1645
FBU.decodeToUtf8 duplicates the BB so other threads can trust its position. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1645
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError during initial compaction,CASSANDRA-425,12434898,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jbellis,jbellis,jbellis,05/Sep/09 06:52,16/Apr/19 17:33,22/Mar/23 14:57,08/Sep/09 01:18,0.4,0.5,,,0,,,,,,"Note that the FutureTask the compaction is wrapped in hides the error from the logs, since get() is never called on the task.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Sep/09 10:35;jbellis;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-425-log-errors-in-futuretasks-in-DScheduledT.txt;https://issues.apache.org/jira/secure/attachment/12418692/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-425-log-errors-in-futuretasks-in-DScheduledT.txt","05/Sep/09 10:35;jbellis;ASF.LICENSE.NOT.GRANTED--0002-fix-compaction-bug-only-one-of-the-SSTR-constructor.txt;https://issues.apache.org/jira/secure/attachment/12418693/ASF.LICENSE.NOT.GRANTED--0002-fix-compaction-bug-only-one-of-the-SSTR-constructor.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19682,,,Tue Sep 08 12:35:49 UTC 2009,,,,,,,,,,"0|i0fyun:",91244,,,,,Critical,,,,,,,,,,,,,,,,,"05/Sep/09 10:37;jbellis;02
    fix compaction bug -- only one of the SSTR constructors was adding to openedFiles

01
    log errors in futuretasks in DScheduledTPE like in DTPE (and clean out unused threadlocal hack).

01 should be applied to 0.4 and 0.5.  02 is only needed on 0.5 (fixes regression caused by CASSANDRA-413);;;","08/Sep/09 00:04;urandom;Looks good, +1.;;;","08/Sep/09 01:18;jbellis;committed as described above;;;","08/Sep/09 20:35;hudson;Integrated in Cassandra #191 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/191/])
    fix compaction bug -- only one of the SSTR constructors was adding to openedFiles.  (fixes regression from #413).
patch by jbellis; reviewed by Eric Evans for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLI: issue with keys being interpreted as hex and causing SET statement to fail,CASSANDRA-2497,12504586,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,cdaw,cdaw,19/Apr/11 03:40,16/Apr/19 17:33,22/Mar/23 14:57,21/Apr/11 00:01,0.8.0 beta 2,,,,0,,,,,,"*Original Summary*: Issues with Update Column Family and adding a key_validation_class
_Changed summary because the issue repros on drop/create.  see comment._

*Reproduction Steps*
{code}
create column family users with comparator = UTF8Type 
and column_metadata = [{column_name: password, validation_class: UTF8Type}];

update column family users with key_validation_class=UTF8Type;

set users['jsmith']['password']='ch@ngem3';          
{code}


*EXPECTED RESULT:* After the UPDATE statement, the SET statement should go through successfully.


*ACTUAL RESULT:*  The SET statement gives the same error message, regardless of the UPDATE statement: 
{code}
org.apache.cassandra.db.marshal.MarshalException: cannot parse 'jsmith' as hex bytes
{code}


*Output from describe keyspace*
{code}
    ColumnFamily: users
      Key Validation Class: org.apache.cassandra.db.marshal.UTF8Type
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.UTF8Type
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.29062499999999997/62/1440 (millions of ops/MB/minutes)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      Built indexes: []
      Column Metadata:
        Column Name: password
          Validation Class: org.apache.cassandra.db.marshal.UTF8Type

{code}
","* Single Node instance on MacOSX

* Nightly Compiled Build from 4/18/2011

* Installed from: https://builds.apache.org/hudson/job/Cassandra/lastSuccessfulBuild/artifact/cassandra/build/apache-cassandra-2011-04-18_11-02-29-bin.tar.gz",bilal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Apr/11 06:55;xedin;CASSANDRA-2497.patch;https://issues.apache.org/jira/secure/attachment/12476663/CASSANDRA-2497.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20656,,,Fri Jul 29 02:52:34 UTC 2011,,,,,,,,,,"0|i0gbpj:",93327,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"19/Apr/11 03:49;cdaw;This may not be related to the UPDATE.
I dropped and recreated the CF and still had the same issue.

{code}
[default@cathy] drop column family users;
72a86490-69f4-11e0-0000-242d50cf1fd4
Waiting for schema agreement...
... schemas agree across the cluster

[default@cathy] create column family users with comparator = UTF8Type and key_validation_class=UTF8Type and column_metadata = [{column_name: password, validation_class: UTF8Type}];
8a09a720-69f4-11e0-0000-242d50cf1fd4
Waiting for schema agreement...
... schemas agree across the cluster

[default@cathy] set users['jsmith']['password']='ch@ngem3';
org.apache.cassandra.db.marshal.MarshalException: cannot parse 'jsmith' as hex bytes

{code}


*Output from describe keyspace after drop/create*
{code}
    ColumnFamily: users
      Key Validation Class: org.apache.cassandra.db.marshal.UTF8Type
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.UTF8Type
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.29062499999999997/62/1440 (millions of ops/MB/minutes)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      Built indexes: []
      Column Metadata:
        Column Name: password
          Validation Class: org.apache.cassandra.db.marshal.UTF8Type
{code};;;","19/Apr/11 05:02;jbellis;I think the cli is not reloading the CF metadata.  Try quitting and restarting the cli, after the recreate.

Pavel: we also want an ""assume"" for key types.;;;","19/Apr/11 05:53;cdaw;Restarting the CLI did not fix the issue.
Adding an assume for the key type did.

{code}
[default@cathy] set users['jsmith']['password']='ch@ngem3';
org.apache.cassandra.db.marshal.MarshalException: cannot parse 'jsmith' as hex bytes

[default@cathy] assume users keys as ascii;
Assumption for column family 'users' added successfully.

[default@cathy] set users['jsmith']['password']='ch@ngem3';
Value inserted.
{code}
;;;","19/Apr/11 06:11;xedin;I figured out that problem is in the CLI, will attach a patch tomorrow morning! (sorry for previous misleading comment);;;","20/Apr/11 08:11;jbellis;Pavel, can you describe the problem + fix?;;;","20/Apr/11 17:45;xedin;The problem was in the getKeyAsBytes method - it wasn't using information provided by cfdef.getKey_validation_class() (only comparator set by 'assume' statement or BytesType if it wasn't set). 

The fix was pretty trivial - make getKeyAsBytes use cfdef.getKey_validation_class() + printSliceList method was fixed to use getKeyComparatorForCF instead of just value from 'assume' statement.;;;","21/Apr/11 00:01;jbellis;committed, w/ the addition of ""assert defaultValidationClass != null;""
;;;","21/Apr/11 04:02;hudson;Integrated in Cassandra-0.8 #30 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/30/])
    ;;;","27/Apr/11 06:24;cdaw;Retested and verified this is fixed in current build.;;;","10/May/11 16:24;bilal;I am getting the same issue in 0.8.0.beta2 version.

set Constructors['Ferrari']['principal'] = 'Stefano Domenicali';
org.apache.cassandra.db.marshal.MarshalException: cannot parse 'Ferrari' as hex bytes

It worked after adding:
assume Constructors keys as Ascii;  ;;;","10/May/11 20:47;jbellis;That is working as designed, if you don't want to use assume you need to add a key_validation_class.;;;","11/May/11 00:51;bilal;Thanks Jonathan!

Regards,
Bilal

Sent from my iPhone


;;;","17/Jun/11 09:07;dongfan;Thanks !

I Have been very helpful;;;","05/Jul/11 14:52;mohctp;apache-cassandra-0.8.1

set Users['user1']['fname']='fname1';
rg.apache.cassandra.db.marshal.MarshalException: cannot parse 'fname' as hex bytes

assume Users keys as ascii;

no effect, set still gives the same error.;;;","05/Jul/11 17:53;xedin;As you can see from your example this is not a problem with key but rather with column name. So use `assume <cf> comparator as <type>;` or re-create your CF with a valid comparator.;;;","29/Jul/11 10:52;hollo08;Thanks !;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException on startup after upgrade,CASSANDRA-1545,12475072,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jhermes,bterm,bterm,25/Sep/10 03:33,16/Apr/19 17:33,22/Mar/23 14:57,28/Sep/10 06:38,0.7 beta 2,,,,0,,,,,,"Running a cluster on trunk of 0.7.0beta-2 and updated to tip of trunk. On startup of node got the following NullPointerException. Was using r997774 and switched to r1000247

ERROR [main] 2010-09-22 12:30:14,110 AbstractCassandraDaemon.java (line 216) Exception encountered during startup.
java.lang.NullPointerException
    at org.apache.cassandra.config.CFMetaData.inflate(CFMetaData.java:373)
    at org.apache.cassandra.config.KSMetaData.inflate(KSMetaData.java:118)
    at org.apache.cassandra.db.DefsTable.loadFromStorage(DefsTable.java:106)
    at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:441)
    at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:109)
    at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:54)
    at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:199)
    at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:133)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Sep/10 05:21;jhermes;namechanges.txt;https://issues.apache.org/jira/secure/attachment/12455758/namechanges.txt","28/Sep/10 05:55;jhermes;nullprimitive.txt;https://issues.apache.org/jira/secure/attachment/12455763/nullprimitive.txt",,,,,,,,,,,,,2.0,jhermes,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20192,,,Tue Sep 28 13:31:33 UTC 2010,,,,,,,,,,"0|i0g5vz:",92384,,stuhood,,stuhood,Critical,,,,,,,,,,,,,,,,,"25/Sep/10 03:34;jhermes;Current working theory: looks like CASSANDRA-1437 rears it head, as it did in CASSANDRA-891.;;;","25/Sep/10 06:34;jbellis;At worst blowing away your schema definitions in the system keyspace and recreating them should fix.;;;","25/Sep/10 08:35;jhermes;Yeah, I'm not sure what's causing the NPE after looking at the config and doing some debug testing.
If this is seen from beta1-> or from 0.6.5->, then it would be a major bug, but from an arbitrary trunk to current trunk is not so important.;;;","25/Sep/10 08:40;jbellis;right, we've broken stuff at various revisions, and i don't think we can necessarily fix that retroactively.  as long as beta1 -> beta2 works i think we're fine.;;;","28/Sep/10 05:21;jhermes;Going from beta1 to trunk has several problems.
The first of which is that we stored the old classnames for o.a.c.locator and are now reading them out and failing to find the matching classes.
We have special case logic in DD to fix this, so it's being pushed into CFMetaData.inflate().

Now I can repro this bug from beta1 ->.;;;","28/Sep/10 05:55;jhermes;Was trying to jam a null into an int and not an Integer.
That was subtle.;;;","28/Sep/10 05:56;jhermes;After both patches, on-disk from beta1 is read safely by beta2.;;;","28/Sep/10 06:20;stuhood;+1
Works for me with the definitions in our default cassandra.yaml.;;;","28/Sep/10 06:38;jbellis;pulled the replace ops into KSMD.convertOldStrategyName and committed;;;","28/Sep/10 06:38;bterm;+1 
looks good;;;","28/Sep/10 21:31;hudson;Integrated in Cassandra #549 (See [https://hudson.apache.org/hudson/job/Cassandra/549/])
    fix reading beta1 schema from beta2.  patch by jhermes; reviewed by Stu Hood and jbellis for CASSANDRA-1545
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
`show config file` in cli causes server to throw NPE ,CASSANDRA-129,12424519,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,sandeep_tata,jmhodges,jmhodges,04/May/09 21:33,16/Apr/19 17:33,22/Mar/23 14:57,06/May/09 08:35,0.3,,,,0,,,,,,"Booting up the cli and running the command ""show config file"" in the lastest from trunk (r771019) on a fresh and empty cassandra instance causes a NullPointerError to be thrown. By looking at the code (but being a not-so-hot java developer), it looks like the problem is simply DatabaseDescriptor.getConfigFileName() returning a null because configFileName_ never gets set anywhere in the code.

The error in question:

ERROR - Error occurred during processing of message.
java.lang.NullPointerException
	at java.io.FileInputStream.<init>(FileInputStream.java:133)
	at java.io.FileInputStream.<init>(FileInputStream.java:96)
	at org.apache.cassandra.service.CassandraServer.getStringProperty(CassandraServer.java:485)
	at org.apache.cassandra.service.Cassandra$Processor$getStringProperty.process(Cassandra.java:1294)
	at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:860)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:252)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:713)

","OS X, Java 1.7 (via soylatte)",hammer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/May/09 07:33;sandeep_tata;129.patch;https://issues.apache.org/jira/secure/attachment/12407296/129.patch","06/May/09 08:45;jmhodges;config_file_name.pach;https://issues.apache.org/jira/secure/attachment/12407302/config_file_name.pach",,,,,,,,,,,,,2.0,sandeep_tata,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19560,,,Thu May 07 13:35:05 UTC 2009,,,,,,,,,,"0|i0fx1z:",90953,,,,,Low,,,,,,,,,,,,,,,,,"06/May/09 07:33;sandeep_tata;Reproduced the error.
The config file wasn't being initialized. This patch should fix that.;;;","06/May/09 08:35;jbellis;applied, thanks;;;","06/May/09 08:45;jmhodges;Patch from Sandeep for config flie plus unit test.;;;","06/May/09 08:46;jmhodges;Ah, shoot. Too slow at writing the test for this, I guess.;;;","06/May/09 08:55;sandeep_tata;+1 :-);;;","06/May/09 10:00;jbellis;committed Jeff's test, too.  thanks!;;;","07/May/09 21:35;hudson;Integrated in Cassandra #63 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/63/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BufferUnderflowException occurs in RowMutationVerbHandler,CASSANDRA-1617,12477257,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,moores,moores,14/Oct/10 01:01,16/Apr/19 17:33,22/Mar/23 14:57,21/Oct/10 03:18,0.7 beta 3,,,,0,,,,,,"There might be a bug in hinted handoff?

I have a cluster of 8, replication factor of 3, doing reads/writes with QUORUM.
I have a single thread doing reads/writes of about 2kb across all nodes, running about 200hps.
When I shut down one node, within a few seconds I start seeing some very big recent write latencies, 4-5 seconds.
I looked at the system.log on the node with the adjacent token to the node that I shut down, and see a bad looking BufferUnderflowException:

INFO [WRITE-kv2-app02.dev.real.com/172.27.109.32] 2010-10-12 12:13:36,712 
OutboundTcpConnection.java (line 115) error writing to 
kv2-app02.dev.real.com/172.27.109.32
 INFO [WRITE-kv2-app02.dev.real.com/172.27.109.32] 2010-10-12 12:13:50,336 
OutboundTcpConnection.java (line 115) error writing to 
kv2-app02.dev.real.com/172.27.109.32
 INFO [Timer-0] 2010-10-12 12:14:22,792 Gossiper.java (line 196) InetAddress 
/172.27.109.32 is now dead.
ERROR [MUTATION_STAGE:1315] 2010-10-12 12:14:24,917 
DebuggableThreadPoolExecutor.java (line 103) Error in ThreadPoolExecutor
java.nio.BufferUnderflowException
        at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:127)
        at java.nio.ByteBuffer.get(ByteBuffer.java:675)
        at 
org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:62)
        at 
org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at 
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at 
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
ERROR [MUTATION_STAGE:1315] 2010-10-12 12:14:24,918 
AbstractCassandraDaemon.java (line 88) Fatal exception in thread 
Thread[MUTATION_STAGE:1315,5,main]
java.nio.BufferUnderflowException
        at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:127)
        at java.nio.ByteBuffer.get(ByteBuffer.java:675)
        at 
org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:62)
        at 
org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at 
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at 
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
ERROR [MUTATION_STAGE:1605] 2010-10-12 12:14:28,919 
DebuggableThreadPoolExecutor.java (line 103) Error in ThreadPoolExecutor
java.nio.BufferUnderflowException
        at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:127)
        at java.nio.ByteBuffer.get(ByteBuffer.java:675)
        at 
org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:62)
        at 
org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at 
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at 
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
....
....

I restarted the previously stopped node, and the system recovers, but with a 
few more underlflow exceptions:

 INFO [GOSSIP_STAGE:1] 2010-10-12 12:15:44,537 Gossiper.java (line 594) Node 
/172.27.109.32 has restarted, now UP again
 INFO [HINTED-HANDOFF-POOL:1] 2010-10-12 12:15:44,537 HintedHandOffManager.java 
(line 196) Started hinted handoff for endpoint /172.27.109.32
 INFO [GOSSIP_STAGE:1] 2010-10-12 12:15:44,537 StorageService.java (line 643) 
Node /172.27.109.32 state jump to normal
 INFO [HINTED-HANDOFF-POOL:1] 2010-10-12 12:15:44,538 HintedHandOffManager.java 
(line 252) Finished hinted handoff of 0 rows to endpoint /172.27.109.32
 INFO [GOSSIP_STAGE:1] 2010-10-12 12:15:44,538 StorageService.java (line 650) 
Will not change my token ownership to /172.27.109.32
ERROR [MUTATION_STAGE:1635] 2010-10-12 12:15:45,083 
DebuggableThreadPoolExecutor.java (line 103) Error in ThreadPoolExecutor
java.nio.BufferUnderflowException
        at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:127)
        at java.nio.ByteBuffer.get(ByteBuffer.java:675)
        at 
org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:62)
        at 
org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at 
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at 
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)","Centos 5.4, jdk 1.6.0_20-b02, 16 core xeon, 8 node cluster",gdusbabek,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Oct/10 02:55;brandon.williams;1617.txt;https://issues.apache.org/jira/secure/attachment/12457691/1617.txt",,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20219,,,Thu Oct 21 14:55:50 UTC 2010,,,,,,,,,,"0|i0g6bz:",92456,,,,,Normal,,,,,,,,,,,,,,,,,"15/Oct/10 04:42;jbellis;did you upgrade this cluster from 0.6?;;;","15/Oct/10 05:20;moores;Yea we stopped using 0.6 a while back.  All data was created from scratch with 0.7-beta2.;;;","16/Oct/10 04:25;franklovecchio;We have the nightly build as of yesterday of 0.7 beta 2, and now get the error below when trying to insert a record:  (CLI sytax is something like set CF ['something']['something1']['something2'] = 'value' )

ERROR [MutationStage:2278] 2010-10-15 20:09:39,523 DebuggableThreadPoolExecutor.java (line 103) Error in ThreadPoolExecutor
java.nio.BufferUnderflowException
    at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:145)
    at java.nio.ByteBuffer.get(ByteBuffer.java:692)
    at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:62)
    at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:636)
ERROR [MutationStage:2278] 2010-10-15 20:09:39,524 AbstractCassandraDaemon.java (line 88) Fatal exception in thread Thread[MutationStage:2278,5,main]
java.nio.BufferUnderflowException
    at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:145)
    at java.nio.ByteBuffer.get(ByteBuffer.java:692)
    at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:62)
    at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:636);;;","19/Oct/10 07:35;brandon.williams;In CASSANDRA-1439, we changed the hint destinations from fixed-length encoding to variable-length encoding (by converting UTF-8, to make OPP happy.)  We neglected to frame the message, however.  Patch adds framing and fixes a debug message that is broken.;;;","20/Oct/10 11:35;jbellis;use FBUtilities.read/writeShortByteArray?;;;","21/Oct/10 00:49;brandon.williams;Updated.;;;","21/Oct/10 01:48;jbellis;writeshortbytearray applied to oldhint will create a new length ""frame,"" i think you just want to use out.writeFully(old) or something like that;;;","21/Oct/10 02:55;brandon.williams;Good catch.  Updated to fix that, and name the oldHint variable more aptly so it's clearer why this is done.;;;","21/Oct/10 03:13;jbellis;+1;;;","21/Oct/10 03:18;brandon.williams;Committed.;;;","21/Oct/10 22:55;hudson;Integrated in Cassandra #572 (See [https://hudson.apache.org/hudson/job/Cassandra/572/])
    Add framing to hint destinations.  Patch by brandonwilliams, reviewed by jbellis for CASSANDRA-1617
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL: create keyspace does not the replication factor argument and allows invalid sql to pass thru,CASSANDRA-2525,12504840,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jbellis,cdaw,cdaw,21/Apr/11 09:35,16/Apr/19 17:33,22/Mar/23 14:57,23/Apr/11 04:50,0.8.0 beta 2,,,,0,cql,,,,,"I ran the following SQL in cqlsh and immediately received a socket closed error.  After that point, I couldn't run nodetool, and then got an exception starting up the cluster.

Please Note:  The following syntax is valid in 0.74 but invalid in 0.8.
The 0.8 cassandra-cli errors on the following statement, so the resolution of the bug is to have cqlsh block this incorrect syntax.

{code}
create keyspace testcli with replication_factor=1
and placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy';
{code}

{code}
CREATE KEYSPACE testcql with replication_factor = 1 and strategy_class = 'org.apache.cassandra.locator.SimpleStrategy';	
{code}


{code}
ERROR 01:29:26,989 Exception encountered during startup.
java.lang.RuntimeException: org.apache.cassandra.config.ConfigurationException: SimpleStrategy requires a replication_factor strategy option.
	at org.apache.cassandra.db.Table.<init>(Table.java:278)
	at org.apache.cassandra.db.Table.open(Table.java:110)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:160)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:314)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)
Caused by: org.apache.cassandra.config.ConfigurationException: SimpleStrategy requires a replication_factor strategy option.
	at org.apache.cassandra.locator.SimpleStrategy.validateOptions(SimpleStrategy.java:79)
	at org.apache.cassandra.locator.AbstractReplicationStrategy.createReplicationStrategy(AbstractReplicationStrategy.java:262)
	at org.apache.cassandra.db.Table.createReplicationStrategy(Table.java:328)
	at org.apache.cassandra.db.Table.<init>(Table.java:274)
	... 4 more
Exception encountered during startup.
java.lang.RuntimeException: org.apache.cassandra.config.ConfigurationException: SimpleStrategy requires a replication_factor strategy option.
	at org.apache.cassandra.db.Table.<init>(Table.java:278)
	at org.apache.cassandra.db.Table.open(Table.java:110)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:160)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:314)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)
Caused by: org.apache.cassandra.config.ConfigurationException: SimpleStrategy requires a replication_factor strategy option.
	at org.apache.cassandra.locator.SimpleStrategy.validateOptions(SimpleStrategy.java:79)
	at org.apache.cassandra.locator.AbstractReplicationStrategy.createReplicationStrategy(AbstractReplicationStrategy.java:262)
	at org.apache.cassandra.db.Table.createReplicationStrategy(Table.java:328)
	at org.apache.cassandra.db.Table.<init>(Table.java:274)
	... 4 more
{code}","Cluster: 3 node Rackspace cluster running Centos 5.5.

Build: https://builds.apache.org/hudson/job/Cassandra/859/artifact/cassandra/build/apache-cassandra-2011-04-20_10-01-29-bin.tar.gz
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Apr/11 13:06;jbellis;2525.txt;https://issues.apache.org/jira/secure/attachment/12476960/2525.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20675,,,Tue Apr 26 22:14:18 UTC 2011,,,,,,,,,,"0|i0gbvj:",93354,,urandom,,urandom,Critical,,,,,,,,,,,,,,,,,"21/Apr/11 10:27;yukim;I got the same problem, and after making change in the statement like following

{code}
CREATE KEYSPACE CqlTest WITH strategy_class = SimpleStrategy AND strategy_options:replication_factor = 1;
{code}

everything works fine.
(I'm using apache-cassandra-2011-04-12_18-15-17 build version.)

I think CQL doc should also be updated, because it states that ""replication_factor"" is required.;;;","21/Apr/11 13:06;jbellis;patch to reject malformed keyspaces from cql.

(also patches CQL.textile docs source, but I have not yet figured out how to generate the html from that.);;;","21/Apr/11 22:24;jbellis;opened CASSANDRA-2526 for doc generation;;;","23/Apr/11 04:28;urandom;+1;;;","23/Apr/11 04:50;jbellis;committed;;;","23/Apr/11 09:26;hudson;Integrated in Cassandra-0.8 #36 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/36/])
    validate CQL create keyspace options
patch by jbellis; reviewed by eevans for CASSANDRA-2525
;;;","27/Apr/11 06:14;cdaw;retested and verified;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get_range_slice NPE,CASSANDRA-578,12441537,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,dispalt,dispalt,24/Nov/09 09:30,16/Apr/19 17:33,22/Mar/23 14:57,26/Nov/09 07:16,0.5,,,,0,,,,,,"If I call get_range_slice with arguments in the SliceRange structure, then it seems to NPE.  I think it only does it when there is nothing in the range specified in the column slice start and end.


ERROR - Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:55)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.Row.addColumnFamily(Row.java:96)
        at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1469)
        at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:41)
        ... 4 more
ERROR - Fatal exception in thread Thread[ROW-READ-STAGE:8,5,main]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:55)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.Row.addColumnFamily(Row.java:96)
        at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1469)
        at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:41)
        ... 4 more
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Nov/09 03:35;jbellis;0003-allow-serializing-null-CF-add-get_range_slice-test.patch;https://issues.apache.org/jira/secure/attachment/12426136/0003-allow-serializing-null-CF-add-get_range_slice-test.patch","24/Nov/09 10:57;jbellis;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-578-r-m-unused-Row-code-and-move-table-vari.txt;https://issues.apache.org/jira/secure/attachment/12425928/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-578-r-m-unused-Row-code-and-move-table-vari.txt","24/Nov/09 10:57;jbellis;ASF.LICENSE.NOT.GRANTED--0002-make-Row-contain-a-single-final-CF-reference.txt;https://issues.apache.org/jira/secure/attachment/12425929/ASF.LICENSE.NOT.GRANTED--0002-make-Row-contain-a-single-final-CF-reference.txt",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19763,,,Thu Nov 26 12:34:02 UTC 2009,,,,,,,,,,"0|i0fzsn:",91397,,,,,Normal,,,,,,,,,,,,,,,,,"24/Nov/09 10:58;jbellis;02
    make Row contain a single, final CF reference

01
    r/m unused Row code, and move table variable into callers rather than serializing it redundantly
;;;","24/Nov/09 11:18;dispalt;Still get this error with the patches applied.

DEBUG - range_slice
DEBUG - reading org.apache.cassandra.db.RangeSliceCommand@4ef18d37 from 31@/127.0.0.1
ERROR - Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:53)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.ColumnFamilySerializer.serialize(ColumnFamilySerializer.java:58)
        at org.apache.cassandra.db.RowSerializer.serialize(Row.java:92)
        at org.apache.cassandra.db.RangeSliceReply.getReply(RangeSliceReply.java:53)
        at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:46)
        ... 4 more
ERROR - Fatal exception in thread Thread[ROW-READ-STAGE:11,5,main]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:53)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.ColumnFamilySerializer.serialize(ColumnFamilySerializer.java:58)
        at org.apache.cassandra.db.RowSerializer.serialize(Row.java:92)
        at org.apache.cassandra.db.RangeSliceReply.getReply(RangeSliceReply.java:53)
        at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:46)
        ... 4 more
;;;","24/Nov/09 12:20;jbellis;can you reproduce against the default set of columnfamilies?;;;","24/Nov/09 12:31;dispalt;Let me try to do that.

just fyi in org.apache.cassandra.db.row.RowSerializer row.cf is null which is causing the NPE.;;;","24/Nov/09 12:36;jbellis;I can reproduce it here, I'm good.;;;","24/Nov/09 12:50;jbellis;03
    allow serializing null CF; add get_range_slice test exercising this;;;","24/Nov/09 13:09;dispalt;I am getting a couple errors that I wasn't getting before and I think its related.


2009-11-24_05:06:54.65928 java.lang.NullPointerException
2009-11-24_05:06:54.65928       at org.apache.cassandra.db.Row.digest(Row.java:75)
2009-11-24_05:06:54.65928       at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:103)
2009-11-24_05:06:54.65928       at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:46)
2009-11-24_05:06:54.65928       at org.apache.cassandra.service.QuorumResponseHandler.get(QuorumResponseHandler.java:88)
2009-11-24_05:06:54.65928       at org.apache.cassandra.service.StorageProxy.strongRead(StorageProxy.java:453)
2009-11-24_05:06:54.65928       at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:381)
2009-11-24_05:06:54.65928       at org.apache.cassandra.service.CassandraServer.readColumnFamily(CassandraServer.java:92)
2009-11-24_05:06:54.65928       at org.apache.cassandra.service.CassandraServer.getSlice(CassandraServer.java:170)
2009-11-24_05:06:54.65928       at org.apache.cassandra.service.CassandraServer.multigetSliceInternal(CassandraServer.java:245)
2009-11-24_05:06:54.65928       at org.apache.cassandra.service.CassandraServer.get_slice(CassandraServer.java:208)
2009-11-24_05:06:54.65928       at org.apache.cassandra.service.Cassandra$Processor$get_slice.process(Cassandra.java:758)
2009-11-24_05:06:54.65928       at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:712)
2009-11-24_05:06:54.65928       at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
2009-11-24_05:06:54.65928       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
2009-11-24_05:06:54.65928       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
2009-11-24_05:06:54.65928       at java.lang.Thread.run(Thread.java:636)
2009-11-24_05:06:56.82921 ERROR - Internal error processing get_slice
2009-11-24_05:06:56.82921 java.lang.NullPointerException
2009-11-24_05:06:56.82921       at org.apache.cassandra.db.Row.digest(Row.java:75)
2009-11-24_05:06:56.82921       at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:103)
2009-11-24_05:06:56.82921       at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:46)
2009-11-24_05:06:56.82921       at org.apache.cassandra.service.QuorumResponseHandler.get(QuorumResponseHandler.java:88)
2009-11-24_05:06:56.82921       at org.apache.cassandra.service.StorageProxy.strongRead(StorageProxy.java:453)
2009-11-24_05:06:56.82921       at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:381)
2009-11-24_05:06:56.82921       at org.apache.cassandra.service.CassandraServer.readColumnFamily(CassandraServer.java:92)
2009-11-24_05:06:56.82921       at org.apache.cassandra.service.CassandraServer.getSlice(CassandraServer.java:170)
2009-11-24_05:06:56.82921       at org.apache.cassandra.service.CassandraServer.multigetSliceInternal(CassandraServer.java:245)
2009-11-24_05:06:56.82921       at org.apache.cassandra.service.CassandraServer.get_slice(CassandraServer.java:208)
2009-11-24_05:06:56.82921       at org.apache.cassandra.service.Cassandra$Processor$get_slice.process(Cassandra.java:758)
2009-11-24_05:06:56.82921       at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:712)
2009-11-24_05:06:56.82921       at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
2009-11-24_05:06:56.82921       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
2009-11-24_05:06:56.82921       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
2009-11-24_05:06:56.82921       at java.lang.Thread.run(Thread.java:636)



2009-11-24_05:06:57.54647 java.lang.NullPointerException
2009-11-24_05:06:57.54647       at org.apache.cassandra.db.Row.digest(Row.java:75)
2009-11-24_05:06:57.54647       at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:85)
2009-11-24_05:06:57.54647       at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)
2009-11-24_05:06:57.54647       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
2009-11-24_05:06:57.54647       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
2009-11-24_05:06:57.54647       at java.lang.Thread.run(Thread.java:636)
2009-11-24_05:06:57.62647 ERROR - Error in ThreadPoolExecutor
2009-11-24_05:06:57.62647 java.lang.NullPointerException
2009-11-24_05:06:57.62647       at org.apache.cassandra.db.Row.digest(Row.java:75)
2009-11-24_05:06:57.62647       at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:85)
2009-11-24_05:06:57.62647       at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)
2009-11-24_05:06:57.62647       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
2009-11-24_05:06:57.62647       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
2009-11-24_05:06:57.62647       at java.lang.Thread.run(Thread.java:636)
2009-11-24_05:06:57.62647 ERROR - Fatal exception in thread Thread[ROW-READ-STAGE:3,5,main]
2009-11-24_05:06:57.62647 java.lang.NullPointerException
2009-11-24_05:06:57.62647       at org.apache.cassandra.db.Row.digest(Row.java:75)
2009-11-24_05:06:57.62647       at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:85)
2009-11-24_05:06:57.62647       at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)
2009-11-24_05:06:57.62647       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
2009-11-24_05:06:57.62647       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
2009-11-24_05:06:57.62647       at java.lang.Thread.run(Thread.java:636)
;;;","24/Nov/09 13:16;jbellis;updated patch 03;;;","24/Nov/09 14:01;dispalt;

2009-11-24_06:00:42.63108 ERROR - Internal error processing get_slice
2009-11-24_06:00:42.63108 java.lang.NullPointerException
2009-11-24_06:00:42.63108       at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:130)
2009-11-24_06:00:42.63108       at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:46)
2009-11-24_06:00:42.63108       at org.apache.cassandra.service.QuorumResponseHandler.get(QuorumResponseHandler.java:88)
2009-11-24_06:00:42.63108       at org.apache.cassandra.service.StorageProxy.strongRead(StorageProxy.java:453)
2009-11-24_06:00:42.63108       at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:381)
2009-11-24_06:00:42.63108       at org.apache.cassandra.service.CassandraServer.readColumnFamily(CassandraServer.java:92)
2009-11-24_06:00:42.63108       at org.apache.cassandra.service.CassandraServer.getSlice(CassandraServer.java:170)
2009-11-24_06:00:42.63108       at org.apache.cassandra.service.CassandraServer.multigetSliceInternal(CassandraServer.java:245)
2009-11-24_06:00:42.63108       at org.apache.cassandra.service.CassandraServer.get_slice(CassandraServer.java:208)
2009-11-24_06:00:42.63108       at org.apache.cassandra.service.Cassandra$Processor$get_slice.process(Cassandra.java:758)
2009-11-24_06:00:42.63108       at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:712)
2009-11-24_06:00:42.63108       at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
2009-11-24_06:00:42.63108       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
2009-11-24_06:00:42.63108       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
2009-11-24_06:00:42.63108       at java.lang.Thread.run(Thread.java:636)
;;;","24/Nov/09 21:45;jbellis;updated 03 again;;;","25/Nov/09 15:01;dispalt;I now seem to be getting tihs.


2009-11-25_06:42:26.20349 java.lang.NullPointerException                                                                                                                                
2009-11-25_06:42:26.20349       at org.apache.cassandra.db.ColumnFamily.delete(ColumnFamily.java:252)                                                                                   
2009-11-25_06:42:26.20349       at org.apache.cassandra.db.ColumnFamily.resolve(ColumnFamily.java:402)                                                                                  
2009-11-25_06:42:26.20349       at org.apache.cassandra.db.Row.resolve(Row.java:62)                                                                                                     
2009-11-25_06:42:26.20349       at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:122)                                                             
2009-11-25_06:42:26.20349       at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:46)                                                              
2009-11-25_06:42:26.20349       at org.apache.cassandra.service.QuorumResponseHandler.get(QuorumResponseHandler.java:88)                                                                
2009-11-25_06:42:26.20349       at org.apache.cassandra.service.StorageProxy.strongRead(StorageProxy.java:477)                                                                          
2009-11-25_06:42:26.20349       at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:381)                                                                        
2009-11-25_06:42:26.20349       at org.apache.cassandra.service.CassandraServer.readColumnFamily(CassandraServer.java:92)                                                               
2009-11-25_06:42:26.20349       at org.apache.cassandra.service.CassandraServer.getSlice(CassandraServer.java:170)                                                                      
2009-11-25_06:42:26.20349       at org.apache.cassandra.service.CassandraServer.multigetSliceInternal(CassandraServer.java:245)                                                         
2009-11-25_06:42:26.20349       at org.apache.cassandra.service.CassandraServer.get_slice(CassandraServer.java:208)                                                                     
2009-11-25_06:42:26.20349       at org.apache.cassandra.service.Cassandra$Processor$get_slice.process(Cassandra.java:758)                                                               
2009-11-25_06:42:26.20349       at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:712)                                                                         
2009-11-25_06:42:26.20349       at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)                                                             
2009-11-25_06:42:26.20349       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)                                                                      
2009-11-25_06:42:26.20349       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)                                                                      
2009-11-25_06:42:26.20349       at java.lang.Thread.run(Thread.java:636)


And I am not sure if this is related but it seems new too:

java.lang.RuntimeException: Unable to load comparator class ''.  probably this means you have obsolete sstables lying around                                  
2009-11-25_06:55:56.50945       at org.apache.cassandra.db.ColumnFamilySerializer.readComparator(ColumnFamilySerializer.java:136)                                                       
2009-11-25_06:55:56.50945       at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:107)                                                          
2009-11-25_06:55:56.50945       at org.apache.cassandra.db.RowSerializer.deserialize(Row.java:110)                                                                                      
2009-11-25_06:55:56.50945       at org.apache.cassandra.db.ReadResponseSerializer.deserialize(ReadResponse.java:122)                                                                    
2009-11-25_06:55:56.50945       at org.apache.cassandra.db.ReadResponseSerializer.deserialize(ReadResponse.java:98)                                                                     
2009-11-25_06:55:56.50945       at org.apache.cassandra.service.ReadResponseResolver.isDataPresent(ReadResponseResolver.java:154)                                                       
2009-11-25_06:55:56.50945       at org.apache.cassandra.service.QuorumResponseHandler.response(QuorumResponseHandler.java:97)                                                           
2009-11-25_06:55:56.50945       at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:37)                                                                     
2009-11-25_06:55:56.50945       at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)                                                                        
2009-11-25_06:55:56.50945       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)                                                                      
2009-11-25_06:55:56.50945       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)                                                                      
2009-11-25_06:55:56.50945       at java.lang.Thread.run(Thread.java:636)                                                                                                                
2009-11-25_06:55:56.50945 Caused by: java.lang.ClassNotFoundException:                                                                                                                  
2009-11-25_06:55:56.50945       at java.lang.Class.forName0(Native Method)                                                                                                              
2009-11-25_06:55:56.50945       at java.lang.Class.forName(Class.java:186)                                                                                                              
2009-11-25_06:55:56.50945       at org.apache.cassandra.db.ColumnFamilySerializer.readComparator(ColumnFamilySerializer.java:132)                                                       
2009-11-25_06:55:56.50945       ... 11 more                                                                                                                                             
2009-11-25_06:55:56.50945 ERROR - Fatal exception in thread Thread[RESPONSE-STAGE:2,5,main]                                                                                             
2009-11-25_06:55:56.50945 java.lang.RuntimeException: Unable to load comparator class ''.  probably this means you have obsolete sstables lying around                                  
2009-11-25_06:55:56.50945       at org.apache.cassandra.db.ColumnFamilySerializer.readComparator(ColumnFamilySerializer.java:136)                                                       
2009-11-25_06:55:56.50945       at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:107)                                                          
2009-11-25_06:55:56.50945       at org.apache.cassandra.db.RowSerializer.deserialize(Row.java:110)                                                                                      
2009-11-25_06:55:56.50945       at org.apache.cassandra.db.ReadResponseSerializer.deserialize(ReadResponse.java:122)                                                                    
2009-11-25_06:55:56.50945       at org.apache.cassandra.db.ReadResponseSerializer.deserialize(ReadResponse.java:98)                                                                     
2009-11-25_06:55:56.50945       at org.apache.cassandra.service.ReadResponseResolver.isDataPresent(ReadResponseResolver.java:154)                                                       
2009-11-25_06:55:56.50945       at org.apache.cassandra.service.QuorumResponseHandler.response(QuorumResponseHandler.java:97)                                                           
2009-11-25_06:55:56.50945       at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:37)                                                                     
2009-11-25_06:55:56.50945       at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)                                                                        
2009-11-25_06:55:56.50945       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)                                                                      
2009-11-25_06:55:56.50945       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)                                                                      
2009-11-25_06:55:56.50945       at java.lang.Thread.run(Thread.java:636)                                                                                                                
2009-11-25_06:55:56.50945 Caused by: java.lang.ClassNotFoundException:                                                                                                                  
2009-11-25_06:55:56.50945       at java.lang.Class.forName0(Native Method)                                                                                                              
2009-11-25_06:55:56.50945       at java.lang.Class.forName(Class.java:186)                                                                                                              
2009-11-25_06:55:56.50945       at org.apache.cassandra.db.ColumnFamilySerializer.readComparator(ColumnFamilySerializer.java:132)                                                       
2009-11-25_06:55:56.50945       ... 11 more                ;;;","26/Nov/09 01:04;dispalt;It seems for the comparator problem the """" is getting read as \u0000.    But then if I make that change I start getting:

2009-11-25_17:00:45.31534 INFO - Exception was generated at : 11/25/2009 17:00:45 on thread RESPONSE-STAGE:2                     
2009-11-25_17:00:45.31534                                                                                                        
2009-11-25_17:00:45.31534 java.io.EOFException                                                                                   
2009-11-25_17:00:45.31534       at java.io.DataInputStream.readFully(DataInputStream.java:197)                                   
2009-11-25_17:00:45.31534       at java.io.DataInputStream.readUTF(DataInputStream.java:609)                                     
2009-11-25_17:00:45.31534       at java.io.DataInputStream.readUTF(DataInputStream.java:564)                                     
2009-11-25_17:00:45.31534       at org.apache.cassandra.db.ColumnFamilySerializer.readComparator(ColumnFamilySerializer.java:124)
2009-11-25_17:00:45.31534       at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:107)   
2009-11-25_17:00:45.31534       at org.apache.cassandra.db.RowSerializer.deserialize(Row.java:110)                               
2009-11-25_17:00:45.31534       at org.apache.cassandra.db.ReadResponseSerializer.deserialize(ReadResponse.java:122)             
2009-11-25_17:00:45.31534       at org.apache.cassandra.db.ReadResponseSerializer.deserialize(ReadResponse.java:98)              
2009-11-25_17:00:45.31534       at org.apache.cassandra.service.ReadResponseResolver.isDataPresent(ReadResponseResolver.java:154)
2009-11-25_17:00:45.31534       at org.apache.cassandra.service.QuorumResponseHandler.response(QuorumResponseHandler.java:97)    
2009-11-25_17:00:45.31534       at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:37)              
2009-11-25_17:00:45.31534       at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)                 
2009-11-25_17:00:45.31534       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)               
2009-11-25_17:00:45.31534       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)               
2009-11-25_17:00:45.31534       at java.lang.Thread.run(Thread.java:636);;;","26/Nov/09 01:09;jbellis;the comparator problem is because we are changing Row internals which affects the commit log.  the simplest is to just wipe out the commitlogs, otherwise, go back to the old version and do a nodeprobe flush to clear them out before patching

working on a fix for the other NPE now;;;","26/Nov/09 03:35;jbellis;updated patch 03 with fix for latest NPE;;;","26/Nov/09 05:37;dispalt;I think that did it!  Thank you so much!;;;","26/Nov/09 07:16;jbellis;committed;;;","26/Nov/09 20:34;hudson;Integrated in Cassandra #269 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/269/])
    allow serializing null CF; add get_range_slice test exercising this
patch by jbellis; tested by Dan Di Spaltro for 
make Row contain a single, final CF reference
patch by jbellis; tested by Dan Di Spaltro for 
r/m unused Row code, and move table variable into callers rather than serializing it redundantly
patch by jbellis; tested by Dan Di Spaltro for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mx4j does not load,CASSANDRA-1832,12492636,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,rantav,rstml,rstml,08/Dec/10 00:28,16/Apr/19 17:33,22/Mar/23 14:57,08/Dec/10 09:01,0.7.0 rc 3,,Legacy/Tools,,0,,,,,,"Adding mx4j-tools.jar (latest) to the library causes following exception:

{code}
 WARN 20:22:25,123 Could not start register mbean in JMX
java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.cassandra.utils.Mx4jTool.maybeLoad(Mx4jTool.java:67)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:169)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:55)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:216)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:134)
Caused by: java.net.BindException: Address already in use
	at java.net.PlainSocketImpl.socketBind(Native Method)
	at java.net.PlainSocketImpl.bind(PlainSocketImpl.java:365)
	at java.net.ServerSocket.bind(ServerSocket.java:319)
	at java.net.ServerSocket.<init>(ServerSocket.java:185)
	at mx4j.tools.adaptor.PlainAdaptorServerSocketFactory.createServerSocket(PlainAdaptorServerSocketFactory.java:24)
	at mx4j.tools.adaptor.http.HttpAdaptor.createServerSocket(HttpAdaptor.java:672)
	at mx4j.tools.adaptor.http.HttpAdaptor.start(HttpAdaptor.java:478)
	... 9 more
{code}",CentOS 5.5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Dec/10 05:48;rantav;CASSANDRA-1832.patch;https://issues.apache.org/jira/secure/attachment/12465736/CASSANDRA-1832.patch",,,,,,,,,,,,,,1.0,rantav,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20332,,,Sat Dec 11 07:35:18 UTC 2010,,,,,,,,,,"0|i0g7on:",92675,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"08/Dec/10 01:24;rstml;I did a bit more tests and here are some results which might help:

1. JMX port set to 9090 in cassandra-env.sh
2. On the machine where another service running on 8080 we get exception above
3. On the machine where no service running on 8080 we don't get any exception and MX4J runs on port 9090

Seems like something checks for port 8080 even though it is configured to run on 9090.;;;","08/Dec/10 05:40;rantav;I think there's a confusion. 
There are two ports in business, one is the JMX port (default is 8080) and one is the MX4J port (default 8081)
If the JMX port is used when cassandra starts you see the following exception, which is different from what's pasted in this bug report:

apache-cassandra-0.7.0-rc1 $ bin/cassandra -f 
Error: Exception thrown by the agent : java.rmi.server.ExportException: Port already in use: 8080; nested exception is: 
	java.net.BindException: Address already in use

If mx4j's port, by default 8081, is used, then you see the report from above, which btw isn't fatal, the server operates correctly but without mx4j.

WARN 20:22:25,123 Could not start register mbean in JMX
java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.cassandra.utils.Mx4jTool.maybeLoad(Mx4jTool.java:67)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:169)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:55)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:216)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:134)
Caused by: java.net.BindException: Address already in use
	at java.net.PlainSocketImpl.socketBind(Native Method)
	at java.net.PlainSocketImpl.bind(PlainSocketImpl.java:365)
	at java.net.ServerSocket.bind(ServerSocket.java:319)
	at java.net.ServerSocket.<init>(ServerSocket.java:185)
	at mx4j.tools.adaptor.PlainAdaptorServerSocketFactory.createServerSocket(PlainAdaptorServerSocketFactory.java:24)
	at mx4j.tools.adaptor.http.HttpAdaptor.createServerSocket(HttpAdaptor.java:672)
	at mx4j.tools.adaptor.http.HttpAdaptor.start(HttpAdaptor.java:478)
	... 9 more


So the problem in this case. I believe was that mx4j's port was bound to a different process.
To control the port used by mx4j use  -Dmx4jport=8082. See https://issues.apache.org/jira/browse/CASSANDRA-1068 for more details.

I think this is not a bug and recommend to close it as such.
I will, however, attach a patch for trunk to make this more obvious and add -Dmx4jport=8081 to conf/cassandra-env.sh;;;","08/Dec/10 05:48;rantav;Patch that adds the variables MX4J_ADDRESS and MX4J_PORT to conf/cassandra-env.sh make configuration for mx4j obvious.;;;","08/Dec/10 06:03;rstml;You right Ran, I checked this machine again and I have another service listening on 8081.
For some reason I thought that MX4J uses same port.

With config options we can close it now.;;;","08/Dec/10 09:01;jbellis;committed, thanks!;;;","11/Dec/10 15:35;hudson;Integrated in Cassandra-0.7 #70 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/70/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add INSERT support to CQL,CASSANDRA-2409,12503053,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,jbellis,jbellis,01/Apr/11 02:13,16/Apr/19 17:33,22/Mar/23 14:57,05/Apr/11 04:05,0.8 beta 1,,Legacy/CQL,,0,,,,,,"There are two reasons to support INSERT:

- It helps new users feel comfortable (everyone's first statement will be to try to INSERT something, we should make that a positive experience instead of slapping them)
- Even though it is synonymous with update it is still useful in your code to have both to help communicate intent, similar to choosing good variable names

The only downside is explaining how INSERT isn't a ""true"" insert because it doesn't error out if the row already exists -- but we already have to explain that same concept for UPDATE; the cognitive load is extremely minor.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Apr/11 02:05;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2409-CQL-INSERT-implementation.txt;https://issues.apache.org/jira/secure/attachment/12475392/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2409-CQL-INSERT-implementation.txt","03/Apr/11 01:09;xedin;CASSANDRA-2409.patch;https://issues.apache.org/jira/secure/attachment/12475286/CASSANDRA-2409.patch",,,,,,,,,,,,,2.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20606,,,Mon Apr 04 20:05:45 UTC 2011,,,,,,,,,,"0|i0gb7b:",93245,,urandom,,urandom,Normal,,,,,,,,,,,,,,,,,"01/Apr/11 02:45;urandom;I don't think this is a good idea. {{UPDATE}} already has semantics that are going to be a surprise to folks coming from SQL. Implementing {{INSERT}} to have identical semantics is going to be (IMO) more surprising still, not less.

But, I don't want to get wrapped up in a protracted debate about it, so I'll leave this as ""-0"", and let you guys carry-on as you see fit.

P.S. Please don't wait too long, the freeze is looming.;;;","01/Apr/11 02:50;xedin;Maybe I have a solution which will satisfy both sides - we can just make INSERT as an alias for UPDATE, so no need to implement anything just a simple change in the grammar, what do you think? ;;;","02/Apr/11 07:04;urandom;I don't think this what Jonathan is after, he's looking for the equivalent of SQL's {{INSERT}}. For example:

{code:style=SQL}
INSERT INTO cf (foo, bar, baz) VALUES (1, 20 300) WHERE KEY = fnord;
{code};;;","02/Apr/11 08:56;jbellis;Close.

{code}
INSERT INTO cf (KEY, foo, bar, baz) VALUES (fnord, 1, 20 300);
{code}
;;;","03/Apr/11 01:09;xedin;Format:
{noformat}
INSERT INTO
    <CF>
    (KEY, <column>, <column>, ...)
VALUES
    (<key>, <value>, <value>, ...)
(USING
  CONSISTENCY <level>)?;
{noformat}

Consistency level is set to ONE by default;;;","03/Apr/11 01:10;xedin;Only thing for you, Eric, to change is exception message when column names size not equal to values size...;;;","03/Apr/11 02:40;urandom;Thanks Pavel, I'll have a look at it and get back to you.;;;","05/Apr/11 02:08;urandom;Attached is an updated version of the patch that refactors things a bit.  The biggest problem here is that we need to raise Thrift exceptions.

Also included is some minimal system tests.;;;","05/Apr/11 02:58;xedin;Can you please explane a part about exceptions, what should be done?;;;","05/Apr/11 03:28;urandom;If you raise a RuntimeException from the lexer or parser, it will not be propagated down to the client (it's swallowed and turned into an internal server error).

If you look at the revised patch, I moved the size equality test and conversion to Map into {{UpdateStatement.getColumns()}} where an {{InvalidRequestException}} can be raised (everything that calls {{getColumns()}} already throws {{InvalidRequestException}}. ;;;","05/Apr/11 03:33;xedin;Oh, yeah, I've seen it, thats why I wrote you to change exception... I misunderstood your previous comment and I thought that there are still problems with exceptions left...

+1 on your patch.;;;","05/Apr/11 04:05;urandom;committed; thanks Pavel!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Repair hangs if one of the neighbor is dead,CASSANDRA-2290,12500799,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,09/Mar/11 03:09,16/Apr/19 17:33,22/Mar/23 14:57,19/Apr/11 20:54,0.7.5,0.8 beta 1,,,0,,,,,,"Repair don't cope well with dead/dying neighbors. There is 2 problems:

  # Repair don't check if a node is dead before sending a TreeRequest; this is easily fixable.
  # If a neighbor dies mid-repair, the repair will also hang forever.

The second point is not easy to deal with. The best approach is probably CASSANDRA-1740 however. That is, if we add a way to query the state of a repair, and that this query correctly check all neighbors and also add a way to cancel a repair, this would probably be enough.
",,jborgstrom,stuhood,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,CASSANDRA-1740,,"09/Mar/11 21:16;slebresne;0001-Don-t-start-repair-if-a-neighbor-is-dead.patch;https://issues.apache.org/jira/secure/attachment/12473125/0001-Don-t-start-repair-if-a-neighbor-is-dead.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20543,,,Tue Apr 19 14:04:21 UTC 2011,,,,,,,,,,"0|i0gahz:",93131,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"09/Mar/11 21:16;slebresne;Attaching patch for the first problem above. It checks that all neighbors are alive before attempting the repair. If not, it don't start the repair. Another option would be to still do the repair with whomever neighbor are alive (if any). But I think that refusing to repair is a saner default and I'm fine waiting that someone needs the second option before considering adding it.
;;;","10/Mar/11 02:32;amorton;Not sure if this helps. I found a place where AES was hanging while testing failure during streaming transfer for CASSANDRA-2088 (against 0.7). I broke the FileStresmTask to only send one range and close the sending channel. 

The  IncomingStreamReader.readFile() got stuck in an infinite loop because it does not check the return from FileChannel.transferFrom(). It was returning 0 bytes read. Also the FileStreamTask does not check the bytes sent by transferTo()

While stuck in the loop the socket it was reading from was (127.0.0.1 was in the loop, .0.2 was sending) 
java      25371 aaron   73u  IPv4 0xffffff8010742ff8      0t0  TCP 127.0.0.1:7000->127.0.0.2:52759 (CLOSE_WAIT)

When I was debugging the socketChannel was still reporting it was open. 

Update: Modified FileStresmTask to call System.exit() after sending the first section and got the same result.;;;","19/Apr/11 06:11;jbellis;+1 the check-neighbors patch;;;","19/Apr/11 06:23;slebresne;Committed (to 0.7 and 0.8) the check-neighbor patch. I'm closing this as the rest of this issue is a duplicate CASSANDRA-2433 and could go there.;;;","19/Apr/11 08:25;jbellis;oops, need to fix AESTest now;;;","19/Apr/11 18:28;hudson;Integrated in Cassandra #856 (See [https://hudson.apache.org/hudson/job/Cassandra/856/])
    ;;;","19/Apr/11 18:39;slebresne;Tests fixed, sorry about that.;;;","19/Apr/11 18:46;hudson;Integrated in Cassandra-0.8 #19 (See [https://hudson.apache.org/hudson/job/Cassandra-0.8/19/])
    Fix unit tests for CASSANDRA-2290
;;;","19/Apr/11 22:04;hudson;Integrated in Cassandra-0.7 #447 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/447/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Check for null super column for SC CF in ThriftValidation (and always validate the sc key),CASSANDRA-2571,12505255,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,mbulman,mbulman,27/Apr/11 06:53,16/Apr/19 17:33,22/Mar/23 14:57,30/Apr/11 01:11,0.7.6,0.8.0 beta 2,,,0,,,,,,"Run the following via cli:
{noformat}
[default@test] use test;
Authenticated to keyspace: test
[default@test] create column family super with column_type=Super and default_validation_class=CounterColumnType;
d41df8e0-7055-11e0-0000-242d50cf1fbf
Waiting for schema agreement...
... schemas agree across the cluster
[default@test] incr super['0']['0'];
Value incremented.
[default@test] incr super['0']['0']['0'];
null
{noformat}

Obviously the first incr call is invalid, even though it reports otherwise, as well as generates this exception:
{noformat}
ERROR 17:38:05,871 Fatal exception in thread Thread[COMMIT-LOG-WRITER,5,main]
java.lang.RuntimeException: java.lang.ClassCastException: org.apache.cassandra.db.CounterColumn cannot be cast to org.apache.cassandra.db.SuperColumn
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.ClassCastException: org.apache.cassandra.db.CounterColumn cannot be cast to org.apache.cassandra.db.SuperColumn
        at org.apache.cassandra.db.SuperColumnSerializer.serialize(SuperColumn.java:353)
        at org.apache.cassandra.db.SuperColumnSerializer.serialize(SuperColumn.java:337)
        at org.apache.cassandra.db.ColumnFamilySerializer.serializeForSSTable(ColumnFamilySerializer.java:88)
        at org.apache.cassandra.db.ColumnFamilySerializer.serialize(ColumnFamilySerializer.java:74)
        at org.apache.cassandra.db.RowMutation$RowMutationSerializer.serialize(RowMutation.java:353)
        at org.apache.cassandra.db.RowMutation.getSerializedBuffer(RowMutation.java:236)
        at org.apache.cassandra.db.commitlog.CommitLogSegment.write(CommitLogSegment.java:111)
        at org.apache.cassandra.db.commitlog.CommitLog$LogRecordAdder.run(CommitLog.java:480)
        at org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:49)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 1 more
{noformat}

But the second, proper incr call results in a bunch of exceptions and not a real increment.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Apr/11 00:50;slebresne;0001-Improve-ThriftValidation.patch;https://issues.apache.org/jira/secure/attachment/12477563/0001-Improve-ThriftValidation.patch","29/Apr/11 03:26;jbellis;2571.txt;https://issues.apache.org/jira/secure/attachment/12477683/2571.txt",,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20704,,,Mon May 02 19:54:50 UTC 2011,,,,,,,,,,"0|i0gc5r:",93400,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"28/Apr/11 00:50;slebresne;Turns out this is not a counter related bug. We just don't check when doing a (single) insert (and thus a add) on a super CF that the super column is not null.

Attached patch add a system test and fix it. It also fix another hole in ThriftValidation where the sc key was not validated. The patch is against 0.7 because it's not 0.8 specific.

Note that in 0.8 the cli ""hides"" this error for non counter column, but it is still not counter specific. The fact that the cluster is then ""in a bad state"", is because since we don't refuse the insert, some insert with a 'null' key is inserted in the column family map and thus messed up following insert (I think at least, haven't check super closely).;;;","29/Apr/11 03:26;jbellis;I find boolean (or other :) flags that change method behavior subtly confusing.  Attached is a version that inlines the new check into insert(), which is the only place that wants it.;;;","30/Apr/11 00:27;hudson;Integrated in Cassandra-0.7 #463 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/463/])
    Reject queries with missing mandatory super column and always validate super column name
patch by jbellis and slebresne for CASSANDRA-2571
;;;","30/Apr/11 01:11;slebresne;Committed;;;","30/Apr/11 01:36;hudson;Integrated in Cassandra #872 (See [https://builds.apache.org/hudson/job/Cassandra/872/])
    merge CASSANDRA-2571 from 0.8
;;;","03/May/11 03:54;hudson;Integrated in Cassandra-0.8 #58 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/58/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OutOfMemory on heavy inserts,CASSANDRA-1177,12466573,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Duplicate,,tcurdt,tcurdt,09/Jun/10 23:03,16/Apr/19 17:33,22/Mar/23 14:57,03/Jul/10 01:11,,,,,0,,,,,,"We have cluster of 6 Cassandra 0.6.2 nodes running under SunOS (see environment).

On initial import (using the thrift API) we see some weird behavior of half the cluster. While cas04-06 look fine as you can see from the attached munin graphs, the other 3 nodes kept on GCing (see log file) until they became unreachable and went OOM. (This is also why the stats are so spotty - munin could no longer reach the boxes) We have seen the same behavior on 0.6.2 and 0.6.1. This started after around 100 million inserts.

Looking at the hprof (which is of course to big to attach) we see lots of ConcurrentSkipListMap$Node's and quite some Column objects. Please see the stats attached.

This looks similar to https://issues.apache.org/jira/browse/CASSANDRA-1014 but we are not sure it really is the same.","SunOS 5.10, x86 32bit, Jave Hotspot Server VM 11.2-b01 mixed mode
Sun SDK 1.6.0_12-b04",anty,danielkluesing_bk,johanoskarsson,yannk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-1014,,,,,,"09/Jun/10 23:04;tcurdt;bug report.zip;https://issues.apache.org/jira/secure/attachment/12446696/bug+report.zip","17/Jun/10 21:47;alx;commitlog.txt;https://issues.apache.org/jira/secure/attachment/12447343/commitlog.txt","17/Jun/10 21:47;alx;data.txt;https://issues.apache.org/jira/secure/attachment/12447342/data.txt",,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20021,,,Fri Jul 02 17:11:39 UTC 2010,,,,,,,,,,"0|i0g3gv:",91992,,,,,Critical,,,,,,,,,,,,,,,,,"09/Jun/10 23:14;jbellis;CSLM sucking up the memory sounds like you just have too much unflushed data in your memtables.

Do you have balanced tokens/""load"" across the machines?  (""nodetool ring"")

I would
 - balance nodes (with move) if necessary as described at the top of http://wiki.apache.org/cassandra/Operations
 - increase heap size and/or decrease memtable size + op count flush thresholds, or possibly if you have some memtables way more active than others, leave the flush thresholds high but reduce MemtableFlushAfterMinutes to flush out the less frequently used ones instead.;;;","10/Jun/10 22:50;tcurdt;The output of ""nodetool ring"" is attached. It should be balanced alright. It should match the disk sizes.

We also already played with the thresholds.

I can ask the engineer that worked on this what exactly he tried.;;;","10/Jun/10 23:10;jbellis;if you're balancing by ""disk size"" then you're basically creating hot spots on the ring deliberately.  that's not a good idea unless you are disk space bound and you're sure your disk-heavy machines can handle the extra load, which doesn't look like the case here. :)

cassandra doesn't do backpressure yet (see CASSANDRA-685) so when you are OOMing it under load then you can mitigate it by (a) giving the JVM more heap (or adding machines) and (b) when you get a TimedOutException on the client, sleep 100ms before retrying.  

You may well also be consuming a lot of heap in (a) compaction of Activities or (b) compaction or scanning of hinted handoff rows (once one node starts going down, say, the 12GB one to start with, that will start generating hints on the other ones that can add to the memory pressure they see).

We can continue troubleshooting here or on the list / irc, but I'm resolving NAP because it's almost certainly not a bug per se.;;;","10/Jun/10 23:13;alx;We tried to reduce the MemtableOperationsInMillions from 1 to 0.1 and MemtableFlushAfterMinutes 1. I also increased and decreased the heap size. As you can see in the attachment all nodes are kinda even loaded. Only 10.12.22.117 is showing a huge difference, but this happened after the crashes, before it was equal to the other nodes.

None of the actions helped. We also experienced a flapping with the Gossiper:


 INFO [GC inspection] 2010-06-10 16:10:38,790 GCInspector.java (line 110) GC for ConcurrentMarkSweep: 23943 ms, 8915640 reclaimed leaving 2151863720 used; max is 2263941120
 INFO [GMFD:1] 2010-06-10 16:10:38,790 Gossiper.java (line 568) InetAddress /10.12.22.116 is now UP
 INFO [Timer-1] 2010-06-10 16:10:55,846 Gossiper.java (line 179) InetAddress /10.12.22.116 is now dead.
 INFO [GC inspection] 2010-06-10 16:10:55,846 GCInspector.java (line 110) GC for ConcurrentMarkSweep: 16730 ms, 8592904 reclaimed leaving 2152186664 used; max is 2263941120
 INFO [GMFD:1] 2010-06-10 16:10:55,846 Gossiper.java (line 568) InetAddress /10.12.22.116 is now UP
 INFO [Timer-1] 2010-06-10 16:11:20,004 Gossiper.java (line 179) InetAddress /10.12.22.116 is now dead.
 INFO [GC inspection] 2010-06-10 16:11:20,004 GCInspector.java (line 110) GC for ConcurrentMarkSweep: 24118 ms, 8148936 reclaimed leaving 2152641776 used; max is 2263941120
 INFO [Timer-1] 2010-06-10 16:11:20,004 Gossiper.java (line 179) InetAddress /10.12.22.115 is now dead.
 INFO [GMFD:1] 2010-06-10 16:11:20,004 Gossiper.java (line 568) InetAddress /10.12.22.116 is now UP
 INFO [GMFD:1] 2010-06-10 16:11:20,004 Gossiper.java (line 568) InetAddress /10.12.22.115 is now UP
 INFO [Timer-1] 2010-06-10 16:11:36,610 Gossiper.java (line 179) InetAddress /10.12.22.116 is now dead.
 INFO [GC inspection] 2010-06-10 16:11:36,910 GCInspector.java (line 110) GC for ConcurrentMarkSweep: 16591 ms, 7905120 reclaimed leaving 2152871040 used; max is 2263941120
 INFO [GMFD:1] 2010-06-10 16:11:36,910 Gossiper.java (line 568) InetAddress /10.12.22.116 is now UP
 INFO [Timer-1] 2010-06-10 16:12:01,268 Gossiper.java (line 179) InetAddress /10.12.22.116 is now dead.
 INFO [Timer-1] 2010-06-10 16:12:01,268 Gossiper.java (line 179) InetAddress /10.12.22.115 is now dead.;;;","10/Jun/10 23:46;tcurdt;Please see the comment from Alexander.;;;","11/Jun/10 00:01;alx;To clarify the situation even more. We tried it with a equally balanced ring, means all nodes have the same range assigned. The same problem occurred. The most interesting fact is, that the problematic nodes ending up in a GC storm without any load on the ring. Since the problems started we stopped writing from it. So no external interaction is happening, but the nodes ending up in the endless cycles.;;;","11/Jun/10 00:27;jbellis;The problem is that your cluster state now is not the same as the cluster state before you started seeing this.  Your rows are larger (compaction) and you have hinted handoff complicating the picture.;;;","16/Jun/10 02:29;urandom;Alexander, Torsten, are you still having problems with this?

bq. The most interesting fact is, that the problematic nodes ending up in a GC storm without any load on the ring. Since the problems started we stopped writing from it. So no external interaction is happening, but the nodes ending up in the endless cycles.

To be clear, are you saying that a cluster brought up cold with no traffic has nodes that GC storm, or that even after removing all load they continue to storm? The former would be a memory leak ,the latter would not be surprising (once it starts thrashing like that, it's not likely to recover on its own).;;;","16/Jun/10 02:59;jbellis;It's much more likely to be compaction related than a memory leak.;;;","16/Jun/10 22:53;alx;A cold cluster is performing well. And even under heavy load up to ~100million inserts with a replica factor of 3. But then half or more of the nodes (cluster total is 6) going into the GC storm. After removing the load the nodes still hanging in the GC and even restarting the nodes didn't helped. ;;;","17/Jun/10 21:47;alx;After getting even further with our import, because we lowered the thresholds for the Memtable flushing, we seeing one node not answering over JMX and start to print more and more GC messages to the log. However we looked into the data and commitlog directory and find that the listing of what's inside might be helpful to solve our problem.;;;","18/Jun/10 06:58;urandom;bq. After getting even further with our import, because we lowered the thresholds for the Memtable flushing, we seeing one node not answering over JMX and start to print more and more GC messages to the log. However we looked into the data and commitlog directory and find that the listing of what's inside might be helpful to solve our problem.

I don't think there is anything particularly telling here, other than that the behavior you're seeing still see falls within the range of what is expected (based on what we know).

I would suggest we move this discussion to user@cassandra.apache.org; the mailing list is a better forum for this sort of discussion. If it becomes apparent that there is indeed a bug, we can move it back here along with a summary or a pointer to the thread.

Be sure to include the rate of write operations, the size of the writes, the consistency level being used, how many nodes are involved, along with your most recent configuration.

;;;","03/Jul/10 00:48;tcurdt;Some more information:

We had an old-ish JDK in place. Upgrading to the latest helped but did not fix it.
But we are still seeing those problems for un-throttled inserts through THRIFT.

 - replication factor 3
 - consistency level ONE
 - 6 nodes involved
 - 4 workers at 200-300 writes/s
 - writes are about 300 bytes in size

We switched to use the low level StorageProxy for the bulk import.
Using that the cluster behaved just beautiful. No problems there.
Much much faster!

So I assume as long as we don't insert too fast we should be OK.
But that it's quite a scary situation if the ring does not recover properly.
...and 1200 writes/s for a 6 node cluster is not really that much.;;;","03/Jul/10 01:11;jbellis;CASSANDRA-685 and CASSANDRA-981 will address this in 0.7.  For 0.6 the best solution is to throttle writes if you start seeing TimedOutExceptions.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
memtable_throughput_in_mb can not support sizes over 2.2 gigs because of an integer overflow.,CASSANDRA-2158,12498478,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,trisk,trisk,13/Feb/11 14:54,16/Apr/19 17:33,22/Mar/23 14:57,19/Feb/11 06:10,0.7.4,,,,0,,,,,,"If memtable_throughput_in_mb is set past 2.2 gigs, no errors are thrown.  However, as soon as data starts being written it is almost immediately being flushed.  Several hundred SSTables are created in minutes.  I am almost positive that the problem is that when memtable_throughput_in_mb is being converted into bytes the result is stored in an integer, which is overflowing.

From memtable.java:

    private final int THRESHOLD;
    private final int THRESHOLD_COUNT;

...
this.THRESHOLD = cfs.getMemtableThroughputInMB() * 1024 * 1024;
this.THRESHOLD_COUNT = (int) (cfs.getMemtableOperationsInMillions() * 1024 * 1024);


NOTE:
I also think currentThroughput also needs to be changed from an int to a long.  I'm not sure if it is as simple as this or if this also is used in other places.",,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,"15/Feb/11 06:12;jbellis;2158.txt;https://issues.apache.org/jira/secure/attachment/12471026/2158.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20471,,,Sat Mar 12 00:23:38 UTC 2011,,,,,,,,,,"0|i0g9o7:",92997,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"13/Feb/11 22:33;jbellis;While you are correct, you almost certainly shouldn't have throughput set that high, because if you are tuning things correctly you will hit your operations count limit first for 99.9% of workloads.;;;","14/Feb/11 01:46;trisk;I think I have a usecase where larger memtables would help a lot.  I have a combination of fat columns that can update frequently, and I have lots of memory (currently 96 gb).  I know I could also handle this by putting more boxes in the cluster, but I think I can get a lot more out of the boxes I have.  I am experimenting with breaking up my cf into multiple ones to get the same effect as the bigger sstable.  So far it seems to perform well, but feels hacky.

Even if you decide to have a hard limit on the memtable size, it should probably fail loudly instead of generating hundreds of sstables.  With my understanding of the current defaults, any default install of cassandra with more than 32 gb of memory will default to this state and will be hard for new users to understand (32/2 -> 16 gig heap | 16/8 -> 2gb default CF memtable throughput).  I would much prefer the option than the hard limit though :).;;;","15/Feb/11 06:12;jbellis;patch to make ints into longs and validate input;;;","17/Feb/11 02:16;brandon.williams;+1;;;","19/Feb/11 06:10;jbellis;committed;;;","19/Feb/11 06:51;hudson;Integrated in Cassandra-0.7 #296 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/296/])
    update memtable_throughput to be a long
patch by jbellis; reviewed by brandonwilliams for CASSANDRA-2158
;;;","12/Mar/11 08:11;jbellis;Erik Forkalsrud commented on the mailing list,

{noformat}
It looks like the fix isn't entirely correct.  The bug is still in 0.7.3.   In Memtable.java, the line:
  THRESHOLD = cfs.getMemtableThroughputInMB() * 1024 * 1024;

should be changed to:
  THRESHOLD = cfs.getMemtableThroughputInMB() * 1024L * 1024L;

Here's some code that illustrates the difference:

   public void testMultiplication() {
       int memtableThroughputInMB = 2300;
       long thresholdA = memtableThroughputInMB * 1024 * 1024;
       long thresholdB = memtableThroughputInMB * 1024L * 1024L;
       System.out.println(""a="" + thresholdA + "" b="" + thresholdB);
   }
{noformat}

Made this change for 0.7.4;;;","12/Mar/11 08:23;hudson;Integrated in Cassandra-0.7 #376 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/376/])
    fix memtable thresholds better
patch by Erik Foralsrud; reviewed by jbellis for CASSANDRA-2158
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLI should loop on describe_schema until agreement or fatel exit with stacktrace/message if no agreement after X seconds,CASSANDRA-2044,12496600,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,mdennis,mdennis,25/Jan/11 04:36,16/Apr/19 17:33,22/Mar/23 14:57,26/Jan/11 03:57,0.7.1,,,,0,,,,,,"see CASSANDRA-2026 for brief background.

It's easy to enter statements into the CLI before the schema has settled, often causing problems where it is no longer possible to get the nodes in agreement about the schema without removing the system directory.

The alleviate the most common problems with this, the CLI should issue the modification statement and loop on describe_schema until all nodes agree or until X seconds has passed.  If the timeout has been exceeded, the CLI should exit with an error and inform the user that the schema has not settled and further migrations are ill-advised until it does.

number_of_nodes/2+1 seconds seems like a decent wait time for schema migrations to start with.

Bonus points for making the value configurable.",,,,,,,,,,,,,,,,,,,,,,";26/Jan/11 04:14;xedin;7200",14400,0,7200,50%,14400,0,7200,,,,,,,,,,,,,,,,,,,"25/Jan/11 09:01;xedin;CASSANDRA-2044.patch;https://issues.apache.org/jira/secure/attachment/12469224/CASSANDRA-2044.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20411,,,Tue Jan 25 20:14:33 UTC 2011,,,,,,,,,,"0|i0g8zb:",92885,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"25/Jan/11 09:01;xedin;I have added option ""--schema-mwt"" to set wait time for schema migration in seconds from command line, if it was not set system will use node_count/2 + 1 secs or 5 secs if couldn't connect to JMX.

CLI checks node agreement on every keyspace create/drop and if nodes were not able to agree in given wait time system will print error message ""The schema has not settled in %d seconds and further migrations are ill-advised until it does."" and exit with -1 code.;;;","26/Jan/11 03:57;jbellis;committed with a couple changes:

- default wait changed to flat 10s; schema propagation does not use gossip and is not expected to be proportional to number of nodes in cluster
- added validation to keyspace update and cf create/update/drop;;;","26/Jan/11 04:14;hudson;Integrated in Cassandra-0.7 #209 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/209/])
    CLI attemptsto block for new schemato propagate
patch by Pavel Yaskevich; reviewed by jbellis for CASSANDRA-2044
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hinted Handoff Exception,CASSANDRA-634,12443423,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jaakko,lenn0x,lenn0x,16/Dec/09 01:36,16/Apr/19 17:33,22/Mar/23 14:57,24/Dec/09 03:44,0.5,,,,0,,,,,,"Updated to the latest codebase from cassandra-0.5 branch. All nodes booted up fine and then I start noticing this error:

ERROR [HINTED-HANDOFF-POOL:1] 2009-12-14 22:05:34,191 CassandraDaemon.java (line 71) Fatal exception in thread Thread[HINTED-HANDOFF-POOL:1,5,main]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.db.HintedHandOffManager$1.run(HintedHandOffManager.java:253)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.gms.FailureDetector.isAlive(FailureDetector.java:146)
        at org.apache.cassandra.db.HintedHandOffManager.sendMessage(HintedHandOffManager.java:106)
        at org.apache.cassandra.db.HintedHandOffManager.deliverAllHints(HintedHandOffManager.java:177)
        at org.apache.cassandra.db.HintedHandOffManager.access$000(HintedHandOffManager.java:75)
        at org.apache.cassandra.db.HintedHandOffManager$1.run(HintedHandOffManager.java:249)
        ... 3 more",,hammer,rrabah,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Dec/09 20:00;jaakko;634-1st-part-gossip-about-all-nodes.patch;https://issues.apache.org/jira/secure/attachment/12428285/634-1st-part-gossip-about-all-nodes.patch","22/Dec/09 00:47;jbellis;634-discard-obsolete.patch;https://issues.apache.org/jira/secure/attachment/12428637/634-discard-obsolete.patch",,,,,,,,,,,,,2.0,jaakko,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19791,,,Wed Dec 23 19:44:13 UTC 2009,,,,,,,,,,"0|i0g053:",91453,,,,,Normal,,,,,,,,,,,,,,,,,"16/Dec/09 01:37;lenn0x;I'm wondering if this might be due to the fact that one of the node's in the cluster is currently down. This might offer a clue.;;;","16/Dec/09 22:34;jaakko;Very strange one.

Line 146 in FailureDetector is this:

EndPointState epState = Gossiper.instance().getEndPointStateForEndPoint(ep);

According to the stacktrace it does not even reach getEndPointStateForEndPoint, and even if it did, ep comes from InetAddress.getByAddress, so it cannot be null. Also, if the culprit was Gossiper constructor, that should also show in the trace.

This would mean that Gossiper.instance() returns null, but I don't know how that can happen. ""volatile"" in gossiper_ is actually not needed, but I don't know if having it there could cause such thing. ""volatile"" was added here pretty recently, so it just might be one possible explanation why this came up now.

BTW what version of java you're using?
;;;","16/Dec/09 22:42;jbellis;No, 146 in the 0.5 branch is the next line:

        return epState.isAlive();
;;;","16/Dec/09 23:03;jaakko;Oops, sorry. I need a bigger font :-(

(but still ""volatile"" is not needed there :-)

OK, could it be like this: There are nodes A and B in the cluster, with replication factor 2. Node B goes down and node C is introduced as a new node after this. Now A knows there are A, B and C in the cluster, but C only knows about A. Suppose at this time client sends a write request to A, which falls into A's range (and replica to B's range). B is offline, so instead a hinted write will go to C. Problem is, C will try to deliver this hint later to B, but its Gossiper has never heard of B, so endpointstate will be null.

If this is the cause, then a simple fix

if (epState == null)
    return false;

before line 146 should do the trick.
;;;","16/Dec/09 23:07;jbellis;Are you sure we don't gossip state for down nodes to new cluster members?  If we don't, the token ranges will be wrong, which is a bigger problem.;;;","16/Dec/09 23:24;jaakko;Gossip SYN only includes digests for live endpoints. 

You're right, this would indeed cause wrong ranges on new nodes that have not seen the dead node when it was still alive. Write would still go to the right node, but it would lack HINT flag.

Simply gossiping state about all nodes (dead or alive) would solve this problem, but have to have another look tomorrow morning if it would cause any side effects with failuredetector on the new node (might momentarily consider the dead node to be alive, although this would probably not be a problem at all).
;;;","17/Dec/09 20:00;jaakko;The way node's alive/dead status gets determined currently is as follows:

1st time there is gossip about certain node:
- add new node to endpoint state and mark it alive (onAlive will be called)
- call onJoin

2nd and subsequent gossip about this node
- notify failuredetector whenever there is gossip about this endpoint -> failuredetector starts to monitor this node and set node's status dead if needed (it will not set it to alive)
- node is marked alive whenever there is gossip about it

The important things here are: (1) node is assumed to be alive when 1st info about it arrives and (2) failuredetector does not know anything about the node before 2nd gossip. That means we cannot simply start gossiping info about dead nodes, as their status would remain ""alive"" forever (that is, until the dead node comes online and activates failure detector)

Proposed fix (patch attached):

1st time gossip:
- add new node to endpoint state, but set its status as dead
- call onJoin (token metadata will be updated)

2nd and subsequent gossips:
- Unchanged. This 2nd gossip will trigger markAlive (and call onAlive) and activate failuredetector -> normal situation

In short: assume node to be dead unless otherwise proven by subsequent gossip. If the node is alive, it will be marked so within seconds. If it is dead, we have knowledge about its existence, but we consider it (correctly) to be dead.

There is a possibility of false ""alive"" interpretation, though: Cluster has nodes A, B and C. Suppose C has just gossiped to B and dies. At this time C's status in A is different (older) than in B. Now suppose at this instant node D enters the cluster and first gossips with A. In this case D will get the old gossip and only later the new one. This second newer gossip will cause C to be marked alive even though it is already dead. However, since second gossip will also activate failure detector, C will be correctly marked as dead in a few seconds, so this is probably OK (and anyway a very rare occurence).

Two open issues:
- Now that we're gossiping about dead nodes as well, gossip digest continues to grow without boundary when nodes come and go. This information will never disappear as it will be propagated to new nodes no matter how old and obsolete it is. To counter this, we need some mechanism to (1) either remove dead node from endpointstateinfo or (2) at some point stop to gossip about it, or both.

For (1): when we get removetoken command, it is probably safe to remove the endpoint immediately (STATE_LEFT is broadcasted by different endpoint, so info about token removal will remain in the gossiper). Another thing we could do is to keep track of nodes that have left. If nothing is heard about it for some time, we could assume that it is gone for good and remove it from gossiper after giving its STATE_LEFT enough time to spread.

For (2): We could gossip info only about nodes in either liveEndpoints or unreachableEndpoints (as opposed to endPointStateMap). Nodes are removed from unreachableEndpoints after three days of silence, so this would discard old information from the gossiper. Side effect would of course be that a node that is down more than three days but comes back later, might miss some of its data (new nodes that booted after the three day period would know nothing about this node).

(Attached patch should work as such, but does not take into account these last two issues)
;;;","19/Dec/09 00:19;jbellis;committed to 0.5 and trunk.

Created CASSANDRA-644 for the gossip membership problem.;;;","20/Dec/09 16:23;lenn0x;Does this patch address the NullException? I tried latest from branch 05 and am still seeing this exception.;;;","21/Dec/09 17:53;jaakko;It was supposed to solve it, but obviously it did not fully do so.

Problem in your case might be because hinted handoff data is persistent and gossiper data is not. Suppose there are nodes A and B. Suppose B goes down and A stores hinted data for it. Later A is restarted -> A still has hinted data for B, but after restart its gossiper knows nothing about B. It does not help even if we gossip about dead nodes, as nobody has ever heard of B. If B is gone forever, A can never get rid of hinted data.

Don't know what would be the best thing to do here. removetoken command could make efforts to redirect hints to new destination in case a hinted target is removed. However, if the endpoint has been lost from gossip/tokenmetadata, then there is nothing it can do as it does not know who the endpoint was. Another option would be to add manual command to redirect hinted data.

Other options?
;;;","22/Dec/09 00:06;jbellis;IMO we should just drop the HH data with a warning (in case ops does intend to bring the dead node back).  In almost all cases, if a node is down that long it is going to be replaced entirely, and the replacement will bootstrap and not need the old HH data.

Since you should repair after bringing a dead node online anyway (b/c there is a window before the FD is aware that we should start doing HH), HH is just an optimization and this is OK.;;;","22/Dec/09 00:47;jbellis;discard hints for nodes that are no longer part of the gossip network;;;","22/Dec/09 08:49;jaakko;If dropping HH data is OK, then this patch looks good to me.

One small thing: After a node starts, it will take some time before it knows about all cluster nodes. During this time if there is a request to deliver hints (not possible?), HH data will be discarded just because the node does not know about the other endpoint yet.
;;;","22/Dec/09 10:25;jbellis;hint delivery is attempted when a node is signaled to be up, or every 1h (starting after a 1h delay), so this seems OK.;;;","23/Dec/09 13:55;lenn0x;can you rebase this on 05? i am fixing it manually, strange it complained you just added a comment to INTERVAL_IN_MS.;;;","24/Dec/09 03:25;lenn0x;+1 no more exceptions and I am seeing HH being disregarded for the node we removed now.;;;","24/Dec/09 03:44;jbellis;committed to 0.5 and trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
more missing svn properties,CASSANDRA-429,12435016,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,urandom,urandom,urandom,07/Sep/09 23:27,16/Apr/19 17:33,22/Mar/23 14:57,08/Sep/09 03:32,0.4,0.5,,,0,,,,,,"Per http://www.mail-archive.com/general@incubator.apache.org/msg22473.html

svn ps svn:eol-style CRLF bin/cassandra.bat
svn ps svn:mime-type application/octet-stream lib/libthrift-r808609.jar
svn ps svn:mime-type application/octet-stream lib/slf4j-api-1.5.8.jar
svn ps svn:mime-type application/octet-stream lib/slf4j-log4j12-1.5.8.jar
svn ps svn:eol-style native src/java/org/apache/cassandra/client/RingCache.java
svn ps svn:eol-style native
test/unit/org/apache/cassandra/client/TestRingCache.java
svn ps svn:eol-style native
test/unit/org/apache/cassandra/db/marshal/BytesTypeTest.java
svn ps svn:eol-style native
test/unit/org/apache/cassandra/dht/CollatingOrderPreservingPartitionerTest.java
svn ps svn:eol-style native
test/unit/org/apache/cassandra/dht/PartitionerTestCase.java

Likewise, I think this means updating http://wiki.apache.org/cassandra/HowToContribute to include configuring in auto-props:

*.jar = svn:mime-type application/octet-stream
*.bar = svn ps svn:eol-style CRLF bin/cassandra.bat


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19685,,,Mon Sep 07 19:32:07 UTC 2009,,,,,,,,,,"0|i0fyvj:",91248,,,,,Low,,,,,,,,,,,,,,,,,"08/Sep/09 01:15;jbellis;my fault, I was missing enable-auto-props :-|;;;","08/Sep/09 03:32;urandom;SVN properties and wiki documentation updated.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed to delete commitlog when restarting service,CASSANDRA-1348,12470702,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,vjevdokimov,vjevdokimov,03/Aug/10 16:42,16/Apr/19 17:33,22/Mar/23 14:57,08/Sep/10 00:25,0.6.6,0.7 beta 2,,,0,,,,,,"When restarting any Cassandra node we've got exception:

ERROR 09:42:27,869 Exception encountered during startup.
java.io.IOException: Failed to delete C:\cassandra\data\commitlog\CommitLog-1280817512228.log
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:45)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:177)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:120)
	at org.apache.cassandra.service.AbstractCassandraDaemon.init(AbstractCassandraDaemon.java:57)
	at org.apache.cassandra.contrib.windows.service.WindowsService.start(Unknown Source)
	at org.apache.cassandra.contrib.windows.service.WindowsService.main(Unknown Source)
 INFO 09:42:27,869 Exception encountered during startup.
 INFO 09:42:27,869 Cassandra Service Finished: Tue Aug 03 09:42:27 EEST 2010

Aftrer exception was thrown and Cassandra didn't started, within commitlog directory there's only one file: CommitLog-1280817512228.log
and no CommitLog-1280817512228.log.header. Looks like CommitLog-1280817512228.log was a new file, not the last one after stopping service.

After few restarts .log file is deleted and Cassandra is working fine until next restart.",Windows Server 2008 R2 64bit,vjevdokimov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Sep/10 23:24;jbellis;1348-2.txt;https://issues.apache.org/jira/secure/attachment/12454023/1348-2.txt","03/Aug/10 23:15;jbellis;1348.txt;https://issues.apache.org/jira/secure/attachment/12451125/1348.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20094,,,Tue Sep 07 16:25:49 UTC 2010,,,,,,,,,,"0|i0g4i7:",92160,,gdusbabek,,gdusbabek,Low,,,,,,,,,,,,,,,,,"03/Aug/10 23:16;jbellis;windows gets pissy when you try to delete an open file.  this patch closes it before the delete.;;;","03/Aug/10 23:38;gdusbabek;+1;;;","03/Aug/10 23:48;jbellis;committed;;;","04/Aug/10 21:25;hudson;Integrated in Cassandra #509 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/509/])
    close commitlog reader before deleting it
patch by jbellis; reviewed by gdusbabek for CASSANDRA-1348
;;;","03/Sep/10 16:49;vjevdokimov;Still could happen with 0.7 beta 1. Not always, but happens.;;;","03/Sep/10 22:18;jbellis;new stacktrace?;;;","04/Sep/10 15:58;vjevdokimov; INFO 13:38:04,311 Received start command
 INFO 13:38:04,311 Cassandra Service Starting: Fri Sep 03 13:38:04 CEST 2010
 INFO 13:38:04,467 DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
 INFO 13:38:04,592 Sampling index for D:\cassandra\data\data\system\Statistics-e-501-<>
 INFO 13:38:04,639 Sampling index for D:\cassandra\data\data\system\Statistics-e-502-<>
 INFO 13:38:04,670 Sampling index for D:\cassandra\data\data\system\Schema-e-1-<>
 INFO 13:38:04,670 Sampling index for D:\cassandra\data\data\system\Schema-e-2-<>
 INFO 13:38:04,670 Sampling index for D:\cassandra\data\data\system\Schema-e-3-<>
 INFO 13:38:04,701 Sampling index for D:\cassandra\data\data\system\Migrations-e-1-<>
 INFO 13:38:04,701 Sampling index for D:\cassandra\data\data\system\Migrations-e-2-<>
 INFO 13:38:04,701 Sampling index for D:\cassandra\data\data\system\Migrations-e-3-<>
 INFO 13:38:04,717 Sampling index for D:\cassandra\data\data\system\LocationInfo-e-9-<>
 INFO 13:38:04,717 Sampling index for D:\cassandra\data\data\system\LocationInfo-e-10-<>
 INFO 13:38:04,717 Sampling index for D:\cassandra\data\data\system\LocationInfo-e-11-<>
 INFO 13:38:04,717 Sampling index for D:\cassandra\data\data\system\HintsColumnFamily-e-24-<>
 INFO 13:38:04,732 Sampling index for D:\cassandra\data\data\system\HintsColumnFamily-e-25-<>
 INFO 13:38:04,732 Sampling index for D:\cassandra\data\data\system\HintsColumnFamily-e-26-<>
 INFO 13:38:04,732 Sampling index for D:\cassandra\data\data\system\HintsColumnFamily-e-28-<>
 INFO 13:38:04,732 Sampling index for D:\cassandra\data\data\system\HintsColumnFamily-e-30-<>
 INFO 13:38:04,732 Sampling index for D:\cassandra\data\data\system\HintsColumnFamily-e-33-<>
 INFO 13:38:04,763 Loading schema version 6e908998-aa17-11df-bf55-cacedb26d926
 INFO 13:38:05,013 Sampling index for D:\cassandra\data\data\BehavioralTargeting\CookieTrackingSetupMemberships-e-25-<>
 INFO 13:38:07,588 Sampling index for D:\cassandra\data\data\BehavioralTargeting\CookieTrackingSetupMemberships-e-150-<>
 INFO 13:38:10,837 Sampling index for D:\cassandra\data\data\BehavioralTargeting\CookieTrackingSetupMemberships-e-279-<>
 INFO 13:38:14,550 Sampling index for D:\cassandra\data\data\BehavioralTargeting\CookieTrackingSetupMemberships-e-412-<>
 INFO 13:38:17,747 Sampling index for D:\cassandra\data\data\BehavioralTargeting\CookieTrackingSetupMemberships-e-461-<>
 INFO 13:38:18,948 Creating new commitlog segment E:/cassandra/data/commitlog\CommitLog-1283513898948.log
 INFO 13:38:18,964 Deleted D:\cassandra\data\data\BehavioralTargeting\CookieTrackingSetupMemberships-e-470-Data.db
 INFO 13:38:18,964 Deleted D:\cassandra\data\data\BehavioralTargeting\CookieTrackingSetupMemberships-e-479-Data.db
 INFO 13:38:18,979 Deleted D:\cassandra\data\data\BehavioralTargeting\CookieTrackingSetupMemberships-e-492-Data.db
 INFO 13:38:18,979 Deleted D:\cassandra\data\data\BehavioralTargeting\CookieTrackingSetupMemberships-e-497-Data.db
 INFO 13:38:18,979 Sampling index for D:\cassandra\data\data\BehavioralTargeting\CookieTrackingSetupMemberships-e-498-<>
 INFO 13:38:20,118 Replaying E:\cassandra\data\commitlog\CommitLog-1283513833929.log, E:\cassandra\data\commitlog\CommitLog-1283513898948.log
 INFO 13:38:20,134 E:\cassandra\data\commitlog\CommitLog-1283513833929.log.header incomplete, missing or corrupt.  Everything is ok, don't panic.  CommitLog will be replayed from the beginning
 INFO 13:38:20,150 Finished reading E:\cassandra\data\commitlog\CommitLog-1283513833929.log
 INFO 13:38:20,150 Finished reading E:\cassandra\data\commitlog\CommitLog-1283513898948.log
 INFO 13:38:20,150 switching in a fresh Memtable for Statistics at CommitLogContext(file='E:/cassandra/data/commitlog\CommitLog-1283513898948.log', position=592)
 INFO 13:38:20,165 Enqueuing flush of Memtable-Statistics@1582759704(15004 bytes, 4 operations)
 INFO 13:38:20,165 Writing Memtable-Statistics@1582759704(15004 bytes, 4 operations)
 INFO 13:38:20,508 Completed flushing D:\cassandra\data\data\system\Statistics-e-503-Data.db
 INFO 13:38:20,508 Recovery complete
ERROR 13:38:20,508 Exception encountered during startup.
java.io.IOException: Failed to delete E:\cassandra\data\commitlog\CommitLog-1283513898948.log
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:45)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:178)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:120)
	at org.apache.cassandra.service.AbstractCassandraDaemon.init(AbstractCassandraDaemon.java:57)
	at org.apache.cassandra.contrib.windows.service.WindowsService.start(Unknown Source)
	at org.apache.cassandra.contrib.windows.service.WindowsService.main(Unknown Source)
 INFO 13:38:20,555 Exception encountered during startup.
 INFO 13:38:20,555 Cassandra Service Finished: Fri Sep 03 13:38:20 CEST 2010
;;;","07/Sep/10 23:24;jbellis;it logs ""Finished reading"" which means it's successfully closed the reader, so there's nothing in Cassandra blocking the delete.  patch -2 makes failure to delete a non-fatal error.;;;","07/Sep/10 23:27;jbellis;backported try/finally from original patch to 0.6.  (failure to delete is already non-fatal there.);;;","07/Sep/10 23:49;gdusbabek;+1 on the new patch.;;;","08/Sep/10 00:25;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Corrupt SSTable,CASSANDRA-452,12436357,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,sammy.yu,sammy.yu,23/Sep/09 06:06,16/Apr/19 17:33,22/Mar/23 14:57,10/Nov/09 13:03,0.5,,,,0,,,,,,"We noticed on one of our node the number of SStables is growing.  The compaction thread is alive and running.  We can see that it is constantly trying to compact the same set of sstables.  However, it is failing because one of the sstable is corrupt:

ERROR [ROW-READ-STAGE:475] 2009-09-21 00:29:17,068 DebuggableThreadPoolExecutor.java (line 125) Error in ThreadPoolExecutor
java.lang.RuntimeException: java.io.EOFException
        at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:110)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:44)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.EOFException
        at java.io.RandomAccessFile.readFully(RandomAccessFile.java:383)
        at java.io.RandomAccessFile.readFully(RandomAccessFile.java:361)
        at org.apache.cassandra.utils.FBUtilities.readByteArray(FBUtilities.java:390)
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:64)
        at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:349)
        at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:309)
        at org.apache.cassandra.db.filter.SSTableNamesIterator.<init>(SSTableNamesIterator.java:102)
        at org.apache.cassandra.db.filter.NamesQueryFilter.getSSTableColumnIterator(NamesQueryFilter.java:69)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1467)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1401)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1420)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1401)
        at org.apache.cassandra.db.Table.getRow(Table.java:589)
        at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:65)
        at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:78)
        ... 4 more","Pre 0.4 based on r805615 on trunk w/ #370, #392, #394, #405, #406, #418",hammer,johanoskarsson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Sep/09 07:15;sammy.yu;FriendActions-17120.tar.gz;https://issues.apache.org/jira/secure/attachment/12420322/FriendActions-17120.tar.gz","23/Sep/09 06:54;sammy.yu;FriendActions-17122-Data.db;https://issues.apache.org/jira/secure/attachment/12420317/FriendActions-17122-Data.db","23/Sep/09 06:54;sammy.yu;FriendActions-17122-Filter.db;https://issues.apache.org/jira/secure/attachment/12420318/FriendActions-17122-Filter.db","23/Sep/09 06:54;sammy.yu;FriendActions-17122-Index.db;https://issues.apache.org/jira/secure/attachment/12420319/FriendActions-17122-Index.db","14/Oct/09 09:32;sammy.yu;FriendActions-22478.tar.gz;https://issues.apache.org/jira/secure/attachment/12422052/FriendActions-22478.tar.gz","23/Sep/09 12:01;jbellis;keys.txt;https://issues.apache.org/jira/secure/attachment/12420346/keys.txt",,,,,,,,,6.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19694,,,Tue Nov 10 04:21:33 UTC 2009,,,,,,,,,,"0|i0fz0v:",91272,,,,,Normal,,,,,,,,,,,,,,,,,"23/Sep/09 06:54;sammy.yu;corrupt sstable
;;;","23/Sep/09 07:15;sammy.yu;good sstable;;;","23/Sep/09 10:12;jbellis;What is the definition for this CF?;;;","23/Sep/09 10:14;lenn0x;<ColumnFamily ColumnType=""Super"" CompareWith=""BytesType"" CompareSubcolumnsWith=""BytesType"" Name=""FriendActions""/>;;;","23/Sep/09 11:55;jbellis;17120 is corrupt, all right.

The local deletion time is 0, which is nonsense (it's generated from System.CurrentTimeMillis) and the timestamp associated with that delete doesn't look like your other timestamps.  After that it's clearly reading nonsense and eventually EOFs while trying to read 19988495 columns.  Some debug output from my local code:

DEBUG - key is 148447622005950731053602871503233733033:itemdiggs15891086
...
DEBUG - reading name of length 7                                                                                     
DEBUG - deserializing SC ironeus deleted @0/21025451015143424; reading 19988495 columns                              
DEBUG - deserializing subcolumn 0                                                                                    
DEBUG - reading name of length 27237                                                                                 
DEBUG - deserializing rryjamesstoneJ�� ...

I can't see how the compaction code could cause this kind of corruption.  (Your logs should show: is this even the product of a compaction?  Or is it a direct result of a memtable or BMt?)

If it is a product of compaction, do you have a snapshot of that sstable any time prior to that compaction?  Can you reproduce the bug compacting those files?

I hate to blame hardware but there are a couple things that indicate this might actually be caused by that.  First, a localDeletionTime of zero is exactly 1 bit away from Integer.MIN_VALUE in 2's complement.  All the other localDT values are Integer.MIN_VALUE as would be expected if no deletes are done.  Second, no bytes are being skipped (which is often how you see some expected small number of columns get huge) -- the row sizes are correct and all the keys are readable where they should be.

I will attach a list of keys in this sstable so you can force read repair on them.  Tomorrow I will patch compaction to be able to recover from this error, and if Sammy or Chris can do CASSANDRA-426 then we will be able to reproduce any such future errors (assuming they are compaction related, rather than memtable/BMt).;;;","23/Sep/09 12:01;jbellis;keys from corrupt sstable file;;;","24/Sep/09 02:09;sammy.yu;We determined last night that 17120 is a result of compaction.  I'm also working on CASSANDRA-453 which will help us validate offline the integrity of sstables.

;;;","14/Oct/09 09:32;sammy.yu;Another corrupt sstable:
Digg/FriendActions-22478-Data.db: could not be fully read last key=106990576908512342565493723105254981184:itemdiggs15991619 at position 1039957;;;","14/Oct/09 19:52;jbellis;Is this still your almost-0.4 code, or trunk?;;;","14/Oct/09 20:26;sammy.yu;This is still almost 0.4.
;;;","15/Oct/09 00:04;jbellis;I don't think I can do a whole lot until someone runs this with SnapshotBeforeCompaction (from CASSANDRA-426) turned on, and gives me a ""compacting this set of sstables produces this corrupt row"" case (or less likely, proves that a row was already corrupt when it was first flushed from a memtable).  0.4.1 and trunk both include this patch now;;;","10/Nov/09 03:05;jbellis;haven't heard anything on this in a while -- does it look like the new compaction code in 0.5 fixes it then?;;;","10/Nov/09 12:21;lenn0x;This is safe to close for now, we haven't run into a corruption issue yet.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition during decommission,CASSANDRA-2072,12497003,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,28/Jan/11 08:06,16/Apr/19 17:33,22/Mar/23 14:57,04/Feb/11 04:17,0.7.1,,,,0,,,,,,"Occasionally when decommissioning a node, there is a race condition that occurs where another node will never remove the token and thus propagate it again with a state of down.  With CASSANDRA-1900 we can solve this, but it shouldn't occur in the first place.

Given nodes A, B, and C, if you decommission B it will stream to A and C.  When complete, B will decommission and receive this stacktrace:

ERROR 00:02:40,282 Fatal exception in thread Thread[Thread-5,5,main]
java.util.concurrent.RejectedExecutionException: ThreadPoolExecutor has shut down
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1.rejectedExecution(DebuggableThreadPoolExecutor.java:62)
        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:767)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:658)
        at org.apache.cassandra.net.MessagingService.receive(MessagingService.java:387)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:91

At this point A will show it is removing B's token, but C will not and instead its failure detector will report that B is dead, and nodetool ring on C shows B in a leaving/down state.  In another gossip round, C will propagate this state back to A.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Jan/11 04:28;brandon.williams;0001-announce-having-left-the-ring-for-RING_DELAY-on-deco.patch;https://issues.apache.org/jira/secure/attachment/12469696/0001-announce-having-left-the-ring-for-RING_DELAY-on-deco.patch","29/Jan/11 04:28;brandon.williams;0002-Improve-TRACE-logging-for-Gossiper.patch;https://issues.apache.org/jira/secure/attachment/12469697/0002-Improve-TRACE-logging-for-Gossiper.patch","01/Feb/11 03:44;brandon.williams;0003-Remove-endpoint-state-when-expiring-justRemovedEndpo.patch;https://issues.apache.org/jira/secure/attachment/12469851/0003-Remove-endpoint-state-when-expiring-justRemovedEndpo.patch",,,,,,,,,,,,3.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20427,,,Thu Feb 03 20:47:49 UTC 2011,,,,,,,,,,"0|i0g95j:",92913,,gdusbabek,,gdusbabek,Low,,,,,,,,,,,,,,,,,"29/Jan/11 04:28;brandon.williams;Here is what is happening:

B sends LEFT to C, C calls removeEndpoint and drops the endpoint state.  B never gets to send to A (because it only waits 2s to announce, which can be just one round) and A still thinks it's LEAVING.  C sees B in a gossip digest from A, and  not knowing anything about it, calls requestAll, but A refuses to tell C anything about it because A has B in justRemovedEndpoints.  Eventually, QUARANTINE_DELAY expires and A unhelpfully propagates the LEAVING state back to C.

The obvious solution here is that B should announce LEFT for RING_DELAY, simply because it's the right thing to do as opposed to a one-off delay of 2 seconds.

However, this exposes a more subtle problem.  When removeEndpoint is called, we drop the state right away and track the endpoint in justRemovedEndpoints.  Instead, we should hold on to the state so it is still propagated in further gossip digests, and expire it when we expire justRemovedEndpoints.

Either of these changes is technically enough to solve this issue, but both together add an extra safeguard.  Changing where we expire the endpoint state is the more impacting of the two, however the gossip generation and version checks always prevent any negative consequences from doing this.;;;","04/Feb/11 02:15;gdusbabek;+1;;;","04/Feb/11 04:17;brandon.williams;Committed.;;;","04/Feb/11 04:47;hudson;Integrated in Cassandra-0.7 #244 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/244/])
    Fix race condition during decommission by announcing for RING_DELAY and
not removing endpoint state until removing the ep from
justRemovedEndpoints.
Patch by brandonwilliams, reviewed by gdusbabek for CASSANDRA-2072
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
array index out of bounds on compact & repair,CASSANDRA-2086,12497284,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Duplicate,,jdamick,jdamick,01/Feb/11 12:26,16/Apr/19 17:33,22/Mar/23 14:57,01/Feb/11 12:50,,,,,0,,,,,,"
We're seeing array index out of bounds exceptions (below) on 0.7.0 when running compact.
 
The repair seems to hang indefinitely on all nodes (also throws index oob).


On 1 node in our cluster (running compact):

 INFO [CompactionExecutor:1] 2011-01-31 20:07:12,140 CompactionManager.java (line 272) Compacting [org.apache.cassandra.io.sstable.SSTableReader(path='/var/lib/cassandra/data/xxxxxxxx/XXXXXXX-e-318-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='/var/lib/cassandra/data/xxxxxxxxxxx/xxxxxxxxxxx-e-317-Data.db')]
ERROR [CompactionExecutor:1] 2011-01-31 20:07:12,295 AbstractCassandraDaemon.java (line 91) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.ArrayIndexOutOfBoundsException: 7
    at org.apache.cassandra.db.marshal.TimeUUIDType.compareTimestampBytes(TimeUUIDType.java:58)
    at org.apache.cassandra.db.marshal.TimeUUIDType.compare(TimeUUIDType.java:45)
    at org.apache.cassandra.db.marshal.TimeUUIDType.compare(TimeUUIDType.java:29)
    at java.util.concurrent.ConcurrentSkipListMap$ComparableUsingComparator.compareTo(Unknown Source)
    at java.util.concurrent.ConcurrentSkipListMap.doPut(Unknown Source)
    at java.util.concurrent.ConcurrentSkipListMap.putIfAbsent(Unknown Source)



And another node (running compact):

 INFO [StreamStage:1] 2011-01-31 20:03:48,663 StreamOutSession.java (line 174) Streaming to /xxx.xxx.xxx.xxx
ERROR [CompactionExecutor:1] 2011-01-31 20:03:52,587 AbstractCassandraDaemon.java (line 91) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.ArrayIndexOutOfBoundsException
ERROR [CompactionExecutor:1] 2011-01-31 20:03:54,216 AbstractCassandraDaemon.java (line 91) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.ArrayIndexOutOfBoundsException: 6
        at org.apache.cassandra.db.marshal.TimeUUIDType.compareTimestampBytes(TimeUUIDType.java:56)
        at org.apache.cassandra.db.marshal.TimeUUIDType.compare(TimeUUIDType.java:45)
        at org.apache.cassandra.db.marshal.TimeUUIDType.compare(TimeUUIDType.java:29)
        at java.util.concurrent.ConcurrentSkipListMap$ComparableUsingComparator.compareTo(Unknown Source)
        at java.util.concurrent.ConcurrentSkipListMap.doPut(Unknown Source)
        at java.util.concurrent.ConcurrentSkipListMap.putIfAbsent(Unknown Source)
        at org.apache.cassandra.db.ColumnFamily.addColumn(ColumnFamily.java:218)


Is this related to: CASSANDRA-1959 or CASSANDRA-1992?

This has left some of my data in an unrecoverable & inaccessible state - how can i repair this situation? 

",,brandon.williams,jdamick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20436,,,Tue Feb 01 15:50:18 UTC 2011,,,,,,,,,,"0|i0g987:",92925,,,,,Critical,,,,,,,,,,,,,,,,,"01/Feb/11 12:50;jbellis;this is CASSANDRA-1992.;;;","01/Feb/11 12:54;jdamick;but is there is any way to repair the problem without deleting all of my data? ;;;","01/Feb/11 23:50;brandon.williams;If the node has never successfully compacted after streaming, a simple reboot will work.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.net.SocketException: Invalid argument / java.net.NoRouteToHostException: Network is unreachable,CASSANDRA-628,12443144,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,urandom,urandom,urandom,12/Dec/09 04:55,16/Apr/19 17:33,22/Mar/23 14:57,30/Oct/10 05:40,0.7.0 rc 1,,,,1,,,,,,"This manifests as either a SocketException that occurs when starting a cassandra node, or a NoRouteToHostException which occurs when connecting with a client.

On Linux systems this is caused by IPV6_V6ONLY being set true. The docs (ipv6(7)) say that when set this causes sockets to be created IPv6 only, while the previous behavior also allowed sending and receiving packets using an  IPv4-mapped IPv6 address.

The quick fix is to either launch applications using the -Djava.net.preferIPv4Stack=true property, or on Linux systems set net.ipv6.bindv6only=0 (see sysctl(8)).

My limited understanding is that the previous behavior (IPV6_V6ONLY=0) was always considered a hack to be used until IPv6 was more mature/had gained traction and that a change in defaults was always inevitable, so in the long-term a Real Fix will be needed.

http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6342561
http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=560056
","Linux, FreeBSD, (possibly others)",tzz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19787,,,Sat Oct 30 12:48:33 UTC 2010,,,,,,,,,,"0|i0g03r:",91447,,,,,Normal,,,,,,,,,,,,,,,,,"23/May/10 01:08;jbellis;Should we apply the ""quick fix"" for now?;;;","23/May/10 01:08;jbellis;that is, -Djava.net.preferIPv4Stack=true;;;","23/May/10 10:58;urandom;We've at least one user who uses IPv6, and as far as I know, I'm the only one that has run into this particular problem; I think that setting -Djava.net.preferIPv4Stack=true would be optimizing for the wrong case. 

Also, this problem and the workarounds for it are pretty well known at this point, so it's possible that it isn't much of an issue because folks who run Java are already setting IPV6_V6ONLY=0. 

I mostly submitted this to document.;;;","23/May/10 21:53;jbellis;Okay, resolving as not-a-problem then.;;;","22/Oct/10 10:40;benjaminblack;Given the decreasing average skill level of users and the defaults on common distributions, this issue is becoming both more common and confusing more people.  Distributions are not going to be changing their defaults.  We should reconsider added the preferIPv4 setting to the default config, perhaps based on inspecting the setting of IPV6_V6ONLY.;;;","22/Oct/10 15:16;scode;I agree. FWIW, this is an issue on FreeBSD too, so it's not limited to Linux (I'm not sure whether there is an equivalent to the sysctl work-around).

Googling for this issue I know lots of people seem to be running into this and asking about it on forums etc (not specifically to Cassandra). In the case of Cassandra, for many people it may be their first use of a production Java app. Assuming a non-programmer user, failing on this issue by default and having to figure out or ask about a stack trace is not a very good first-time experience.

While ideologically one might go for the ""it's the user's responsibility to sort out his networking"" argument, until IPv6 is more widely used it really seems like a completely sensible default. The level of expected clue expected if you're running in an IPv6 environment is currently high enough that having to nudge this system property is not an issue (especially if there's a faq entry/note about its use somewhere).
;;;","30/Oct/10 05:40;urandom;committed.;;;","30/Oct/10 20:48;hudson;Integrated in Cassandra #581 (See [https://hudson.apache.org/hudson/job/Cassandra/581/])
    document CASSANDRA-628 change (prefer IPv4)

Patch by eevans
make -Djava.net.preferIPv4Stack=true default

Patch by eevans for CASSANDRA-628
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
keys are now byte[] but hashmap cannot have byte[] as keys so they need to be fixed,CASSANDRA-1020,12462918,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,24/Apr/10 12:54,16/Apr/19 17:33,22/Mar/23 14:57,27/Apr/10 23:15,0.7 beta 1,,,26/Apr/10 00:00,0,,,,,,Thrift client calls use hashmap and needs this fix,Linux/Mac Cassandra,rschildmeijer,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Apr/10 04:57;vijay2win@yahoo.com;1020-v1.txt;https://issues.apache.org/jira/secure/attachment/12442896/1020-v1.txt",,,,,,,,,,,,,,1.0,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19959,,,Tue Apr 27 15:15:07 UTC 2010,,,,,,,,,,"0|i0g2if:",91837,,,,,Critical,,,,,,,,,,,,,,,,,"24/Apr/10 21:52;jbellis;as discussed on IRC, it looks like the only places we use byte[] keys in maps are in CassandraServer where using object identity is OK since we don't need to worry about two different byte[] w/ the same contents;;;","25/Apr/10 00:07;vijay2win@yahoo.com;Thanks Jonathan, but cfamilies.get(command.key) will be fixed in cassandraserver?;;;","25/Apr/10 00:43;jbellis;you're right, that one is still a potential bug.;;;","27/Apr/10 04:57;vijay2win@yahoo.com;This Solves the problem which i noticed earlier... will this work?;;;","27/Apr/10 23:15;jbellis;committed, with a similar patch for avro.CassandraServer;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stream*Manager doesn't clean up broken Streams,CASSANDRA-1438,12472639,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,nickmbailey,nickmbailey,27/Aug/10 03:41,16/Apr/19 17:33,22/Mar/23 14:57,16/May/12 01:44,,,,,0,,,,,,StreamInManager and StreamOutManager only remove stream contexts/managers when a stream completes successfully.  Any broken streams will cause objects to hang around and never get garbage collected.,,gdusbabek,jbellis,jghoman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Nov/10 01:17;jbellis;1438.txt;https://issues.apache.org/jira/secure/attachment/12460382/1438.txt",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20138,,,Tue May 15 17:44:53 UTC 2012,,,,,,,,,,"0|i0g527:",92250,,,,,Normal,,,,,,,,,,,,,,,,,"25/Nov/10 01:17;jbellis;Patch based on Erik's from CASSANDRA-1766.  Changes:

- Merged StreamOutManager.cancelPendingFiles with StreamOutManager.remove
- allow StreamingService.cancelOutboundStreams to propagate UnknownHostException up so JMX caller gets feedback about the problem
- removes unused StreamOutManager.pendingDestinations
- Removed manual catch in FileStreamTask. Exceptions will be logged by the executor (MessagingService.streamExecutor_ is a DebuggableThreadPoolExecutor like almost all our executors, which will log uncaught exceptions from its tasks
- No finishAndStartNext on FST failure; if streaming fails we don't want to pretend it's succeeded
- Similarly SOM.remove does not notify its condition, which would be taken to mean success by waiting threads

Those last two illustrate the hard part here -- how do we un-jam those threads, without having them treat it as success, which it's not?;;;","16/May/12 01:44;jbellis;fixed in CASSANDRA-4051;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLI does not allow connection to hostname with dashes,CASSANDRA-914,12460002,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Duplicate,,riffraff,riffraff,23/Mar/10 22:56,16/Apr/19 17:33,22/Mar/23 14:57,23/Mar/10 23:07,,,,,0,antlr,cli,,,,"the cli syntax does not allow a dash in an identifier, which means that it is impossible to connect to a hostname such as foo-bar.com. 
It also forbids it in keyspace and column family names while it seems they are both supported by cassandra itself. 


Adding ""_"" as an allowed ""identifier"" character seems to fix both problems. I think the grammar is still broken in that it allows ""_"" in hostnames, but that seems a non issue. ",,,,,,,,,,,,,,,,,,,,,,,300,300,,0%,300,300,,,,,,,,,CASSANDRA-677,,,,,,,,,,,"23/Mar/10 22:58;riffraff;CASSANDRA-914.patch;https://issues.apache.org/jira/secure/attachment/12439573/CASSANDRA-914.patch",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19916,,,Tue Mar 23 21:33:57 UTC 2010,,,,,,,,,,"0|i0g1uv:",91731,,,,,Low,,,,,,,,,,,,,,,,,"23/Mar/10 22:58;riffraff;adds ""-"" as a valid character in Identitifier rules (used for keyspace, column family, hostname). All test still passing, but none added.  It's probably easier to rewrite the patch than apply it.;;;","23/Mar/10 23:07;jbellis;please move this to CASSANDRA-677.  thanks!;;;","24/Mar/10 04:48;urandom;This issue and CASSANDRA-677 are duplicates, and gabriele renzi's patch attached here is identical to  Roger Schildmeijer's first patch in that issue.

The problem with this approach is that it allows hyphens in keyspace and column family names, which is probably a no-no (at least for column families). However, after thinking about this more, it's probably worth applying since it's really not the place of the cli to be the arbiter of allowable column family names, and more people are probably tripped up by the reported issue.

It would be nice to see some of these irritating nits fixed once and for all, but maybe CASSANDRA-912 will manage that.;;;","24/Mar/10 04:56;riffraff;sorry hadn't seen CASSANDRA-677. I was dubious about the hyphen in CF names, so I tried, and apparently, they did work

cassandra> connect local-host/9160
Connected to: ""Test Cluster"" on local-host/9160
cassandra> set test-ks.Standard1-foo['x']['y']='hello'
Value inserted.
cassandra> get test-ks.Standard1-foo['x']['y']        
=> (column=79, value=hello, timestamp=1269377615172)

Now I see that although they work on a freshly started instance, cassandra will fail handling them when restarted. I believe in this case, an exception should be raised when cassandra is starting up?  ;;;","24/Mar/10 05:27;urandom;Your example above shows a keyspace with a hyphen, which according to my tests seems to work. But yeah, as I suspected, a CF name containing a hyphen causes an sstable write to blow up. I'll submit a separate ticket for that.;;;","24/Mar/10 05:33;urandom;see CASSANDRA-915;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cpu Spike to > 100%. ,CASSANDRA-2054,12496730,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,tbritz,tbritz,26/Jan/11 05:56,16/Apr/19 17:33,22/Mar/23 14:57,27/Jan/11 05:55,,,,,0,,,,,,"I see sudden spikes of cpu usage where cassandra will take up an enormous amount of cpu (uptime load > 1000). 

My application executes both reads and writes.

I tested this with https://hudson.apache.org/hudson/job/Cassandra-0.7/193/artifact/cassandra/build/apache-cassandra-2011-01-24_06-01-26-bin.tar.gz.

I disabled JNA, but this didn't help.

Jstack won't work anymore when this happens:

-bash-4.1# jstack 27699 > /tmp/jstackerror
27699: Unable to open socket file: target process not responding or HotSpot VM not loaded
The -F option can be used when the target process is not responding

Also, my entire application comes to a halt as long as the node is in this state, as the node is still marked as up, but won't respond (cassandra is taking up all the cpu on the first node) to any requests.

/software/cassandra/bin/nodetool -h localhost ring
Address Status State Load Owns Token
ffffffffffffffff
192.168.0.1 Up Normal 3.48 GB 5.00% 0cc
192.168.0.2 Up Normal 3.48 GB 5.00% 199
192.168.0.3 Up Normal 3.67 GB 5.00% 266
192.168.0.4 Up Normal 2.55 GB 5.00% 333
192.168.0.5 Up Normal 2.58 GB 5.00% 400
192.168.0.6 Up Normal 2.54 GB 5.00% 4cc
192.168.0.7 Up Normal 2.59 GB 5.00% 599
192.168.0.8 Up Normal 2.58 GB 5.00% 666
192.168.0.9 Up Normal 2.33 GB 5.00% 733
192.168.0.10 Down Normal 2.39 GB 5.00% 7ff
192.168.0.11 Up Normal 2.4 GB 5.00% 8cc
192.168.0.12 Up Normal 2.74 GB 5.00% 999
192.168.0.13 Up Normal 3.17 GB 5.00% a66
192.168.0.14 Up Normal 3.25 GB 5.00% b33
192.168.0.15 Up Normal 3.01 GB 5.00% c00
192.168.0.16 Up Normal 2.48 GB 5.00% ccc
192.168.0.17 Up Normal 2.41 GB 5.00% d99
192.168.0.18 Up Normal 2.3 GB 5.00% e66
192.168.0.19 Up Normal 2.27 GB 5.00% f33
192.168.0.20 Up Normal 2.32 GB 5.00% ffffffffffffffff

The interesting part is that after a while (seconds or minutes), I have seen cassandra nodes return to a normal state again (without restart). I have also never seen this happen at 2 nodes at the same time in the cluster (the node where it happens differes, but there seems to be scheme for it to happen on the first node most of the times).

In the above case, I restarted node 192.168.0.10 and the first node returned to normal state. (I don't know if there is a correlation)

I attached the jstack of the node in trouble (as soon as I could access it with jstack, but I suspect this is the jstack when the node was running normal again).

The heap usage is still moderate:

/software/cassandra/bin/nodetool -h localhost info
0cc
Gossip active    : true
Load             : 3.49 GB
Generation No    : 1295949691
Uptime (seconds) : 42843
Heap Memory (MB) : 1570.58 / 3005.38


I will enable the GC logging tomorrow.
",,dkuebric,eonnen,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jan/11 19:32;tbritz;gc.log;https://issues.apache.org/jira/secure/attachment/12469407/gc.log","26/Jan/11 19:32;tbritz;jstack.txt;https://issues.apache.org/jira/secure/attachment/12469408/jstack.txt","26/Jan/11 05:58;tbritz;jstackerror.txt;https://issues.apache.org/jira/secure/attachment/12469339/jstackerror.txt",,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20416,,,Thu Jan 27 04:07:26 UTC 2011,,,,,,,,,,"0|i0g91j:",92895,,,,,Normal,,,,,,,,,,,,,,,,,"26/Jan/11 05:59;jbellis;What JVM version?;;;","26/Jan/11 06:03;tbritz;java -version
java version ""1.6.0_22""
Java(TM) SE Runtime Environment (build 1.6.0_22-b04)
Java HotSpot(TM) 64-Bit Server VM (build 17.1-b03, mixed mode)


 /usr/bin/java -XX:+UseThreadPriorities -XX:ThreadPriorityPolicy=42 -Xms3022M -Xmx3022M -XX:+HeapDumpOnOutOfMemoryError -XX:+UseCompressedOops -Xss128k -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:SurvivorRatio=8 -XX:MaxTenuringThreshold=1 -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -Djava.net.preferIPv4Stack=true -Dcom.sun.management.jmxremote.port=8080 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Dlog4j.configuration=log4j-server.properties -cp /software/cassandra/bin/../conf:/software/cassandra/bin/../build/classes:/software/cassandra/bin/../lib/antlr-3.1.3.jar:/software/cassandra/bin/../lib/apache-cassandra-2011-01-24_06-01-26.jar:/software/cassandra/bin/../lib/avro-1.4.0-fixes.jar:/software/cassandra/bin/../lib/avro-1.4.0-sources-fixes.jar:/software/cassandra/bin/../lib/commons-cli-1.1.jar:/software/cassandra/bin/../lib/commons-codec-1.2.jar:/software/cassandra/bin/../lib/commons-collections-3.2.1.jar:/software/cassandra/bin/../lib/commons-lang-2.4.jar:/software/cassandra/bin/../lib/concurrentlinkedhashmap-lru-1.1.jar:/software/cassandra/bin/../lib/guava-r05.jar:/software/cassandra/bin/../lib/high-scale-lib.jar:/software/cassandra/bin/../lib/ivy-2.1.0.jar:/software/cassandra/bin/../lib/jackson-core-asl-1.4.0.jar:/software/cassandra/bin/../lib/jackson-mapper-asl-1.4.0.jar:/software/cassandra/bin/../lib/jetty-6.1.21.jar:/software/cassandra/bin/../lib/jetty-util-6.1.21.jar:/software/cassandra/bin/../lib/jline-0.9.94.jar:/software/cassandra/bin/../lib/json-simple-1.1.jar:/software/cassandra/bin/../lib/jug-2.0.0.jar:/software/cassandra/bin/../lib/libthrift-0.5.jar:/software/cassandra/bin/../lib/log4j-1.2.16.jar:/software/cassandra/bin/../lib/servlet-api-2.5-20081211.jar:/software/cassandra/bin/../lib/slf4j-api-1.6.1.jar:/software/cassandra/bin/../lib/slf4j-log4j12-1.6.1.jar:/software/cassandra/bin/../lib/snakeyaml-1.6.jar org.apache.cassandra.thrift.CassandraDaemon


I added "" -XX:+UseCompressedOops"". About 40-50 column families, througput_in_mb reduced from 64 to 16)
;;;","26/Jan/11 13:06;jbellis;Are you running on EC2 by chance?;;;","26/Jan/11 16:56;tbritz;No, these are dedicated machines with no virtualization layer. I will test today with gc logging enabled.;;;","26/Jan/11 19:32;tbritz;All nodes were up when the error occured, this time on node 192.168.0.3. I stopped our application and the node returned to normal state.

(jstack is from when the node was accessible again)


/software/cassandra/bin/nodetool -h localhost info
266
Gossip active    : true
Load             : 6.26 GB
Generation No    : 1296040182
Uptime (seconds) : 1208
Heap Memory (MB) : 907.55 / 3005.38


                                                       ffffffffffffffff
192.168.0.1     Up     Normal  4.6 GB          5.00%   0cc
192.168.0.2     Up     Normal  4.6 GB          5.00%   199
192.168.0.3     Up     Normal  5.35 GB         5.00%   266
192.168.0.4     Up     Normal  2.54 GB         5.00%   333
192.168.0.5     Up     Normal  2.59 GB         5.00%   400
192.168.0.6     Up     Normal  2.55 GB         5.00%   4cc
192.168.0.7     Up     Normal  2.61 GB         5.00%   599
192.168.0.8     Up     Normal  2.59 GB         5.00%   666
192.168.0.9     Up     Normal  2.34 GB         5.00%   733
192.168.0.10    Up     Normal  1.74 GB         5.00%   7ff
192.168.0.11    Up     Normal  2.41 GB         5.00%   8cc
192.168.0.12    Up     Normal  2.73 GB         5.00%   999
192.168.0.13    Up     Normal  3.18 GB         5.00%   a66
192.168.0.14    Up     Normal  3.26 GB         5.00%   b33
192.168.0.15    Up     Normal  3.02 GB         5.00%   c00
192.168.0.16    Up     Normal  2.5 GB          5.00%   ccc
192.168.0.17    Up     Normal  2.42 GB         5.00%   d99
192.168.0.18    Up     Normal  2.31 GB         5.00%   e66
192.168.0.19    Up     Normal  2.28 GB         5.00%   f33
192.168.0.20    Up     Normal  2.33 GB         5.00%   ffffffffffffffff
;;;","26/Jan/11 23:10;tjake;I think the problem here is thread context switching.

In your thread stack you have 2200 threads.

This seems to be caused by two problems:

1. In MessagingService we create a thread per connection.  This should be switched to a thread pool with a fixed max.
2. On the thrift size our CustomTThreadPoolServer has no max worker limit,  which i believe is the root cause of CASSANDRA-2058


;;;","26/Jan/11 23:20;jbellis;I don't think this is actually the problem.  Modern OSes are designed to handle a lot more than a couple thousand threads.

1. No, the MessagingService connections are persistent; pooling doesn't make sense.  Also, note that there's 2 MS conns per node in the cluster, it's not a lot of threads.

2. We've been using CustomTThreadPoolServer basically unchanged since June 2010, we're looking for something more recent (0.6.8 to 0.6.10 in CASSANDRA-2058).;;;","26/Jan/11 23:32;tbritz;The jstack was taken after the node returned to ""normal"" state. I'm unable to do a jstack if the node is taking away all the cpu.

The connections could also all come from the threads from my application (multiple threads) trying to connect while the node is in the ""not normal"" state, as the node is still marked as up.
;;;","27/Jan/11 01:55;eonnen;Sorry to but in.  Often, a kill -3 will still work when jstack does not. The output goes to stdout with this approach so it can be harder to locate but IME it's often more reliable.

I'm curious, does this always happen on the same node? Even in ""normal"" state there are over 100 threads waiting on the JRE to do basic slab allocation tasks. Stacks like these are all over the thread dump:

""pool-1-thread-1523"" prio=10 tid=0x00007f44ec6e7000 nid=0x366c runnable [0x00007f4274ee8000]
   java.lang.Thread.State: RUNNABLE
	at org.cliffc.high_scale_lib.NonBlockingHashMap.initialize(NonBlockingHashMap.java:259)
	at org.cliffc.high_scale_lib.NonBlockingHashMap. (NonBlockingHashMap.java:250)
	at org.cliffc.high_scale_lib.NonBlockingHashMap. (NonBlockingHashMap.java:243)
	at org.cliffc.high_scale_lib.NonBlockingHashSet. (NonBlockingHashSet.java:26)
	at org.apache.cassandra.net.MessagingService.putTarget(MessagingService.java:274)
	at org.apache.cassandra.net.MessagingService.sendRR(MessagingService.java:336)
	at org.apache.cassandra.service.StorageProxy.fetchRows(StorageProxy.java:381)
	at org.apache.cassandra.service.StorageProxy.read(StorageProxy.java:315)
	at org.apache.cassandra.thrift.CassandraServer.readColumnFamily(CassandraServer.java:98)
	at org.apache.cassandra.thrift.CassandraServer.get(CassandraServer.java:289)
	at org.apache.cassandra.thrift.Cassandra$Processor$get.process(Cassandra.java:2655)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2555)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)

That particular code is just allocating an array, should be very fast and I wouldn't expect to see > 100 threads waiting on memory allocation unless there's a problem in the underlying host (bad RAM) or JVM. ;;;","27/Jan/11 03:08;jbellis;I think that either

- we are ""leaking"" keys in that NBHM (MessagingService.targets -- it's supposed to be active for the duration of a request and then removed) or
- we are running into the same subtle NBHM problem that we did in CASSANDRA-157; see also http://sourceforge.net/tracker/?func=detail&aid=2828100&group_id=194172&atid=948362 and http://sourceforge.net/tracker/?func=detail&aid=2835696&group_id=194172&atid=948362;;;","27/Jan/11 03:29;jbellis;Third possibility:

- We're simply beating the hell out of the allocation subsystem by instantiating so many NBHS as part of the value in each targets map entry;;;","27/Jan/11 05:55;jbellis;Definitely looks like the same thing as CASSANDRA-2058.  Posted a patch there (for 0.6).;;;","27/Jan/11 12:07;jbellis;For those following this but not 2058, I've attached a patch there for 0.6 and 0.7.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deleted columns are resurrected after a flush,CASSANDRA-1837,12492744,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,gdusbabek,brandon.williams,brandon.williams,08/Dec/10 23:25,16/Apr/19 17:33,22/Mar/23 14:57,15/Dec/10 23:24,0.7.0 rc 3,,,,1,,,,,,"Easily reproduced with the cli:

{noformat}
[default@unknown] create keyspace testks;
2785d67c-02df-11e0-ac09-e700f669bcfc
[default@unknown] use testks;
Authenticated to keyspace: testks
[default@testks] create column family testcf;
2fbad20d-02df-11e0-ac09-e700f669bcfc
[default@testks] set testcf['test']['foo'] = 'foo';
Value inserted.
[default@testks] set testcf['test']['bar'] = 'bar';
Value inserted.
[default@testks] list testcf;
Using default limit of 100
-------------------
RowKey: test
=> (column=626172, value=626172, timestamp=1291821869120000)
=> (column=666f6f, value=666f6f, timestamp=1291821857320000)

1 Row Returned.
[default@testks] del testcf['test'];
row removed.
[default@testks] list testcf;
Using default limit of 100
-------------------
RowKey: test

1 Row Returned.
{noformat}

Now flush testks and look again:

{noformat}

[default@testks] list testcf;
Using default limit of 100
-------------------
RowKey: test
=> (column=626172, value=626172, timestamp=1291821869120000)
=> (column=666f6f, value=666f6f, timestamp=1291821857320000)

1 Row Returned.
{noformat}",,dave111,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Dec/10 01:52;gdusbabek;1837-0.6-unit-test.diff;https://issues.apache.org/jira/secure/attachment/12466228/1837-0.6-unit-test.diff","15/Dec/10 01:29;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-unit-tests-that-show-bug.txt;https://issues.apache.org/jira/secure/attachment/12466225/ASF.LICENSE.NOT.GRANTED--v1-0001-unit-tests-that-show-bug.txt","15/Dec/10 01:29;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0002-copy-deletion-time-to-reduced-column-families.txt;https://issues.apache.org/jira/secure/attachment/12466226/ASF.LICENSE.NOT.GRANTED--v1-0002-copy-deletion-time-to-reduced-column-families.txt","15/Dec/10 06:26;jbellis;v2-0002.txt;https://issues.apache.org/jira/secure/attachment/12466259/v2-0002.txt",,,,,,,,,,,4.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20337,,,Wed Jan 19 01:21:24 UTC 2011,,,,,,,,,,"0|i0g7pr:",92680,,jbellis,,jbellis,Critical,,,,,,,,,,,,,,,,,"08/Dec/10 23:36;brandon.williams;I had a suspicion it was related to CASSANDRA-1780, but I backed it out and the problem persists.;;;","09/Dec/10 04:40;jbellis;can you port to TableTest?  it has reTest to check for same behavior before-and-after-flush.;;;","11/Dec/10 04:10;gdusbabek;This is happening because of bug in the way deleted rows are [not] interpreted once they leave the memtable in the CFS.getRangeSlice code.  CFS.getColumnFamily doesn't exhibit the bug, but the codepath is pretty different.

I've got a fix that addresses standard columns, but it breaks a few of the nosetests for super columns.  Looking into that now.;;;","15/Dec/10 01:31;gdusbabek;This might be a problem in 0.6 too.  Porting the unit test over isn't trivial though...;;;","15/Dec/10 01:52;gdusbabek;Ported unit test to 0.6. Verifies non-breakage there.;;;","15/Dec/10 06:26;jbellis;Thanks for tracking that down, Gary!

I think it makes sense to centralize the lastReducedAt logic in getReduced to avoid having a field that is initialized in one method and cleared in another (which looks like a bug in v1, that it doesn't get cleared between rows).  v2 attached w/ this approach.;;;","15/Dec/10 06:47;gdusbabek;bq. I think it makes sense to centralize the lastReducedAt logic in getReduced...
Right. My mistake.;;;","15/Dec/10 23:12;jbellis;+1;;;","15/Dec/10 23:24;gdusbabek;committed.;;;","15/Dec/10 23:44;hudson;Integrated in Cassandra-0.7 #84 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/84/])
    track row deletions when merging cols to form a row. patch by gdusbabek and jbellis. CASSANDRA-1837
;;;","16/Dec/10 00:15;hudson;Integrated in Cassandra #631 (See [https://hudson.apache.org/hudson/job/Cassandra/631/])
    changes for CASSANDRA-1837
track row deletions when merging cols to form a row. patch by gdusbabek and jbellis. CASSANDRA-1837
unit tests that show bug. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1837
;;;","19/Jan/11 09:19;dave111;just wanted to take credit for originally discovering and reporting this to driftx in the chat. my original example- http://pastie.org/1358812.  thanks for the fix and quick turnaround.;;;","19/Jan/11 09:21;brandon.williams;Thanks for finding this, Dave.  (I am driftx on irc);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cannot move a node,CASSANDRA-1670,12478510,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,gdusbabek,mdennis,mdennis,28/Oct/10 06:28,16/Apr/19 17:33,22/Mar/23 14:57,03/Nov/10 21:12,0.7.0 rc 1,,,,0,,,,,,"two node cluster (node0, node1).  node0 is listed as the only seed on both nodes.  Listen addresses explicitly set to an IP on both nodes. No initial token, no autobootstrap (but see below).  Bring up the ring.  Everything is fine on both nodes.

decom node1.  verify decom completed correctly by reading the logs on both nodes.  rm all data/logs on node1.  bring node1 up again.

One of two things happen:

* node0 thinks it is in a ring by itself, node1 thinks both nodes are in the ring.
* both node0 and node1 think they are in rings by themselves

If you restart node0 after decom, it appears to work normally.

Similar issues seem to present if you kill node1 (either when autobootstrapping before it completes or after it is in the ring) and removetoken.

",RAX,mbulman,thepaul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Nov/10 23:59;gdusbabek;1670-0.6.txt;https://issues.apache.org/jira/secure/attachment/12458550/1670-0.6.txt","02/Nov/10 00:28;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-code-that-tidied-Gossiper.justRemovedEndpoints_-was-no.txt;https://issues.apache.org/jira/secure/attachment/12458552/ASF.LICENSE.NOT.GRANTED--v1-0001-code-that-tidied-Gossiper.justRemovedEndpoints_-was-no.txt",,,,,,,,,,,,,2.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20250,,,Wed Nov 03 13:07:03 UTC 2010,,,,,,,,,,"0|i0g6nz:",92510,,,,,Normal,,,,,,,,,,,,,,,,,"30/Oct/10 05:38;mbulman;Because move is decommission+bootstrap, the same behavior occurs when moving node1 as well.;;;","01/Nov/10 23:54;gdusbabek;Looks like this bug goes all the way back to 0.6.;;;","01/Nov/10 23:59;gdusbabek;The code that removes endpoints from Gossiper.justRemovedEndpoints after RING_DELAY was only getting called if the endpoint had a state attached to it.  Since state is removed for decommissioned nodes, the code was never getting called. ;;;","02/Nov/10 00:06;jbellis;how does moving the removal out of the for loop fix the state-attached-to-it problem?;;;","02/Nov/10 00:38;gdusbabek;When a node is decommissioned, it gets added to justRemovedEndpoints_, but removed from endpointStateMap_.  The old code will only remove a node from justRemovedEndpoints_ if it currently exists in endpointStateMap_.  If the node stays in justRemovedEndpoints_ (which it will currently), it can never be recognized as part of the ring because of the check in Gossiper.handleNewJoin().;;;","02/Nov/10 06:47;mbulman;Running from nodetool as well as ripcord decommission code (direct call to StorageService) gets:

Exception in thread ""main"" java.lang.AssertionError
        at org.apache.cassandra.service.StorageService.getLocalToken(StorageService.java:1128)
        at org.apache.cassandra.service.StorageService.startLeaving(StorageService.java:1527)
        at org.apache.cassandra.service.StorageService.decommission(StorageService.java:1546)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:111)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:45)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:226)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:251)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:857)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:795)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1450)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:90)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1285)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1383)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:807)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
        at sun.rmi.transport.Transport$1.run(Transport.java:177)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636);;;","02/Nov/10 22:11;gdusbabek;Mike, can you provide a more complete log?  That trace is unrelated to the patch and likely indicates a different problem.;;;","02/Nov/10 23:24;mbulman;That's all I get.  The only other thing I can add is that the node being decommissioned logs ""DECOMMISSIONING"" when level is set to DEBUG, and that the exception comes back almost immediately.  fwiw, I'm running this on r1029870 of the .7 branch;;;","02/Nov/10 23:34;gdusbabek;There should be more. What I'm looking for is some indication that the node is not in the middle of a bootstrap operation, which would trigger this exception.;;;","03/Nov/10 00:34;mbulman;INFO 15:48:22,176 Joining: getting load information                                                                                                                                                                        
 INFO 15:48:22,177 Sleeping 90000 ms to wait for load information...
DEBUG 15:48:23,053 GC for ParNew: 12 ms, 15822112 reclaimed leaving 15102720 used; max is 1268449280                                                                                                                        
DEBUG 15:48:23,166 attempting to connect to /184.106.231.n0
 INFO 15:48:23,553 Node /184.106.231.n0 is now part of the cluster                                                                                                                                                         
DEBUG 15:48:23,554 Resetting pool for /184.106.231.n0
DEBUG 15:48:23,559 Node /184.106.231.n0 state normal, token 104110673354167092736227093944218730763                                                                                                                        
DEBUG 15:48:23,559 New node /184.106.231.n0 at token 104110673354167092736227093944218730763
DEBUG 15:48:23,559 clearing cached endpoints                                                                                                                                                                                
DEBUG 15:48:24,167 attempting to connect to /184.106.231.n0
DEBUG 15:48:24,169 Disseminating load info ...                                                                                                                                                                              
 INFO 15:48:24,554 InetAddress /184.106.231.n0 is now UP
 INFO 15:48:24,554 Started hinted handoff for endpoint /184.106.231.n0                                                                                                                                                  
 INFO 15:48:24,557 Finished hinted handoff of 0 rows to endpoint /184.106.231.n0
DEBUG 15:49:24,169 Disseminating load info ...                                                                                                                                                                              
DEBUG 15:49:39,106 GC for ParNew: 16 ms, 16111512 reclaimed leaving 85537648 used; max is 1268449280
DEBUG 15:49:52,177 ... got load info                                                                                              
 INFO 15:49:52,177 Joining: getting bootstrap token
DEBUG 15:49:52,183 attempting to connect to /184.106.231.n0                                                                                                                                                                
DEBUG 15:49:52,191 Processing response on a callback from 270@/184.106.231.n0
 INFO 15:49:52,192 New token will be 19040081623932476870383442086276677899 to assume load from /184.106.231.n0                                                                                                             
DEBUG 15:49:52,193 clearing cached endpoints
DEBUG 15:49:52,194 Will try to load mx4j now, if it's in the classpath                                                                                                                                                      
 INFO 15:49:52,194 Will not load MX4J, mx4j-tools.jar is not in the classpath
 INFO 15:49:52,220 Binding thrift service to /0.0.0.0:9160                                                                                                                                                                  
 INFO 15:49:52,222 Using TFramedTransport with a max frame size of 15728640 bytes.
 INFO 15:49:52,226 Listening for thrift clients...                                                                                                                                                                          
DEBUG 15:50:24,170 Disseminating load info ...
DEBUG 15:50:52,247 DECOMMISSIONING                                                                                                                                                                                          
DEBUG 15:51:24,170 Disseminating load info ...
DEBUG 15:52:24,171 Disseminating load info ...                                                                                                                                                                              


From n0:

root@ripcord:/usr/src/cassandra/branches/cassandra-0.7# bin/nodetool -h 184.106.231.n0  ring                                                                                                                           
Address         Status State   Load            Token
                                       104110673354167092736227093944218730763                                                                                                                                             
184.106.228.n1 Up     Normal  5.3 KB          19040081623932476870383442086276677899
184.106.231.n0 Up     Normal  10.27 KB        104110673354167092736227093944218730763                                                                                                                                     

root@ripcord:/usr/src/cassandra/branches/cassandra-0.7# bin/nodetool -h 184.106.228.n1 decommission 
     <TRACE FROM MY PREVIOUS COMMENT>

root@ripcord:/usr/src/cassandra/branches/cassandra-0.7# bin/nodetool -h 184.106.231.n0 ring
Address         Status State   Load            Token                                       
                                       104110673354167092736227093944218730763    
184.106.228.n1 Up     Normal  5.3 KB          19040081623932476870383442086276677899      
184.106.231.n0 Up     Normal  10.27 KB        104110673354167092736227093944218730763     
;;;","03/Nov/10 06:25;mbulman;Ok got it working properly.  Patch fixes the issue described.

As a node, that patch doesn't work in 0.7 branch because justRemovedEndPoints_ (.6) is now justRemovedEndpoints (.7).  Not sure how you guys handle that, but the change is simple enough.;;;","03/Nov/10 06:58;jbellis;bq. The old code will only remove a node from justRemovedEndpoints_ if it currently exists in endpointStateMap_

Isn't it ""remove from jRE if _any_ [other] node exists in eSM?""  Which means this is only a bug in 2-node clusters?

+1 if so, just trying to understand.;;;","03/Nov/10 21:07;gdusbabek;bq. sn't it ""remove from jRE if any [other] node exists in eSM?"" Which means this is only a bug in 2-node clusters?
Yes. good observation.  I'll only bother committing this to 0.7/trunk then.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"In a cluster, get_range_slices() does not return all the keys it should",CASSANDRA-1198,12467113,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,cgist,cgist,17/Jun/10 02:26,16/Apr/19 17:33,22/Mar/23 14:57,26/Jun/10 06:12,0.6.3,,,,0,,,,,,"Row iteration with get_range_slices() does not return all keys. This behaviour only occurs with more than one node and depends on how the nodes are located on the ring.

To reproduce, insert some records into a cluster with more than one node. A subsequent row iteration will return fewer records than were inserted. This has been observed when 1) inserting into a single node, bootstrapping a second node then using get_range_slices() and 2) inserting into a cluster of several nodes then using get_range_slices().

This appears to be similar to https://issues.apache.org/jira/browse/CASSANDRA-781","Linux 2.6.18-128.1.10.el5.xs5.5.0.51xen
Java build 1.6.0_17-b04
Cassandra 0.7 2010-06-15 including patch for https://issues.apache.org/jira/browse/CASSANDRA-1130",bryantower,johanoskarsson,joosto,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jun/10 13:51;jbellis;1198.txt;https://issues.apache.org/jira/secure/attachment/12447546/1198.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20031,,,Fri Jun 25 22:12:51 UTC 2010,,,,,,,,,,"0|i0g3lj:",92013,,,,,Normal,,,,,,,,,,,,,,,,,"17/Jun/10 02:39;jbellis;Christopher reported on IRC that he cannot reproduce in 0.6, so this does appear to be a 0.7-only bug.;;;","20/Jun/10 11:58;cgist;It looks like this issue is a regression of https://issues.apache.org/jira/browse/CASSANDRA-1042 so it should in fact affect 0.6.3, though I haven't confirmed.

When doing a row iteration starting from token 0, since the ranges are no longer sorted by ring order after #1042 but by wrap order, the iteration could start with the range to the left of 0. This results in either iterating through fewer than all keys, or iterating through duplicate keys. I have seen the following different behaviour, varying based on the ring tokens, key count and distribution, and range count:
1) too few keys because range slice command had lower count than keys in (z,0] range and subsequent range slice was restricted to only the (z,0] range
2) duplicate keys because range slice command had higher count than keys in (z,0] range and keys in range (z,0] were repeated
3) too few keys because ranges were handled out of token order, eg. (z,0] before (y,z] so no keys in (y,z] were returned

Also it would appear that token ranges passed into get_range_slices() are in fact start inclusive, contrary to the wiki. Is this correct?;;;","20/Jun/10 13:51;jbellis;Excellent work.  Yes, the sort that was removed was in fact necessary (which is stated in the docstring to getRestrictedRanges, but then the sort was done elsewhere so the confusion was understandable).

This patch adds back the sort (this time in gRR) and moves the endpoints-handling to getRangeSlice (where doing just-in-time liveness checking has the added benefit of improving availability for large queries) to make gRR less muddled.

(Token-based range queries are turned into Bounds objects, which are start-inclusive, as opposed to Range objects, which are not.);;;","20/Jun/10 19:52;jeromatron;Oh crumb!

We had fixed the problem manifest in 1045 but overlooked that part of it. Nice catch.

I'll go ahead and try that patch out on the word count example to make sure it works tomorrow.

Thanks Christopher and Jonathan.;;;","21/Jun/10 23:57;jeromatron;Hmmm, I tried latest vanilla 0.6 branch with this patch and the duplicates have come back in the word count example. ;;;","26/Jun/10 06:09;urandom;+1;;;","26/Jun/10 06:12;jbellis;committed.  (CASSANDRA-1042 is re-opened for fixing the original bug.);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
json2sstable fails due to OutOfMemory,CASSANDRA-2189,12498991,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,skamio,skamio,18/Feb/11 10:53,16/Apr/19 17:33,22/Mar/23 14:57,09/Dec/11 00:39,0.8.9,1.0.6,Legacy/Tools,,0,,,,,,"I have a json file created with sstable2json for a column family of super column type. Its size is about 1.9GB. (It's a dump of all keys because I cannot find out how to specify keys to dump in sstable2json.)
When I tried to create sstable from the json file, it failed with OutOfMemoryError as follows.

 WARN 00:31:58,595 Schema definitions were defined both locally and in cassandra.yaml. Definitions in cassandra.yaml were ignored.
Exception in thread ""main"" java.lang.OutOfMemoryError: PermGen space
        at java.lang.String.intern(Native Method)
        at org.codehaus.jackson.util.InternCache.intern(InternCache.java:40)
        at org.codehaus.jackson.sym.BytesToNameCanonicalizer.addName(BytesToNameCanonicalizer.java:471)
        at org.codehaus.jackson.impl.Utf8StreamParser.addName(Utf8StreamParser.java:893)
        at org.codehaus.jackson.impl.Utf8StreamParser.findName(Utf8StreamParser.java:773)
        at org.codehaus.jackson.impl.Utf8StreamParser.parseLongFieldName(Utf8StreamParser.java:379)
        at org.codehaus.jackson.impl.Utf8StreamParser.parseMediumFieldName(Utf8StreamParser.java:347)
        at org.codehaus.jackson.impl.Utf8StreamParser._parseFieldName(Utf8StreamParser.java:304)
        at org.codehaus.jackson.impl.Utf8StreamParser.nextToken(Utf8StreamParser.java:140)
        at org.codehaus.jackson.map.deser.UntypedObjectDeserializer.mapObject(UntypedObjectDeserializer.java:93)
        at org.codehaus.jackson.map.deser.UntypedObjectDeserializer.deserialize(UntypedObjectDeserializer.java:65)
        at org.codehaus.jackson.map.deser.MapDeserializer._readAndBind(MapDeserializer.java:197)
        at org.codehaus.jackson.map.deser.MapDeserializer.deserialize(MapDeserializer.java:145)
        at org.codehaus.jackson.map.deser.MapDeserializer.deserialize(MapDeserializer.java:23)
        at org.codehaus.jackson.map.ObjectMapper._readValue(ObjectMapper.java:1261)
        at org.codehaus.jackson.map.ObjectMapper.readValue(ObjectMapper.java:517)
        at org.codehaus.jackson.JsonParser.readValueAs(JsonParser.java:897)
        at org.apache.cassandra.tools.SSTableImport.importUnsorted(SSTableImport.java:208)
        at org.apache.cassandra.tools.SSTableImport.importJson(SSTableImport.java:197)
        at org.apache.cassandra.tools.SSTableImport.main(SSTableImport.java:421)

So, what I had to is that split the json file with ""split"" command and modify them to be correct json file. Create sstable for each small json files.

Could you change json2sstable to avoid OutOfMemory?",linux,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,"08/Dec/11 07:54;jbellis;2189-2.txt;https://issues.apache.org/jira/secure/attachment/12506542/2189-2.txt","18/Feb/11 14:58;jbellis;2189.txt;https://issues.apache.org/jira/secure/attachment/12471370/2189.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20491,,,Thu Dec 08 17:37:14 UTC 2011,,,,,,,,,,"0|i0g9v3:",93028,,,,,Low,,,,,,,,,,,,,,,,,"18/Feb/11 11:00;jbellis;That kind of looks like a jackson bug to me.  I'll ask Tatu.;;;","18/Feb/11 14:58;jbellis;patch to disable interning.  (Thanks to Tatu for pointing me at the right Feature.);;;","22/Feb/11 05:13;jbellis;Shotaro, can you test the patch?;;;","24/Feb/11 07:28;jbellis;committed;;;","24/Feb/11 07:46;hudson;Integrated in Cassandra-0.7 #313 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/313/])
    turnoff string interning in json2sstable
patch by jbellis for CASSANDRA-2189
;;;","08/Dec/11 07:46;jbellis;This didn't actually fix the problem.  Tatu again:

""This calls configure on parser; but at that point the symbol table has already been created. So configure must be called on the factory first (and only need to be called once, really), and then settings will be passed as expected."";;;","08/Dec/11 07:54;jbellis;Patch attached to move configuration to the factory.;;;","09/Dec/11 00:39;jbellis;Customer testing indicates that this is a big improvement, especially when combined with an upgrade to Jackson 1.9.2.  I'll commit this patch to 0.8.9 and 1.0.6, and upgrade Jackson in trunk.;;;","09/Dec/11 01:37;hudson;Integrated in Cassandra-0.8 #413 (See [https://builds.apache.org/job/Cassandra-0.8/413/])
    turn off string interning in json2sstable, take 2
patch by jbellis; tested by George Ciubotaru for CASSANDRA-2189

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1211976
Files : 
* /cassandra/branches/cassandra-0.8
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/tools/SSTableImport.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
clean up better after streaming,CASSANDRA-550,12440628,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,14/Nov/09 00:28,16/Apr/19 17:33,22/Mar/23 14:57,14/Nov/09 05:00,0.5,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Nov/09 04:46;jbellis;550-2.patch;https://issues.apache.org/jira/secure/attachment/12424894/550-2.patch","14/Nov/09 00:45;jbellis;550.patch;https://issues.apache.org/jira/secure/attachment/12424862/550.patch",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19750,,,Sat Nov 14 12:33:59 UTC 2009,,,,,,,,,,"0|i0fzmf:",91369,,,,,Low,,,,,,,,,,,,,,,,,"14/Nov/09 02:11;rays;Getting an error after restarting with this patch applied.

ERROR [main] 2009-11-13 14:41:07,289 CassandraDaemon.java (line 184) Exception encountered during startup.
java.lang.NullPointerException
	at org.apache.cassandra.db.SystemTable.initMetadata(SystemTable.java:150)
	at org.apache.cassandra.service.StorageService.start(StorageService.java:259)
	at org.apache.cassandra.service.CassandraServer.start(CassandraServer.java:72)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:94)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:166)

full log: http://pastebin.com/m25468765;;;","14/Nov/09 03:32;jbellis;looks like a separate bug...  this should fix it;;;","14/Nov/09 04:13;rays;should I file as new?

ERROR - Exception encountered during startup.
java.lang.ClassCastException: [B cannot be cast to java.lang.Comparable
at java.util.concurrent.ConcurrentSkipListMap.comparable(ConcurrentSkipListMap.java:621)
at java.util.concurrent.ConcurrentSkipListMap.doPut(ConcurrentSkipListMap.java:862)
at java.util.concurrent.ConcurrentSkipListMap.putIfAbsent(ConcurrentSkipListMap.java:1893)
at java.util.concurrent.ConcurrentSkipListSet.add(ConcurrentSkipListSet.java:202)
at org.apache.cassandra.db.SystemTable.initMetadata(SystemTable.java:122)
at org.apache.cassandra.service.StorageService.start(StorageService.java:259)
at org.apache.cassandra.service.CassandraServer.start(CassandraServer.java:72)
at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:94)
at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:166)
Exception encountered during startup.;;;","14/Nov/09 04:53;rays;Its working now after 500-2.patch was applied and successfully cleaned up some old junk on my nodes.;;;","14/Nov/09 05:00;jbellis;committed;;;","14/Nov/09 20:33;hudson;Integrated in Cassandra #258 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/258/])
    specifically look for TOKEN and GENERATION columns
patch by jbellis; tested by Ray Slakinski for 
clean up temporary for-streaming files when done
patch by jbellis; tested by Ray Slakinski for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Range queries do not yet span multiple nodes,CASSANDRA-212,12427015,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,phatduckk,jbellis,jbellis,03/Jun/09 17:22,16/Apr/19 17:33,22/Mar/23 14:57,01/Aug/09 02:28,0.4,,,,0,,,,,,"Need ability to continue a query on the next node in the ring, if necessary",,euphoria,hammer,sammy.yu,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Jul/09 08:23;phatduckk;0001-All-patches-for-212.-All-comments-in-https-issues.patch;https://issues.apache.org/jira/secure/attachment/12415079/0001-All-patches-for-212.-All-comments-in-https-issues.patch","29/Jul/09 05:39;phatduckk;0001-Cassandra-212.patch;https://issues.apache.org/jira/secure/attachment/12414807/0001-Cassandra-212.patch","01/Aug/09 02:10;phatduckk;rebased-212-against-9164940f41972e3611d1ad38a903ca39562e6feb.patch;https://issues.apache.org/jira/secure/attachment/12415135/rebased-212-against-9164940f41972e3611d1ad38a903ca39562e6feb.patch",,,,,,,,,,,,3.0,phatduckk,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19596,,,Sat Aug 01 12:34:01 UTC 2009,,,,,,,,,,"0|i0fxjz:",91034,,,,,Normal,,,,,,,,,,,,,,,,,"01/Jul/09 04:02;jbellis;All this needs is a bool added to RangeReply that is true if it stopped b/c the next local key was not part of the range, or the asked-for limit was reached.

If a reply comes back with that bool false, then the coordinator node, the one talking to the client, sends the query to the next node in the ring down from the one it just got a reply from and appends those results to the list it's going to return to the client.

Repeat until the i'm-done-bool comes back true.
;;;","14/Jul/09 22:49;markr;Sounds like a pretty serious bug in the general case, but might not happen very often in practice.;;;","22/Jul/09 06:47;jbellis;(the relevant starting point is StorageProxy.getKeyRange);;;","22/Jul/09 07:24;junrao;This can be a bit tricky because the key ranges btw 2 consecutive nodes overlap (because of replication). When moving to the next node, you want to be careful not to pick up duplicated keys.

Also, the approach that Jonathan described forces nodes to be scanned sequentially. Sometimes, it is more efficient to scan multiple nodes in parallel, especially if maxResult is unspecified.;;;","22/Jul/09 07:52;jbellis;> When moving to the next node, you want to be careful not to pick up duplicated keys. 

that is why you give the last value received from node #1 as the start_with parameter to node #2.  so at most there will be one duplicate.

> Sometimes, it is more efficient to scan multiple nodes in parallel

Unless you have trivially small amounts of data on each node, in which case it doesn't matter, any number that's going to fit in memory is going to be better served by sequential scanning since the odds are excellent that you won't have to cross to another node.;;;","28/Jul/09 09:07;phatduckk;first cut at http://github.com/phatduckk/Cassandra/commit/b535b00f2917995f93f5838a98e08931e2b52680
I did a bunch of refactoring to get a lot of the get*Endpoint*() type methods to take an offset.

still need to work on the replicas.

anyways... wanted feedback on the approach. anyone wanna take a look and lemme know what you think?;;;","28/Jul/09 10:57;jbellis;looks good, modulo the headers being in all the diffs (whitespace?)

> still need to work on the replicas

all you need to do is change

command = new RangeCommand(command.table, command.columnFamily, command.startWith, command.stopAt, command.maxResults - rangeKeys.size());

to pass in the last key from the previous node as the start.  right?  or are you talking about something else?;;;","29/Jul/09 05:39;phatduckk;attempt at #212

unit tests and nosetests pass;;;","29/Jul/09 11:10;jbellis;i'm a little confused by the git commit message.

which of those are ""mission accomplished"" and which are ""to-dos"" if any? :);;;","29/Jul/09 11:21;jbellis;we still need this part

> this needs a bool added to RangeReply that is true if it stopped b/c the next local key was not part of the range, or the asked-for limit was reached

otherwise we have to do extra queries for every range command that doesn't hit the max results or the end key.

also,

            while (endPoint != null)

                if (endPoint.toString().equals(firstEndpoint)) break

these seem redundant, shouldn't it be while (true) if (endpoint == null) { throw } ?
;;;","30/Jul/09 06:55;phatduckk;created an object called RangeResult that encapsulates the keys and a boolean flag called isInRange.

i plumbed this thru RangeReply and to StorageProxy.getKeyRange

there's a squashed commit with all changes at:
http://github.com/phatduckk/Cassandra/commit/a5b2d264a1567a97b04e6d7874e0114a3ebd32d8

is the RangeResult stuff ok? It seemed to be the best way to flag whether a node had gone thru its entire range or not

(ignore ws diff - i'll prune those when i make a patch);;;","30/Jul/09 10:46;jbellis;Looks good to me overall.

RangeResult looks an awful lot like RangeReply w/ less methods.  Could we just use RangeReply and cut out the middleman?  I dunno.  Java feels so clunky here.

One way to return multiple values would be like this

        return new HashMap<String, Object>() {{
            put(""keys"", listOfStrings);
            put(""finished"", true);
        }};

Who says constructor blocks aren't useful? :)

Can we the bool in RangeReply it something besides isInRange?  In the range does not imply ""contains the entire range"" to me.  Rather the opposite relationship.  If you don't like ""finished"" maybe isComplete, rangeComplete, rangeCompletedLocally, ...;;;","30/Jul/09 12:02;phatduckk;i thought about the HashMap thing but Sammy and I thought that was kinda hacky. I'll see about merging RangeResult with RangeReply.

I'll have some alternative(s) tomorrow. 

Also - i have no problem renaming the boolean. rangeCompletedLocally sounds most descriptive to me so ill go with that;;;","31/Jul/09 08:15;phatduckk;single patch file to address the range queries;;;","31/Jul/09 08:23;phatduckk;new patch with previous comments addressed;;;","31/Jul/09 23:09;jbellis;does not apply to trunk (because of CASSANDRA-111 maybe?), can you rebase?;;;","01/Aug/09 02:10;phatduckk;rabased my last patch against trunk @ 9164940f41972e3611d1ad38a903ca39562e6feb;;;","01/Aug/09 02:28;jbellis;committed, thanks!;;;","01/Aug/09 20:34;hudson;Integrated in Cassandra #154 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/154/])
    Make range queries continue on the next node when necessary.
Patch by Arin Sarkissian; reviewed by jbellis for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
race condition prevents startup under Xen vm,CASSANDRA-97,12423605,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,24/Apr/09 01:47,16/Apr/19 17:33,22/Mar/23 14:57,29/Apr/09 10:06,0.3,,,,0,,,,,,Reported by Soo Hwan Park on the mailing list.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Apr/09 11:21;jbellis;0001-r-m-unused-code-do-minor-cleanup.patch;https://issues.apache.org/jira/secure/attachment/12406324/0001-r-m-unused-code-do-minor-cleanup.patch","24/Apr/09 11:21;jbellis;0002-r-m-unnecessary-race-prone-cancelled-read-write-ke.patch;https://issues.apache.org/jira/secure/attachment/12406325/0002-r-m-unnecessary-race-prone-cancelled-read-write-ke.patch","24/Apr/09 11:22;jbellis;0003-renaming-and-final-cleanup.patch;https://issues.apache.org/jira/secure/attachment/12406326/0003-renaming-and-final-cleanup.patch","29/Apr/09 05:13;jbellis;0004.patch.2;https://issues.apache.org/jira/secure/attachment/12406694/0004.patch.2","24/Apr/09 11:33;jbellis;cassandra_race_condition_in_xen.patch;https://issues.apache.org/jira/secure/attachment/12406329/cassandra_race_condition_in_xen.patch",,,,,,,,,,5.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19548,,,Wed Apr 29 02:06:41 UTC 2009,,,,,,,,,,"0|i0fwuv:",90921,,,,,Normal,,,,,,,,,,,,,,,,,"24/Apr/09 11:35;jbellis;The race condition patch is Soo's original one.

But rather than proliferate hacks on top of hacks I think it's best to remove all the unnecessary scaffolding around the Selector entirely.

03
    renaming and final cleanup

02
    r/m unnecessary & race-prone cancelled/read/write key sets (a main reason to use
    select is that select does this for you!)

01
    r/m unused code & do minor cleanup
;;;","28/Apr/09 03:08;jbellis;here is the thread where the bug was reported: http://groups.google.com/group/cassandra-user/browse_thread/thread/1617bb8fd6e80041/bb3fa9cf12535162;;;","28/Apr/09 06:43;urandom;First off, I think these patches (000[1-3]) look good. I don't see a reason for the key state collections that currently exist; IMO this is how the code should look, +1 for applying it.

However, I still believe there is a race condition here. Or rather, I am experiencing what seems to be a race condition between the select() call in SelectorManager.run and the invocation of SelectorManager.register() from the main thread.

Is no one else seeing this?


;;;","29/Apr/09 05:10;jbellis;committed patches 1-3.

added patch 4: start SelectorManager after all the selector registration is done, to workaround a bug in some linux environments.  (Debian etch under vmware player or under EC2 seem to exhibit this problem consistently.)  the bug symptom is, the selector register hangs indefinitely even though the select() call is only for 100ms (after which it should stop blocking and let the register through).;;;","29/Apr/09 05:12;jbellis;oops, needed to rebase post-113.;;;","29/Apr/09 05:41;urandom;+1 for 0004.patch.2, that seems to do the trick.;;;","29/Apr/09 10:06;jbellis;committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cache capacity settings done via nodetool get reset on memtable flushes,CASSANDRA-1079,12464310,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,kingryan,kingryan,12/May/10 08:18,16/Apr/19 17:33,22/Mar/23 14:57,18/May/10 06:33,0.6.2,,,,0,,,,,,"In an experiment we set cache capacities via nodetool. The config file had the KeyCache for this CF at 1000000, we set the RowCache to 10000000 via nodetool.

The next time we flushed a memtable for that CF, the cache capacity settings got reverted to what is in the conf file. We repeated the experiment with the same results.",,schubertzhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/May/10 23:40;jbellis;1079.txt;https://issues.apache.org/jira/secure/attachment/12444694/1079.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19985,,,Mon May 17 22:33:09 UTC 2010,,,,,,,,,,"0|i0g2vb:",91895,,,,,Normal,,,,,,,,,,,,,,,,,"12/May/10 08:33;jbellis;this is working as designed.  the config file is The Source Of Truth for settings it contains.;;;","12/May/10 08:43;kingryan;If we're not going to let the jmx-based setting live longer than one memtable, we should probably remove the ability to set it that way. The current behavior is too surprising.;;;","12/May/10 08:48;jbellis;It's not surprising if you remember that the conf is The Source Of Truth. :)

I'm big on ""there should only be one way to do it"" but restarting to test cache size effects is too painful.;;;","12/May/10 10:58;jmhodges;??I'm big on ""there should only be one way to do it"" but restarting to test cache size effects is too painful.??

I'm confused. Did you mean ""isn't""?;;;","12/May/10 11:35;stuhood;I think the disconnect is that Ryan and Jeff would like to be able to tune cache sizes on a cluster that is receiving writes, which is difficult when it is reset per-memtable.;;;","12/May/10 11:58;jbellis;Oh.  Sometimes I need to read things twice -- yes, it's a bug if it's not persisting between flushes.  (I'm saying that it's not supposed to persist b/t restarts, unless you change the conf file too.);;;","12/May/10 14:12;kingryan;Yes, ""not persisting  between restarts"" is expected, it currently gets reset when at every flush.;;;","13/May/10 01:05;kingryan;It appears the problem is that SSTableTracker's replace method calls updateCacheSizes, which reads the cache settings from DatabaseDescriptor (which doesn't get updated by the jmx command).;;;","17/May/10 23:40;jbellis;patch to not recalculate cache capacity if it's been manually modified;;;","18/May/10 01:04;kingryan;Patch looks good to me.;;;","18/May/10 06:33;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get_slice needs to support desc from last column,CASSANDRA-263,12429006,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,junrao,lenn0x,lenn0x,27/Jun/09 06:55,16/Apr/19 17:33,22/Mar/23 14:57,31/Jul/09 23:06,0.4,,,,0,,,,,,At the moment there's no way to ask for a slice starting with the last column and going desc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Jul/09 07:24;junrao;issue263.patchv1;https://issues.apache.org/jira/secure/attachment/12415072/issue263.patchv1",,,,,,,,,,,,,,1.0,junrao,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19613,,,Sat Aug 01 12:34:00 UTC 2009,,,,,,,,,,"0|i0fxvb:",91085,,,,,Normal,,,,,,,,,,,,,,,,,"27/Jun/09 07:31;junrao;Is the problem that you don't know what the last column is? Normally, an application should be able to insert a fake column that's always larger than any real columns. Do you think that's good enough?;;;","27/Jun/09 08:22;jbellis;Yes.

I was thinking that we could have the ""CF"" format (w/ no "":"") stand for ""start w/ first if asc or last if desc.""  Reading the last from the column index should be reasonably efficient, right?;;;","30/Jun/09 01:09;junrao;Using the column index to start with the last column group is possible and is efficient. I am just wondering if it is a good idea to give empty starting column different meanings depending on the ordering.;;;","30/Jun/09 01:18;jbellis;I'm open to alternative APIs, but that one makes sense to me and doesn't require adding extra parameters :);;;","31/Jul/09 07:24;junrao;Attach a fix. If startColumn is empty and isAscending is false, assume that we scan from the largest column in descending order.

Also fix a bug in SliceFilterQuery.filterSuperColumn() that didn't handle descending order properly.
;;;","31/Jul/09 23:06;jbellis;committed, thanks!;;;","01/Aug/09 20:34;hudson;Integrated in Cassandra #154 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/154/])
    allow start of [] to mean ""start with the largest value"" when ascending=false.  patch by Jun Rao; reviewed by jbellis for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
invalid column name length 0,CASSANDRA-2518,12504753,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,lichenglingl,lichenglingl,20/Apr/11 12:03,16/Apr/19 17:33,22/Mar/23 14:57,10/Aug/11 03:48,,,,,0,,,,,,"one of the three nodes cassandra 0.7.3 report error after start up:
ERROR [CompactionExecutor:1] 2011-04-16 22:18:39,281 PrecompactedRow.java (line 82) Skipping row DecoratedKey(3813860378406449638560060231106122758, 79616e79776275636b65743030303030303030312f6f626a303030303030323534) in /opt/cassandra/data/Keyspace/cf-f-4715-Data.db
org.apache.cassandra.db.ColumnSerializer$CorruptColumnException: invalid column name length 0
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:68)
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:35)
        at org.apache.cassandra.db.ColumnFamilySerializer.deserializeColumns(ColumnFamilySerializer.java:129)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.getColumnFamilyWithColumns(SSTableIdentityIterator.java:176)
        at org.apache.cassandra.io.PrecompactedRow.<init>(PrecompactedRow.java:78)
        at org.apache.cassandra.io.CompactionIterator.getCompactedRow(CompactionIterator.java:139)
        at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:108)
        at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:43)
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:73)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
        at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
        at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
        at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:449)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:124)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:94)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
and few minutes later,
ERROR [CompactionExecutor:1] 2011-04-16 22:20:20,073 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.OutOfMemoryError: Java heap space
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.readBytes(BufferedRandomAccessFile.java:267)
        at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:310)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithLength(ByteBufferUtil.java:267)
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:94)
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:35)
        at org.apache.cassandra.db.ColumnFamilySerializer.deserializeColumns(ColumnFamilySerializer.java:129)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.getColumnFamilyWithColumns(SSTableIdentityIterator.java:176)
        at org.apache.cassandra.io.PrecompactedRow.<init>(PrecompactedRow.java:78)
        at org.apache.cassandra.io.CompactionIterator.getCompactedRow(CompactionIterator.java:139)
        at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:108)
        at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:43)
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:73)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
        at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
        at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
        at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:449)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:124)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:94)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
and we restart cassandra several times, but OOM like above also.","three nodes, 
JVM:
-XX:+UseThreadPriorities -XX:ThreadPriorityPolicy=42 -Xms6G -Xmx6G -Xmn2400M -XX:+HeapDumpOnOutOfMemoryError -Xss128k -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:SurvivorRatio=8 -XX:MaxTenuringThreshold=1 -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -Djava.net.preferIPv4Stack=true",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20671,,,Tue Aug 09 19:48:25 UTC 2011,,,,,,,,,,"0|i0gbu7:",93348,,,,,Normal,,,,,,,,,,,,,,,,,"21/Apr/11 12:27;lichenglingl;cf-f-4715-Data.db is 19GB.
After we changed JVM to -Xms16G -Xmx16G -Xmn6400M ,cassandra not OOM again.But more data files ,cf-f-*-Data.db size 20M per,had been created crazy when we inserted data,and error log ""invalid column name length 0"" appeard sometimes.;;;","10/Aug/11 03:48;jbellis;probably CASSANDRA-2675, fixed in 0.7.7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
make cassandra not allow itself to run out of memory during sustained inserts,CASSANDRA-157,12424977,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,daishi,daishi,09/May/09 06:42,16/Apr/19 17:33,22/Mar/23 14:57,28/Jul/09 06:41,,,,,0,,,,,,"Tv on IRC pointed out to me that the issue that I've been encountering
is probably point 2. in this roadmap:

    http://www.mail-archive.com/cassandra-dev@incubator.apache.org/msg00160.html

I was unable to find any existing issue for this topic, so I'm creating a new one.

Since this issue would block our use of Cassandra I'm happy to look into it,
but if this is a known issue perhaps there's already a plan for addressing it
that could be clarified?",,johanoskarsson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Jul/09 06:23;jbellis;157.patch;https://issues.apache.org/jira/secure/attachment/12414671/157.patch","13/May/09 01:15;daishi;Cassandra-157_Unregister_Memtable_MBean.diff;https://issues.apache.org/jira/secure/attachment/12407898/Cassandra-157_Unregister_Memtable_MBean.diff",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19574,,,Tue Jul 28 13:29:27 UTC 2009,,,,,,,,,,"0|i0fx7z:",90980,,,,,Normal,,,,,,,,,,,,,,,,,"09/May/09 08:44;jbellis;if you're running out of memory, start by decreasing these settings in conf/storage-conf.  svn up first because objectcount used to default to 0.1 (i.e. 100k objects).

    <!--
      The maximum amount of data to store in memory before flushing to
      disk. Note: There is one memtable per column family, and this threshold
      is based solely on the amount of data stored, not actual heap memory
      usage (there is some overhead in indexing the columns).
    -->
    <MemtableSizeInMB>32</MemtableSizeInMB>

    <!--
      The maximum number of columns in millions to store in memory
      before flushing to disk.  This is also a per-memtable setting.
      Use with MemtableSizeInMB to tune memory usage.
    -->
    <MemtableObjectCountInMillions>0.02</MemtableObjectCountInMillions>

;;;","09/May/09 08:49;jbellis;Ah, I see you've had this discussion with urandom on irc.

Like he said, play with jconsole, it may be your limits are still not low enough or possibly there is a bug in its internal accounting.;;;","12/May/09 01:32;urandom;I think we need a wiki page which details what we've learned about tuning thresholds (and yes, I suppose I am volunteerin :).;;;","13/May/09 01:15;daishi;This patch removes the MBean reference to Memtable objects,
reducing the memory pressure under sustained inserts somewhat.
(This helps, but I think there are other remaining issues).;;;","13/May/09 01:56;jbellis;great fix!  committed.;;;","13/May/09 17:26;hudson;Integrated in Cassandra #73 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/73/])
    unregister mbean on flush.  patch by daishi; reviewed by jbellis for 
;;;","16/May/09 03:44;urandom;http://wiki.apache.org/cassandra/MemtableThresholds;;;","05/Jun/09 06:41;jbellis;Is this still an issue?;;;","05/Jun/09 09:14;daishi;Sorry I haven't had a chance to follow up on this.

If I remember correctly the main unbounded memory growth
that caused OOMs did seem to go away, but that the exposed
properties didn't control the memory usage in a predictable way.

In particular I think the objCount property would schedule
a Memtable.flush(), but ColumnFamilyStore.memtablesPendingFlush
would be actually flushed according to a different schedule.
I haven't read enough of the code to know whether it would
be possible/make sense to allow user-control of latter schedule,
but that was the direction I was thinking in when I had to drop
this for a bit (maybe you can let me know if that makes no
sense or if there's a better way to think about things).
;;;","05/Jun/09 09:18;jbellis;> ColumnFamilyStore.memtablesPendingFlush  would be actually flushed according to a different schedule

flushes are processed one at a time, first come first served, to avoid swamping the system with IO.

does that jive with what you saw?;;;","05/Jun/09 09:42;daishi;> flushes are processed one at a time, first come first served, to avoid swamping the system with IO.
> does that jive with what you saw? 

I guess that depends on what ""first come first served"" means.
Basically what I observed was that I could control how
frequently Memtable.flush() happened, but that only
after N memtables had been ""queued up"" to flush
would they actually be flushed to disk (I presume -
my view is based only on observing memory usage).
I think N was 10-20, but I don't remember exactly right now.
;;;","05/Jun/09 10:04;jbellis;That's strange.  Let me know if you can reproduce that behavior still with 0.3 or trunk.  It shouldn't be queueing up unless the flusher is busy with a different memtable.;;;","28/Jul/09 06:23;jbellis;Found the (a?) problem.

memtablesPendingFlush used NonBlockingHashSets to store the memtables-to-flush.  But NBHS uses a NBHMap under the hood, which when remove() is called, assigns a tombstone value to the key instead of actually removing it.  (See http://sourceforge.net/tracker/?func=detail&aid=2828100&group_id=194172&atid=948362.)
;;;","28/Jul/09 06:34;brandon.williams;+1  The heap usage is much improved for me with this patch.;;;","28/Jul/09 06:41;jbellis;committed;;;","28/Jul/09 21:29;hudson;Integrated in Cassandra #151 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/151/])
    use CSLS instead of NBHS for memtablesPendingFlush. See explanation here: http://sourceforge.net/tracker/?func=detail&aid=2828100&group_id=194172&atid=948362
patch by jbellis; reviewed by Brandon Williams for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
secondary indexes aren't created on pre-existing or streamed data,CASSANDRA-2244,12499629,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,25/Feb/11 03:29,16/Apr/19 17:33,22/Mar/23 14:57,04/Mar/11 01:42,0.7.4,,Feature/2i Index,,0,,,,,,"The repaired node neither receives indexes from the replicas, nor does it generate them afterwards.  The same bug prevents generation of new indexes against existing data.",,jborgstrom,skamio,stuhood,,,,,,,,,,,,,,,,,,,57600,57600,,0%,57600,57600,,,,,,,,,,,,,,,,,,,,"04/Mar/11 01:04;jbellis;2244-v2.txt;https://issues.apache.org/jira/secure/attachment/12472561/2244-v2.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20523,,,Thu Mar 03 17:55:22 UTC 2011,,,,,,,,,,"0|i0ga7r:",93085,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"28/Feb/11 15:31;jbellis;Doesn't repair use normal Streaming?  That should take care of generating the index CFs.;;;","02/Mar/11 07:33;brandon.williams;I added some logging, it does do something after streaming:

{noformat}
 INFO 23:27:32,050 Opening /var/lib/cassandra/data/Keyspace1/Standard1-f-1
 INFO 23:27:34,843 Opening /var/lib/cassandra/data/Keyspace1/Standard1-f-2
 INFO 23:27:35,232 Opening /var/lib/cassandra/data/Keyspace1/Standard1-f-3
 INFO 23:27:35,246 Building index for ColumnFamilyStore(table='Keyspace1', columnFamily='Standard1') [SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1-f-1-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1-f-2-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1-f-3-Data.db')] [java.nio.HeapByteBuffer[pos=0 lim=2 cap=2]]
 INFO 23:29:53,448 Finished streaming session 19283352579402862 from /10.179.65.102
{noformat}

During the ~2.5 minutes between the last 2 lines it appeared to generate the index, however flushing afterwards shows no index was generated:

{noformat}
 INFO 23:32:06,591 switching in a fresh Memtable for LocationInfo at CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1299022015229.log', position=12763)
 INFO 23:32:06,592 Enqueuing flush of Memtable-LocationInfo@1459852990(35 bytes, 1 operations)
 INFO 23:32:06,593 Writing Memtable-LocationInfo@1459852990(35 bytes, 1 operations)
 INFO 23:32:06,623 Completed flushing /var/lib/cassandra/data/system/LocationInfo-f-3-Data.db (89 bytes)
{noformat}


;;;","04/Mar/11 00:34;jbellis;The ""don't bother flushing dropped CFs"" check was unaware that index CFs are never part of the global CF metadata.

(I think that check is race-prone to begin with, so we should probably drop it and do something more robust, but for now it's better than nothing -- it will *usually* prevent creating new sstables for dropped CFs.);;;","04/Mar/11 01:04;jbellis;Gary points out that migration does acquire the flushlock.;;;","04/Mar/11 01:04;jbellis;patch adds test that catches the problem, and adds fix to CFS.;;;","04/Mar/11 01:42;brandon.williams;+1, committed;;;","04/Mar/11 01:55;hudson;Integrated in Cassandra-0.7 #342 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/342/])
    CFS correctly flushes index CFs.
Patch by jbellis, reviewed by brandonwilliams for CASSANDRA-2244
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Loadbalance during gossip issues leaves cluster in bad state,CASSANDRA-1895,12493949,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,stuhood,stuhood,23/Dec/10 15:31,16/Apr/19 17:33,22/Mar/23 14:57,17/Jan/11 07:54,0.7.1,,,,0,,,,,,Running loadbalance against a node in a 4 node cluster leaves gossip in a wonky state.,,,,,,,,,,,,,,,,,,,,,,,28800,28800,,0%,28800,28800,,,,,,,,,,,,,,,,,,,,"04/Jan/11 06:39;brandon.williams;1895.txt;https://issues.apache.org/jira/secure/attachment/12467374/1895.txt","23/Dec/10 15:33;stuhood;logs.tgz;https://issues.apache.org/jira/secure/attachment/12466861/logs.tgz","23/Dec/10 15:33;stuhood;ring-views.txt;https://issues.apache.org/jira/secure/attachment/12466862/ring-views.txt",,,,,,,,,,,,3.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20360,,,Mon Jan 17 00:37:15 UTC 2011,,,,,,,,,,"0|i0g82n:",92738,,,,,Low,,,,,,,,,,,,,,,,,"23/Dec/10 15:33;stuhood;Attaching logs and {{nodetool ring}} output from after running loadbalance against 10.97.29.122.;;;","23/Dec/10 23:31;jbellis;trunk, or 0.7 branch?;;;","24/Dec/10 01:56;nickmbailey;this looks very similar to CASSANDRA-1895;;;","24/Dec/10 02:04;nickmbailey;It also looks like CASSANDRA-1829 :);;;","24/Dec/10 02:24;stuhood;This run is from trunk. I'm traveling today, but I should be able to try it against 0.7 when I get settled this evening.;;;","24/Dec/10 03:59;brandon.williams;I'm unable to repro against 0.7.;;;","24/Dec/10 04:00;tjake;Based on the ring and .122 log it looks like you were missing fix for CASSANDRA-1829

If see this in the log: "" INFO [RMI TCP Connection(2)-10.97.29.122] 2010-12-22 01:33:47,328 StorageService.java (line 249) Bootstrap/move completed! Now serving reads.""

Then the local ring state would say Normal since finishBootstrap() calls setToken() which sets the ring state to normal. hmm.
;;;","24/Dec/10 04:03;tjake;In fact, based on the log message above I can see you didn't have the fix in palace since the line is now 250 after the fix for the above message:

http://svn.apache.org/viewvc/cassandra/trunk/src/java/org/apache/cassandra/service/StorageService.java?r1=1043262&r2=1044034;;;","25/Dec/10 14:34;stuhood;It looks like the patch you mentioned was only partially applied to trunk. The last chunk in the patch should have added {code}+        if(!bootstrapped)
+            setToken(token);{code}
...but didn't.;;;","25/Dec/10 14:39;nickmbailey;Stu,

That  if check was removed in a later commit. The current code will call setToken() no matter what. The only difference is if the node bootstrapped, setToken will be called twice but that shouldn't be a problem and I don't think that is causing the bug here.;;;","28/Dec/10 15:58;stuhood;I think the burden of proof is back on me now, so I'll try and reproduce this once I'm back in the office tomorrow.;;;","29/Dec/10 10:24;stuhood;After a rebase and changes to the EC2 images I was using, I'm no longer able to reproduce this against either 0.7 or trunk.

Nick noticed some gossip related problems in the attached logs, so I'm going to chalk this up to _either_ a bad rebase, or problems related to bootstrapping during gossip problems. Nick: could you chime in with details, and whether you think those gossip issues might be worth pursuing?;;;","30/Dec/10 03:34;nickmbailey;I just noticed that the logs on one of the machines contains a ""Removing token"" line followed by a new node line 3 times. Since Stu only called loadbalance once, I believe this is likely due to gossip issues, where a node completes the decom then gets gossip about the same node leaving and readds it.

I've also talked to one other person in irc who had a similar problem.  Every time a decommission happened, the node would get removed but added 30 seconds later (gossip timeout after removal) due to some other node in the cluster.  ;;;","30/Dec/10 05:33;gdusbabek;We've had this problem before.  I think it was CASSANDRA-1467, but not sure.;;;","31/Dec/10 04:40;stuhood;Is this the sort of problem where having a gossip generation might help? The node that readded an older node should have ignored the outdated gossip.;;;","31/Dec/10 05:55;nickmbailey;We have generations for node states. The problem is after 30 seconds we assume gossip has propagated and forget about nodes that have been removed.  Then when we see the gossip again it looks like a brand new node.;;;","31/Dec/10 06:10;jbellis;sounds like the inverse of CASSANDRA-1730;;;","04/Jan/11 06:39;brandon.williams;Revival of my first try at CASSANDRA-1730: change the quarantine interval for justRemovedEndpoints to RING_DELAY * 2.;;;","04/Jan/11 07:03;nickmbailey;Would it be a good idea to make this configurable? I've seen nodes that have failed to process a REMOVED state for up to 45 minutes, although that was one extreme case. It might be useful in a case like that to temporarily configure this very high in order to process ring operations when the cluster is having issues otherwise.;;;","04/Jan/11 07:08;brandon.williams;I'm guessing in the 45 minute case, what was happening was the state was being re-gossiped just outside of RING_DELAY many times, but finally it propagated fast enough.  I think the next step, if this isn't sufficient, is to expose RING_DELAY as a tunable.;;;","15/Jan/11 13:11;jbellis;would it make more sense to change FatClientTimeout to QUARANTINE_DELAY / 2?;;;","17/Jan/11 02:47;brandon.williams;It wouldn't hurt, though we established in CASSANDRA-1730 the timeout was rather arbitrary.  It could help in the case of a 'flapping' bootstrap, though I tend to think there will be problems there in any case.;;;","17/Jan/11 07:10;jbellis;let's commit w/ that change then;;;","17/Jan/11 07:54;brandon.williams;Committed w/fatclient timeout change.;;;","17/Jan/11 08:37;hudson;Integrated in Cassandra-0.7 #165 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/165/])
    Set quarantine delay to RING_DELAY * 2
Patch by brandonwilliams, reviewed by jbellis for CASSANDRA-1895
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
determine legal keyspace/column family names and validate,CASSANDRA-1185,12466818,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Duplicate,,urandom,urandom,12/Jun/10 23:26,16/Apr/19 17:33,22/Mar/23 14:57,18/Dec/10 10:35,,,,,0,,,,,,"Since keyspace and column family names are used to derive the directory and filenames where data is stored, so some thought should be given to which characters are legally allowed. Then, node-side validation should prevent the creation of namespaces that contain these characters, and produce a meaningful error message in the process.",,hammer,,,,,,,,,,,,,,,,,,,,,0,0,,0%,0,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20027,,,Sat Dec 18 02:35:32 UTC 2010,,,,,,,,,,"0|i0g3in:",92000,,,,,Low,,,,,,,,,,,,,,,,,"13/Jun/10 00:09;jeromatron;Is there a way we could divorce names in Cassandra from names on the filesystem?  It just seems like an odd implementation specific thing.

Maybe marking up special characters in a standard way?

I'm not sure why they are ties together in the first place though.

No matter what though validation and error messages are needed- I was just curious.;;;","13/Jun/10 01:25;urandom;bq. Is there a way we could divorce names in Cassandra from names on the filesystem? It just seems like an odd implementation specific thing.

You could, but I think this is a case where the cure would be worse than the disease. It's rather nice to be able to map keyspace and column family names to their respective locations in the filesystem, and I suspect that a lot of people have come to rely on this.;;;","18/Dec/10 10:35;urandom;I believe this is done (don't know which issue), with the result that names are \w.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Assert in RandomPartitioner causes unending compaction,CASSANDRA-1300,12469640,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,djnym,djnym,20/Jul/10 02:09,16/Apr/19 17:33,22/Mar/23 14:57,20/Jul/10 03:51,,,,,0,,,,,,"I have a set of 4 sstables which equal about 600GB of data, whenever they are compacted after about 2 days I get this exception

ERROR [COMPACTION-POOL:1] 2010-07-17 19:09:49,784 CassandraDaemon.java (line 83) Uncaught exception in thread Thread[COMPACTION-POOL:1,5,main]                  
java.util.concurrent.ExecutionException: java.lang.StringIndexOutOfBoundsException: String index out of range: -1       
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)   
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)              
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(Debug+gableThreadPoolExecutor.java:86)                                               
        at org.apache.cassandra.db.CompactionManager$CompactionExecutor.afterExecute(CompactionManager.java:577)                                                         
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)                                                                           
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)Caused by: java.lang.StringIndexOutOfBoundsException: String index out of range: -1       
        at java.lang.String.substring(String.java:1938)                         
        at org.apache.cassandra.dht.RandomPartitioner.convertFromDiskFormat(RandomPartitioner.java:50)                                                                   
        at org.apache.cassandra.io.IteratingRow.<init>(IteratingRow.java:48)    
        at org.apache.cassandra.io.SSTableScanner$KeyScanningIterator.next(SSTableScanner.java:136)                                                                      
        at org.apache.cassandra.io.SSTableScanner$KeyScanningIterator.next(SSTableScanner.java:113)                                                                      
        at org.apache.cassandra.io.SSTableScanner.next(SSTableScanner.java:105) 
        at org.apache.cassandra.io.SSTableScanner.next(SSTableScanner.java:34)  
        at org.apache.commons.collections.iterators.CollatingIterator.set(CollatingIterator.java:284)                                                                    
        at org.apache.commons.collections.iterators.CollatingIterator.least(CollatingIterator.java:326)                                                                  
        at org.apache.commons.collections.iterators.CollatingIterator.next(CollatingIterator.java:230)                                                                   
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:68)                                                                             
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:135)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:130)  
        at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)                                                                
        at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)                                                                       
        at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:295)                                                                            
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:102)   
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:83)    
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)   
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)             
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)                                                                           
        ... 2 more  

After this expection, the compaction starts over and then crashes again.   This led to several thousand outstanding sstables hanging out waiting to be compacted.
                                                                             ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20069,,,Mon Jul 19 21:10:19 UTC 2010,,,,,,,,,,"0|i0g47z:",92114,,,,,Normal,,,,,,,,,,,,,,,,,"20/Jul/10 03:51;jbellis;Duplicate of CASSANDRA-1170.

Short version: delete the bad sstable and repair.;;;","20/Jul/10 04:23;djnym;So looking at the other bug it seems like the way to detect a bad sstable is to run sstablekeys and see if there is an exception thrown?

If so, I did that on the 4 -Data.db files and got no exceptions.  However, it looks like the -Index.db and -Filter.db files are also sstables, so should I run on each of them?  Also, I assume its then okay to just delete the bad sstable (ie, if I find a bad -Index.db delete it, then repair), or do I need to delete all 3 files (ie, if 3-Index.db is bad, do I delete 3-Data.db, 3-Filter.db and 3-Index.db)?;;;","20/Jul/10 05:10;jbellis;sstable2json is what you use to test the data file;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Easy to OOM on log replay since memtable limits are ignored,CASSANDRA-609,12442690,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,08/Dec/09 10:09,16/Apr/19 17:33,22/Mar/23 14:57,09/Dec/09 12:55,0.5,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Dec/09 01:10;jbellis;609-2.patch;https://issues.apache.org/jira/secure/attachment/12427350/609-2.patch","08/Dec/09 10:32;jbellis;609.patch;https://issues.apache.org/jira/secure/attachment/12427281/609.patch",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19778,,,Wed Dec 09 12:42:41 UTC 2009,,,,,,,,,,"0|i0fzzj:",91428,,,,,Normal,,,,,,,,,,,,,,,,,"08/Dec/09 10:32;jbellis;respect memtable thresholds when replaying commit log; use normal write path, but add a writeCommitLog flag so we don't re-write out another CL entry for the replay;;;","09/Dec/09 00:28;jbellis;To clarify: under low load, you won't much exceed a single memtable's worth of inserts since when a memtable flushes it marks the commitlog header as ""start replay from here.""  But under high load, you can have a significant amount of inserts done while flushes are queued up in sort + write executors, and this is where you run into trouble on replay.;;;","09/Dec/09 00:35;junrao;During recovery, we used to rely on CFS.switchMemtable to call onMemtableFlush(ctx) to discard commit log files. With this patch, onMemtableFlush(ctx) won't be called during recovery. When will the commit log files be deleted?
;;;","09/Dec/09 01:10;jbellis;I was thinking that the final table.flush is a ""normal"" flush so that should take care of it.  But thinking about it more we do have a corner case of ""the last write in the log happened to exactly hit the threshold and triggered a no-discard flush, so the final flush saw a clean memtable and did nothing.""

Patch 2 (applies on top of first) addresses this.  (Most of the patch is converting CL to a true singleton so replay has access to the normal context operations w/o ugly hacks.);;;","09/Dec/09 09:13;junrao;My bad. I got confused about how log files are deleted during recovery. It turns out that all log files are deleted explicitly in RecoveryManager.doRecovery(). The deletion doesn't rely on the flushing logic. So even v1 of the patch is fine.;;;","09/Dec/09 09:36;jbellis;I didn't know that, either.  I think that's a bug -- if something goes wrong during recovery we definitely shouldn't be deleting data that hasn't been replayed.  I'd feel safer taking that out, what do you think?;;;","09/Dec/09 10:15;junrao;Currently, CL.discardCompletedSegments() doesn't really discard log files properly during recovery. The problem is that clHeaders_ is never populated during recovery.

Deleting log files explicitly during recovery may not be that bad. If anything goes wrong during recovery, we will get either an IOException or a RuntimeException. In either case, log files won't be deleted.;;;","09/Dec/09 12:47;jbellis;You're right, because it assumes that when CL is instantiated it's starting fresh.  I'll commit patch 1.;;;","09/Dec/09 20:42;hudson;Integrated in Cassandra #282 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/282/])
    respect memtable thresholds when replaying commit log
patch by jbellis; reviewed by Jun Rao for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid dropping messages off the client request path,CASSANDRA-1676,12478591,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,28/Oct/10 23:36,16/Apr/19 17:33,22/Mar/23 14:57,01/Nov/10 12:07,0.6.7,0.7.0 rc 1,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Oct/10 08:31;jbellis;1676-v2.txt;https://issues.apache.org/jira/secure/attachment/12458496/1676-v2.txt","28/Oct/10 23:37;jbellis;1676.txt;https://issues.apache.org/jira/secure/attachment/12458255/1676.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20254,,,Fri Jan 07 10:01:25 UTC 2011,,,,,,,,,,"0|i0g6pb:",92516,,stuhood,,stuhood,Normal,,,,,,,,,,,,,,,,,"29/Oct/10 12:31;jbellis;Brandon, can you beat up a cluster badly enough to start dropping repair messages like this, now that http://issues.apache.org/jira/browse/CASSANDRA-1677 shows what exactly we're dropping?  AS mentioned in CASSANDRA-1674 we're not really sure what is happening here so I don't want to commit this until we understand that better.;;;","31/Oct/10 08:31;jbellis;rebased and updated post-CASSANDRA-1685 to never drop INTERNAL_RESPONSE messages;;;","01/Nov/10 05:36;stuhood;It doesn't look like CASSANDRA-1685 actually made it in to trunk, so this doesn't compile.;;;","01/Nov/10 07:11;jbellis;patch is against 0.7.;;;","01/Nov/10 11:24;stuhood;+1;;;","01/Nov/10 12:07;jbellis;committed, w/ the slight modification for 0.6 of not dropping any _RESPONSE calls at all since CASSANDRA-1685 isn't backported there.  should be a mostly academic difference since REQUEST_RESPONSE should be essentially instantaneous.;;;","07/Jan/11 18:01;rantav;This bug fixes the ""hanged bootstrap"" problem some which some clusters would see when bootstrapping new nodes. 
See the following threads for more details 
http://www.mail-archive.com/user@cassandra.apache.org/msg07106.html
http://www.mail-archive.com/user@cassandra.apache.org/msg08379.html
http://www.mail-archive.com/user@cassandra.apache.org/msg08441.html;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to start Cassandra in windows if P drive exsists,CASSANDRA-824,12457227,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,gdusbabek,kazw,kazw,24/Feb/10 06:02,16/Apr/19 17:33,22/Mar/23 14:57,24/Feb/10 23:40,0.6,,,,0,Missing-Class,P-Drive,win32,windows,wont-start,"When running bin\cassandra.bat from main dir, Cassandra exits with:
Invalid parameter - P:
Starting Cassandra Server
Listening for transport dt_socket at address: 8888
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/cassandra/service/CassandraDaemon
Caused by: java.lang.ClassNotFoundException: org.apache.cassandra.service.CassandraDaemon
        at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:307)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:248)
Could not find the main class: org.apache.cassandra.service.CassandraDaemon.  Program will exit.",This problem should affect any version of windows with a P drive.,,,,,,,,,,,,,,,,,,,,,,0,0,,0%,0,0,,,,,,,,,,,,,,,,,,,,"24/Feb/10 16:05;wolfeidau;cassandra-bat-tidy.patch;https://issues.apache.org/jira/secure/attachment/12436832/cassandra-bat-tidy.patch",,,,,,,,,,,,,,1.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19880,,,Wed Feb 24 15:40:43 UTC 2010,,,,,,,,,,"0|i0g1av:",91641,,,,,Low,,,,,,,,,,,,,,,,,"24/Feb/10 06:06;kazw;Changing the following lines in bin\cassandra.bat will fix this issue (Substitute ""Q"" for any available drive letter):

subst Q: ""%CASSANDRA_HOME%\lib""
Q:
set CLASSPATH=Q:\

for %%i in (*.jar) do call :append %%i
goto okClasspath

:append
set CLASSPATH=%CLASSPATH%;Q:\%*
goto :eof;;;","24/Feb/10 16:05;wolfeidau;Patch to correct the issue.

I have removed the use of subst which  was mapping the P: this is unneccessary on newer windows releases.

I will give it a go on some test machines at work so far this tests fine on windows 7.;;;","24/Feb/10 23:24;gdusbabek;That patch didn't work perfectly for me.  It wasn't including the jar files located in %CASSANDRA_HOME%/build/lib/jars.  Once I added that, everything was good.

+1 on this patch.;;;","24/Feb/10 23:40;gdusbabek;r915824 (0.6), r915828 (trunk);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handle mmapping index files greater than 2GB,CASSANDRA-669,12444719,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,05/Jan/10 23:57,16/Apr/19 17:33,22/Mar/23 14:57,07/Jan/10 10:58,0.6,,,,0,,,,,,"""Who would ever have an index file larger than 2GB?"" I thought.  Turns out it's not that hard with narrow rows... :)",,brandon.williams,hammer,johanoskarsson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Jan/10 05:43;jbellis;ASF.LICENSE.NOT.GRANTED--0001-store-data-information-for-any-index-entries-spanning-.txt;https://issues.apache.org/jira/secure/attachment/12429581/ASF.LICENSE.NOT.GRANTED--0001-store-data-information-for-any-index-entries-spanning-.txt","07/Jan/10 05:43;jbellis;ASF.LICENSE.NOT.GRANTED--0002-add-support-for-multiple-mmapped-index-segments-and-ad.txt;https://issues.apache.org/jira/secure/attachment/12429582/ASF.LICENSE.NOT.GRANTED--0002-add-support-for-multiple-mmapped-index-segments-and-ad.txt","07/Jan/10 05:43;jbellis;ASF.LICENSE.NOT.GRANTED--0003-instead-of-providing-a-RandomAccessFile-like-interface.txt;https://issues.apache.org/jira/secure/attachment/12429583/ASF.LICENSE.NOT.GRANTED--0003-instead-of-providing-a-RandomAccessFile-like-interface.txt",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19811,,,Fri Jan 08 12:38:35 UTC 2010,,,,,,,,,,"0|i0g0cv:",91488,,,,,Normal,,,,,,,,,,,,,,,,,"06/Jan/10 05:14;jbellis;02
    add support for multiple mmapped index segments, and add mmap_index_only option

01
    store data information for any index entries spanning a mmap segment boundary when reading the index (with a BufferedRAF)
;;;","06/Jan/10 07:21;brandon.williams;Received the following traceback after testing (during compaction):

ERROR - Error in executor futuretask
java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: Negative position
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
        at java.util.concurrent.FutureTask.get(FutureTask.java:111)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:53)
        at org.apache.cassandra.db.CompactionManager$CompactionExecutor.afterExecute(CompactionManager.java:591)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1118)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.IllegalArgumentException: Negative position
        at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:739)
        at org.apache.cassandra.io.SSTableReader.mmap(SSTableReader.java:273)
        at org.apache.cassandra.io.SSTableReader.<init>(SSTableReader.java:234)
        at org.apache.cassandra.io.SSTableWriter.closeAndOpenReader(SSTableWriter.java:157)
        at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:307)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:102)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:83)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
;;;","06/Jan/10 11:54;jbellis;heh, that's actually a 2nd bug.  patch 03 attached to fix.;;;","06/Jan/10 13:02;jbellis;fix the -- in the xml in 02, and change BUFFER_SIZE to long in 03 as well to force the promotion of the position parameter.;;;","06/Jan/10 23:35;brandon.williams;+1;;;","07/Jan/10 00:07;brandon.williams;Oops, nevermind my +1, receiving the following traceback when trying to get_slice now:

Caused by: java.lang.AssertionError
        at org.apache.cassandra.io.util.MappedFileDataInput.length(MappedFileDataInput.java:36)
        at org.apache.cassandra.io.util.MappedFileDataInput.read(MappedFileDataInput.java:52)
        at java.io.InputStream.read(InputStream.java:171)
        at org.apache.cassandra.io.util.MappedFileDataInput.readUnsignedShort(MappedFileDataInput.java:358)
        at java.io.DataInputStream.readUTF(DataInputStream.java:589)
        at org.apache.cassandra.io.util.MappedFileDataInput.readUTF(MappedFileDataInput.java:381)
        at org.apache.cassandra.io.SSTableReader.getPosition(SSTableReader.java:419)
        at org.apache.cassandra.io.SSTableReader.getFileDataInput(SSTableReader.java:537)
        at org.apache.cassandra.db.filter.SSTableSliceIterator.<init>(SSTableSliceIterator.java:54)
        at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:63)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamilyInternal(ColumnFamilyStore.java:859)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:817)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:786)
        at org.apache.cassandra.db.Table.getRow(Table.java:405)
        at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:59)
        at org.apache.cassandra.service.StorageProxy$weakReadLocalCallable.call(StorageProxy.java:692)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        ... 3 more
;;;","07/Jan/10 01:24;jbellis;patch 04 to fix.;;;","07/Jan/10 05:44;jbellis;... latest patches fix the fd leak and index reading bugs, and rebased bugfixes into 01 and 02.;;;","07/Jan/10 10:50;brandon.williams;+1, for real this time.  40M narrow keys used to create a 2.2GB index when fully compacted, all is well.;;;","07/Jan/10 10:58;jbellis;committed;;;","07/Jan/10 21:08;hudson;Integrated in Cassandra #316 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/316/])
    instead of providing a RandomAccessFile-like interface in FileDataInput implementing seek and trying to keep people from shooting themselves in the foot by forgetting that it may only represent a 2GB segment of a larger file, provide an InputStream-like interface emphasizing mark/reset
patch by jbellis; tested by Brandon Williams for 
add support for multiple mmapped index segments, and add mmap_index_only option
patch by jbellis; tested by Brandon Williams for 
store data information for any index entries spanning a mmap segment boundary when reading the index (with a BufferedRAF) so we don't have to deal with that at read time.
patch by jbellis; tested by Brandon Williams for 
;;;","08/Jan/10 20:38;hudson;Integrated in Cassandra #317 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/317/])
    fix bad rebase causing regression in the  patchset (use index path to open index file).  patch by jbellis
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Server fails to join cluster if IPv6 only,CASSANDRA-969,12461722,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,gdusbabek,cody.lerum,cody.lerum,11/Apr/10 02:16,16/Apr/19 17:33,22/Mar/23 14:57,14/Apr/10 21:37,0.6.1,,,,0,,,,,,"When configuring Cassandra for IPv6 connectivity on the server to server side the addition of a second node causes the both servers to loop on ArrayIndexOutOfBoundsExection for 5 minutes

The first server has 

Caused by: java.lang.ArrayIndexOutOfBoundsException: 65536
        at org.apache.cassandra.net.HeaderSerializer.deserialize(Header.java:155)

While the second has

Caused by: java.lang.ArrayIndexOutOfBoundsException: 131072
        at org.apache.cassandra.net.HeaderSerializer.deserialize(Header.java:155)

the index is double.

These servers work find in a cluster together if they are configured IPv4

server1 in the output is 2607:f3d0:0:2::16
server2 is 2607:f3d0:0:1::f","Ubuntu 9.10 x64
java: Java(TM) SE Runtime Environment (build 1.6.0_15-b03)
cassandra 0.6.0-rc1",cody.lerum,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Apr/10 02:25;cody.lerum;ASF.LICENSE.NOT.GRANTED--cassandra-v6-2-filtered.pcap;https://issues.apache.org/jira/secure/attachment/12441347/ASF.LICENSE.NOT.GRANTED--cassandra-v6-2-filtered.pcap","11/Apr/10 02:18;cody.lerum;ASF.LICENSE.NOT.GRANTED--cassandra-v6.pcap;https://issues.apache.org/jira/secure/attachment/12441346/ASF.LICENSE.NOT.GRANTED--cassandra-v6.pcap","12/Apr/10 21:01;gdusbabek;ASF.LICENSE.NOT.GRANTED--encode_decode_ipv6_addresses_safely.txt;https://issues.apache.org/jira/secure/attachment/12441480/ASF.LICENSE.NOT.GRANTED--encode_decode_ipv6_addresses_safely.txt","11/Apr/10 02:16;cody.lerum;ASF.LICENSE.NOT.GRANTED--server1.log;https://issues.apache.org/jira/secure/attachment/12441344/ASF.LICENSE.NOT.GRANTED--server1.log","11/Apr/10 02:16;cody.lerum;ASF.LICENSE.NOT.GRANTED--server2.log;https://issues.apache.org/jira/secure/attachment/12441345/ASF.LICENSE.NOT.GRANTED--server2.log",,,,,,,,,,5.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19942,,,Wed Apr 14 04:40:26 UTC 2010,,,,,,,,,,"0|i0g273:",91786,,,,,Low,,,,,,,,,,,,,,,,,"11/Apr/10 02:16;cody.lerum;system.log files for both servers;;;","11/Apr/10 02:18;cody.lerum;wireshark capture of the networking traffic on server1;;;","11/Apr/10 02:25;cody.lerum;this file shows the initial server startup networking traffic as well.

second server starts up at about 6 seconds in ;;;","12/Apr/10 20:59;gdusbabek;Looks like CompactEndPointSerializationHelper is assuming a 4 byte address during deserialization, but actually writes a full 16 byte IPv6 address during serialization.;;;","12/Apr/10 21:01;gdusbabek;This patch should address your specific problem.  What we really need to do is audit the code for this problem.  There are quite a few places where we send addresses over the wire.;;;","12/Apr/10 21:10;jbellis;+1;;;","12/Apr/10 21:30;cody.lerum;I'll try and recompile later today and test.;;;","13/Apr/10 05:43;cody.lerum;I checked out the .6 branch and built off that.

Still getting errors

ERROR [MESSAGE-DESERIALIZER-POOL:1] 2010-04-12 15:37:57,020 DebuggableThreadPoolExecutor.java (line 94) Error in executor futuretask
java.util.concurrent.ExecutionException: java.lang.ArrayIndexOutOfBoundsException: 65536
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:86)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 65536
        at org.apache.cassandra.net.HeaderSerializer.deserialize(Header.java:155)
        at org.apache.cassandra.net.HeaderSerializer.deserialize(Header.java:113)
        at org.apache.cassandra.net.MessageSerializer.deserialize(Message.java:136)
        at org.apache.cassandra.net.MessageDeserializationTask.run(MessageDeserializationTask.java:45)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        ... 2 more
;;;","13/Apr/10 05:45;cody.lerum;actually I take that back

-rw-r--r-- 1 root root 1275022 2010-03-28 09:25 apache-cassandra-0.6.0-rc1.jar

The lib in my new build is still old.;;;","13/Apr/10 06:19;cody.lerum;ok got it built..


The joining server shows

ERROR 16:17:52,106 Error in executor futuretask
java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.io.EOFException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:86)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.RuntimeException: java.io.EOFException
        at org.apache.cassandra.net.MessageDeserializationTask.run(MessageDeserializationTask.java:49)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        ... 2 more
Caused by: java.io.EOFException
        at java.io.DataInputStream.readFully(DataInputStream.java:180)
        at java.io.DataInputStream.readUTF(DataInputStream.java:592)
        at java.io.DataInputStream.readUTF(DataInputStream.java:547)
        at org.apache.cassandra.net.HeaderSerializer.deserialize(Header.java:140)
        at org.apache.cassandra.net.HeaderSerializer.deserialize(Header.java:113)
        at org.apache.cassandra.net.MessageSerializer.deserialize(Message.java:136)
        at org.apache.cassandra.net.MessageDeserializationTask.run(MessageDeserializationTask.java:45)
        ... 6 more

and existing

ERROR 16:17:42,991 Error in executor futuretask
java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.net.UnknownHostException: addr is of illegal length
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:86)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.RuntimeException: java.net.UnknownHostException: addr is of illegal length
        at org.apache.cassandra.net.MessageDeserializationTask.run(MessageDeserializationTask.java:49)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        ... 2 more
Caused by: java.net.UnknownHostException: addr is of illegal length
        at java.net.InetAddress.getByAddress(InetAddress.java:935)
        at java.net.InetAddress.getByAddress(InetAddress.java:1311)
        at org.apache.cassandra.net.CompactEndPointSerializationHelper.deserialize(CompactEndPointSerializationHelper.java:37)
        at org.apache.cassandra.net.HeaderSerializer.deserialize(Header.java:139)
        at org.apache.cassandra.net.HeaderSerializer.deserialize(Header.java:113)
        at org.apache.cassandra.net.MessageSerializer.deserialize(Message.java:136)
        at org.apache.cassandra.net.MessageDeserializationTask.run(MessageDeserializationTask.java:45)
;;;","13/Apr/10 09:18;gdusbabek;As I suspected: we have more code that is not IPv6 friendly.

Cody, can you give me the low-down on how I can turn off IPv4 and test in an environment that is reasonably similar to yours?  Assume I have an ubuntu VM at my disposal.;;;","13/Apr/10 09:50;cody.lerum;Gary,

I'm running on ubuntu with dual stack (both v4 and v6) I am merely only using ipv6 addresses in the storage and seed portions of the storage-conf.xml.

in your simply set

/etc/network/interfaces

iface eth0 inet6 static
        address 2607:f3d0:0:2::A
        netmask 64
        gateway 2607:f3d0:0:2::1

and on the other server

iface eth0 inet6 static
        address 2607:f3d0:0:2::B
        netmask 64
        gateway 2607:f3d0:0:2::1


Then in your storage-conf.xml

<Seeds>
      <Seed>2607:f3d0:0:1::b</Seed>
 </Seeds>
 <ListenAddress>2607:f3d0:0:2::a</ListenAddress>
  <!-- internal communications port -->
  <StoragePort>7000</StoragePort>

As long as both the vm's are on the same network (non-routed) you should be able to test just fine.




;;;","14/Apr/10 01:52;gdusbabek;I am not able to reproduce the latest problem.  I went as far as creating a unit test to test CompactEndPointSerializationHelper for all manner of IPv4 and IPv6 addresses.  It seems to do the job.

The EOF in the new stack trace makes me think that one of the nodes might not be up on the same code.  Cody, can you verify?;;;","14/Apr/10 02:47;cody.lerum;I may have screwed up the build. I will try the latest from Hudson ;;;","14/Apr/10 12:40;cody.lerum;Gary, I tested off http://hudson.zones.apache.org/hudson/job/Cassandra/405/ and it all looks good.

root@cassandra:/opt/cassandra# bin/cassandra -f
 INFO 22:32:35,577 Auto DiskAccessMode determined to be mmap
 WARN 22:32:35,840 Couldn't detect any schema definitions in local storage. I hope you've got a plan.
 INFO 22:32:35,851 Replaying /var/lib/cassandra/commitlog/CommitLog-1271219308493.log
 INFO 22:32:35,899 Creating new commitlog segment /var/lib/cassandra/commitlog/CommitLog-1271219555899.log
 INFO 22:32:35,910 LocationInfo has reached its threshold; switching in a fresh Memtable at CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1271219555899.log', position=163)
 INFO 22:32:35,911 Enqueuing flush of Memtable(LocationInfo)@1971599989
 INFO 22:32:35,913 Writing Memtable(LocationInfo)@1971599989
 INFO 22:32:36,012 Completed flushing /var/lib/cassandra/data/system/LocationInfo-b-1-Data.db
 INFO 22:32:36,025 Log replay complete
 INFO 22:32:36,051 Saved Token found: 149994310325493222650165912864788358013
 INFO 22:32:36,052 Saved ClusterName found: CLEARFLY-1
 INFO 22:32:36,062 Starting up server gossip
 INFO 22:32:36,110 Binding thrift service to /2607:f3d0:0:2:0:0:0:16:9160
 INFO 22:32:36,115 Cassandra starting up...
 INFO 22:33:35,564 Node /2607:f3d0:0:1:0:0:0:f is now part of the cluster
 INFO 22:33:36,553 InetAddress /2607:f3d0:0:1:0:0:0:f is now UP


You can close this out.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tombstone-only rows in sstables can be ignored,CASSANDRA-1063,12464004,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,stuhood,stuhood,08/May/10 00:03,16/Apr/19 17:33,22/Mar/23 14:57,08/May/10 04:09,0.6.2,,,,0,,,,,,"ColumnFamilyStoreTest has two tests that pass, that shouldn't.  These are obscuring bugs in tombstone handling.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/May/10 00:04;stuhood;0001-Remove-implementation-awareness-from-CFSTest.patch;https://issues.apache.org/jira/secure/attachment/12443976/0001-Remove-implementation-awareness-from-CFSTest.patch","08/May/10 02:24;jbellis;1063.txt;https://issues.apache.org/jira/secure/attachment/12443987/1063.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19981,,,Fri May 07 20:09:07 UTC 2010,,,,,,,,,,"0|i0g2rr:",91879,,,,,Normal,,,,,,,,,,,,,,,,,"08/May/10 00:04;stuhood;Patch against 0.6.;;;","08/May/10 02:24;jbellis;There's actually a real bug getting masked here -- the correct answer is not null, but an empty, tombstoned CF.

In fact there are three bugs where we are incorrectly not taking into account row tombstones:

 - we can't just return null if the returnCF is empty, we need to check for tombstones via removeDeleted
 - we can't skip the delete call while building our iterator list, if the column iterator is empty
 - we can't skip deserializing the CF if all the columns in a names predicate are rejected by the bloom filter;;;","08/May/10 02:43;stuhood;+1 for 1063.txt. Good finds!;;;","08/May/10 04:09;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Assertion Exception in COMMIT-LOG-WRITER,CASSANDRA-1376,12471227,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Duplicate,,arya,arya,11/Aug/10 04:04,16/Apr/19 17:33,22/Mar/23 14:57,28/Aug/10 00:43,0.7 beta 2,,,,0,,,,,,"I have a 3 node cluster. The last version which I used to insert data was Trunc on Friday August 6th at 2pm. On Monday the cluster was updated to version Trunc on August 8th. When I tried to insert rows, all 3 nodes gave this exception and the cluster became unresponsive to all requests causing TSocket timeouts. The resolution was to restart all 3 nodes.

ERROR [COMMIT-LOG-WRITER] 2010-08-09 11:30:27,722 CassandraDaemon.java (line 82) Uncaught exception in thread Thread[COMMIT-LOG-WRITER,5,main]
java.lang.AssertionError
        at org.apache.cassandra.db.commitlog.CommitLogHeader$CommitLogHeaderSerializer.serialize(CommitLogHeader.java:157)
        at org.apache.cassandra.db.commitlog.CommitLogHeader.writeCommitLogHeader(CommitLogHeader.java:124)
        at org.apache.cassandra.db.commitlog.CommitLogSegment.writeHeader(CommitLogSegment.java:70)
        at org.apache.cassandra.db.commitlog.CommitLogSegment.write(CommitLogSegment.java:103)
        at org.apache.cassandra.db.commitlog.CommitLog$LogRecordAdder.run(CommitLog.java:521)
        at org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:52)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.lang.Thread.run(Thread.java:636)

I am not sure how to reproduce this again but on Trunc from Monday August 9th, one of my 3 nodes did not start giving the following exception and again the resolution was to restart that node:


java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
        at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:549)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:339)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:174)
        at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:120)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:90)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
        at java.util.concurrent.FutureTask.get(FutureTask.java:111)
        at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:545)
        ... 5 more
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
        at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegments(CommitLog.java:408)
        at org.apache.cassandra.db.ColumnFamilyStore$2.runMayThrow(ColumnFamilyStore.java:445)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 6 more
Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
        at java.util.concurrent.FutureTask.get(FutureTask.java:111)
        at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegments(CommitLog.java:400)
        ... 8 more
Caused by: java.lang.AssertionError
        at org.apache.cassandra.db.commitlog.CommitLogHeader$CommitLogHeaderSerializer.serialize(CommitLogHeader.java:157)
        at org.apache.cassandra.db.commitlog.CommitLogHeader.writeCommitLogHeader(CommitLogHeader.java:124)
        at org.apache.cassandra.db.commitlog.CommitLogSegment.writeHeader(CommitLogSegment.java:70)
        at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegmentsInternal(CommitLog.java:450)
        at org.apache.cassandra.db.commitlog.CommitLog.access$300(CommitLog.java:75)
        at org.apache.cassandra.db.commitlog.CommitLog$6.call(CommitLog.java:394)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:52)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 1 more","CentOS 5.2
Trunc",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-1435,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20109,,,Fri Aug 27 16:43:22 UTC 2010,,,,,,,,,,"0|i0g4of:",92188,,,,,Critical,,,,,,,,,,,,,,,,,"19/Aug/10 06:36;messi;This looks suspicious.;;;","27/Aug/10 23:09;bterm;can confirm, had this happen on a 6 node cluster on 3 nodes at about same time.

Server 2 - ERROR [COMMIT-LOG-WRITER] 2010-08-27 09:51:30,942 CassandraDaemon.java (line 82) Uncaught exception in thread Thread[COMMIT-LOG-WRITER,5,main]
Server 4 - ERROR [COMMIT-LOG-WRITER] 2010-08-27 09:43:56,882 CassandraDaemon.java (line 82) Uncaught exception in thread Thread[COMMIT-LOG-WRITER,5,main]
Server 5 - ERROR [COMMIT-LOG-WRITER] 2010-08-27 09:43:56,891 CassandraDaemon.java (line 82) Uncaught exception in thread Thread[COMMIT-LOG-WRITER,5,main]

java.lang.AssertionError
        at org.apache.cassandra.db.commitlog.CommitLogHeader$CommitLogHeaderSerializer.serialize(CommitLogHeader.java:157)
        at org.apache.cassandra.db.commitlog.CommitLogHeader.writeCommitLogHeader(CommitLogHeader.java:124)
        at org.apache.cassandra.db.commitlog.CommitLogSegment.writeHeader(CommitLogSegment.java:70)
        at org.apache.cassandra.db.commitlog.CommitLogSegment.write(CommitLogSegment.java:103)
        at org.apache.cassandra.db.commitlog.CommitLog$LogRecordAdder.run(CommitLog.java:521)
        at org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:52)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.lang.Thread.run(Thread.java:619);;;","28/Aug/10 00:43;jbellis;duplicate of CASSANDRA-1435 (where there is a patch attached);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Index Out Of Bounds during Validation Compaction (Repair),CASSANDRA-2373,12502240,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,bcoverston,bcoverston,24/Mar/11 07:19,16/Apr/19 17:33,22/Mar/23 14:57,12/Apr/11 23:32,,,,,0,,,,,,"Stack Trace is below:

ERROR [CompactionExecutor:1] 2011-03-23 19:11:39,488 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.IndexOutOfBoundsException
        at java.nio.Buffer.checkIndex(Unknown Source)
        at java.nio.HeapByteBuffer.getInt(Unknown Source)
        at org.apache.cassandra.db.DeletedColumn.getLocalDeletionTime(DeletedColumn.java:57)
        at org.apache.cassandra.db.ColumnFamilyStore.removeDeletedStandard(ColumnFamilyStore.java:879)
        at org.apache.cassandra.db.ColumnFamilyStore.removeDeletedColumnsOnly(ColumnFamilyStore.java:866)
        at org.apache.cassandra.db.ColumnFamilyStore.removeDeleted(ColumnFamilyStore.java:857)
        at org.apache.cassandra.io.PrecompactedRow.<init>(PrecompactedRow.java:94)
        at org.apache.cassandra.io.CompactionIterator.getCompactedRow(CompactionIterator.java:147)
        at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:108)
        at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:43)
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:73)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
        at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
        at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
        at org.apache.cassandra.db.CompactionManager.doValidationCompaction(CompactionManager.java:822)
        at org.apache.cassandra.db.CompactionManager.access$800(CompactionManager.java:56)
        at org.apache.cassandra.db.CompactionManager$6.call(CompactionManager.java:358)
        at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
        at java.util.concurrent.FutureTask.run(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)",CentOS on EC2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20587,,,Tue Apr 12 15:32:46 UTC 2011,,,,,,,,,,"0|i0gb07:",93213,,,,,Normal,,,,,,,,,,,,,,,,,"24/Mar/11 08:36;jbellis;Did you scrub first?;;;","25/Mar/11 06:21;bcoverston;Yes, I scrubbed first on both nodes before the repair.;;;","26/Mar/11 03:01;jbellis;is the sstable small enough to attach?;;;","27/Mar/11 11:44;bcoverston;It doesn't look like Validation compactions log a message before they begin. Makes it hard to identify the offending SSTable.;;;","28/Mar/11 22:56;slebresne;Out of curiosity, are you able to major compact (or scrub) without problems ? I would be surprised if it was a validation specific thing, since it really doesn't do anything fancy as far as deserialization is involved.;;;","28/Mar/11 23:16;bcoverston;I was able to scrub the node without any issues.;;;","28/Mar/11 23:29;slebresne;Oh yeah, but actually now that I think about it, scrub is not a good test. What I suspect is that there is some crappy data, such that it is deserialized ""correctly"" as a tombstone, but the corruption is such that the value is < 4 bytes. At least provided the stack trace, the problem is when accessing the deletion time, which scrub won't. I'd be more curious to see if a major compaction triggers it. If is is possible to get the sstable(s), I'd happily test it here.;;;","28/Mar/11 23:38;jbellis;so we should enhance scrub to check fields?;;;","28/Mar/11 23:49;slebresne;Yes, wouldn't be a bad idea. We could check that tombstone values are 4 bytes long (that almost could be an assert in ColumnSerializer.deserialize() actually, and I assume this is the problem here) and non negative value. Expiration time shouldn't be negative either. And if we validate the name and value through the validators, that would give an even better confidence that nothing is rotten.;;;","12/Apr/11 23:32;jbellis;CASSANDRA-2460 is open now to fully deserialize during scrub.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra_cli: CREATE CF HELP should list option as key_cache_save_period instead of keys_cached_save_period,CASSANDRA-2572,12505258,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,cdaw,cdaw,27/Apr/11 08:36,16/Apr/19 17:33,22/Mar/23 14:57,27/Apr/11 23:33,0.8.0 beta 2,,Legacy/Tools,,0,,,,,,"*cassandra-cli: output from create cf command*
{noformat}
[default@cqldb]  create column family supa_dupa2 with keys_cached_save_period = 124000;

No enum const class org.apache.cassandra.cli.CliClient$ColumnFamilyArgument.KEYS_CACHED_SAVE_PERIOD
{noformat}

*cassandra-cli: help create column family*
{noformat}
- keys_cached_save_period: Duration in seconds after which Cassandra should
  safe the keys cache. Caches are saved to saved_caches_directory as
  specified in conf/Cassandra.yaml. Default is 14400 or 4 hours.

  Saved caches greatly improve cold-start speeds, and is relatively cheap in
  terms of I/O for the key cache. Row cache saving is much more expensive and
  has limited use.
{noformat}

*cqlsh: documentation for create column family options*
{noformat}
key_cache_save_period_in_seconds	14400	Number of seconds between saving key caches.
{noformat}

*cqlsh: this actually works*
{noformat}
cqlsh>  CREATE COLUMNFAMILY cf1 (KEY varchar PRIMARY KEY) WITH key_cache_save_period_in_seconds=10000;
{noformat}

*cassandra-cli: CF definition via show keyspace*
{noformat}
    ColumnFamily: cf1
      Key Validation Class: org.apache.cassandra.db.marshal.UTF8Type
      Default column value validator: org.apache.cassandra.db.marshal.UTF8Type
      Columns sorted by: org.apache.cassandra.db.marshal.UTF8Type
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/10000
      Memtable thresholds: 0.140625/30/1440 (millions of ops/MB/minutes)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: true
      Built indexes: []
{noformat}
",,dw,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Apr/11 10:31;dw;CASSANDRA-0.7-2572.patch;https://issues.apache.org/jira/secure/attachment/12477471/CASSANDRA-0.7-2572.patch","27/Apr/11 18:17;xedin;CASSANDRA-2572.patch;https://issues.apache.org/jira/secure/attachment/12477492/CASSANDRA-2572.patch",,,,,,,,,,,,,2.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20705,,,Wed Apr 27 17:18:12 UTC 2011,,,,,,,,,,"0|i0gc5z:",93401,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"27/Apr/11 09:34;jbellis;may also be a bug in 0.7;;;","27/Apr/11 09:53;dw;I'm seeing the following in org.apache.cassandra.cli.CliClient:

 protected enum ColumnFamilyArgument
    {
    ....
    KEY_CACHE_SAVE_PERIOD,
    ....
    }

Instead of executing:

create column family supa_dupa2 with keys_cached_save_period = 124000;

one executes:

create column family supa_dupa2 with key_cache_save_period = 124000;

The second statement successfully executes.;;;","27/Apr/11 11:33;cdaw;Thanks David.  Will make this a bug about help.;;;","27/Apr/11 18:17;xedin;changed doc for 0.8 (can be applied on both cassandra-0.8 and trunk without any problems), 0.7 is not affected by this bug.;;;","27/Apr/11 23:33;jbellis;committed;;;","28/Apr/11 01:18;hudson;Integrated in Cassandra-0.8 #46 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/46/])
    fix cli help typo
patch by Pavel Yaskevich for CASSANDRA-2572
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Removed/Dead Node keeps reappearing,CASSANDRA-2371,12502232,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,acctech,acctech,24/Mar/11 06:03,16/Apr/19 17:33,22/Mar/23 14:57,19/Apr/11 02:59,0.7.5,,Legacy/Tools,,2,,,,,,"The removetoken option does not seem to work. The original node 10.240.50.63 comes back into the ring, even after the EC2 instance is no longer in existence. Originally I tried to add a new node 10.214.103.224 with the same token, but there were some complications with that. I have pasted below all the INFO log entries found with greping the system log files.

Seems to be a similar issue seen with http://cassandra-user-incubator-apache-org.3065146.n2.nabble.com/Ghost-node-showing-up-in-the-ring-td6198180.html 

INFO [GossipStage:1] 2011-03-16 00:54:31,590 StorageService.java (line 745) Nodes /10.214.103.224 and /10.240.50.63 have the same token 95704415696513900000000000000000000000.  /10.214.103.224 is the new owner
 INFO [GossipStage:1] 2011-03-16 17:26:51,083 StorageService.java (line 865) Removing token 95704415696513900000000000000000000000 for /10.214.103.224
 INFO [GossipStage:1] 2011-03-19 17:27:24,767 StorageService.java (line 865) Removing token 95704415696513900000000000000000000000 for /10.214.103.224
 INFO [GossipStage:1] 2011-03-19 17:29:30,191 StorageService.java (line 865) Removing token 95704415696513900000000000000000000000 for /10.214.103.224
 INFO [GossipStage:1] 2011-03-19 17:31:35,609 StorageService.java (line 865) Removing token 95704415696513900000000000000000000000 for /10.214.103.224
 INFO [GossipStage:1] 2011-03-19 17:33:39,440 StorageService.java (line 865) Removing token 95704415696513900000000000000000000000 for /10.214.103.224
 INFO [GossipStage:1] 2011-03-23 17:22:55,520 StorageService.java (line 865) Removing token 95704415696513900000000000000000000000 for /10.240.50.63


 INFO [GossipStage:1] 2011-03-10 03:52:37,299 Gossiper.java (line 608) Node /10.240.50.63 is now part of the cluster
 INFO [GossipStage:1] 2011-03-10 03:52:37,545 Gossiper.java (line 600) InetAddress /10.240.50.63 is now UP
 INFO [HintedHandoff:1] 2011-03-10 03:53:36,168 HintedHandOffManager.java (line 304) Started hinted handoff for endpoint /10.240.50.63
 INFO [HintedHandoff:1] 2011-03-10 03:53:36,169 HintedHandOffManager.java (line 360) Finished hinted handoff of 0 rows to endpoint /10.240.50.63
 INFO [GossipStage:1] 2011-03-15 23:23:43,770 Gossiper.java (line 623) Node /10.240.50.63 has restarted, now UP again
 INFO [GossipStage:1] 2011-03-15 23:23:43,771 StorageService.java (line 726) Node /10.240.50.63 state jump to normal
 INFO [HintedHandoff:1] 2011-03-15 23:28:48,957 HintedHandOffManager.java (line 304) Started hinted handoff for endpoint /10.240.50.63
 INFO [HintedHandoff:1] 2011-03-15 23:28:48,958 HintedHandOffManager.java (line 360) Finished hinted handoff of 0 rows to endpoint /10.240.50.63
 INFO [ScheduledTasks:1] 2011-03-15 23:37:25,071 Gossiper.java (line 226) InetAddress /10.240.50.63 is now dead.
 INFO [GossipStage:1] 2011-03-16 00:54:31,590 StorageService.java (line 745) Nodes /10.214.103.224 and /10.240.50.63 have the same token 95704415696513900000000000000000000000.  /10.214.103.224 is the new owner
 WARN [GossipStage:1] 2011-03-16 00:54:31,590 TokenMetadata.java (line 115) Token 95704415696513900000000000000000000000 changing ownership from /10.240.50.63 to /10.214.103.224
 INFO [GossipStage:1] 2011-03-18 23:37:09,158 Gossiper.java (line 610) Node /10.240.50.63 is now part of the cluster
 INFO [GossipStage:1] 2011-03-21 23:37:10,421 Gossiper.java (line 610) Node /10.240.50.63 is now part of the cluster
 INFO [GossipStage:1] 2011-03-21 23:37:10,421 StorageService.java (line 726) Node /10.240.50.63 state jump to normal
 INFO [GossipStage:1] 2011-03-23 17:22:55,520 StorageService.java (line 865) Removing token 95704415696513900000000000000000000000 for /10.240.50.63
 INFO [ScheduledTasks:1] 2011-03-23 17:22:55,521 HintedHandOffManager.java (line 210) Deleting any stored hints for 10.240.50.63
",Large Amazon EC2 instances. Ubuntu 10.04.2 ,haruska,jborgstrom,jeromatron,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Mar/11 06:43;brandon.williams;2371.txt;https://issues.apache.org/jira/secure/attachment/12474449/2371.txt",,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20586,,,Mon Apr 18 19:43:59 UTC 2011,,,,,,,,,,"0|i0gazr:",93211,,,,,Low,,,,,,,,,,,,,,,,,"24/Mar/11 06:43;brandon.williams;My guess is what is happening is that after aVeryLongTime when the state is evicted, another gossip round occurs and the state is repopulated.  Patch to re-quarantine on eviction to avoid this.;;;","24/Mar/11 07:30;acctech;This looks like it's changing the source code. Can we deploy this on a live cluster? ;;;","01/Apr/11 12:09;jbellis;Did you get a chance to try a patched build?;;;","02/Apr/11 00:48;acctech;Actually the issue was resolved when we did a restart of the cluster, and ran the nodetool removetoken command when only a portion of the nodes had started up. The main reason I didn't apply the patch yet, is merely because I need to learn how to patch the code and integrate the new build first... Thank you!;;;","07/Apr/11 08:05;brandon.williams;There is a second problem here.  We don't populate the gossiper's application state to LEFT for removetoken, only for decommission.  The problem with adding it, however, is that SS.handleStateLeft gets called every gossip round and runs through the hint removal process.  One option may be to check if the node is locally persisted and if not, just ignore the message since we never knew about it anyway.  Another is to just not remove hints when we see the LEFT state, because they'll expire anyway and large unaccessed rows aren't a problem, so this seems like a throwback from the <=0.6 days.;;;","10/Apr/11 17:03;jeromatron;I applied the patch and within a few days even with no restarting of any of the Cassandra nodes, the removed token came back. Just FYI.;;;","12/Apr/11 00:11;jbellis;bq. The problem with adding it, however, is that SS.handleStateLeft gets called every gossip round and runs through the hint removal process

But it only runs hint removal once per node, right?  Or is ""onChange"" not an accurate method name?

bq. One option may be to check if the node is locally persisted and if not, just ignore the message since we never knew about it anyway.

Locally persisted... with a token in SystemTable?  Ignore the ... hint message?

bq. Another is to just not remove hints when we see the LEFT state

We should issue a delete to the hints row, but we should not force a major compaction. The rule of thumb is, avoiding inflicting a performance hit on the cluster trumps immediate disk space cleanup.;;;","15/Apr/11 06:55;brandon.williams;It looks like the bad news here is that gossip makes many assumptions about a node being alive when it hears about it, and has no provisions for keeping removed node state.  This is bad, but highly exacerbated by keeping the removed node state for 3 days instead of 30s.  I think the best solution right now is to revert CASSANDRA-2115 until we can overhaul gossip to handle this.  The bug will still exist as it always has, but the window to trigger it is far shorter.;;;","15/Apr/11 08:50;jbellis;Sounds reasonable.;;;","19/Apr/11 02:59;brandon.williams;Reverted CASSANDRA-2115 and created CASSANDRA-2496 to fix this correctly.;;;","19/Apr/11 03:43;hudson;Integrated in Cassandra-0.7 #440 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/440/])
    Revert ""Keep endpoint state until aVeryLongTime when not a fat client""

This reverts commit db9164ffd96ebc7752fc5789c90c7211ba323ad2.

For CASSANDRA-2371.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Read repair does not always work correctly,CASSANDRA-1316,12470095,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,25/Jul/10 07:50,16/Apr/19 17:33,22/Mar/23 14:57,28/Jul/10 00:47,0.6.4,,,,0,,,,,,"Read repair does not always work.  At the least, we allow violation of the CL.ALL contract.  To reproduce, create a three node cluster with RF=3, and json2sstable one of the attached json files on each node.  This creates a row whose key is 'test' with 9 columns, but only 3 columns are on each machine.  If you get_count this row in quick succession at CL.ALL, sometimes you will receive a count of 6, sometimes 9.  After the ReadRepairManager has sent the repairs, you will always get 9, which is the desired behavior.

I have another data set obtained in the wild which never fully repairs for some reason, but it's a bit large to attach (600ish columns per machine.)  I'm still trying to figure out why RR isn't working on this set, but I always get different results when reading at any CL including ALL, no matter how long I wait or how many reads I do.",,brandon.williams,johanoskarsson,sayap,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jul/10 23:24;brandon.williams;001_correct_responsecount_in_RRR.txt;https://issues.apache.org/jira/secure/attachment/12450474/001_correct_responsecount_in_RRR.txt","27/Jul/10 10:41;jbellis;1316-RRM.txt;https://issues.apache.org/jira/secure/attachment/12450552/1316-RRM.txt","27/Jul/10 00:08;jbellis;RRR-v2.txt;https://issues.apache.org/jira/secure/attachment/12450479/RRR-v2.txt","26/Jul/10 04:15;brandon.williams;cassandra-1.json;https://issues.apache.org/jira/secure/attachment/12450423/cassandra-1.json","26/Jul/10 04:15;brandon.williams;cassandra-2.json;https://issues.apache.org/jira/secure/attachment/12450424/cassandra-2.json","26/Jul/10 04:15;brandon.williams;cassandra-3.json;https://issues.apache.org/jira/secure/attachment/12450425/cassandra-3.json",,,,,,,,,6.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20077,,,Sat Dec 11 07:35:22 UTC 2010,,,,,,,,,,"0|i0g4bb:",92129,,,,,Normal,,,,,,,,,,,,,,,,,"25/Jul/10 07:53;brandon.williams;I tried to bisect this issue and went as far back as 0.4.2 without finding a successful version.  I will see if the data that never repairs can be scrubbed so I can attach it to this ticket for debugging.;;;","25/Jul/10 08:02;brandon.williams;It looks like the key difference between the real data and the toy data that is attached is that the real data has the key in multiple sstables.  If left this way, RR never fully works, but if I force a compaction then it succeeds.;;;","26/Jul/10 04:18;brandon.williams;Updated json files illustrate one possible scenario:  nodes 1 and 2 have a column, and node 3 has the column tombstoned with the same timestamp.  It looks like tombstones aren't taking precedence.;;;","26/Jul/10 23:24;brandon.williams;Patch to solve one problem: use derterminBlockFor to set the correct response count passed to RRR, so CL.ALL works.;;;","27/Jul/10 00:08;jbellis;v2 updates QRH arguments to use responseCount as well, even though it's ignored;;;","27/Jul/10 06:11;brandon.williams;Committed RRRv2.;;;","27/Jul/10 10:41;jbellis;patch that simplifies debugging by removing ReadRepairManager which is mostly 100-odd lines of obfuscation around MessagingService.sendOneWay.  (backport from CASSANDRA-1077 which was applied to trunk two months ago);;;","28/Jul/10 00:47;jbellis;Brandon's first patch fixing reads at CL.ALL turns out to be the only bug.  The rest is obscure-but-valid behavior when expired tombstones haven't been replicated across the cluster (i.e., the tombstones exist on some nodes, but not all).  Let me give an example:

say node A has columns x and y, where x is an expired tombstone with timestamp T1, and node B has live column x, at time T2 where T2 < T1.

if you read at ALL you will see x from B and y from A.  you will _not_ see x from A -- since it is expired, it is no longer relevant off-node.  thus, the ALL read will send a repair of column x to A, since it was ""missing.""

But next time you read from A the tombstone will supress the newly-written copy of x-from-B still, because its timestamp is higher.  So the replicas won't converge.

This is not a bug, because the design explicitly allows that behavior when tombstones expire before being propagated to all nodes; see http://wiki.apache.org/cassandra/DistributedDeletes.  The best way to avoid this of course is to run repair frequently enough to ensure that tombstones are propagated within GCGraceSeconds of being written.

But if you do find yourself in this situation, you have two options to get things to converge again:

1) the simplest option is to simply perform a major compaction on each node, which will eliminate all expired tombstones.

2) but if you want to propagate as many of the tombstones as possible first, increase your GCGraceSeconds setting everywhere (requires rolling restart), and perform a full repair as described in http://wiki.apache.org/cassandra/Operations.  After the repair is complete you can put GCGraceSeconds back to what it was.
;;;","11/Dec/10 15:35;hudson;Integrated in Cassandra-0.7 #70 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/70/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
incorrect neighbor calculation in repair,CASSANDRA-924,12460506,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,stuhood,hujn,hujn,29/Mar/10 14:00,16/Apr/19 17:33,22/Mar/23 14:57,06/Apr/10 05:04,0.6.1,,,,0,,,,,,"With Replicationfactor=2, if a server is brought down and its data directory wiped out, it doesn't restore its data replica after restart and nodeprobe repair.
Steps to reproduce:
1) Bring up a cluster with three servers cs1,2,3, with their initial token set to 'foo3', 'foo6', and 'foo9', respectively. ReplicationFactor is set to 2 on all 3.
2) Insert 9 columns with keys from 'foo1' to 'foo9', and flush. Now I have foo1,2,3,7,8,9 on cs1, foo1,2,3,4,5,6, on cs2, and foo4,5,6,7,8,9
on cs3. So far so good
3) Bring down cs3 and wipe out its data directory
4) Bring up cs3
5) run nodeprobe repair Keyspace1 on cs3, the flush
At this point I expect to see cs3 getting its data back. But there's nothing in its data directory. I also tried getting all columns with
ConsistencyLevel::ALL to see if that'll do a read pair. But still cs3's data directory is empty.
",CentOS 5.2,rschildmeijer,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Apr/10 13:08;stuhood;0001-Calculate-neighbors-using-getRangeToEndpointMap.patch;https://issues.apache.org/jira/secure/attachment/12440577/0001-Calculate-neighbors-using-getRangeToEndpointMap.patch","02/Apr/10 13:08;stuhood;0002-Always-trigger-streaming-repairs.patch;https://issues.apache.org/jira/secure/attachment/12440578/0002-Always-trigger-streaming-repairs.patch","02/Apr/10 13:08;stuhood;0003-Unit-test-for-AEService-neighbor-calculation.patch;https://issues.apache.org/jira/secure/attachment/12440579/0003-Unit-test-for-AEService-neighbor-calculation.patch","30/Mar/10 08:39;hujn;cs1.log;https://issues.apache.org/jira/secure/attachment/12440155/cs1.log","30/Mar/10 05:33;hujn;cs1.log;https://issues.apache.org/jira/secure/attachment/12440113/cs1.log","30/Mar/10 08:39;hujn;cs2.log;https://issues.apache.org/jira/secure/attachment/12440156/cs2.log","30/Mar/10 05:33;hujn;cs2.log;https://issues.apache.org/jira/secure/attachment/12440114/cs2.log","30/Mar/10 08:39;hujn;cs3.log;https://issues.apache.org/jira/secure/attachment/12440157/cs3.log","30/Mar/10 05:33;hujn;cs3.log;https://issues.apache.org/jira/secure/attachment/12440115/cs3.log","30/Mar/10 00:59;hujn;error.log;https://issues.apache.org/jira/secure/attachment/12440082/error.log","30/Mar/10 00:09;hujn;error.log;https://issues.apache.org/jira/secure/attachment/12440075/error.log","29/Mar/10 14:02;hujn;error.log;https://issues.apache.org/jira/secure/attachment/12440040/error.log","29/Mar/10 14:02;hujn;storage-conf.xml;https://issues.apache.org/jira/secure/attachment/12440039/storage-conf.xml",,13.0,stuhood,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19922,,,Mon Apr 05 21:04:38 UTC 2010,,,,,,,,,,"0|i0g1x3:",91741,,,,,Normal,,,,,,,,,,,,,,,,,"29/Mar/10 14:02;hujn;conf file used in the test and error log;;;","29/Mar/10 22:32;stuhood;Do you have the logs from one of the other machines? AntiEntropyService on the failed box appears to have given the correct result, but both of the other boxes should have had tree mismatches.;;;","30/Mar/10 00:09;hujn;log file from cs1;;;","30/Mar/10 00:35;rschildmeijer;I could be wrong but isn't this exception very simliar to the ""upgrade to jdk build 18""-problem?;;;","30/Mar/10 00:59;hujn;log file from cs2;;;","30/Mar/10 02:47;stuhood;This one is pretty serious... apparently AntiEntropyService is always calculating who to initiate repairs with using getNaturalEndpoints(mytoken), so when RF is less than the number of nodes, it is likely that trees are only sent in one direction (clockwise around the ring). Additionally, we need to do the inverse of getNaturalEndpoints, and determine which other nodes we are holding replicas for.;;;","30/Mar/10 03:44;stuhood;Uses getRangeToAddressMap to calculate all endpoints that node A is storing replicas for, in addition to endpoints storing replicas for node A.;;;","30/Mar/10 04:06;stuhood;Backport of the 924.patch for 0.5.

Jianing: would you mind testing the appropriate patch for your version?;;;","30/Mar/10 05:33;hujn;still doesn't work for me. i've attached logs from all servers.
(i added the ""get neighbors"" log just to be sure i was running the patched code);;;","30/Mar/10 06:51;stuhood;Soo, we resolved one issue, and exposed another. Because less than 5% of possible keys (in the UTF-8 space) were out of sync between the nodes, AntiEntropyService decided not to do a repair.

I'll update the patches with a solution this evening. Thanks for your patience!;;;","30/Mar/10 07:39;jbellis;is the problem that it's guessing for % of ""utf8 space"" instead of ""keys actually on the servers?"";;;","30/Mar/10 07:53;stuhood;Yea, it's a naive comparison based only on the trees. But fixing the stubbed out 'range read-repair' option is much easier.;;;","30/Mar/10 08:06;stuhood;Patches updated to remove stubbed out 'range read-repair' option.

Jianing: can you give your test one more try?;;;","30/Mar/10 08:39;hujn;still no luck. log files attached.;;;","30/Mar/10 09:34;stuhood;I think you might have used the wrong version of the patch (I see log entries that I removed in the most recent version). I just deleted the older version, and I've tested that this version works on a 3 node cluster with RF=2.

Very sorry to use you like a guinea pig like that, but thank you very much for trying out each version.;;;","30/Mar/10 09:53;hujn;Sorry my bad. I was indeed using the wrong patch in my last test (wget renamed the file). The latest patch works!

Thank you so much for bearing with me and getting it fixed so fast.;;;","30/Mar/10 10:04;jbellis;is this something we can add a unit test for?;;;","02/Apr/10 13:08;stuhood;Patchset for 0.6 and trunk, including a test for neighbor calculation.;;;","06/Apr/10 03:44;jbellis;why does the test patch add

endpoints.add(FBUtilities.getLocalAddress());

to forceTableRepair?;;;","06/Apr/10 03:49;stuhood;> why does the test patch add endpoints.add to forceTableRepair?
Rather than implementing the neighbor calculation in two places, I wanted to reuse the logic in AES. In addition to the neighbors, forceTableRepair sends a tree request to localhost, so we add it back to the neighbor list.;;;","06/Apr/10 05:04;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Faulty memory causes adjacent nodes to have invalid data and fail compactation (java.io.IOException: Keys must be written in ascending order.),CASSANDRA-2408,12502987,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,tbritz,tbritz,31/Mar/11 16:52,16/Apr/19 17:33,22/Mar/23 14:57,29/Apr/11 21:06,,,,,0,,,,,,"Hi,

We had to replace a node with faulty ram. Besides the cassandra cluster getting unresponsive, we also observerd a ""keys must be written in ascending order"" exception on all adjacent quorum nodes, causing their compactation to fail.
We had another node with faulty ram a week earlier, and it was causing the same errors on it's neighbours.

A faulty node shouldn't affect the key ordering of adjacent nodes?

 INFO [CompactionExecutor:1] 2011-03-31 09:57:29,529 CompactionManager.java (line 396) Compacting [SSTableReader(path='/cassandra/data/table_lists/table_lists-f-1793-Data.db'),SSTableReader(path='/cassandra/data/table_lists/table_lists-f-1798-Data.db'),SSTableReader(path='/cassandra/data/table_lists/table_lists-f-1803-Data.db'),SSTableReader(path='/cassandra/data/table_lists/table_lists-f-1808-Data.db'),SSTableReader(path='/cassandra/data/table_lists/table_lists-f-1832-Data.db'),SSTableReader(path='/cassandra/data/table_lists/table_lists-f-1841-Data.db'),SSTableReader(path='/cassandra/data/table_lists/table_lists-f-1852-Data.db')]
 INFO [CompactionExecutor:1] 2011-03-31 09:58:26,437 SSTableWriter.java (line 108) Last written key : DecoratedKey(ba578bed5e75a06d1156dabccc68abd5_www.abc.com_hostinlinks_9223370735413988125_870cc25d-7e4f-437b-aa83-a311dfbf7f88, 62613537386265643565373561303664313135366461626363633638616264355f7777772e6b6f7475736f7a6c756b2e636f6d5f686f7374696e6c696e6b735f393232333337303733353431333938383132355f38373063633235642d376534662d343337622d616138332d613331316466626637663838)
 INFO [CompactionExecutor:1] 2011-03-31 09:58:26,515 SSTableWriter.java (line 109) Current key : DecoratedKey(ba578bed5e75a06d1156dabccc68abd5_www.abc.com_hostinlinks_9223370735413988136_17070303-ab24-4207-83bd-1604e2c159e4, 62613537386265643565373561303664313135366461626363633638616264355f7777772e6b677475736f7a6c756b2e636f6d5f686f7374696e6c696e6b735f393232333337303733353431333938383133365f31373037303330332d616232342d343230372d383362642d313630346532633135396534)
 INFO [CompactionExecutor:1] 2011-03-31 09:58:26,515 SSTableWriter.java (line 110) Writing into file /cassandra/data/table_lists/table_lists-tmp-f-1861-Data.db
ERROR [CompactionExecutor:1] 2011-03-31 09:58:26,516 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.io.IOException: Keys must be written in ascending order.
      at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:111)
      at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:128)
      at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:452)
      at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:124)
      at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:94)
      at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
      at java.util.concurrent.FutureTask.run(FutureTask.java:138)
      at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
      at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
      at java.lang.Thread.run(Thread.java:662)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20605,,,Fri Apr 29 13:06:53 UTC 2011,,,,,,,,,,"0|i0gb73:",93244,,,,,Normal,,,,,,,,,,,,,,,,,"19/Apr/11 22:17;jbellis;Did you run any repairs _before_ seeing this?

SSTableWriter checks order (of data written out of a memtable) at write time, so the only way to get data out-of-order should be to stream it from a corrupt node.

Repair would explain why it affects adjacent nodes too.;;;","19/Apr/11 22:44;tbritz;I might have run a repair on the faulty node before, but I don't think I ran a repair on the adjacent nodes, as there was no logical need to do this. However, I can't completely exclude it.

We also replaced about 8 machines so far with faulty memory, and this problem only occured twice (on 6 different machines).

If I do a write ONE on the faulty node, the faulty node might also be buffering delayed writes for the other nodes? What if the a bit flip happens there?

;;;","20/Apr/11 01:24;jbellis;If the coordinator flips bits before sending the write to a data node, the data node will write out the flipped key, but in the correct order (for the flipped version).;;;","29/Apr/11 21:06;jbellis;CASSANDRA-1717 will address not propagating corrupt data.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scan results out of order,CASSANDRA-1332,12470387,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,drevell,drevell,29/Jul/10 06:43,16/Apr/19 17:33,22/Mar/23 14:57,06/Aug/10 08:50,0.7 beta 1,,,,0,,,,,,"After inserting 10 keys ('0', '1', ... '9') and running scan() with start_key='' and count=7, scan() returns the keys  ['7', '3', '6', '5', '0', '8', '2']. When I scan() again with start_key='2' and count=7, I get the keys  ['2', '1', '9', '4', '7']. Notice that key ""7"" appears in both result sets, and the relative order of keys ""7"" and ""2"" is inconsistent between the two scan results. 

I see the problem when running on a 4-node cluster. When I run on a 1-node cluster, the problem does not occur. So the attached system test always passes, since system tests use a 1-node cluster, so the test doesn't actually demonstrate the problem.

A standalone Python program that reproduces the problem is at: http://pastebin.com/FwitG4wf","CentOS 5, Java 1.6.0, Cassandra trunk as of 28 July 2010",drevell,hammer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Jul/10 06:54;drevell;scan_test.patch;https://issues.apache.org/jira/secure/attachment/12450766/scan_test.patch",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20085,,,Fri Aug 06 00:50:39 UTC 2010,,,,,,,,,,"0|i0g4ev:",92145,,,,,Normal,,,,,,,,,,,,,,,,,"29/Jul/10 06:54;drevell;System test that reproduces the problem on a 4-node cluster;;;","29/Jul/10 08:25;brandon.williams;FWIW, Dave also told me that increasing the CL doesn't help, so it's not a consistency issue.;;;","29/Jul/10 09:43;jbellis;is this also present in 0.6.4?;;;","29/Jul/10 09:52;drevell;It isn't present in exactly the same form in 0.6.4 because scan() is new in 0.7. Would it be worth testing 0.6.4 get_range_slices and looking for similar behavior?;;;","30/Jul/10 01:29;drevell;Bisecting the SVN history shows that this worked in r966733 but became broken in r966742. Both of those revisions are dated July 22.;;;","06/Aug/10 04:50;jbellis;I believe this is fixed by the changes to StorageProxy made in CASSANDRA-1156.  Can you re-test?;;;","06/Aug/10 04:50;jbellis;I note that scan wasn't supposed to really work at all across multiple nodes, prior to 1156. :);;;","06/Aug/10 07:07;drevell;jbellis: I can't retest, scan() no longer exists (in cassandra.thrift).;;;","06/Aug/10 07:34;jbellis;get_range_slices is un-deprecated instead;;;","06/Aug/10 08:12;drevell;It seems fixed. After updating to r982821 and running the same test, it now passes (with scan switched to get_range_slices).;;;","06/Aug/10 08:50;jbellis;Great, thanks for the help!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
separate assignment of current keyspace from login(),CASSANDRA-1022,12463043,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,todd,urandom,urandom,27/Apr/10 02:35,16/Apr/19 17:33,22/Mar/23 14:57,05/May/10 00:01,0.7 beta 1,,,,0,,,,,,"With the completion of CASSANDRA-714, it is now a requirement that login() be called, even when using the AllowAllAuthenticator (effectively disabling auth), since this is how the current/connected keyspace is set. These two disparate functions (assigning keyspace and authentication) should be disentangled.

I propose that the keyspace argument be removed from calls to {{login()}}, and that a new method ({{use_keyspace(string)}}?), be added.
",,jeromatron,rschildmeijer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Apr/10 18:16;todd;CASSANDRA-1022.patch;https://issues.apache.org/jira/secure/attachment/12443277/CASSANDRA-1022.patch",,,,,,,,,,,,,,1.0,todd,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19961,,,Sat Jul 03 12:48:09 UTC 2010,,,,,,,,,,"0|i0g2iv:",91839,,,,,Low,,,,,,,,,,,,,,,,,"27/Apr/10 02:45;jbellis;mild preference for set_keyspace here;;;","27/Apr/10 04:42;todd;Since this relates closely with CASSANDRA-714, I'll spend the time logically separating these.;;;","30/Apr/10 18:16;todd;Patch attached.;;;","02/May/10 08:13;urandom;This has been committed with a few changes:

* Fixed some style nits, (braches on newlines, etc). See http://wiki.apache.org/cassandra/CodeStyle.
* Reinstated debug log statement in CassandraServer.login()
* Reinstated css_.debug test when printing stacktrace in the CLI
* Removed unused _login() function from system tests
* Reworded a few error messages.
* Updated contrib/utils/service/CassandraServiceTest for login() change
* Disabled system_add_keyspace() when authentication is enabled (login() requires a keyspace).
* Reset access level (forcing a new login) when keyspace is changed using set_keyspace().

Thanks Todd!
;;;","04/May/10 22:53;tzz;Eric, please remember http://thread.gmane.org/gmane.comp.db.cassandra.user/1038/focus=1404 where you asked me to merge login() with setKeyspace() and now they are pulled apart again.  What has changed?  Shouldn't we be insisting on the ""one keyspace per connection"" concept?  Or has the philosophy changed?

If set_keyspace() is to be used, it should be the one returning an AccessLevel.  login() should return void.  In other words, the AccessLevel is per-keyspace, not per-user.

Also, login() should no longer throw AuthorizationException since it doesn't do authorization, while set_keyspace() should throw it.

Please let me know if I need to implement these changes or if someone else will do it or if there are any questions.

Thanks!;;;","05/May/10 00:01;urandom;{quote}
Eric, please remember http://thread.gmane.org/gmane.comp.db.cassandra.user/1038/focus=1404 where you asked me to merge login() with setKeyspace() and now they are pulled apart again. What has changed? Shouldn't we be insisting on the ""one keyspace per connection"" concept? Or has the philosophy changed?
If set_keyspace() is to be used, it should be the one returning an AccessLevel. login() should return void. In other words, the AccessLevel is per-keyspace, not per-user.
{quote}

I remember; what has changed is (anecdotal) experience with how people are (or more importantly are not) using this, and how well the interface is holding up over time (read: it's not).

Personally,  I am  stronger in my convictions now that we should _not_ be rolling our own AAA, that this is not The Way, and I am seeking to make this (still experimental) API as optional as possible while working toward something better, (AVRO-341).

{quote}
Also, login() should no longer throw AuthorizationException since it doesn't do authorization, while set_keyspace() should throw it.
{quote}

{{set_keyspace()}} is the required call now, {{login()}} works exactly as before only it uses the keyspace specified in {{set_keyspace()}} instead of having one passed in (and {{set_keyspace()}} invalidates any previous {{login()}}).

This was the whole point of this issue, to divorce the function of assigning a keyspace from authentication.

{quote}
Please let me know if I need to implement these changes or if someone else will do it or if there are any questions.
{quote}

I don't see any changes that need to be implemented here.;;;","05/May/10 00:29;tzz;Thanks for explaining.  I didn't understand your motivation earlier.;;;","03/Jul/10 04:31;messi;Commit #940127 introduced a bug in CassandraServer (line 889 in trunk): if statement is empty because of semicolon.;;;","03/Jul/10 05:54;jbellis;removed the semicolon.  thanks!;;;","03/Jul/10 20:48;hudson;Integrated in Cassandra #484 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/484/])
    r/m errant semicolon.  patch by Folke Behrens; reviewed by jbellis for CASSANDRA-1022
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CFMetaData.convertToThrift makes subcomparator_type empty string instead of null,CASSANDRA-1480,12473524,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jhermes,jeromatron,jeromatron,08/Sep/10 11:27,16/Apr/19 17:33,22/Mar/23 14:57,14/Sep/10 03:22,0.7 beta 2,,Legacy/CQL,,0,,,,,,"As a result of CASSANDRA-891 adding a CFMetaData.convertToThrift method, the values such as subcomparator_type are defaulted to empty string instead of null.  That makes it so, for example, in ColumnFamilyRecordReader, in its RowIterator, the check for only null is insufficient.  It also needs to check for a blank value.

After a discussion about it in IRC, Jonathan said it was probably easier to just change the creation to give a null value instead of empty string.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-1425,CASSANDRA-891,,,,,"14/Sep/10 02:56;jhermes;1480.txt;https://issues.apache.org/jira/secure/attachment/12454470/1480.txt",,,,,,,,,,,,,,1.0,jhermes,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20160,,,Mon Sep 13 19:22:51 UTC 2010,,,,,,,,,,"0|i0g5bj:",92292,,jeromatron,,jeromatron,Normal,,,,,,,,,,,,,,,,,"14/Sep/10 00:19;jhermes;(This code is only touched in describe keyspace(s) in the API.)
Attaching a debugger shows the subcomparator_type of the result of the call to be null (as expected) until it gets sent, at which point the result is then converted to the empty string.
I'm not sure when this behaviour changed, but it appears to be systemic and not a simple error.

It would be less time to change the code to accept the empty string (which may come up naturally anyway) than it would be to continue debugging thrift.;;;","14/Sep/10 00:24;jeromatron;I kind of wondered if it would be complicated like that based on how thrift handled it.  It's a one line change to fix it in ColumnFamilyRecordReader.  I didn't know if it would affect anything else similarly.  Hopefully system/unit tests catch a lot of that.  We could do some simple searches in the code for null checks I suppose - and in those cases, if they're strings, do a o.a.commons.lang.StringUtils.isNotBlank instead.  Not sure if it's worth it though since that could have other side effects.;;;","14/Sep/10 00:41;jhermes;To pose a related question: assume the user sets the subcomparator_type (or the comparator or the validator or the default_validator) to the empty string. All perfectly legal, as the empty string is a valid string.
What should they expect to see? Right now it's a stack trace.;;;","14/Sep/10 01:41;jbellis;This is not a bug; subcomparator_type has a default of """", so if you leave it null (i.e., unspecified), it gets changed to its default.

Removing the default should restore the old behavior.

Defaults on reconciler and comment should also be removed.;;;","14/Sep/10 01:52;jhermes;Massive palm to the face.
Testing now.;;;","14/Sep/10 02:56;jhermes;convertToCFMetaData had to be changed as well, it was expecting subcomparator to default to empty string. I don't think this was a recent change; instead it looks more like we changed our assumption about the default.

In any event, subcomp, reconciler, and comment all default to null now and all tests/methods are working correctly.;;;","14/Sep/10 03:14;jeromatron;Ran the word count again with the 1480.txt patch.  It no longer gets the exception and completes properly now.;;;","14/Sep/10 03:22;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
orphaned data files may be created during migration race,CASSANDRA-2381,12502323,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,25/Mar/11 04:55,16/Apr/19 17:33,22/Mar/23 14:57,29/Mar/11 03:07,0.7.5,,,,0,,,,,,"We try to prevent creating orphans by locking Table.flusherLock in maybeSwitchMemtable and the Migration process, but since the actual writing is done asynchronously in Memtable.writeSortedContents there is a race window, where we acquire lock in maybeSwitch, we're not dropped so we queue the flush and release the lock, Migration does the drop, then Memtable writes itself out.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Mar/11 04:59;jbellis;2381-0.8.txt;https://issues.apache.org/jira/secure/attachment/12474557/2381-0.8.txt","29/Mar/11 00:01;jbellis;2381-v3.txt;https://issues.apache.org/jira/secure/attachment/12474782/2381-v3.txt","28/Mar/11 22:10;jbellis;2831-v2.txt;https://issues.apache.org/jira/secure/attachment/12474778/2831-v2.txt",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20593,,,Mon Mar 28 20:16:26 UTC 2011,,,,,,,,,,"0|i0gb1z:",93221,,gdusbabek,,gdusbabek,Low,,,,,,,,,,,,,,,,,"25/Mar/11 04:59;jbellis;Patch against 0.8:

- moves Table.flusherLock to Memtable.flushLock; acquire during memtable writing
- replaces flushLock use in Table.maybeSwitchMemtables with a synchronized block
- removes beforeApplyModel; snapshotting is moved into applyModel for drops and removed entirely for updates;;;","28/Mar/11 21:21;gdusbabek;Is it necessary to make the MT lock static?  I do not think there would be a problem with concurrently flushing two memtables of different column families (the CL serialization is still preserved by the synchronization in CFS).;;;","28/Mar/11 22:10;jbellis;You're right. v2 attached and rebased. Had to make lock/unlock responsibility of individual Migration classes since lock is no longer global.;;;","28/Mar/11 22:54;gdusbabek;I'm seeing unit test failures in CliTest and DefsTest with v2 applied (never ran them on v1).;;;","29/Mar/11 00:01;jbellis;v3 grabs CF reference before removing it from CFMetadata map.  tests pass.;;;","29/Mar/11 01:49;gdusbabek;+1.;;;","29/Mar/11 03:07;jbellis;committed;;;","29/Mar/11 04:16;hudson;Integrated in Cassandra-0.7 #411 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/411/])
    r/m unnecessary isDropped check from maybeSwitchMemtable (the one in Memtable.writeSortedContents it the important one)
patch by jbellis for CASSANDRA-2381
add actual dropped check to Memtable.flush for CASSANDRA-2381
patch by jbellis
fix migration race vs flush
patch by jbellis; reviewed by gdusbabek for CASSANDRA-2381
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RMI call times out on large repair jobs,CASSANDRA-2126,12497888,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Duplicate,,mdennis,mdennis,08/Feb/11 03:20,16/Apr/19 17:33,22/Mar/23 14:57,26/Jan/13 04:40,,,Legacy/Tools,,1,,,,,,"It looks like when a repair is started via nodetool and the repair takes a long time the blocking RMI call can timeout before the repair finishes.  The repair will continue to run and correctly complete, but the caller receives a stack trace similar to:

{noformat}
Exception in thread ""main"" java.rmi.UnmarshalException: Error unmarshaling return header; nested exception is:
        java.io.EOFException
        at sun.rmi.transport.StreamRemoteCall.executeCall(StreamRemoteCall.java:209)
        at sun.rmi.server.UnicastRef.invoke(UnicastRef.java:142)
        at com.sun.jmx.remote.internal.PRef.invoke(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl_Stub.invoke(Unknown Source)
        at javax.management.remote.rmi.RMIConnector$RemoteMBeanServerConnection.invoke(RMIConnector.java:993)
        at javax.management.MBeanServerInvocationHandler.invoke(MBeanServerInvocationHandler.java:288)
        at $Proxy0.forceTableRepair(Unknown Source)
        at org.apache.cassandra.tools.NodeProbe.forceTableRepair(NodeProbe.java:155)
        at org.apache.cassandra.tools.NodeCmd.optionalKSandCFs(NodeCmd.java:635)
        at org.apache.cassandra.tools.NodeCmd.main(NodeCmd.java:546)
Caused by: java.io.EOFException
        at java.io.DataInputStream.readByte(DataInputStream.java:250)
        at sun.rmi.transport.StreamRemoteCall.executeCall(StreamRemoteCall.java:195)
        ... 9 more
{noformat}
",,dallsopp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20455,,,Fri Jan 25 20:40:32 UTC 2013,,,,,,,,,,"0|i0g9gv:",92964,,,,,Low,,,,,,,,,,,,,,,,,"22/Jul/11 19:47;dallsopp;I've just had almost the same exception for a nodetool loadbalance operation (on v0.7.6):

{noformat}
$ nodetool -h dev2 -p8080 loadbalance
Exception in thread ""main"" java.rmi.UnmarshalException: Error unmarshaling return header; nested exception is: 
	java.io.EOFException
	at sun.rmi.transport.StreamRemoteCall.executeCall(StreamRemoteCall.java:227)
	at sun.rmi.server.UnicastRef.invoke(UnicastRef.java:160)
	at com.sun.jmx.remote.internal.PRef.invoke(Unknown Source)
	at javax.management.remote.rmi.RMIConnectionImpl_Stub.invoke(Unknown Source)
	at javax.management.remote.rmi.RMIConnector$RemoteMBeanServerConnection.invoke(RMIConnector.java:1001)
	at javax.management.MBeanServerInvocationHandler.invoke(MBeanServerInvocationHandler.java:305)
	at $Proxy0.loadBalance(Unknown Source)
	at org.apache.cassandra.tools.NodeProbe.loadBalance(NodeProbe.java:352)
	at org.apache.cassandra.tools.NodeCmd.main(NodeCmd.java:541)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readByte(DataInputStream.java:267)
	at sun.rmi.transport.StreamRemoteCall.executeCall(StreamRemoteCall.java:213)
	... 8 more
{noformat};;;","26/Jan/13 04:40;jbellis;Added JMX notifications to repair in CASSANDRA-4767;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Cassandra silently loses data when a single row gets large (under ""heavy load"")",CASSANDRA-9,12419172,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,neophytos,neophytos,22/Mar/09 19:35,16/Apr/19 17:33,22/Mar/23 14:57,16/Apr/09 04:31,0.3,,,,0,,,,,,"When you insert a large number of columns in a single row, cassandra silently loses some or all of these inserts while flushing memtable to disk (potentialy leaving you with zero-sized data files). This happens when the memtable threshold is violated, i.e. when currentSize_ >= threshold_ (MemTableSizeInMB) OR  currentObjectCount_ >= thresholdCount_ (MemTableObjectCountInMillions). This was a problem with the old code in code.google and the code with the jdk7 dependencies also. No OutOfMemory errors are thrown, there is nothing relevant in the logs. It is not clear why this happens under heavy load (when no throttle is used) as it works fine when when you pace requests. I have confirmed this with another member of the community.


In storage-conf.xml:

   <HashingStrategy>RANDOM</HashingStrategy>
   <MemtableSizeInMB>32</MemtableSizeInMB>
   <MemtableObjectCountInMillions>1</MemtableObjectCountInMillions>
   <Tables>
      <Table Name=""MyTable"">
          <ColumnFamily ColumnType=""Super"" ColumnSort=""Name"" Name=""MySuper""></ColumnFamily>
      </Table>
  </Tables>

You can also test it with different values for thresholdCount_ In db/Memtable.java, say:
    private int thresholdCount_ = 512*1024;


Here is a small program that will help you reproduce this (hopefully):

    private static void doWrite() throws Throwable
    {
        int numRequests=0;
        int numRequestsPerSecond = 3;
        Table table = Table.open(""MyTable"");
        Random random = new Random();
        byte[] bytes = new byte[8];
        String key = ""MyKey"";
        int totalUsed = 0;
        int total = 0;
        for (int i = 0; i < 1500; ++i) {
            RowMutation rm = new RowMutation(""MyTable"", key);
            random.nextBytes(bytes);
            int[] used = new int[500*1024];
            for (int z=0; z<500*1024;z++) {
                used[z]=0;
            }
            int n = random.nextInt(16*1024);
            for ( int k = 0; k < n; ++k ) {
                int j = random.nextInt(500*1024);
                if ( used[j] == 0 ) {
                    used[j] = 1;
                    ++totalUsed;
                    //int w = random.nextInt(4);
                    int w = 0;
                    rm.add(""MySuper:SuperColumn-"" + j + "":Column-"" + i, bytes, w);
                }
            }
            rm.apply();
            total += n;
            System.out.println(""n=""+n+ "" total=""+ total+"" totalUsed=""+totalUsed);
            //Thread.sleep(1000*numRequests/numRequestsPerSecond);
            numRequests++;
        }
        System.out.println(""Write done"");
    }


PS. Please note that (a) I'm no java guru and (b) I have tried this initially with a C++ thrift client. The outcome is always the same: zero-sized data files under heavy load --- it works fine when you pace requests.","code in trunk, linux-2.6.27-gentoo-r1/, java version ""1.7.0-nio2"", 4GB, Intel Core 2 Duo",avinash.lakshman@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-59,CASSANDRA-76,,,,,,,,,,,,,,"09/Apr/09 23:29;jbellis;0001-better-fix-for-9-v2.patch;https://issues.apache.org/jira/secure/attachment/12405077/0001-better-fix-for-9-v2.patch","07/Apr/09 05:45;jbellis;0001-better-fix-for-9.patch;https://issues.apache.org/jira/secure/attachment/12404773/0001-better-fix-for-9.patch","24/Mar/09 04:58;jbellis;executor.patch;https://issues.apache.org/jira/secure/attachment/12403456/executor.patch","24/Mar/09 07:36;neophytos;shutdown-before-flush-against-trunk.patch;https://issues.apache.org/jira/secure/attachment/12403467/shutdown-before-flush-against-trunk.patch","24/Mar/09 22:53;jbellis;shutdown-before-flush-v2.patch;https://issues.apache.org/jira/secure/attachment/12403525/shutdown-before-flush-v2.patch","25/Mar/09 00:03;jbellis;shutdown-before-flush-v3-trunk.patch;https://issues.apache.org/jira/secure/attachment/12403528/shutdown-before-flush-v3-trunk.patch","24/Mar/09 06:16;jbellis;shutdown-before-flush.patch;https://issues.apache.org/jira/secure/attachment/12403465/shutdown-before-flush.patch",,,,,,,,7.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19513,,,Wed Apr 15 20:31:11 UTC 2009,,,,,,,,,,"0|i0fwc7:",90837,,,,,Normal,,,,,,,,,,,,,,,,,"22/Mar/09 23:44;junrao;Neo,

Is the problem specific to super CF or does it show up in regular CF too? Also, have you tried flushing smaller amount of data to disk. In cassandra, if you insert a row with key ""FlushKey"", it forces a flush on the CF referenced in the insertion.

Jun;;;","23/Mar/09 00:25;neophytos;Hi Jun, I've done most of the tests using super CFs. Having said that I just tried it with a name-sorted regular CF and the outcome seems to be the same (zero-sized data files when no throttle is used). Please don't take my word for regular CFs and try it out. One of the reasons it took me so long to report this in public was that I was not sure if (a) it was a problem specific to the hardware I'm using or (b) misuse of Cassandra's constructs. 

For the case of super CFs, I did extensive testing with the code.google codebase and I did try lowering the thresholds (i.e. threshold_ and thresholdCount_ in Memtable.java). The result of that was that it would write some of the files fine while others were zero-sized. I've tried it by forcing flushes (FlushKey), no.;;;","23/Mar/09 00:27;neophytos;Last line should read: I've not tried it by forcing flushes (FlushKey), no. ;;;","24/Mar/09 04:58;jbellis;This is your old friend the ConcurrentModificationException, Neophytos.  Only the ThreadPoolExecutor is eating the exception.  Took me hours to figure out where the hell the exception was disappearing to...  Here's a patch that exposes it.  Not sure what the fix for the CME is yet but at least it's out in the open and reproducible. :);;;","24/Mar/09 05:53;jbellis;Okay, I see the problem.

Doing an insert (-> CFS.apply -> MT.put) checks for threshold violation; if it's okay, it schedules a Putter with the mutation on the Memtable.apartments thread pool.

If it is NOT okay, it schedules a flush -- _on another thread pool_ (MemtableManager's flusher).

So, what happens is, you have a bunch of Putter objects, each with a reference to the old Memtable, in the first threadpool, when the flush starts in the 2nd.  These Putters cause the CME when they get to resolve() while the flush is computing the column index.  (This is why it is easier to make this happen on large rows: index computation takes longer).

I think the easiest fix will be to make the apartments threadpool (executorservice) non-static, and just have one per memtable; then flush could wait for the service to finish before doing its thing.  Memory and thread churn will be nominal since memtable flush is so rare, relatively speaking.  Creating a new thread after flush is no big deal.

I'll get on the patch, just wanted to post an update in the meantime so nobody else needs to bang his head on this.;;;","24/Mar/09 06:16;jbellis;Here is the patch, following my proposed fix above.  Works like a champ.;;;","24/Mar/09 07:26;neophytos;Confirmed. Thank you Jonathan.;;;","24/Mar/09 07:36;neophytos;Just a quick diff against code in trunk for your convenience. Please verify with Jonathan's patch before commit.;;;","24/Mar/09 22:53;jbellis;Here is a cleaner solution that adds the flush to the ExecutorService terminated() method which seems cleaner than having the flush itself (running in the Manager service) reach back to the Memtable service and block while waiting for shutdown.  (In a busy system we don't want to block the Manager service.)

Note that this also handles waiting for gets() to finish before flushing -- any logic purely in put() will not be able to do that, because get never checks threshold or acquires a lock.;;;","24/Mar/09 23:50;avinash.lakshman@gmail.com;I am going to look at this once I get into work. I will apply/fix this today.;;;","25/Mar/09 00:04;jbellis;v3 applies cleanly against trunk.;;;","25/Mar/09 05:58;avinash.lakshman@gmail.com;The problem was identified by Jonathan Ellis. I have a fix checked in that requires a change only in the Memtable class. Neophytos has verified that my change actually works. But the credit goes to Jonathan for identifying the problem which was the harder part of this whole exercise. I am deeming this case as closed.;;;","25/Mar/09 06:12;jbellis;the problem with r758044 is it does not address GETs -- you can still have Getter ops on the the service added after the Flusher, so they will execute during / after the flush.  that is why I split the apartments_ into a per-memtable instance var and run the flush on terminate.  it's the cleanest way to be correct w/ gets w/o introducing explicit locks.;;;","25/Mar/09 06:32;avinash.lakshman@gmail.com;Hmm. Should that matter. Gets do not modify the collection. I was under the impression that CME occurs when one thread tries to modify a collection when another is iterating over it. I will continue to look. Of course the whole apartment concept was introduced to eliminate locks.;;;","25/Mar/09 07:18;jbellis;Gets won't cause the CME, flush will. :)  calling columnFamily.clear(); as it goes can cause a CME as Get looks through it.  Of course even if it did not you would get back invalid results operating on a half-cleared-out Memtable.


In general it is just difficult to reason about concurrency when an object is being accessed from multiple threads at a time, even if it were okay today to ""cheat"" a bit, it will probably bite us down the road.;;;","25/Mar/09 07:49;avinash.lakshman@gmail.com;Not sure if there can ever be a Getter on the queue after a Flusher has been enqueued. Once you are in the state where a Flusher() has been enqueued there can be no Getter() for the same Memtable. Anyways I will look into it tonight again.;;;","25/Mar/09 08:21;jbellis;> Not sure if there can ever be a Getter on the queue after a Flusher has been enqueued. Once you are in the state where a Flusher() has been enqueued there can be no Getter() for the same Memtable.

That is how easy it is to be fooled in these things -- that is what we want, but we are not enforcing that.

In particular note that the line

    		cf = apartments_.get(cfName_).submit(call).get();

is not atomic.

GET thread can execute apartments_.get(cfName_)

then PUT thread gets CPU (or executes concurrently on another core), switches memtable, and queues Flusher.

Thread A gets the CPU back and calls submit.  Getter is now on queue after Flusher.;;;","25/Mar/09 08:26;avinash.lakshman@gmail.com;Ahh. I see. For some reason I was seeing from inside the apartment. This is no good. I will fix it tonight. ;;;","25/Mar/09 08:36;jbellis;Sorry, that's the right result but the wrong explanation.  (Son was howling at me -- very distracting.)

It is the getter creation / submit that is problematic, not the apartment get/submit.

that is,

{code}
thread A                                                              thread B
new Getter(key, cfName, filter);
                                                                             new Flusher(cLogCtx);
                                                                             submit(flusher);
submit(getter);
{code};;;","25/Mar/09 08:43;sandeep_tata;I think today, because of the way the code stands, you won't enqueue Getters on the table after you enqueue a flusher. But, I don't see how simply adding a flusher to the apartment's DebuggableThreadPool (instead of running the flusher in a separate thread) guarantees that there are no concurrent Putters/Getters still in the threadpool. Am I'm missing something? 

I agree that running flush in terminated() by overriding the method is the cleanest approach. You don't have to rely on the fact that the rest of the code (today) is such that you won't end up queuing a Getter after a Flusher (I'm guessing this is what Jonathan meant by ""cheat"" a bit today). This guarantee is precisely the reason ThreadPoolExecutor provides this hook.



;;;","25/Mar/09 09:05;sandeep_tata;Ah, there's a whole bunch of worker threads talking to CFStore (and therefore memtable) -- I see how we can end up with Getters after adding a Flusher;;;","25/Mar/09 10:44;avinash.lakshman@gmail.com;It is an issue that is actually a non-issue. In the worst case the Getter will return NULL since it read an empty memtable (maybe memtable got cleared). But that is fine because now the disk read will happen from buffer cache. It is not incorrect. No harm will be done.;;;","25/Mar/09 11:04;jbellis;Worst case, the flush clear() happens while the getter is iterating columns, and you get CME.;;;","25/Mar/09 11:32;avinash.lakshman@gmail.com;Get rid of clear() :). It is a useless call anyway.;;;","25/Mar/09 11:54;avinash.lakshman@gmail.com;Actually I take that back. There is no CME unless iterators are involved. But nevertheless the safest thing would be to not do the clear(). And I think everything will be good.;;;","25/Mar/09 12:07;jbellis;I thought the point of clear was to free up memory as the flush progresses.  Isn't that worth a dozen more lines of code?;;;","25/Mar/09 19:15;jbellis;Ah, I see, we were talking about different clear(). Yes, the one from the end that you removed is always irrelevant (and not going to cause a CME).

It is the columnFamily.clear() in the middle that is still there that both frees up memory (\yes, of course by ""free up memory"" I mean ""make it available to be GC'd"") and can cause CME on the iterations that GET does.;;;","27/Mar/09 04:50;junrao;Looking at the latest code. Both flush and put on a CF are submitted to the same ExecutorPool for that CF. Since the ExecutorPool has 1 thread in it, this means that the flushing of an old memtable will not run concurrently with the updates on a new memtable in the same CF. This limits concurrency.
 ;;;","27/Mar/09 04:55;avinash.lakshman@gmail.com;The put thread does not run the flush. You submit to the put thread. It submits it to the manager service. Maybe I am missing something here?;;;","27/Mar/09 04:56;jbellis;Wrong.  The Flusher that goes on the Memtable executor is just a stub that kicks off the real flush in the MemtableManager's executor.

So when you have a Getter queued after that flusher, which can happen as I described above, the getter can get a CME while it is iterating through columns at the same time as the real flush calls cf.clear().;;;","27/Mar/09 04:59;jbellis;(I was writing my comment at the same time as Avinash, so my ""Wrong"" was referring to Jun's assertion that ""the flushing of an old memtable will not run concurrently with the updates on a new memtable"".);;;","27/Mar/09 05:15;junrao;OK. I see it now. The real work of Flush is done in a separate thread. Sorry for the false alarm.;;;","07/Apr/09 05:45;jbellis;Fixes potential CME with GETs.;;;","09/Apr/09 23:04;jbellis;(Todd pointed out that having a per-Memtable executor is also more efficient by not needing to hash CF names to look up the executor.);;;","09/Apr/09 23:29;jbellis;rebased to HEAD;;;","10/Apr/09 02:02;johanoskarsson;From my limited understanding of that code the latest patch gets a +1, looks clean. But I'd recommend that someone with a bit more experience look at it.;;;","10/Apr/09 23:09;avinash.lakshman@gmail.com;I am just confused about one thing here. Why is there a chance of a CME on a get? I mean as far as I know a CME occurs when one thread is iterating (using an iterator) and another tries to modify the collection. That is not something that can happen here on a get, I think. So if that is the case there is no need for this change. Hash function cf name lookup is a non issue here.;;;","10/Apr/09 23:16;jbellis;Right.  Get iterates over the columns, depending on the filter used.  Flush still clears out each CF as it is flushed:

                ssTable.append(partitioner.decorateKey(key), buffer);
                bf.add(key);
                columnFamily.clear();

This is behavior I want to keep since the overhead can be relatively high when column values are small.  And the new code is simpler to reason about since you only ever have one thread accessing things rather than executing gets during the flush.  (If we took the clear() out, we would be ok for now, but what if someone changes flush in six months?  One thread at a time is safer, especially vs _almost_ always one thread at a time which becomes easy to forget the exceptions.);;;","10/Apr/09 23:50;jbellis;Forgot to mention one more benefit to executor-per-memtable: this lets us easily call forceFlush in tests and then wait for the flush to finish before proceeding to do tests on the flushed sstable.  (That is why #59 blocks on this.);;;","11/Apr/09 00:03;avinash.lakshman@gmail.com;Why would you want to wait to do tests? In the real world that is not what happens. You should be able to do reads even before the flush is complete. It should be seamless. Even a new memtable is served out the old one is maintained till the flush is complete. So this should really matter. If you just want to test the writes into SSTable then write into it and then test. I think this should not be a reason for the proposed change. May I missing something here.;;;","11/Apr/09 00:06;jbellis;It's a side benefit for the change, not a motivation.

Certainly testing a flush and making sure the resulting sstable has the same data that the memtable did is a good test to have.;;;","14/Apr/09 08:33;tlipcon;Here's a review against the newest patch:

First, some style nits in Memtable.java:
  - runningExecutorServices member variable should have a trailing _ for style consistency
  - same with executor_

Regarding the actual contents of the patch, I sort of dislike subclassing the executor to do work on terminate, but it's the cleanest solution I can think of, so +1;;;","14/Apr/09 13:03;jbellis;I will make the style changes; thanks for the review, Todd.

Any further discussion needed before commit?;;;","16/Apr/09 04:31;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StorageProxy sends same message multiple times,CASSANDRA-2557,12505158,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,skamio,skamio,26/Apr/11 14:45,16/Apr/19 17:33,22/Mar/23 14:57,26/Apr/11 15:00,0.8.0 beta 2,,,,0,,,,,,"A cassandra node gets multiple mutation messages (in number of times of replication factor at maximum) for an insert. It may cause high load on the node. The mutation should be only once for each insert.

This bug is visible via MutationStage count in nodetool tpstats.
For instance, if you have 6 node cluster (initial keys are 31, 32, 33, 34, 35 and 36) with replication factor = 4 and a single data (for example, key='2') is inserted, MutationStage count will be as follows:

node 1: MutationStage 0 0 4
node 2: MutationStage 0 0 3
node 3: MutationStage 0 0 2
node 4: MutationStage 0 0 1
node 5: MutationStage 0 0 0
node 6: MutationStage 0 0 0

As you can see, the counts are different in each node.
",linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Apr/11 14:48;skamio;StorageProxy.java.patch;https://issues.apache.org/jira/secure/attachment/12477367/StorageProxy.java.patch",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20696,,,Tue Apr 26 15:22:04 UTC 2011,,,,,,,,,,"0|i0gc2n:",93386,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"26/Apr/11 14:48;skamio;The problem occurs because the sendMessages() is within for-loop of StorageProxy. A patch to fix that is attached.;;;","26/Apr/11 15:00;slebresne;Good catch. Committed, thanks!;;;","26/Apr/11 15:29;hudson;Integrated in Cassandra-0.8 #40 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/40/])
    Fix sending mutation messages multiple times
patch by skamio; reviewed by slebresne for CASSANDRA-2557
;;;","26/Apr/11 23:22;hudson;Integrated in Cassandra #865 (See [https://builds.apache.org/hudson/job/Cassandra/865/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ColumnFamilyOutputFormat only writes the first column,CASSANDRA-1842,12492961,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jeromatron,brandon.williams,brandon.williams,11/Dec/10 01:24,16/Apr/19 17:33,22/Mar/23 14:57,12/Dec/10 12:03,0.7.0 rc 3,,,,0,,,,,,"In CASSANDRA-1774 we fixed a problem where only the last column was being written.  However, it appears that we only write the first one now.  This is easy to observe in the word count example:

{noformat}
RowKey: text2
=> (column=word1, value=1, timestamp=1291666461685000)
{noformat}

is what the output for text2 looks like, but there should be another column, word2.  If the word count is run without CFOF it works correctly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Dec/10 11:12;jeromatron;1842-2.txt;https://issues.apache.org/jira/secure/attachment/12466092/1842-2.txt","12/Dec/10 10:35;jbellis;1842.txt;https://issues.apache.org/jira/secure/attachment/12466091/1842.txt",,,,,,,,,,,,,2.0,jeromatron,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20340,,,Sun Dec 12 03:49:56 UTC 2010,,,,,,,,,,"0|i0g7qv:",92685,,,,,Normal,,,,,,,,,,,,,,,,,"12/Dec/10 10:35;jbellis;This is a bug where the WordCount example incorrectly wraps a byte array that is subsequently re-used w/o copying.  Fix attached.;;;","12/Dec/10 11:12;jeromatron;Added some housekeeping to the patch like a semi-colon to the README and some naming in WordCount.java - also tested.;;;","12/Dec/10 11:49;hudson;Integrated in Cassandra-0.7 #71 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/71/])
    copy Text bytes because it gets re-used
patch by jbellis; tested by Jeremy Hanna for CASSANDRA-1842
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bootstrapping doesn't work on new clusters,CASSANDRA-696,12445444,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,gdusbabek,gdusbabek,gdusbabek,14/Jan/10 06:19,16/Apr/19 17:33,22/Mar/23 14:57,15/Jan/10 00:48,0.5,,,,0,,,,,,"This is an edge case.

1. start a clean 3 node cluster with autobootstrap on.
2. load some data.
3. bootstrap in a 4th node.

the logs in the 4th node will indicate that data was not received.  If you restart the cluster in between steps 1 and 2, or 2 and 3, boot strapping works fine.  

I find that waiting on the table flush when making the streaming request solves the problem (see patch).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-682,,,,,,,,,"14/Jan/10 06:20;gdusbabek;wait_for_memtable_flush.patch;https://issues.apache.org/jira/secure/attachment/12430180/wait_for_memtable_flush.patch",,,,,,,,,,,,,,1.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19825,,,Thu Jan 14 16:48:42 UTC 2010,,,,,,,,,,"0|i0g0in:",91514,,,,,Low,,,,,,,,,,,,,,,,,"14/Jan/10 06:20;gdusbabek;block on the table flush.;;;","14/Jan/10 06:29;jbellis;is this a 0.5 bug?;;;","14/Jan/10 07:12;gdusbabek;Just tested on 0.5... yes, the bug is there too.;;;","14/Jan/10 07:19;gdusbabek;To be precise, it looks like anything still in the memtables will not be streamed to the bootstrapping node since we're not blocking on the table.flush() call.  I haven't researched it enough, but I suspect the same thing will happen in an established cluster: everything already committed to SSTables gets streamed, and anything still left in the memtable is left behind.  It's just that in a brand new cluster, there never are any SSTables in the first place--it makes the bug more obvious.;;;","14/Jan/10 07:22;jbellis;+1.  can you commit to 0.5 and merge to trunk?

;;;","14/Jan/10 07:22;jbellis;(minor tweak: can you change the re-throw from Interrupted to AssertionError?);;;","15/Jan/10 00:48;gdusbabek;r899280 (0.5)
r899290 (trunk);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tombstones not collected post-repair,CASSANDRA-2279,12500654,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,j.casares,j.casares,08/Mar/11 01:42,16/Apr/19 17:33,22/Mar/23 14:57,09/Apr/11 06:23,0.7.5,,Legacy/Tools,,0,,,,,,"The keys would only show up in sstables2json and look like this:

(root@aps4):/opt/cassandra/storage/queue/data/Panama Wed Feb 23 07:24:34am 
===> /opt/cassandra/bin/sstable2json Queue-2527-Data.db -k waq:publicMessageIndexingWorkArea:PUM8a65ce95-9d35-4941-928c-dd5965e8b29b 
2011-02-23 07:24:43,710 INFO [org.apache.cassandra.config.DatabaseDescriptor] - DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap 
2011-02-23 07:24:43,972 INFO [org.apache.cassandra.io.SSTableReader] - Opening /opt/cassandra/storage/queue/data/Panama/Queue-2527-Data.db 
{ 
""waq:publicMessageIndexingWorkArea:PUM8a65ce95-9d35-4941-928c-dd5965e8b29b"": [] 
} 
(root@aps4):/opt/cassandra/storage/queue/data/Panama Wed Feb 23 07:24:44am 
===>

The steps that I took to reproduce it were:
Create a keyspace, column family, and a key
Delete the key on Node 1 using the cli (del cf['key'];)
Flush 
Repair on a cluster with more than 1 node
Wait GCSeconds 
Compact
And the empty row would appear on Node 2

However, when I was able to get rid of the empty rows, I was following these steps on a single machine: 
Create a keyspace, column family, and a key
Delete the key
Flush
Sample write (writing to some temporary key)
Deleting the attribute to that temporary key (not the entire key)
Flush
Compact

or these steps:
Create a keyspace, column family, and a key
Delete the key
Flush 
Wait GCseconds
Compact

",,cburroughs,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Mar/11 02:18;slebresne;RowIteration-unit-tests.patch;https://issues.apache.org/jira/secure/attachment/12473595/RowIteration-unit-tests.patch","15/Mar/11 02:18;slebresne;fix-RowIteratorFactory.patch;https://issues.apache.org/jira/secure/attachment/12473596/fix-RowIteratorFactory.patch","15/Mar/11 06:27;j.casares;nodeA.txt;https://issues.apache.org/jira/secure/attachment/12473620/nodeA.txt","15/Mar/11 06:27;j.casares;nodeB.txt;https://issues.apache.org/jira/secure/attachment/12473621/nodeB.txt",,,,,,,,,,,4.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20537,,,Fri Apr 08 22:23:58 UTC 2011,,,,,,,,,,"0|i0gafj:",93120,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"09/Mar/11 23:35;slebresne;I'm not able to reproduce. And I don't see why repair would screw up with tombstones collection. Only compaction should remove them and repair shouldn't have anything to do with this. If sstable2json is the only one to show it, it's even weirder (that is, listing with the cli should also show an empty row if there was problem with tombstone collection).

Are you sure you compacted node 2 (on which the empty row showed up) ? Are you sure there wasn't a Queue-2527-Compacted file alongside Queue-2527-Data.db ? Are you sure node 2 local time wasn't highly out of sync with node 1 ?;;;","11/Mar/11 18:30;slebresne;Ok, strongest hypothesis: this is a sign of CASSANDRA-2305.
Joaquim: was some row cache enabled on the incriminated CF ?;;;","12/Mar/11 01:35;j.casares;The entire thing was reproduced on a fresh install and I was just using the standard Keyspace that already exists, I believe. I'll go through and follow my steps again and give a more detailed account.;;;","14/Mar/11 21:44;slebresne;It's a bit of a shot in the dark since I'm not sure exactly how was produced and how to reproduce this, but I found a place in RowIteratorFactor where we don't handle a CF localDeletionTime correctly. Since RowIterator is used for repair and for sstable2json, it sounds like a promising candidate for this (and it's a legit problem even if not the one at hand here).

Patch is against 0.7.;;;","14/Mar/11 22:28;jbellis;how hard would it be to create a unit test that catches this?;;;","14/Mar/11 22:38;slebresne;Should be doable. I'll do that.;;;","15/Mar/11 00:50;slebresne;Attaching the unit test.;;;","15/Mar/11 01:09;jbellis;what's the significance of gc_grace_seconds: 2 in the test CF?  I don't see any sleeps in the test.;;;","15/Mar/11 01:19;slebresne;Sorry, that was me trying stuff and forgetting to remove it. I update the test to use an existing column with standard gc_grace;;;","15/Mar/11 02:18;slebresne;There was actually 2 related bugs in RowIteratorFactory. Re-attaching new patch with the 2 unit tests and the fix (fixing both problem and simplifying, I think, RowIteratorFactory).;;;","15/Mar/11 03:32;jbellis;that's a big improvement over the existing factory code!  committed.

we'll need a separate patch for 0.6 -- RowIteratorFactory doesn't exist (the analogous code is inline in getRangeSlice).;;;","15/Mar/11 03:48;hudson;Integrated in Cassandra-0.7 #378 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/378/])
    fix tombstone handling in repair andsstable2json
patch by slebresne; reviewed by jbellis for CASSANDRA-2279
;;;","15/Mar/11 06:27;j.casares;Here are all the commands that were run. I was messing around with T2 references and the bug didn't show up the first time, so I tried it again.

The second time it worked following the exact steps that I listed. I set the GCSeconds to be 30 minutes to wait for the repair to finish 100%.

This time however, the SSTables actually have the old values as well.;;;","18/Mar/11 22:30;slebresne;bq. we'll need a separate patch for 0.6

I don't think 0.6 suffers from the problem fixed by the attached patch. It uses CFStore.getFamily() for range slices to do its bidding which handles correctly the column family deletion times as far as I can tell.

Which make me think that there could be something else at hand here. I'll have a look at Joaquin instructions to try to reproduce what he is seeing. ;;;","31/Mar/11 06:11;j.casares;I just read this and was wondering if this may be the case:
http://wiki.apache.org/cassandra/Operations#Dealing_with_the_consequences_of_nodetool_repair_not_running_within_GCGraceSeconds

GCGraceSeconds was set to 30 minutes to allow the repair to finish and I waited 35 before running the compaction on the above steps.;;;","09/Apr/11 06:06;jbellis;compaction should be runnable any time, as long as you do repair before gcgrace expires.
;;;","09/Apr/11 06:15;slebresne;For the record, we did try to reproduce with Joaquin and weren't able to find anything wrong in there. In particular, in one instance when we though we had a column coming back from the dead, we were actually looking at a compacted sstable (which sstable2json won't avoid).

So I do not know if there is an actual problem here.;;;","09/Apr/11 06:23;jbellis;Sounds good, thanks for looking into that.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gossiper Starvation,CASSANDRA-2253,12499862,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,mikaels,mikaels,mikaels,28/Feb/11 00:03,16/Apr/19 17:33,22/Mar/23 14:57,01/Mar/11 07:53,0.7.3,,,,0,,,,,,"Gossiper periodic task will get into starvation in case large sstable files need to be deleted.
Indeed the SSTableDeletingReference uses the same scheduledTasks pool (from StorageService) as the Gossiper and other periodic tasks, but the gossiper tasks should run each second to assure correct cluster status (liveness of nodes). In case of large sstable files to be deleted (several GB) the delete operation can take more than 30 sec, thus making the whole cluster going into a wrong state where nodes are marked as not living while they are!
This will lead to unneeded additional load like hinted hand off, wrong cluster state, increase in latency.

One of the possible solution is to use a separate pool for periodic and non periodic tasks. 
I've implemented such change and it resolves the problem. 
I can provide a patch ","linux, windows",mikaels,,,,,,,,,,,,,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,,,,,"01/Mar/11 07:02;mikaels;CASSANDRA-0.7-2253.txt;https://issues.apache.org/jira/secure/attachment/12472246/CASSANDRA-0.7-2253.txt",,,,,,,,,,,,,,1.0,mikaels,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20526,,,Tue Mar 01 00:04:48 UTC 2011,,,,,,,,,,"0|i0ga9r:",93094,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"28/Feb/11 00:57;jbellis;bq. use a separate pool for periodic and non periodic tasks

that's reasonable; so might splitting Gossiper off to its own executor;;;","28/Feb/11 02:05;mikaels;I also though having gossiper in its own executor, nevertheless it means that other periodic tasks may come to starvation because of the heavy non periodic tasks. Therefore i chose to use a separate pool for the heavy one instead.
;;;","28/Feb/11 06:44;jbellis;Sounds good to me.  Were you going to submit a patch?;;;","28/Feb/11 11:43;mikaels;yes;;;","01/Mar/11 07:01;mikaels;Add a new pool for non periodic heavyweight tasks, to eliminate the starvation of periodic short time execution task like Gossiper.
Additionally add debug statement for periodic and non periodic task ;;;","01/Mar/11 07:02;mikaels;patch for bug 2253, Gossip starvation;;;","01/Mar/11 07:53;jbellis;committed the executor change

omitted the debug log statements; the ""right"" way to do that is to create an executor subclass that logs that for us instead of relying on boilerplate code in the caller;;;","01/Mar/11 08:04;hudson;Integrated in Cassandra-0.7 #333 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/333/])
    movefile deletions off of scheduledtasks executor
patch by Mikael Sitruk; reviewed by jbellis for CASSANDRA-2253
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BRAF read can loop infinitely instead of detecting EOF,CASSANDRA-2241,12499622,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,25/Feb/11 02:53,16/Apr/19 17:33,22/Mar/23 14:57,25/Feb/11 23:00,0.7.3,,,,0,,,,,,"(marking this Minor since normally we never try to read past the end of an SSTable, but CASSANDRA-2240 is running into it.)",,,,,,,,,,,,,,,,,,,,,,,21600,21600,,0%,21600,21600,,,,,,,,,,,,,,,,,,,,"25/Feb/11 02:59;jbellis;2241.txt;https://issues.apache.org/jira/secure/attachment/12471862/2241.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19345,,,Fri Feb 25 15:15:13 UTC 2011,,,,,,,,,,"0|i0ga73:",93082,,tjake,,tjake,Low,,,,,,,,,,,,,,,,,"25/Feb/11 02:59;jbellis;Adds tests for this problem and similar ones around EOF behavior.

Fixes bug and makes some changes to make BRAF more straightforward:

- ByteBuffer buffer -> byte[] (no more confusion between ""current"" and buffer's internal markers)

- Instead of bufferEnd (easy to confuse with bufferOffset + buffer.length), use validBufferBytes

- removes hitEOF

- gives read() the behavior of readAtMost(), which was the behavior read() is *supposed* to have

- avoid allocating new byte[1] for each call to write(int)

- adds asserts for expected internal state

- fixes length() when we seek to an earlier position that is still inside the current buffer;;;","25/Feb/11 03:00;jbellis;(At least some of these are regressions introduced by CASSANDRA-1470.);;;","25/Feb/11 04:32;tjake;I don't see anything here that is a problem, but I'll run it through some workloads...;;;","25/Feb/11 09:27;tjake;Ok, I ran this through some more tests and it looks good to me +1;;;","25/Feb/11 23:00;jbellis;committed;;;","25/Feb/11 23:15;hudson;Integrated in Cassandra-0.7 #321 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/321/])
    fix BufferedRandomAccessFile bugs
patch by jbellis; reviewed by tjake for CASSANDRA-2241
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"time-based slicing does not work correctly w/ ""historical"" memtables",CASSANDRA-223,12427409,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,09/Jun/09 06:09,16/Apr/19 17:33,22/Mar/23 14:57,27/Jun/09 01:26,0.4,,,,0,,,,,,"TimeFilter assumes that it is done as soon as it finds a column stamped earlier than what it is filtering on, but when you have a group of ""historical"" memtables whose columns were written in an arbitrary order this is not a safe assumption.

It is not even a safe assumption when dealing with a single memtable + sstable pair, as the attached new test shows.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jun/09 05:33;jbellis;223-2-v2.patch;https://issues.apache.org/jira/secure/attachment/12411864/223-2-v2.patch","26/Jun/09 05:24;jbellis;223-2.patch;https://issues.apache.org/jira/secure/attachment/12411863/223-2.patch","09/Jun/09 06:10;jbellis;223.patch;https://issues.apache.org/jira/secure/attachment/12410183/223.patch",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19600,,,Fri Jun 26 17:26:47 UTC 2009,,,,,,,,,,"0|i0fxmf:",91045,,,,,Normal,,,,,,,,,,,,,,,,,"09/Jun/09 22:02;jbellis;Thinking about it more, the current behavior is sort of reasonable if you assume that timestamp values are strictly increasing.  I prefer not to rely on this because it's impossible to sync clocks with perfect accuracy, but it's a reasonable optimization to make and consistent with the rest of Cassandra's design.

In this case though, clock sync problems can lead to permanently inconsistent behavior -- different queries will not agree on what data the node contains.;;;","10/Jun/09 01:02;junrao;The assumption that columns in an SSTable with a larger file name always have more recent timestamp also breaks in the following cases:
1. after SSTables are compacted (since the compaction works on a group of SSTables at a time)
2. hinted data is delivered
3. columns fixed through read repairs
;;;","10/Jun/09 22:39;junrao;get_column is also designed to stop iterating SSTables as soon as the requested column is found. Therefore, it could suffer from the above problems too.
;;;","22/Jun/09 23:51;junrao;Since it's hard to guarantee any timestamp ordering among memtable and sstables because of the reasons listed above, the only way that we can get the correct answer is do look at the memtable and ALL sstables (this is what HBase has been doing). This affects the following APIs.
get_column
get_slice_by_names
get_slice_by_name_range
get_slice_since

The implication is that those APIs will potentially run slower since there are more files to read. One can probably tune the performance by setting a proper compaction policy.
;;;","23/Jun/09 00:04;jbellis;I came to the same conclusion.

One partial answer to the files-to-read is to change compaction to guarantee log(n) sstable files instead of the current ad-hoc behavior, where n is the maximum sstable ""generation"" number.  (Where ""generation"" is the number of compactions done.)

For each CF, when you flush, you compact until there is nothing already at the same generation to compact with.  For example,

flush 1: nothing to merge.  memtable becomes sstable-gen0
flush 2: there is already a sstable-gen0 so you merge.  now you have sstable-gen1
flush 3: no gen0, so you store there.  now you have sstable-gen0, sstable-gen1
flush 4: 0 and 1 exist, so you compact (with the new one) to sstable-gen2

etc.

Generation tracking can be done in the sstable filename.;;;","23/Jun/09 02:40;junrao;Well, as for SSTable compaction, we probably can learn from Lucene. Overall, Lucene tries to keep on the order of log(n) index segments, where n is the total number of segments generated. It does that by keeping merging index segments (up to a merging factor) of about the same size. The current compaction code in cassandra seems to try to do that too. It's worth revisiting it though.
;;;","26/Jun/09 05:24;jbellis;add brute-force fix for the bug: always merge in results from all memtables and sstables.  (this is what bigtable does.);;;","26/Jun/09 05:33;jbellis;oops.  already had to rebase.;;;","27/Jun/09 01:02;junrao;The patch looks good to me.
;;;","27/Jun/09 01:26;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix junit related build issues,CASSANDRA-164,12425240,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,johanoskarsson,johanoskarsson,johanoskarsson,13/May/09 00:51,16/Apr/19 17:33,22/Mar/23 14:57,13/May/09 02:51,0.3,,,,0,,,,,,Since the junit switch no xml report files are generated and the build doesn't fail properly if a test fails,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/May/09 00:51;johanoskarsson;CASSANDRA-164.patch;https://issues.apache.org/jira/secure/attachment/12407896/CASSANDRA-164.patch",,,,,,,,,,,,,,1.0,johanoskarsson,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19579,,,Wed May 13 14:59:32 UTC 2009,,,,,,,,,,"0|i0fx9j:",90987,,,,,Normal,,,,,,,,,,,,,,,,,"13/May/09 02:51;jbellis;applied, with the change to retain
      <formatter type=""brief"" usefile=""false""/>

instead of printsummary.  (the brief formatter includes the stacktrace for failures which is nice to have right there.);;;","13/May/09 17:16;johanoskarsson;The problem is that then we won't get the junit xml files, which means hudson can't pick them up and tell us what went wrong. I'll see if we can get both. 

As a side note I wish you wouldn't commit patches with additional, non reviewed changes. Personally I think it's better to comment on the issue and let the developer make the changes or argue a case against them.;;;","13/May/09 17:18;johanoskarsson;Guess I spoke to soon, the xml one is there too. My other comment still stands though :);;;","13/May/09 17:26;hudson;Integrated in Cassandra #73 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/73/])
    fix junit-related build issues.  patch by johano; reviewed by jbellis for 
;;;","13/May/09 22:50;jbellis;> I wish you wouldn't commit patches with additional, non reviewed changes

Refrain from judgement for a moment and let's look at how the workflow compares.

Without taking shortcuts:

If my change is good:

1) original patch uploaded
2) my patch uploaded
3) my patch approved
4) commit

If my change is not good:

1) original patch uploaded
2) my patch uploaded
3) new patch from reviewer
4) commit

(obviously repeat 2-3 as necessary, let's keep things simple though).

Now, with shortcuts:

If my change is good:

1) original patch uploaded
2) commit patch w/ my changes

If my change is not good:

1) original patch uploaded
2) commit patch w/ my changes
3) new patch from reviewer against my commit
4) commit new patch

So in the case where my change is not good, we have basically the same steps and our only loss is an extra svn commit at step 2.

But when my change _is_ good we cut the steps where someone is waiting on someone else in half.

That seems like the risk is worth it to me.  (And my track record is very good here; I can't remember ever making a change that the author/reviewer disagreed with.  Part of that is if I make a nontrivial change I do in fact upload patches first.);;;","13/May/09 22:59;johanoskarsson;I know we are not going to agree on this, but wanted to throw it out there anyway.

I didn't mean that you as a committer should upload a new, revised patch, that would be quite a workload. Instead what I was suggesting was that you suggest the changes needed in the ticket, wait for the developer to either submit a new patch or discuss your suggested changes. 

It's not that I don't trust your code changes. The main reason is that imho the latest patch in jira should be the one applied to svn for a few reasons, transparency being one and from my experience it is very helpful for power users to be able to download the patch and apply it to older versions of the software. We do this all the time at Last.fm with our Hadoop installation.

I'm not going to push this further, just wanted it said.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C* .deb installs C* init.d scripts such that C* comes up before mdadm and related,CASSANDRA-2481,12504307,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thepaul,mdennis,mdennis,15/Apr/11 02:51,16/Apr/19 17:33,22/Mar/23 14:57,23/May/11 23:45,0.7.6,0.8.0,Packaging,,0,,,,,,the C* .deb packages install the init.d scripts at S20 which is before mdadm and various other services.  This means that when a node reboots that C* is started before the RAID sets are up and mounted causing C* to think it has no data and attempt bootstrapping again.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/May/11 00:44;thepaul;2481-fix.txt;https://issues.apache.org/jira/secure/attachment/12479618/2481-fix.txt","13/May/11 02:26;thepaul;2481.txt;https://issues.apache.org/jira/secure/attachment/12479001/2481.txt",,,,,,,,,,,,,2.0,thepaul,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20644,,,Fri May 20 16:51:36 UTC 2011,,,,,,,,,,"0|i0gbmf:",93313,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"19/Apr/11 00:26;hudson;Integrated in Cassandra-0.8 #14 (See [https://hudson.apache.org/hudson/job/Cassandra-0.8/14/])
    add optional replication_factor fields to KsDef to make supporting both 0.8 and 0.7 easier for client devs
patch by jbellis; reviewed by Nate McCall for CASSANDRA-2481
;;;","13/May/11 02:27;thepaul;patch solverizes this problem. uses priority 50, or requires mdadm to be started first if using dependency-based init ordering.;;;","13/May/11 03:27;jbellis;Out of curiosity, why aren't start/stop symmetrical?

{noformat}
+	dh_installinit -u'start 50 2 3 4 5 . stop 50 0 1 6'
{noformat};;;","13/May/11 04:05;thepaul;bq. Out of curiosity, why aren't start/stop symmetrical?

they are: ((start = 50) == 100 - (end = 50))

If you were referring to the ""2 3 4 5"" vs ""0 1 6"", those are the standard ""starting stuff"" and ""stopping stuff"" runlevels for SysV init.;;;","13/May/11 04:46;brandon.williams;Committed.;;;","13/May/11 05:10;hudson;Integrated in Cassandra-0.7 #484 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/484/])
    Start/stop cassandra after more important services such as mdadm in
debian packaging.
Patch by Paul Cannon, reviewed by brandonwilliams for CASSANDRA-2481
;;;","18/May/11 18:05;slebresne;When installing the debian package for 0.7.6 and 0.8.0-rc1 on ubuntu 11.04 (natty), I get
{noformat}
Installing new version of config file /etc/init.d/cassandra ...
update-rc.d: error: start|stop arguments not terminated by "".""
usage: update-rc.d [-n] [-f] <basename> remove
       update-rc.d [-n] <basename> defaults [NN | SS KK]
       update-rc.d [-n] <basename> start|stop NN runlvl [runlvl] [...] .
       update-rc.d [-n] <basename> disable|enable [S|2|3|4|5]
		-n: not really
		-f: force
{noformat}

Given that it works like a charm with 0.7.5, I strongly suspect this is this patch doing.;;;","18/May/11 23:32;gasolwu;{quote}
Installing new version of config file /etc/init.d/cassandra ...
update-rc.d: error: start|stop arguments not terminated by "".""
{quote}

find dh_installinit in debian/rules and edit like following line.

dh_installinit -u'start 50 2 3 4 5 . stop 50 0 1 6 .'

it works for me.;;;","19/May/11 00:44;thepaul;fix for previous;;;","19/May/11 01:10;brandon.williams;Fix committed.;;;","19/May/11 01:48;hudson;Integrated in Cassandra-0.7 #488 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/488/])
    Fix for dh_installinit syntax for CASSANDRA-2481
Patch by Paul Cannon, reviewed by brandonwilliams

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1124338
Files : 
* /cassandra/branches/cassandra-0.7/debian/rules
;;;","20/May/11 23:24;sword2;This is still broken for 0.8.0~rc1.  The last commit to it shows a merge with 0.7 but this fix is not in it.

quick workaround is to modify the cassandra.postinst file.  You can get that from dpkg-query -c cassandra, mine is under /var/lib/dpkg/info/:
{noformat}
sudo cp /var/lib/dpkg/info/cassandra.postinst /var/lib/dpkg/info/cassandra.postinst.original
sudo sed -i 's/50 2 3 4 5 \. stop 50 0 1 6 >/50 2 3 4 5 \. stop 50 0 1 6 \.>/' /var/lib/dpkg/info/cassandra.postinst
#make sure it's correct then run the below line.
#sudo dpkg --configure cassandra
{noformat}

;;;","21/May/11 00:51;jbellis;the fix is in the 0.8.0 branch for -final, but you are right that it is not in rc1.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DatabaseDescriptor static initialization circular reference when initialized through call to StorageService.instance.initClient ,CASSANDRA-1756,12480378,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,gdusbabek,eonnen,eonnen,19/Nov/10 05:58,16/Apr/19 17:33,22/Mar/23 14:57,20/Nov/10 00:31,0.7.0 rc 1,,,,0,,,,,,"In trunk, attempting to invoke StorageService.instance.initClient results in an NPE due to static definition field ordering in StorageService and a circular reference from DatabaseDescriptor back into an uninitialized field (scheduledTasks). Changing the ordering of the static fields such that scheduledTasks is defined before the static partitioner fixes the issue.

I've also marked the scheduledTasks executor as final as it doesn't seem to make sense changing it.

All tests pass with this change locally.

I suspect this hasn't surfaced in tests as calling initServer first in the same JVM will allow later calls to initClient to see the correctly defined scheduledTasks fields.

I'm following the recommended way to do this from ClientOnlyExample, if this isn't the right way to initialize things let me know.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Nov/10 22:34;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-fix-cicular-initialization-problem-between-StorageServ.txt;https://issues.apache.org/jira/secure/attachment/12460006/ASF.LICENSE.NOT.GRANTED--v1-0001-fix-cicular-initialization-problem-between-StorageServ.txt","19/Nov/10 22:34;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0002-StorageService.initClient-unit-test-that-never-finishe.txt;https://issues.apache.org/jira/secure/attachment/12460007/ASF.LICENSE.NOT.GRANTED--v1-0002-StorageService.initClient-unit-test-that-never-finishe.txt","19/Nov/10 22:34;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0003-DynamicEndpointSnitch-shouldn-t-update-if-SS-isn-t-ini.txt;https://issues.apache.org/jira/secure/attachment/12460008/ASF.LICENSE.NOT.GRANTED--v1-0003-DynamicEndpointSnitch-shouldn-t-update-if-SS-isn-t-ini.txt","19/Nov/10 05:59;eonnen;CS-1756;https://issues.apache.org/jira/secure/attachment/12459948/CS-1756",,,,,,,,,,,4.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20295,,,Fri Nov 19 20:05:42 UTC 2010,,,,,,,,,,"0|i0g773:",92596,,,,,Low,,,,,,,,,,,,,,,,,"19/Nov/10 06:01;eonnen;Attached;;;","19/Nov/10 06:57;eonnen;I forgot to add the stack trace resulting in the NPE in the original bug, sorry about that.

Exception in thread ""main"" java.lang.ExceptionInInitializerError
	at org.apache.cassandra.service.StorageService.<clinit>(StorageService.java:199)
	at io.eao.cassandra.http.Main.main(Main.java:24)
Caused by: java.lang.RuntimeException: java.lang.NullPointerException
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:396)
	... 2 more
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.locator.DynamicEndpointSnitch.<init>(DynamicEndpointSnitch.java:74)
	at org.apache.cassandra.config.DatabaseDescriptor.createEndpointSnitch(DatabaseDescriptor.java:403)
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:275);;;","19/Nov/10 21:42;gdusbabek;That patch does fix the circular reference but exposes other problems in the fat client.  I've attached a test case that never finishes.  I suspect over time we've introduced changes that couple a few of the singleton services together in undesirable ways.  

I'll spend some time looking at it this morning and see if I can make sense of it.;;;","19/Nov/10 22:39;gdusbabek;0001 is Erik's patch
0002 is a test case that exposes an bug in the assumption DynamicEndpointSnitch makes about service initialization.
0003 fixes the bug.

Here is the chain of events.
StorageService.initClient() triggers static initializer in DD which instantiates a DynamicEndpointSnitch.  DES, as part of it's constructors schedule some tasks with SS.scheduledTasks which promptly starts executing them.  This causes MessagingService to initialize out of order which tries to access the not-fully-initialized SS, causing deadlock.  

SS cannot finish initializing until MS is done initializing, but MS is waiting on SS to do the same thing, near as I can tell.;;;","19/Nov/10 22:43;jbellis;+1;;;","20/Nov/10 00:31;gdusbabek;committed with a few changes.  It turns out System.exit() in a unit test is not good and I had to change the DES unit test to (rightfully) initialize StorageService.;;;","20/Nov/10 04:05;hudson;Integrated in Cassandra-0.7 #19 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/19/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"get_slice calls do not close files when finished resulting in ""too many open files"" exceptions and rendering C unusable",CASSANDRA-1178,12466596,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,mdennis,mdennis,mdennis,10/Jun/10 03:47,16/Apr/19 17:33,22/Mar/23 14:57,11/Jun/10 08:19,0.7 beta 1,,,,0,,,,,,"insert ~100K rows.  Read them back in a loop.  Notice ""too many open files"" exceptions in log.  SSTableSliceIterator is never closing the files.

",,arya,johanoskarsson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Jun/10 03:49;mdennis;0001-trunk-1178.patch;https://issues.apache.org/jira/secure/attachment/12446719/0001-trunk-1178.patch","11/Jun/10 00:34;mdennis;0002-trunk-1178.patch;https://issues.apache.org/jira/secure/attachment/12446771/0002-trunk-1178.patch",,,,,,,,,,,,,2.0,mdennis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20022,,,Fri Jun 11 12:45:40 UTC 2010,,,,,,,,,,"0|i0g3h3:",91993,,,,,Normal,,,,,,,,,,,,,,,,,"10/Jun/10 07:11;mortazavi;Out of curiosity . . . What is the point, in this patch, in switching import statements around . . . Is there a policy to re-order them? . . . To the extent I can tell, the order of import statements is consistent with what one sees in the other files?

The rest of the fix seems OK to me as long as the ""close"" method in the IColumnIterator interface to SSTableSliceIterator  is used properly to clean up resources . . . (although -- on a different plane -- objects that know, themselves, that they have nothing more to do are more friendly objects to use and harder to design) . . . 




;;;","10/Jun/10 07:59;mdennis;intellij often automatically reorders imports and removes unused ones (they call it ""optimizing imports"").  I just let it do it's thing in this regard (to be honest I've just gotten in the habit of ignoring the import section).

At least for get_slice, close is properly called (as evident by the patch fixing the problem).   It may be the case that for other calls close is not correctly called, I didn't specifically look for such cases but 1) I haven't noticed a problem making other calls and 2) I didn't notice anything in the code while debugging this issue.

In general I agree that self contained objects are better but that doesn't necessarily fit in this case as the open file is really a system resource.  Given the way Java does GC, such resources should be explicitly released.

I discussed this particular case with jbellis and the thought for the original patch that introduced this bug was that some callers would want to reuse an existing file.  In such a case, they would be responsible for closing the file but that meant that SSTableSliceIterator shouldn't close it out from under them.;;;","10/Jun/10 11:52;mortazavi;Sounds reasonable . . . 

It seems to me that he idea is that 

(1) If you pass a non-null file object reference to the constructor, you are the best judge of when to close it. 

(2) If you pass a null, instead, you will have to ask SSTableSliceIterator to clean up after you're done by calling ""close"" on it. 

This is then the semantics of this class when it comes to the file variable. 

I think it may be useful to include this contractual/semantic fact in the javadoc for this class. 

(Side note: It would probably be best not to allow IDE's to change the order of imports from the one that's common everywhere else unless there is a policy by this project to make such reordering. It is better to keep things consistent.)

;;;","11/Jun/10 00:34;mdennis;0002-trunk-1178.patch has suggested JavaDoc changes;;;","11/Jun/10 08:19;jbellis;committed;;;","11/Jun/10 20:45;hudson;Integrated in Cassandra #462 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/462/])
    fix FD leak.  patch by mdennis; reviewed by jbellis for CASSANDRA-1178
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Submit Flush is Failing with a RejectedExecutionException,CASSANDRA-471,12437324,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,dispalt,dispalt,06/Oct/09 06:39,16/Apr/19 17:33,22/Mar/23 14:57,07/Oct/09 22:16,0.5,,Legacy/Tools,,0,,,,,,"This is probably very specific to my BMT loading job; however, I have started running into this problem lately.  

$ bin/nodeprobe -host 127.0.0.1 flush_binary MyApp
Exception in thread ""main"" java.util.concurrent.RejectedExecutionException
        at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:1760)
        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:767)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:658)
        at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:78)
        at org.apache.cassandra.db.ColumnFamilyStore.submitFlush(ColumnFamilyStore.java:926)
        at org.apache.cassandra.db.ColumnFamilyStore.forceFlushBinary(ColumnFamilyStore.java:427)
        at org.apache.cassandra.service.StorageService.forceTableFlushBinary(StorageService.java:802)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1426)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1264)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1359)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:788)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
        at sun.rmi.transport.Transport$1.run(Transport.java:159)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
",,johanoskarsson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Oct/09 00:46;jbellis;471.patch;https://issues.apache.org/jira/secure/attachment/12421440/471.patch",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19706,,,Thu Oct 08 12:35:14 UTC 2009,,,,,,,,,,"0|i0fz53:",91291,,,,,Normal,,,,,,,,,,,,,,,,,"07/Oct/09 00:46;jbellis;when enough compactions queued up, it would reject additional ones instead of waiting for a thread to free up.  this patch fixes that.;;;","07/Oct/09 22:16;jbellis;Chris Were reports on -user that this patch fixes the bug for him.  Committed.;;;","08/Oct/09 20:35;hudson;Integrated in Cassandra #221 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/221/])
    use CallerRunsPolicy instead of rejecting runnables on multi-threaded executors w/ blocking queues
patch by jbellis; tested by Chris Were for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup conversions between bytes and strings,CASSANDRA-2367,12502184,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,23/Mar/11 22:22,16/Apr/19 17:33,22/Mar/23 14:57,29/Mar/11 05:06,0.7.5,,,,0,,,,,,"There is a bit of inconsistency in our conversions between ByteBuffers and Strings.
For instance, ByteBufferUtil.string() uses as a default the java default charset, while ByteBufferUtil.bytes(String) assumes UTF8. Moreover, a number of places in the code don't use those functions and uses getBytes() directly. There again, we often encode with the default charset but decode in UTF8 or the contrary.

Using the default charset is probably a bad idea anyway, since this depends on the actual system the node is running on and could lead to a stupid bug when running in heterogeneous systems.

This ticket proposes to always assume UTF8 all over the place (and tries to use the ByteBufferUtil as much as possible to help with that).",,,,,,,,,,,,,,,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,,,,,"23/Mar/11 22:23;slebresne;0001-Cleanup-bytes-string-conversions.patch;https://issues.apache.org/jira/secure/attachment/12474397/0001-Cleanup-bytes-string-conversions.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20583,,,Mon Mar 28 21:06:22 UTC 2011,,,,,,,,,,"0|i0gayv:",93207,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"23/Mar/11 22:23;slebresne;Attaching patch against 0.7.;;;","24/Mar/11 01:11;jbellis;I see two places this fixes bugs:

- HintedHandOffManger: post-delivery hint deletion is now done w/ UTF8 encoding, which matches old and new encoding of ip-address-as-string.
- SystemTable now encodes cluster name as UTF8; before it encoded as system encoding, decoded as UTF8.

Is that accurate?;;;","24/Mar/11 01:27;slebresne;There is also:
  * the avro schema (DEFINITION_SCHEMA_COLUMN_NAME) for mutation. I was encoded in UTF8 (in Migration.java), but decoded using system encoding (in DefsTable.loadFromStorage(), since decoded by ByteBufferUtil.string() with default charset).
  * In HintedHandOffManager, the combined table and cfName is encoded as UTF8 but decoded with system encoding (once again through the use of BBUtil.string() with no specific charset.
;;;","24/Mar/11 02:13;jbellis;committed to 0.7 w/ some build fixes for contrib/ (so you will want to base port to trunk on r1084660);;;","24/Mar/11 02:34;hudson;Integrated in Cassandra-0.7 #401 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/401/])
    fix encoding bugs in HintedHandoffManager, SystemTable when default charset is not UTF8
patch by slebresne; reviewed by jbellis for CASSANDRA-2367
;;;","29/Mar/11 04:38;hudson;Integrated in Cassandra #812 (See [https://hudson.apache.org/hudson/job/Cassandra/812/])
    ;;;","29/Mar/11 05:06;jbellis;(merged to trunk by Sylvain);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Creating a SuperColumnFamily other than BytesType results in incorrect comparator types ,CASSANDRA-1712,12479178,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,ceocoder,ceocoder,05/Nov/10 16:00,16/Apr/19 17:33,22/Mar/23 14:57,09/Nov/10 05:06,0.7.0 rc 1,,Legacy/Tools,,0,,,,,,"CF 1
    ColumnFamily: CFCli (Super)
      Columns sorted by: org.apache.cassandra.db.marshal.LongType/org.apache.cassandra.db.marshal.UTF8Type
      Subcolumns sorted by: org.apache.cassandra.db.marshal.LongType

was created with cli using 

create column family CFCli with column_type= 'Super' and comparator= 'LongType' and subcomparator='UTF8Type'

 CF 2
 ColumnFamily: CFYaml (Super)
      Columns sorted by: org.apache.cassandra.db.marshal.LongType/org.apache.cassandra.db.marshal.UTF8Type
      Subcolumns sorted by: org.apache.cassandra.db.marshal.LongType

was created with yaml using 

  column_families:
        - name: CFYaml
          column_type: Super
          compare_with: LongType
          compare_subcolumns_with: UTF8Type

In both cases Subcolumn comparator was defined as UTF8Type but CF was created with subcomparatortype of LongType

",ubuntu using 0.7.0 beta 3 bin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20267,,,Mon Nov 08 21:06:06 UTC 2010,,,,,,,,,,"0|i0g6xb:",92552,,,,,Low,,,,,,,,,,,,,,,,,"05/Nov/10 16:03;mdennis;{quote}
(03:00:59 AM) ceocoder1: I tried adding column using cli and my client (scromium) each time it fails with ""shit"" can not be converted to Long where shit is value of subcolumn
(03:01:07 AM) ceocoder1: thanks
(03:01:27 AM) ceocoder1: i meant name of subcolumn
(03:02:03 AM) mdennis: that sounds a lot worse than just an output bug
(03:02:09 AM) ceocoder1: y
(03:02:12 AM) mdennis: it sounds like it actually reversed them
(03:02:19 AM) ceocoder1: regular columns are fine
(03:02:24 AM) ceocoder1: just supercolumns
(03:02:44 AM) mdennis: it's karma saying people shouldn't use super columns
{quote};;;","05/Nov/10 20:17;jbellis;what does ""show keyspaces"" on the cli say the comparators are?;;;","06/Nov/10 01:54;ceocoder;Output of show keyspaces

Keyspace: system:
  Replication Factor: 1
  Column Families:
    ColumnFamily: IndexInfo
    ""indexes that have been completed""
      Columns sorted by: org.apache.cassandra.db.marshal.UTF8Type
      Row cache size / save period: 0.0/0
      Key cache size / save period: 0.01/3600
      Memtable thresholds: 0.571875/122/60
      GC grace seconds: 0
      Compaction min/max thresholds: 4/32
    ColumnFamily: Schema
    ""current state of the schema""
      Columns sorted by: org.apache.cassandra.db.marshal.UTF8Type
      Row cache size / save period: 0.0/0
      Key cache size / save period: 0.01/3600
      Memtable thresholds: 0.571875/122/60
      GC grace seconds: 0
      Compaction min/max thresholds: 4/32
    ColumnFamily: Migrations
    ""individual schema mutations""
      Columns sorted by: org.apache.cassandra.db.marshal.TimeUUIDType
      Row cache size / save period: 0.0/0
      Key cache size / save period: 0.01/3600
      Memtable thresholds: 0.571875/122/60
      GC grace seconds: 0
      Compaction min/max thresholds: 4/32
    ColumnFamily: LocationInfo
    ""persistent metadata for the local node""
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period: 0.0/0
      Key cache size / save period: 0.01/3600
      Memtable thresholds: 0.571875/122/60
      GC grace seconds: 0
      Compaction min/max thresholds: 4/32
    ColumnFamily: HintsColumnFamily (Super)
    ""hinted handoff data""
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType/org.apache.cassandra.db.marshal.BytesType
      Subcolumns sorted by: org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period: 0.0/0
      Key cache size / save period: 0.01/3600
      Memtable thresholds: 0.571875/122/60
      GC grace seconds: 0
      Compaction min/max thresholds: 4/32
Keyspace: Skunk:
  Replication Factor: 1
  Column Families:
    ColumnFamily: CFYaml (Super)
      Columns sorted by: org.apache.cassandra.db.marshal.LongType/org.apache.cassandra.db.marshal.UTF8Type
      Subcolumns sorted by: org.apache.cassandra.db.marshal.LongType
      Row cache size / save period: 1000.0/0
      Key cache size / save period: 10000.0/3600
      Memtable thresholds: 0.29/255/59
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
    ColumnFamily: CFCli (Super)
      Columns sorted by: org.apache.cassandra.db.marshal.LongType/org.apache.cassandra.db.marshal.UTF8Type
      Subcolumns sorted by: org.apache.cassandra.db.marshal.LongType
      Row cache size / save period: 0.0/0
      Key cache size / save period: 200000.0/3600
      Memtable thresholds: 0.571875/122/60
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
Keyspace: Lucandra:
  Replication Factor: 1
  Column Families:
    ColumnFamily: Documents
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period: 0.0/0
      Key cache size / save period: 200000.0/3600
      Memtable thresholds: 0.571875/122/60
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
    ColumnFamily: TermInfo (Super)
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType/org.apache.cassandra.db.marshal.BytesType
      Subcolumns sorted by: org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period: 0.0/0
      Key cache size / save period: 200000.0/3600
      Memtable thresholds: 0.571875/122/60
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
;;;","06/Nov/10 03:34;jbellis;Oh, I see.  It's printing the correct comparator/subcomparator types, then there is an additional line with the comparator type incorrectly listed as the subcomparator.  Fixed in r1031741.;;;","06/Nov/10 03:59;ceocoder;For 

ColumnFamily: CFYaml (Super)
Columns sorted by: org.apache.cassandra.db.marshal.LongType/org.apache.cassandra.db.marshal.UTF8Type
Subcolumns sorted by: org.apache.cassandra.db.marshal.LongType

set CFYaml['newrow'][1234567890]['column'] = 'value'
'column' could not be translated into a LongType.

I think type of ['column'] is UTF8Type not sure why I'm getting translated into LongType error.
;;;","06/Nov/10 04:51;jbellis;Pavel, could you have a look at this second part?;;;","06/Nov/10 05:01;xedin;sure! I will start with this issue right after I will finish 1470.;;;","09/Nov/10 05:06;jbellis;this is working fine for me in latest 0.7 code:

{code}
[default@unknown] create keyspace KS1
cfff16aa-eb7b-11df-b5e1-e700f669bcfc
[default@unknown] use KS1
Authenticated to keyspace: KS1
[default@KS1] create column family CFCli with column_type= 'Super' and comparator= 'LongType' and subcomparator='UTF8Type'
d4d6684b-eb7b-11df-b5e1-e700f669bcfc
[default@KS1] set CFCli['newrow'][1234567890]['column'] = 'value'
Value inserted.
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gossip conviction threshold too low,CASSANDRA-610,12442691,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,08/Dec/09 10:15,16/Apr/19 17:33,22/Mar/23 14:57,09/Dec/09 05:28,0.5,,,,0,,,,,,"The current gossip conviction threshold is a bit too low, and can cause hosts to 'flap' under heavy load.  I suspect that the original author of the failure detector implementation originally used both suspect and convict, but at some point decided that there was no action to take when a host was suspected, or that it was not worth doing.  They appear to have short-circuited the code to convict on suspect to eliminate this, however this caused the suspicion threshold (5) to be used for convicting hosts instead of the conviction threshold (8).","debian lenny amd 64 OpenJDK 64-Bit Server VM (build 1.6.0_0-b11, mixed mode)
",hammer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Dec/09 10:17;brandon.williams;610.patch;https://issues.apache.org/jira/secure/attachment/12427279/610.patch",,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19779,,,Wed Dec 09 12:42:40 UTC 2009,,,,,,,,,,"0|i0fzzr:",91429,,,,,Normal,,,,,,,,,,,,,,,,,"08/Dec/09 10:16;brandon.williams;Patch to remove suspect() and replace it with convict() restoring the threshold of 8.;;;","08/Dec/09 10:30;jbellis;LGTM.  Waiting for Dan to test.;;;","09/Dec/09 05:28;jbellis;committed;;;","09/Dec/09 20:42;hudson;Integrated in Cassandra #282 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/282/])
    increase failure conviction threshold.  patch by Brandon Williams; reviewed by jbellis for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix jna errno reporting,CASSANDRA-1694,12478855,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,02/Nov/10 13:20,16/Apr/19 17:33,22/Mar/23 14:57,03/Nov/10 02:29,0.6.7,0.7.0 rc 1,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Nov/10 01:21;jbellis;1694-take-2.txt;https://issues.apache.org/jira/secure/attachment/12458652/1694-take-2.txt","02/Nov/10 13:59;jbellis;1694-v3.txt;https://issues.apache.org/jira/secure/attachment/12458613/1694-v3.txt","02/Nov/10 13:39;jbellis;jna-3.2.7.jar;https://issues.apache.org/jira/secure/attachment/12458611/jna-3.2.7.jar","02/Nov/10 13:20;jbellis;jna.patch;https://issues.apache.org/jira/secure/attachment/12458608/jna.patch",,,,,,,,,,,4.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20262,,,Tue Nov 02 18:29:18 UTC 2010,,,,,,,,,,"0|i0g6tb:",92534,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"02/Nov/10 13:55;jbellis;v2 adds support for crappy old jna versions;;;","02/Nov/10 14:09;brandon.williams;+1.  Needs jna 3.2.7 to compile, but will run (with inferior logging) with lesser jna versions.;;;","02/Nov/10 14:12;jbellis;committed;;;","03/Nov/10 01:19;jbellis;This causes
{code}
ERROR 12:11:56,633 Exception encountered during startup.
java.lang.NoClassDefFoundError: com/sun/jna/LastErrorException
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:67)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
Caused by: java.lang.ClassNotFoundException: com.sun.jna.LastErrorException
	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:307)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:248)
{code}

when jna is not present at runtime.  reverted for now.;;;","03/Nov/10 01:22;jbellis;-take-2 patch adds a pre-emptive call to getLastError in an attempt to fix the problem a different way.
{code}
            // calling getLastError involves system calls that can reset errno, so you don't
            // want the first time you call it to be when you actually have an error to diagnose
{code};;;","03/Nov/10 01:27;brandon.williams;Take-2 has no effect for me regardless of jna version.;;;","03/Nov/10 02:29;jbellis;re-committed v3 with catches changed to look like this instead:
{code}
        catch (RuntimeException e)
        {
            if (!(e instanceof LastErrorException))
                throw e;

            ... handle errno ...
        }
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Schema disagreements when using connections to multiple hosts,CASSANDRA-2536,12504930,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thobbs,thobbs,thobbs,22/Apr/11 06:55,16/Apr/19 17:33,22/Mar/23 14:57,29/Apr/11 23:38,0.7.6,0.8.0 beta 2,,,1,,,,,,"If you have two thrift connections open to different nodes and you create a KS using the first, then a CF in that KS using the second, you wind up with a schema disagreement even if you wait/sleep after creating the KS.

The attached script reproduces the issue using pycassa (1.0.6 should work fine, although it has the 0.7 thrift-gen code).  It's also reproducible by hand with two cassandra-cli sessions.",Two node 0.8-beta1 cluster with one seed and JNA.,gdusbabek,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Apr/11 06:26;thobbs;2536-compare-timestamp.txt;https://issues.apache.org/jira/secure/attachment/12477706/2536-compare-timestamp.txt","22/Apr/11 06:56;thobbs;schema_disagree.py;https://issues.apache.org/jira/secure/attachment/12477050/schema_disagree.py",,,,,,,,,,,,,2.0,thobbs,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20682,,,Fri Apr 29 15:56:06 UTC 2011,,,,,,,,,,"0|i0gbxz:",93365,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"22/Apr/11 07:20;mbulman;I feel like a better, more critical sounding explanation, is:  create a keyspace on node1, create a cf in that keyspace on node2 = hang + schema disagreement.;;;","22/Apr/11 09:37;jbellis;And this is fine in 0.7?;;;","23/Apr/11 00:03;thobbs;Actually, I can also reproduce this with a two node 0.7.4 cluster.  I'm pretty sure that this does not happen with 0.7.3, but I'll go ahead and verify that.;;;","23/Apr/11 00:20;thobbs;Nevermind my thoughts that it doesn't happen in 0.7.3 -- it seems to happen there too.  It appears this is not a recent problem.;;;","25/Apr/11 22:09;1eightt;Just bumped into this on a fresh 0.7.4 install on our test cluster. Does this only happen in a 2 node ring?;;;","28/Apr/11 02:38;ceocoder;encountered this on fresh 0.7.4 - 5 nodes - 100G+ per node. 

decommissioning the bad node and rejoin fixed the problem.;;;","28/Apr/11 10:10;jbellis;Gary, any thoughts on where to start looking?;;;","28/Apr/11 20:42;gdusbabek;bq. any thoughts...
I was going to add some jmx to get the last N schema versions (seems like it would be handy anyway and will be necessary if we ever get the rollback pony). Send schema to node A, verify that schema is propagated to B, send schema to B and watch the problem happen.  The code to start looking at are the Definitions*VerbHandlers.

Schema version is tracked in two places: gossip and in DatabaseDescriptor.defsVersion.  Make sure those are reasonably in sync (was the sourced of one bug in the past). ;;;","29/Apr/11 04:58;thobbs;The issue is the clocks being out of sync between nodes.  Sometimes the v1 UUID generated by the second node has an earlier timestamp than the current schema UUID has.

There are a couple of things that could be fixed here:

1. A node shouldn't accept a schema change if the timestamp for the new schema would be earlier than its current schema.
2. Schema modification calls should accept an optional client-side timestamp that will be used for the v1 UUID.;;;","29/Apr/11 05:08;gdusbabek;bq. Sometimes the v1 UUID generated by the second node has an earlier timestamp than the current schema UUID has.
Wouldn't that update be DOA then?  I thought we checked to make sure the new migration compared after the current migration (as well as making sure the new migration's previous version matches with the current version).

bq. A node shouldn't accept a schema change if the timestamp for the new schema would be earlier than its current schema.
If the clocks are *that* far off sync, I think the cluster has bigger problems (like writes not being applied). Plus, it would be easy for a node whose clock is way head to 'poison' schema updates from the rest of the cluster who are, in effect, behind the times.

bq. Schema modification calls should accept an optional client-side timestamp that will be used for the v1 UUID.
Seems like a better approach.

;;;","29/Apr/11 05:21;jbellis;bq. A node shouldn't accept a schema change if the timestamp for the new schema would be earlier than its current schema.

You need this with or without the client-side timestamp, though; there's no sense in letting people blow their leg off.

And once you have that you don't need to add a client-side timestamp with all the PITA-ness that involves.

(And unlike with data modification, I can't think of a use for doing ""clever"" things w/ a client side timesamp.  So pushing it to the client doesn't really solve anything, just means you need to sync clocks across more machines.);;;","29/Apr/11 05:33;thobbs;{quote}
Wouldn't that update be DOA then? I thought we checked to make sure the new migration compared after the current migration (as well as making sure the new migration's previous version matches with the current version).
{quote}
We do check that the previous version matches, but the migration is applied locally without comparing the current and new uuids.

{quote}
If the clocks are that far off sync, I think the cluster has bigger problems (like writes not being applied).
{quote}
This can theoretically happen with clocks being off by only tens of milliseconds.

{quote}
And unlike with data modification, I can't think of a use for doing ""clever"" things w/ a client side timesamp. So pushing it to the client doesn't really solve anything, just means you need to sync clocks across more machines.
{quote}
Not for clever purposes -- it seems to me that clients making schema modifications are more likely to be centralized, so schema changes coming from a single client will (almost) always have increasing timestamps.;;;","29/Apr/11 06:26;thobbs;Attached patch compares version timestamps before applying migration locally.;;;","29/Apr/11 07:08;thobbs;I personally think the timestamp comparison is good enough for now.  Any interest in opening a new ticket for client-side timestamps?;;;","29/Apr/11 09:11;jbellis;bq. it seems to me that clients making schema modifications are more likely to be centralized

I would have also argued that they are likely to use the same connection (to the same server), and look where that got us. :)

bq. I personally think the timestamp comparison is good enough for now

I am okay with this. What do you think, Gary?

(Nit: the exception message says ""older"" but the comparison is ""older or equal."");;;","29/Apr/11 15:38;slebresne;I'll hijack this conversation by saying that I think we should start advertising that people should try to keep their server clocks in sync unless they have a good reason not too (which would legitimize the fact that ""timestamp comparison is good enough""). Counter removes for instance use server side timestamps and would be screwed up by diverging clocks (and by that I mean more screwed up than they already are by design). And really, is there any reason not to install a ntpd server in the first place anyway?;;;","29/Apr/11 20:35;gdusbabek;I think timestamp comparisons will be fine.;;;","29/Apr/11 23:38;jbellis;committed, thanks!;;;","29/Apr/11 23:56;hudson;Integrated in Cassandra-0.7 #462 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/462/])
    refuse to apply migrations with older timestamps than the current schema
patch by Tyler Hobbs; reviewed by jbellis and gdusbabek for CASSANDRA-2536
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Store AccessLevels externally to IAuthenticator,CASSANDRA-1237,12468077,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,stuhood,stuhood,stuhood,29/Jun/10 02:29,16/Apr/19 17:33,22/Mar/23 14:57,26/Aug/10 04:43,0.7 beta 2,,,,0,,,,,,"Currently, the concept of authentication (proving the identity of a user) is mixed up with permissions (determining whether a user is able to create/read/write databases). Rather than determining the permissions that a user has, the IAuthenticator should only be capable of authenticating a user, and permissions (specifically, an AccessLevel) should be stored consistently by Cassandra.

EDIT: Adding summary

----

In summary, there appear to be 3 distinct options for how to move forward with authorization. Remember that this ticket is about disconnecting authorization (permissions) from authentication (user/group identification), and its goal is to leave authentication pluggable.

Options:
# Leave authentication and authorization in the same interface. If we choose this option, this ticket is invalid, and CASSANDRA-1271 and CASSANDRA-1320 will add-to/improve IAuthenticator
** Pros:
*** Least change
** Cons:
*** Very little actually implemented by Cassandra: burden is on the backend implementors
*** Each combination of authz and authc backends would require a new implementation (PAM for authc + permissions keyspace for authz, for instance), causing an explosion of implementations
# Separate out a pluggable IAuthority interface to implement authorization
## IAuthenticator interface would be called at login time to determine user/groups membership
## IAuthority would be called at operation time with the user/groups determined earlier, and the required permission for the operation
** Pros:
*** Provides the cleanest separation of concerns
*** Allows plugability for permissions
** Cons:
*** Pluggability of permissions gains limited benefit
*** IAuthority would need to support callbacks for keyspace/cf creation and removal to keep existing keyspaces in sync with their permissions (although technically, option 1 suffers from this as well)
# Separate authorization, but do not make it pluggable
** This option is implemented by the existing patchset by attaching permissions to metadata, but could have an alternative implementation that stores permissions in a permissions keyspace.
** Pros:
*** Cassandra controls the scalability of authorization, and can ensure it does not become a bottleneck
** Cons:
*** Would need to support callbacks for user creation and removal to keep existing users in sync with their permissions",,gdusbabek,rschildmeijer,urandom,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-1186,,,,,,,,,,,,,,,,,"29/Jul/10 00:58;stuhood;0001-Consolidate-KSMetaData-mutations-into-copy-methods.patch;https://issues.apache.org/jira/secure/attachment/12450717/0001-Consolidate-KSMetaData-mutations-into-copy-methods.patch","29/Jul/10 00:58;stuhood;0002-Thrift-and-Avro-interface-changes.patch;https://issues.apache.org/jira/secure/attachment/12450718/0002-Thrift-and-Avro-interface-changes.patch","29/Jul/10 00:58;stuhood;0003-Add-user-and-group-access-maps-to-Keyspace-metadata.patch;https://issues.apache.org/jira/secure/attachment/12450719/0003-Add-user-and-group-access-maps-to-Keyspace-metadata.patch","29/Jul/10 00:58;stuhood;0004-Remove-AccessLevel-return-value-from-login-and-retur.patch;https://issues.apache.org/jira/secure/attachment/12450720/0004-Remove-AccessLevel-return-value-from-login-and-retur.patch","29/Jul/10 00:58;stuhood;0005-Move-per-thread-state-into-a-ClientState-object-1-pe.patch;https://issues.apache.org/jira/secure/attachment/12450721/0005-Move-per-thread-state-into-a-ClientState-object-1-pe.patch","29/Jul/10 00:58;stuhood;0006-Apply-access.properties-to-keyspaces-during-an-upgra.patch;https://issues.apache.org/jira/secure/attachment/12450722/0006-Apply-access.properties-to-keyspaces-during-an-upgra.patch","25/Aug/10 10:29;stuhood;sample-usage.patch;https://issues.apache.org/jira/secure/attachment/12453007/sample-usage.patch","30/Jul/10 02:23;messi;simple-jaas-authenticator.patch;https://issues.apache.org/jira/secure/attachment/12450850/simple-jaas-authenticator.patch","25/Aug/10 10:21;stuhood;v2-0001-Remove-AccessLevel-return-value-from-login-since-aut.patch;https://issues.apache.org/jira/secure/attachment/12453002/v2-0001-Remove-AccessLevel-return-value-from-login-since-aut.patch","25/Aug/10 10:29;stuhood;v2-0002-Add-IAuthority-and-split-login-into-authenticate-aut.patch;https://issues.apache.org/jira/secure/attachment/12453005/v2-0002-Add-IAuthority-and-split-login-into-authenticate-aut.patch","25/Aug/10 10:21;stuhood;v2-0003-Factor-out-reflection-based-class-construction.patch;https://issues.apache.org/jira/secure/attachment/12453004/v2-0003-Factor-out-reflection-based-class-construction.patch","25/Aug/10 10:21;stuhood;v2-0004-Add-configuration-for-IAuthority-and-handle-SimpleAu.patch;https://issues.apache.org/jira/secure/attachment/12453003/v2-0004-Add-configuration-for-IAuthority-and-handle-SimpleAu.patch","25/Aug/10 10:29;stuhood;v2-0005-Separate-authentication-and-authorization-into-Clien.patch;https://issues.apache.org/jira/secure/attachment/12453006/v2-0005-Separate-authentication-and-authorization-into-Clien.patch",,13.0,stuhood,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20043,,,Wed Aug 25 20:43:17 UTC 2010,,,,,,,,,,"0|i0g3tz:",92051,,urandom,,urandom,Normal,,,,,,,,,,,,,,,,,"21/Jul/10 07:24;stuhood;Patchset to separate permissions from authentication.;;;","21/Jul/10 07:25;stuhood;The set currently uses our old-school KSMetaData serialization, so it will need a rebase when CASSANDRA-1186 makes it in.;;;","22/Jul/10 03:43;stuhood;Rebased post 1186.;;;","22/Jul/10 03:46;stuhood;This should be ready for review.

One thing that might block it getting committed, though, is that we probably need to automate the conversion from 'access.properties' to per-keyspace access maps. Perhaps during loadFromYaml, we could check for the existence of an 'access.properties' file, and apply the properties found there to the new keyspaces?;;;","22/Jul/10 04:40;messi;Failing to authenticate correctly is not an exception. OTOH, accessing a keyspace you're not authorized for is. All RPC methods except ""login"" should throw AuthorizationException.

While you're at it, please look at CASSANDRA-974. I suggest renaming ""login"" to ""authenticate"" and have it return Map<String,String> to make it future-proof for SASL authentication schemes, like DIGEST-MD5.;;;","23/Jul/10 01:15;stuhood;> Failing to authenticate correctly is not an exception.
(Keeping in mind that I didn't change any of the Exception handling/throwing in this patch) I think I agree with the original decision. A failed authentication should be extremely rare, and therefore exceptional.

> All RPC methods except ""login"" should throw AuthorizationException.
Agreed, but out of the scope of this ticket.

> While you're at it, please look at CASSANDRA-974. I suggest renaming ""login"" to ""authenticate""
I'm thinking of the IAuthenticator interface and login() methods as stopgaps until Avro adds support for SASL in their custom protocol (AVRO-341), so I don't think breaking the Thrift API right now is worth it.;;;","23/Jul/10 04:12;stuhood;Rebased for trunk, and added 0006 to handle upgrades by parsing the deprecated 'access.properties' when users call readTablesFromYaml.

This should be ready for review.;;;","27/Jul/10 03:55;stuhood;Rebased for trunk.;;;","27/Jul/10 10:19;messi;Attached patch shows a simple IAuthenticator for JAAS. With JAAS you can configure LoginModules for Unix users or PAM, for LDAP or Kerberos and you can also write your own LoginModule reading passwd.properties files or even column families. In fact, I have a SimpleLoginModule (similar to SimpleAuthenticator) half ready.

This authenticator is also not finished, yet. I submitted it because I hope it's not too late to urge you to make/keep IAuthenticator as lightweight as possible.
The proposed defaultUser() would make IAuthenticators somewhat stateful. Not good.

If you're interested I can open a new issue and submit my JAAS classes there including sample config files and a programmatic JAAS configuration.;;;","27/Jul/10 10:21;messi;By the way, I think the constants USERNAME_KEY and PASSWORD_KEY should go into IAuthenticator because these are common keys used by all authenticators.;;;","28/Jul/10 02:37;gdusbabek;Folke, I'd be interested in seeing your classes (on a separate ticket).;;;","28/Jul/10 02:38;gdusbabek;0003: unavronateAccessMap is checking for null on the wrong variable.
0004: is there a way to not have a default user? I think it adds some noise to the interface.  

Feel free to make breaking changes (no need to support access.properties) if it simplifies things.  Our authentication API has been explicitly 'experimental' from day one.;;;","28/Jul/10 05:29;messi;In AuthenticatedUser.toString(): String.format() is a static method that takes the format as its first parameter.

Again, please drop defaultUser() and put the ""super admin"" status directly into AuthenticatedUser or use a not configurable pseudo group for super admins.;;;","29/Jul/10 00:58;stuhood;* 0003: Fixed unavronateAccessMap null check (good eye!)
* 0004: Moved 'isSuper' onto AuthenticatedUser
* 0004: Fixed static/instance String.format usage
* 0005: Fixed an error where calling set_keyspace before login would fail

----

I can't think of a good way to remove defaultUser: it replaced lots of (authenticator instanceof AllowAllAuthenticator) calls, which existed to check whether it was necessary for the user to login.;;;","29/Jul/10 03:03;messi;I see.

A few suggestions then:
- rename ""AuthenticatedUser"" to just ""User"".
- add ""isAuthenticated"" state to User.
- add package private getter(s) and setter(s) that allow Authenticators to manipulate their User objects.
- add ""User"" as argument to ""login"" (and ""logout""). Authenticators use the credentials to fill in the details.
- rename ""defaultUser"" to ""createUser"" to make it clear that it's a factory method that must always return a (new) User who may or may not already be authenticated.

I know. It's probably too much.;;;","29/Jul/10 05:05;gdusbabek;+1 committed.;;;","29/Jul/10 11:36;jbellis;committing to a users-and-groups approach seems very wrongheaded to me.  the approach of pluggable authenticators to PAM etc seems much better to me (although it's hard to say now since that patch was deleted -- bad practice there...);;;","29/Jul/10 11:50;jbellis;similarly, the AccessLevel change is moving 180 degrees in the wrong direction -- we _want_ to push that into the authenticator rather than hard-coding it somewhere :(;;;","29/Jul/10 21:14;hudson;Integrated in Cassandra #503 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/503/])
    apply access.properties to KSM during loadSchemaFromYaml. patch by stuhood, reviewed by gdusbabek. CASSANDRA-1237
move CS threadlocals into single object. patch by stuhood, reviewed by gdusbabek. CASSANDRA-1237
new IAuthenticator interface. patch by stuhood, reviewed by gdusbabek. CASSANDRA-1237
push access structures into KSM. patch by stuhood, reviewed by gdusbabek. CASSANDRA-1237
put access structures in KsDef. patch by stuhood, reviewed by gdusbabek. CASSANDRA-1237
move KSM modification code into copy methods. patch by stuhood, reviewed by gdusbabek. CASSANDRA-1237
;;;","30/Jul/10 00:29;urandom;On the one hand, decoupling access levels seems like a Good Move, and from that stand-point this is an improvement over the status quo, but I disagree with the decision to bundle the access levels in keyspace definitions.

Wouldn't it be reasonable to create another interface (IAuthority or whatever) and off-load how access levels are persisted to implementations (similar to how it is done with IAuthenticator)?

-1 (in the ASF-sense), I believe this should be rolled back.

EDIT: and I do apologize for coming into this late, as opposed to when it was actively being discussed.;;;","30/Jul/10 00:32;urandom;
{quote}
Attached patch shows a simple IAuthenticator for JAAS. With JAAS you can configure LoginModules for Unix users or PAM, for LDAP or Kerberos and you can also write your own LoginModule reading passwd.properties files or even column families. In fact, I have a SimpleLoginModule (similar to SimpleAuthenticator) half ready.
This authenticator is also not finished, yet. I submitted it because I hope it's not too late to urge you to make/keep IAuthenticator as lightweight as possible.
The proposed defaultUser() would make IAuthenticators somewhat stateful. Not good.

If you're interested I can open a new issue and submit my JAAS classes there including sample config files and a programmatic JAAS configuration.
{quote}

What happened to this? This sounds quite interesting.;;;","30/Jul/10 01:01;stuhood;> the approach of pluggable authenticators to PAM
> we want to push that into the authenticator rather than hard-coding it somewhere
I feel like you're mixing up 'authentication' with 'permissions/authorization'... the reasoning behind this ticket is that backends like PAM aren't designed to provide storage for permissions. Folke's JAAS example is a great example of authentication (and what he coded will still apply post 1237), but I haven't seen any JAAS/PAM backends that implement permissions storage.

> Wouldn't it be reasonable to create another interface (IAuthority or whatever) and off-load how access levels are persisted
I think that would be a mistake, without a working backend that filled the interface. Then, imagine how that backend might fill that interface, and I expect you'll come up with either storing the permissions in their own Keyspace, storing them in a backend specific manner, or attaching them as metadata to the keyspace. The first option is an alternative that should only be implemented once, and therefore shouldn't have an interface. The second could be handled by the IAuthenticator, and is what we have now. The third option is what is implemented here.;;;","30/Jul/10 01:39;urandom;bq. ...the reasoning behind this ticket is that backends like PAM aren't designed to provide storage for permissions.

PAM is a bad example here, and there are plenty of other (more relevant) services and frameworks that do, (think LDAP, radius, tacacs, etc).

bq. I think that would be a mistake, without a working backend that filled the interface.

Right, you'd need at least the equivalent of SimpleAuthenticator.

bq. Then, imagine how that backend might fill that interface, and I expect you'll come up with either storing the permissions in their own Keyspace, storing them in a backend specific manner, or attaching them as metadata to the keyspace. The first option is an alternative that should only be implemented once, and therefore shouldn't have an interface. The second could be handled by the IAuthenticator, and is what we have now. The third option is what is implemented here.

If authorization should be pluggable (I've argued that is should be), then ""backend specific"" is the only option that makes sense.

;;;","30/Jul/10 02:23;messi;I'm sorry I deleted my patch too early. I wanted to open a new ticket but when I looked over my code I thought that it's not ready and I want to work on it over the weekend.

The attached patch adds several classes:
- +JAASAuthenticator:+ an IAuthenticator implemention that uses a LoginContext to request authentication against configured LoginModules. Very simple stuff.
- +SimpleLoginModule:+ LoginModule impl that could replace SimpleAuthenticator.
- +SimpleLoginModuleConfiguration:+ programmatic configuration of LoginModules.
- +User/Group:+ implementations of java.security.Principal added by SimpleLoginModule to the Subject that is passed from the LoginContext to all configured LoginModules.

This is only proof-of-concept code. I hope I have something cleaner and more robust ready by Sunday.;;;","30/Jul/10 02:45;jbellis;I agree that committing this was premature, and also apologize for not looking at it earlier.;;;","30/Jul/10 03:09;stuhood;> If authorization should be pluggable (I've argued that is should be)
I'd be interested in seeing the reasons for making permissions pluggable, if you know where I can find the thread.;;;","30/Jul/10 03:15;gdusbabek;>> If authorization should be pluggable (I've argued that is should be)
>I'd be interested in seeing the reasons for making permissions pluggable, if you know where I can find the thread.

Separation of concerns.  I tried to make the argument yesterday that mixing KS definitions with KS permissions was not the right design.;;;","30/Jul/10 03:25;stuhood;> Separation of concerns. I tried to make the argument yesterday that mixing KS definitions with KS permissions was not the right design.
Separation of concerns is not an argument for making it pluggable (pluggable implies multiple implementations), although it is an argument for storing the permissions somewhere other than in the metadata for the keyspace.;;;","30/Jul/10 03:43;urandom;{quote}
> If authorization should be pluggable (I've argued that is should be)
I'd be interested in seeing the reasons for making permissions pluggable, if you know where I can find the thread.
{quote}

Directory services like LDAP and Active Directory seem like prominent examples of existing systems that people might want to integrate with for authorization, (as well as authentication). And, I'm sure there are plenty of people with existing databases, web services, etc that would appreciate the opportunity to integrate instead of duplicating that information.;;;","30/Jul/10 04:22;messi;I don't think you can make authorization pluggable: authorization is very specific, it requires predefined permissions and/or group/role names, and authz is done at many places in the code. I recommend an authorization infrastructure that does not authorize users directly but groups or roles. An authenticator must return a list of groups a user belongs to and the authz infrastructure renders a list of permissions. In Cassandra you just ask if UserX.isAllowedTo(READ, ""Keyspace1"");;;;","30/Jul/10 21:37;hudson;Integrated in Cassandra #504 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/504/])
    revert 980215, 980217, 980220, 980222, 980225, 980226. CASSANDRA-1237
;;;","17/Aug/10 03:49;stuhood;In summary, there appear to be 3 distinct options for how to move forward with authorization. Remember that this ticket is about disconnecting authorization (permissions) from authentication (user/group identification), and its goal is to leave authentication pluggable.

Options:
# Leave authentication and authorization in the same interface. If we choose this option, this ticket is invalid, and CASSANDRA-1271 and CASSANDRA-1320 will add-to/improve IAuthenticator
** Pros:
*** Least change
** Cons:
*** Very little actually implemented by Cassandra: burden is on the backend implementors
*** Each combination of authz and authc backends would require a new implementation (PAM for authc + permissions keyspace for authz, for instance), causing an explosion of implementations
# Separate out a pluggable IAuthority interface to implement authorization
## IAuthenticator interface would be called at login time to determine user/groups membership
## IAuthority would be called at operation time with the user/groups determined earlier, and the required permission for the operation
** Pros:
*** Provides the cleanest separation of concerns
*** Allows plugability for permissions
** Cons:
*** Pluggability of permissions gains limited benefit
*** IAuthority would need to support callbacks for keyspace/cf creation and removal to keep existing keyspaces in sync with their permissions (although technically, option 1 suffers from this as well)
# Separate authorization, but do not make it pluggable
** This option is implemented by the existing patchset by attaching permissions to metadata, but could have an alternative implementation that stores permissions in a permissions keyspace.
** Pros:
*** Cassandra controls the scalability of authorization, and can ensure it does not become a bottleneck
** Cons:
*** Would need to support callbacks for user creation and removal to keep existing users in sync with their permissions;;;","17/Aug/10 04:24;urandom;The pressing problem is that the current implementation predates dynamically created keyspaces/column families and as a result it's not possible to add/remove keyspaces with auth enabled.  It's also quite late in the 0.7 cycle so ""least change"" from #1 seems quite appealing.;;;","17/Aug/10 04:37;stuhood;> The pressing problem is that the current implementation predates dynamically created keyspaces/column families
Correct.. CASSANDRA-1271 addresses that issue. I think it is too late to make any of these changes in 0.7, so I'm interested in what you think we can/should do for 0.8.;;;","17/Aug/10 06:15;toulmean;I am in favor of 3. I like that it keeps scalability over checking permissions, and I like it is made pluggable for people to integrate with LDAP or their own framework.;;;","17/Aug/10 06:22;urandom;bq. I am in favor of 3. I like that it keeps scalability over checking permissions, and I like it is made pluggable for people to integrate with LDAP or their own framework.

3 is decidedly _not_ pluggable with respect to authorization (only authentication).   Also, there is also nothing to prevent you from implementing a back-end that stored credentials and/or permissions in Cassandra. ;;;","18/Aug/10 04:07;rnirmal;Here's some probable use cases for Authz:
* User A / Group B -> read/write KS1, KS2: read KS3
* User C / Group D -> create/rename/drop KS/CF
* User E -> read/write KS1-CF1, KS1-CF2: read KS1-CF3
* Admin -> add/modify/delete Users/groups/permissions;;;","24/Aug/10 03:27;stuhood;I think it should be possible to implement option #2 before 0.7 final, if we defer implementing the 'callbacks' until we have a backend that can support modifying its permissions (SimpleAuthenticator cannot).

Split {{AccessLevel IAuthenticator.login(String keyspace, Map credentials)}} into:
* {{AuthenticatedUser IAuthenticator.authenticate(Map credentials)}}
** Where AuthenticatedUser is similar to the implementation in the existing patchset, but without a 'isSuper' flag, since the IAuthority should make that decision
* {{AccessLevel IAuthority.authorize(AuthenticatedUser user,  String keyspace)}}
** In CASSANDRA-1320, AccessLevel will be replaced with a Set<Permission>
** In CASSANDRA-1271, keyspace will be replaced with a generic 'resource' hierarchy

Does this sound reasonable?;;;","24/Aug/10 04:04;urandom;bq. Does this sound reasonable?

It sounds reasonable to me.;;;","25/Aug/10 10:29;stuhood;Implementation of option 2:

v2-0001 - Removes AccessLevel return value from login in the client APIs
v2-0002 - Splits IAuthority out of IAuthenticator: SimpleAuthority and SimpleAuthenticator are backwards compatible
v2-0003 - Removes some reflection-centered duplication
v2-0004 - Allows configuration of an IAuthority, and specifies SimpleAuthority for upgraders who have configured SimpleAuthenticator
v2-0005 - Splits the authc/z ThreadLocals out of the CassandraServers and into a ClientState object;;;","25/Aug/10 10:31;stuhood;Optimistically targetting back to 0.7-beta2 (sorry for the flapping).;;;","26/Aug/10 04:43;urandom;committed; thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StorageProxy.insertBlocking does not perform hinted handoff,CASSANDRA-383,12433523,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,sandeep_tata,sandeep_tata,sandeep_tata,20/Aug/09 08:22,16/Apr/19 17:33,22/Mar/23 14:57,25/Aug/09 00:49,0.4,,,,0,,,,,,"insertBlocking should use getNStorageEndPointMap (like insert) instead of just getNStorageEndPoint so that it can perform hinted handoff.

",all,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-375,,,,,,,,,,,,,,,"20/Aug/09 08:47;sandeep_tata;383-v1.patch;https://issues.apache.org/jira/secure/attachment/12417085/383-v1.patch","21/Aug/09 10:15;sandeep_tata;383-v2.patch;https://issues.apache.org/jira/secure/attachment/12417209/383-v2.patch","22/Aug/09 00:55;sandeep_tata;383-v3.patch;https://issues.apache.org/jira/secure/attachment/12417283/383-v3.patch","22/Aug/09 02:55;sandeep_tata;383-v4.patch;https://issues.apache.org/jira/secure/attachment/12417301/383-v4.patch",,,,,,,,,,,4.0,sandeep_tata,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19662,,,Tue Aug 25 14:21:26 UTC 2009,,,,,,,,,,"0|i0fylj:",91203,,,,,Normal,,,,,,,,,,,,,,,,,"20/Aug/09 08:28;sandeep_tata;Need to fix this so both insert paths can echo writes to the bootstrapping nodes for CASSANDRA-375;;;","20/Aug/09 08:47;sandeep_tata;insertBlocking now uses getNStorageEndPointMap;;;","20/Aug/09 09:08;jbellis;eyeballing it looks ok.  will commit after i get to my machine that has Thrift installed tomorrow to run nosetests. :);;;","21/Aug/09 00:20;jbellis;I get many nosetests failures with this patch (on a single machine ofc);;;","21/Aug/09 10:15;sandeep_tata;Oops, can't quite cast to EndPoint[] like I was doing.
v2 passes nosetests and ant test.;;;","21/Aug/09 14:01;euphoria;I think this is what you'll want to do, getting rid of the temp counter.
{{{
EndPoint[] endPoints = (EndPoint[])endpointMap.keySet().toArray(new EndPoint[endpointMap.size()]);
}}}

It might be pedantic, but I also think we should be making the quorum calculation consistent, instead of
availability style
{{{
(DatabaseDescriptor.getReplicationFactor() / 2) + 1)
}}}
versus blocking style
{{{
(DatabaseDescriptor.getReplicationFactor() >> 1) + 1)
}}}

Apart from these, looks good.;;;","22/Aug/09 00:55;sandeep_tata;Thanks -- v3 with Michaels' suggestions.;;;","22/Aug/09 01:02;jbellis;instead of
            if (endpointMap.size() < (DatabaseDescriptor.getReplicationFactor() / 2) + 1)
            {
                throw new UnavailableException();
            }
            int blockFor = determineBlockFor(consistency_level);

we should have

            int blockFor = determineBlockFor(consistency_level);
            if (endpointMap.size() < blockFor)
            {
                throw new UnavailableException();
            }
;;;","22/Aug/09 01:03;jbellis;minor point: the (EndPoint[]) is redundant in the toArray call;;;","22/Aug/09 01:47;jbellis;rather belatedly I realized that we have a deeper problem here.

we don't want to count hint destinations as counting towards the W consistency level, or we are violating our contract that W + R > N = consistent (or unavailableexception).

we do want to write hints if for instance a quorum write is requested and we have 2 of 3 nodes live, we write the third to a hint.  but that can't count towards the quorum since that node won't be a potential read target.  so getNStorageEndPointMap is the wrong thing to do here.

i think we actually have to go below that abstraction: use getStorageEndPoints directly, count the live nodes to see if we have enough to satisfy consistency_level, write those out, and if that is less than N, then add hinted writes.;;;","22/Aug/09 02:27;sandeep_tata;True, the blockFor semantics so far has simply been a W-durability guarantee.
If we want to guarantee W + R > N = consistent, HHO nodes can't count towards W.
;;;","22/Aug/09 02:55;sandeep_tata;v4 fixes blockFor semantics to actually block for W live nodes and then do hinted handoff after that succeeds.

We don't want to do HHO in parallel to make sure we don't write any value in case the quorum write fails .. clients will see a failed write, but quorum reads might succeed after handoff.;;;","25/Aug/09 00:48;jbellis;committed v4 w/ minor changes (renamed getLiveNodes -> getUnhintedNodes);;;","25/Aug/09 22:21;hudson;Integrated in Cassandra #177 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/177/])
    allow blocking write to create hints if not enough of the ""correct"" nodes are live, but enough are to fulfil the requested consistency level.  patch by Sandeep Tata; reviewed by jbellis and Michael Greene for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CommitLog does not flush on writes,CASSANDRA-367,12433134,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jbellis,lenn0x,lenn0x,15/Aug/09 08:04,16/Apr/19 17:33,22/Mar/23 14:57,19/Aug/09 06:49,0.4,,,,0,,,,,,"When you write data to CommitLog, we are not calling a flush() in class LogRecordAdder.

Is this acceptable? We added flush() and its working now. This bug was introduced when things were consolidated in r799942",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/09 08:05;lenn0x;0001-CASSANDRA-367-Added-flush.patch;https://issues.apache.org/jira/secure/attachment/12416628/0001-CASSANDRA-367-Added-flush.patch","19/Aug/09 06:46;jbellis;367-3.patch;https://issues.apache.org/jira/secure/attachment/12416928/367-3.patch","18/Aug/09 03:16;jbellis;367-v2.patch;https://issues.apache.org/jira/secure/attachment/12416794/367-v2.patch","18/Aug/09 02:54;jbellis;367.patch;https://issues.apache.org/jira/secure/attachment/12416793/367.patch",,,,,,,,,,,4.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19654,,,Wed Aug 19 14:08:32 UTC 2009,,,,,,,,,,"0|i0fyi7:",91188,,,,,Critical,,,,,,,,,,,,,,,,,"15/Aug/09 09:42;jbellis;I'd rather not call flush every add if possible.  What problems are you seeing?

(To be pedantic: the change in the revision in question moved from using an unbuffered writer, to using a buffered one.  Not removing a pre-existing flush.);;;","18/Aug/09 02:54;jbellis;patch to always sync commitlog, either as a batch pre-ack (current CLSync of true) or a new periodic mode defaulting to ever 1000ms;;;","18/Aug/09 03:15;lenn0x;Thread.sleep(600) should be pulling in getCommitLogSyncPeriod();;;","18/Aug/09 03:16;jbellis;v2 w/ config option actually used :);;;","18/Aug/09 07:31;lenn0x;+1;;;","18/Aug/09 07:37;jbellis;committed;;;","19/Aug/09 06:46;jbellis;forgot while loop :);;;","19/Aug/09 06:47;lenn0x;Oops! Yes, this looks better now. +1;;;","19/Aug/09 06:49;jbellis;committed;;;","19/Aug/09 22:08;hudson;Integrated in Cassandra #172 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/172/])
    add missing while loop on periodic commitlog sync thread.
patch by jbellis; reviewed by Chris Goffinet for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bloom filters should avoid huge array allocations to avoid fragmentation concerns,CASSANDRA-2466,12504130,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,m0nstermind,scode,scode,13/Apr/11 11:45,16/Apr/19 17:33,22/Mar/23 14:57,27/Oct/11 03:34,1.0.1,,,,0,,,,,,"The fact that bloom filters are backed by single large arrays of longs is expected to interact badly with promotion of objects into old gen with CMS, due to fragmentation concerns (as discussed in CASSANDRA-2463).

It should be less of an issue than CASSANDRA-2463 in the sense that you need to have a lot of rows before the array sizes become truly huge. For comparison, the ~ 143 million row key limit implied by the use of 'int' in BitSet prior to the switch to OpenBitSet translates roughly to 238 MB (assuming the limitation factor there was the addressability of the bits with a 32 bit int, which is my understanding).

Having a preliminary look at OpenBitSet with an eye towards replacing the single long[] with multiple arrays, it seems that if we're willing to drop some of the functionality that is not used for bloom filter purposes, the bits[i] indexing should be pretty easy to augment with modulo to address an appropriate smaller array. Locality is not an issue since the bloom filter case is the worst possible case for locality anyway, and it doesn't matter whether it's one huge array or a number of ~ 64k arrays.

Callers may be affected like BloomFilterSerializer which cares about the underlying bit array.

If the full functionality of OpenBitSet is to be maintained (e.g., xorCount) some additional acrobatics would be necessary and presumably at a noticable performance cost if such operations were to be used in performance critical places.

An argument against touching OpenBitSet is that it seems to be pretty carefully written and tested and has some non-trivial details and people have seemingly benchmarked it quite carefully. On the other hand, the improvement would then apply to other things as well, such as the bitsets used to keep track of in-core pages (off the cuff for scale, a 64 gig sstable should imply a 2 mb bit set, with one bit per 4k page).


",,cburroughs,kingryan,m0nstermind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-2521,,"17/Oct/11 00:03;m0nstermind;2466v1.patch;https://issues.apache.org/jira/secure/attachment/12499202/2466v1.patch","18/Oct/11 03:34;m0nstermind;2466v2.patch;https://issues.apache.org/jira/secure/attachment/12499421/2466v2.patch","15/Oct/11 01:44;m0nstermind;BloomFilterSerializer.java;https://issues.apache.org/jira/secure/attachment/12499069/BloomFilterSerializer.java","14/Oct/11 14:52;m0nstermind;OpenBitSet.java;https://issues.apache.org/jira/secure/attachment/12498988/OpenBitSet.java",,,,,,,,,,,4.0,m0nstermind,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20640,,,Wed Oct 26 20:06:23 UTC 2011,,,,,,,,,,"0|i0gbjj:",93300,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"13/Apr/11 14:14;cscotta;Peter, it's interesting that you mention this. During the first run of the memory profiling I ran while investigating CASSANDRA-2463 on vanilla 0.7.4, I saw a tremendous amount of allocation of long[]/longs (comprising ~70% of heap allocations at that specific moment), but was not sure of the source. I doubt I'll have time to look into this at the office this week, but if I'm able to outside of work I may check into the allocation a bit.

If you're on it, by all means go for it. Thanks for mentioning this!;;;","14/Apr/11 01:48;kingryan;Moving to smaller arrays would make the allocation easier, but wouldn't reduce the raw amount of memory needed for a large bloom filter.

Would it be worth moving these off-heap completely?;;;","14/Apr/11 02:55;scode;Yes, it's not decreasing the total amount. The idea is that with small values, at least you're now left with a tweakable situation where a suitable heap size and a suitable initial occupancy trigger should be sufficient to avoid concurrent mode failures, as long as you're avoiding the fragmentation induced promotion failures (pragmatically, even if theoretically unclean, one can also help CMS along by inserting some minor application level pauses during allocations of many chunks as part of a larger allocation). But the memory pressure remains.

Regarding off-heap: I didn't suggest it because I'd personally like to avoid JNI/JNA if possible for ""safety"" reasons, and I would also like to avoid adding further dependencies on CMS sweeps for external resources (files, off-heap mem, etc) for the usual reasons. But maybe I'm more paranoid about that than most.

It would be really nice to just have an explicitly managed pool of fixed-size off-heap buffers of a reasonable size (say, a meg a piece, mmap():ed). I'm thinking maybe the bloom filters can be explicitly managed more easily than the mmaps/brafs for data reading; for example by accepting that for the few requests that race with the removal of an sstable, the bloom filter would just pretend all bits are set.

Hmmm...

;;;","21/Apr/11 15:02;scode;If/once the bullet is bitten in CASSANDRA-2521 this should be much simpler to accomplish without the GC tradeoffs.;;;","14/Oct/11 06:41;jbellis;The main problem with moving BF off-heap is that single-item gets from direct buffers are about 1/2 the speed of accessing an on-heap byte[].  That's a pretty big hit to take.;;;","14/Oct/11 14:19;scode;Good point. What's your feeling on the approach of modifying the bitset to use a number of small byte arrays? (Probably at some fixed size to help fragmentation.)

It does mean an additional level of indirection in an array of arrays, and it's not clear (to me at least) that it is expected to usually reside in CPU cache.
;;;","14/Oct/11 14:52;m0nstermind;I made it already for our cassandra deployment (because we have huge bloom filters). Attaching it as is (will rebase it to head if you'll find it useful).;;;","14/Oct/11 20:47;jbellis;What effect did you observe when deploying this, Oleg?;;;","14/Oct/11 22:44;m0nstermind;Promotion failures caused by bloom filters gone away completely.
 
I did not measured performance overhead specifically on bloom filters, so all I can tell that it was not introduced measureable performance degradation on production cluster operation from client perspective. 

(currently we have ~3300 thrift single column read reqs/sec per node with ~1ms avg call duration measured from client; 98% reads are bounced by bloom filters; size of single bloom filter is up to 250Mb)
;;;","15/Oct/11 01:32;jbellis;Is serialization the same, or at least backwards-compatible w/ existing OBS BFs?;;;","15/Oct/11 01:44;m0nstermind;You mean on-disk format ? Yes. Specifics are handled by BloomFilterSerializer. Attached serializer as implemented for 0.6.
(as far as i see no changes are required in BloomFilter itself);;;","15/Oct/11 01:58;jbellis;Are there any other moving parts to this? Could we ask you to submit a patch against trunk?;;;","15/Oct/11 02:14;m0nstermind;Of course. I'll try to prepare it till monday.;;;","15/Oct/11 05:27;jbellis;Great!;;;","17/Oct/11 00:03;m0nstermind;Rebased to trunk. ant test passed.;;;","17/Oct/11 21:11;jbellis;Some comments:

{code}
            for (int i = 0; i < pageSize && bitLength-- > 0; i++)
{code}

Isn't one of those bounds checks redundant?  It looks like wlen/bitLength is the right one to use.

* lots of commented-out methods in OBS. Let's remove them entirely if we're not going to implement.
* OBS.setPage is unnecessary;;;","18/Oct/11 03:41;m0nstermind;Attached v2:
Removed there commented out code as well as OBS.setPage.

regarding  ""i < pageSize && bitLength-- > 0"":
both bound checks are neccessary:
* ""i<pageSize"" ensures there are no out of bounds for every single page, while
* ""bitLength-- > 0"" ensures there are no more than neccessary bytes are written from the last page (otherwise extra zeroes will appear in file, which will break backwards compatibility).;;;","26/Oct/11 02:52;jbellis;Patch looks good to me, just waiting for performance testing now.;;;","27/Oct/11 03:34;jbellis;Tested (by Brandon) and committed. Thanks, Oleg!;;;","27/Oct/11 03:49;scode;Awesome!;;;","27/Oct/11 04:06;m0nstermind;Youre welcome ;-);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReadResponseResolver Race,CASSANDRA-2552,12505091,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,stuhood,stuhood,25/Apr/11 09:21,16/Apr/19 17:33,22/Mar/23 14:57,28/Apr/11 21:26,0.7.6,0.8.0 beta 2,,,0,,,,,,"When receiving a response, ReadResponseResolver uses a 3 step process to decide whether to trigger the condition that enough responses have arrived:
# Add new response
# Check response set size
# Check that data is present

I think that these steps must have been reordered by the compiler in some cases, because I was able to reproduce a case for a QUORUM read where the condition is not properly triggered:
{noformat}
INFO [RequestResponseStage:15] 2011-04-25 00:26:53,514 ReadResponseResolver.java (line 87) post append for 1087367065: hasData=false in 2 messages
INFO [RequestResponseStage:8] 2011-04-25 00:26:53,514 ReadResponseResolver.java (line 87) post append for 1087367065: hasData=true in 1 messages
INFO [pool-1-thread-54] 2011-04-25 00:27:03,516 StorageProxy.java (line 623) Read timeout: java.util.concurrent.TimeoutException: ReadResponseResolver@1087367065(/10.34.131.109=false,/10.34.132.122=true,)
{noformat}
The last line shows that both results were present, and that one of them was holding data.",,johanoskarsson,skamio,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Apr/11 15:36;stuhood;0001-Move-Resolvers-to-atomic-append-count.txt;https://issues.apache.org/jira/secure/attachment/12477369/0001-Move-Resolvers-to-atomic-append-count.txt","28/Apr/11 04:24;jbellis;2552-v2-07.txt;https://issues.apache.org/jira/secure/attachment/12477577/2552-v2-07.txt","28/Apr/11 04:18;jbellis;2552-v2.txt;https://issues.apache.org/jira/secure/attachment/12477576/2552-v2.txt","26/Apr/11 14:08;stuhood;ResolveRaceTest.java;https://issues.apache.org/jira/secure/attachment/12477366/ResolveRaceTest.java",,,,,,,,,,,4.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20693,,,Thu Apr 28 13:44:17 UTC 2011,,,,,,,,,,"0|i0gc1j:",93381,,stuhood,,stuhood,Normal,,,,,,,,,,,,,,,,,"25/Apr/11 10:06;stuhood;I have a patch ready that I believe fixes this: testing it out before posting.;;;","25/Apr/11 21:25;jbellis;Hard to tell exactly what's going on here w/o knowing where your logging was added.

In particular it's important to note that we don't prevent responses from being processed after we've already given up and decided to call a timeout (but before we've torn down the request callback).;;;","26/Apr/11 14:08;stuhood;Here is a cut down testcase that reproduces the race: it looks like two threads can race on step 2 such that neither accounts for the item added by the other, and both think the set of responses is too small.

I have a patch that makes append + size an atomic operation: I'll post it as soon as I clean it up a bit.;;;","26/Apr/11 15:10;stuhood;Chris pointed out that the example passes if you replace NBHM with CHM, but I don't think NBHM is necessarily to blame here: each thread views a locally consistent copy, likely due to Cliff's use of sun.misc.Unsafe references.

It's possible that a similar race applies to RangeSliceResponseResolver, but I think changes to LBQ (like CHM) will be broadcast to all threads.;;;","26/Apr/11 15:36;stuhood;Attaching a patch that replaces NBHM with an AtomicReferenceArray that is appended to and counted atomically. This patch eliminated the timeouts we were seeing.

CHM may also be a legitimate solution, but it feels a bit like an abuse of a map.;;;","27/Apr/11 01:21;jbellis;That sure sounds like a NBHM bug to me. The javadoc says,

bq. Retrievals reflect the results of the most recently completed update operations holding upon their onset... Similarly, Iterators and Enumerations return elements reflecting the state of the hash table at some point at or since the creation of the iterator/enumeration.

I.e., for at least one thread, *both* update operations will have completed when the iterator is created, so it should see all the entries.

(Will review the actual patch shortly, I'm just saying I think we should report a bug too.);;;","27/Apr/11 06:39;stuhood;> I.e., for at least one thread, both update operations will have completed when the iterator is created
Note that the race I observed via the debug output for that test was actually on the size() operation, which doesn't put any such guarantees in its javadocs.;;;","27/Apr/11 07:46;jbellis;You're right: size() is implemented as a org.cliffc.high_scale_lib.Counter object, which says

{code}
  // Add the given value to current counter value.  Concurrent updates will
  // not be lost, but addAndGet or getAndAdd are not implemented because but
  // the total counter value is not atomically updated.
  //public void add( long x );

...

  // Current value of the counter.  Since other threads are updating furiously
  // the value is only approximate, but it includes all counts made by the
  // current thread.  Requires a pass over all the striped counters.
  //public long get();
{code};;;","27/Apr/11 17:38;slebresne;I am no expert of the Java Memory Model, but I can't find anything that preclude this behavior in the CHM docs either (there really is not much on the size function). So I would have liked the CHM solution if we could be sure it always fix that problem (I would have liked it because it was a one line change and I think maps are here to be ""abused""), but as far as I can tell, it may well only make the bug much less frequent or fix it only on some architecture (the code of CHM seems to indicate it is safe but it's complicated enough that I wouldn't bet my life on it).

Note that if that's true, LBQ too could well allow for a race here without breaking it's specification (it seems to use a AtomicInteger for the size internally so it is trivially ok, but if the spec doesn't force anything, I suppose that could change).

So I suppose if we want to do right by the spec, we should probably update both AbstractRowResolver and RangeSliceResponseResolver (note that using an AtomicInteger to count the number of responses could be slightly simpler, but I'm fine with an AtomicReferenceArray). ;;;","27/Apr/11 23:38;jbellis;is there a reason RSRR can't inherit ARR or does it just predate that refactoring?;;;","28/Apr/11 00:07;jbellis;bq. is there a reason RSRR can't inherit ARR or does it just predate that refactoring?

To answer my own ARR assumes we're returning Rows, which would be easy to fix, and that Messages turn into ReadResponse objects, which would be harder since we'd need to have a <T extends ISerializable> interface where ISerializeable gave us a Serializer class declaring ""void serialize(T, outputstream) and T deserialize(inputstream)"", i.e., we start to get into fixing ICompactSerializer and all the mess that would be.;;;","28/Apr/11 00:29;jbellis;I'm not sure I'm a fan of the ARR solution.  Wouldn't it be similar complexity (one O(N) operation per message received) to keep NBHM and implement getMessageCount as an iterate-entries operation?  (the O(N) op in ARR is of course the search-for-free-slot in append.)

I'm -0 on changing RSRR away from LBQ when LBQ is known to work fine in practice.;;;","28/Apr/11 00:37;jbellis;bq. Wouldn't it be similar complexity (one O(N) operation per message received) to keep NBHM and implement getMessageCount as an iterate-entries operation?

We can actually do size-by-iteration for basically free (with a little refactoring), since we're already iterating for isDataPresent. We can just push the iteration into the callers who care about size-and-data-present and do it with one loop.

But if I am honest that is premature optimization.  We are already using the AtomicInteger approach in DatacenterReadCallback.  I'll submit a patch to standardize on that.
;;;","28/Apr/11 01:23;jbellis;v2 w/ AtomicInteger approach;;;","28/Apr/11 04:18;jbellis;updated v2 fixes AsyncRepairCallback and RepairCallback as well;;;","28/Apr/11 04:24;jbellis;and a v2 for 0.7;;;","28/Apr/11 04:45;stuhood;Although I didn't reproduce a race between size() and isDataPresent(), isn't that one still possible? IMO, two operations that are atomic independently shouldn't be trusted to compose. The point of the ARR wasn't to improve runtime, it was simply to make all three steps atomic.;;;","28/Apr/11 04:53;jbellis;the correctness criterion is that once the messages are received, at least one thread running response will see that both blockfor and data are satisfied; this meets that need.

note that received(-messages-that-count-towards-blockfor) is NOT the same as size (see: DRC) so you need a separate variable anyway even with ARR.;;;","28/Apr/11 16:45;stuhood;+1

I still think a Map is overkill here, but I can't reproduce a race with the v2 algorithm.;;;","28/Apr/11 21:26;jbellis;committed;;;","28/Apr/11 21:44;hudson;Integrated in Cassandra-0.7 #459 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/459/])
    fix incorrect use ofNBHM.size in ReadCallback
patch by jbellis; reviewed by stuhood for CASSANDRA-2552
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix contrib/word_count build in 0.7,CASSANDRA-1030,12463154,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jeromatron,jbellis,jbellis,28/Apr/10 05:38,16/Apr/19 17:33,22/Mar/23 14:57,07/May/10 01:23,0.7 beta 1,,,,0,,,,,,CASSANDRA-44 broke word_count setup (see CASSANDRA-1002) so CASSANDRA-992 and CASSANDRA-1029 can't easily be applied to 0.7 as-is.  This ticket will port those to 0.7 and add schema setup.,,johanoskarsson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/May/10 04:58;jeromatron;1030.patch;https://issues.apache.org/jira/secure/attachment/12443632/1030.patch",,,,,,,,,,,,,,1.0,jeromatron,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19965,,,Thu May 06 17:23:58 UTC 2010,,,,,,,,,,"0|i0g2kn:",91847,,,,,Normal,,,,,,,,,,,,,,,,,"01/May/10 01:49;jeromatron;Converted from using the fat client to using thrift rpc for the WordCountSetup.  Also fixed a few things that were amiss in the current hadoop stuff with @stuhood. Needs a patch for cassandra-1022 to be able to login on the wordcount itself.;;;","03/May/10 21:50;jbellis;why do we still need the .yaml?;;;","03/May/10 21:55;gdusbabek;I haven't reviewed the patch yet.;;;","03/May/10 22:30;johanoskarsson;Had a quick look at but unfortunately the word count contrib does not compile for me with the patch applied. 
The fix is trivial, but I can't seem to figure out the reason for changing the input format from SortedMap to a LinkedHashMap in the first place?;;;","03/May/10 23:19;jeromatron;@johan: the reason why we changed from SortedMap to LinkedHashMap was that the TreeMap required a comparable object to instantiate (line 234 of ColumnFamilyRecordReader).  Stu suggested just changing it to the LHM so that it didn't require the comparable object.  I'm wondering what it needed to compile - it had compiled fine for me.;;;","03/May/10 23:19;jeromatron;@jonathan: I removed the need for the yaml file for the WordCountSetup class and tried to run it without it for the WordCount but it errored out.  I wasn't sure if that was just the way it had to be or if I could get rid of that.  I think this line (line 91) in WordCount needs to be updated to grab the dynamic mapping:

 this.columnName = context.getConfiguration().get(CONF_COLUMN_NAME);

but I'll take another look.;;;","03/May/10 23:29;jeromatron;I see the compilation problem after updating - it looks like the patch for cassandra-1022 has been committed.  That changes the args required for a thrift rpc client login.  I'll update the patch and see about getting rid of the dependency on the .yaml at the same time.;;;","03/May/10 23:45;jbellis;""context"" is the hadoop job context, nothing to do w/ our yaml.;;;","03/May/10 23:55;jeromatron;@johan: sorry - not because it requires a comparator, but that LinkedHashMap maintains the insertion order.  That was the reason.

@jonathan: right - sorry, yeah.  Was going to look at it and that was the first thing I skimmed that looked promising, but you're right.  Looking at it though.;;;","04/May/10 01:15;jeromatron;@jonathan - I asked Johan about the error when the cassandra.yaml file is not present.  He said it looked like it's required because it gets the host address from the seed that's in the configuration file.  That's in the org.cassandra.hadoop... code - ColumnFamilyInputFormat line 196 for example.  Would you rather the host be configured by the client itself and remove the dependency on cassandra.yaml?;;;","04/May/10 02:48;jeromatron;Updated the patch with compilation fix from cassandra-1022 changes. Also updated ColumnFamilyInputFormat to use LinkedHashMap in addition to the other places where SortedMap had been used.

Waiting to find out if it would be good to have the client provide the host address or if we should still depend on cassandra.yaml for the seed to get to that.;;;","04/May/10 05:17;jbellis;It looks like the main reason we use DatabaseDescriptor is to get comparator information so we can throw SortedMaps around.

Created CASSANDRA-1047 to clean this up.;;;","05/May/10 04:58;jeromatron;Updated to make sure everything works in the wordcount - dynamically finds the comparator based on describing the keyspace itself.;;;","06/May/10 17:50;johanoskarsson;The patch looks good, but I get unexpected output. In the setup code we insert just one ""word1"" in the text1 column. When the word count runs it finds two instances of ""word1"" in the text1 column. This could be due to other changes in trunk, but worth verifying it is not this patch.;;;","07/May/10 00:38;johanoskarsson;The problem with the output is most likely this one CASSANDRA-1042, so will commit this patch as is;;;","07/May/10 01:02;jeromatron;yeah - it looks like the output is the same on 0.6.0's word count as the trunk cassandra-1030 patch output.  so it would seem to be something external that is causing odd output - hopefully cassandra-1042 will fix that.;;;","07/May/10 01:23;johanoskarsson;Committed to trunk. Thanks Jeremy!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hinted handoff rows never get deleted,CASSANDRA-34,12421771,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,junrao,jbellis,jbellis,01/Apr/09 21:50,16/Apr/19 17:33,22/Mar/23 14:57,06/May/09 01:03,0.3,,,,0,,,,,,"from the list: ""after the hints are delivered, the hinted keys are deleted from the hinted CF only, but not from the application CF.""

Prashant verified that this is a bug that can't be fixed until deletes are fully working.

Note: when we fix this, see if we can do so w/o compromising the immediate-GC of the hinted CF keys.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-33,CASSANDRA-120,,,,,,,,,,,,,,,,"03/May/09 11:28;jbellis;0001-cleanup.patch;https://issues.apache.org/jira/secure/attachment/12407102/0001-cleanup.patch","03/May/09 11:29;jbellis;0002-fix-HHM.patch;https://issues.apache.org/jira/secure/attachment/12407103/0002-fix-HHM.patch","05/May/09 23:30;jbellis;0003-tombstones-take-priority-over-non-tombstones-w-the.patch;https://issues.apache.org/jira/secure/attachment/12407253/0003-tombstones-take-priority-over-non-tombstones-w-the.patch","02/May/09 05:45;junrao;issue34.patch_v1;https://issues.apache.org/jira/secure/attachment/12407039/issue34.patch_v1","02/May/09 06:53;junrao;issue34.patch_v2;https://issues.apache.org/jira/secure/attachment/12407045/issue34.patch_v2",,,,,,,,,,5.0,junrao,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19522,,,Thu May 07 13:35:07 UTC 2009,,,,,,,,,,"0|i0fwhb:",90860,,,,,Normal,,,,,,,,,,,,,,,,,"02/May/09 01:14;junrao;Sending out hinted data needs correct RowMutation support.;;;","02/May/09 05:45;junrao;Attach a fix. About the patch.

1. Make sendMessage blocking.
2. Delete the rows in application CF after hinted data is sent. To do that, we need to collect the largest timestamp among columns in a CF and then delete the CF with the largest timestamp.
3. When a column is in a deleted CF and their timestamps are the same, the rule is that the deleted CF wins. This rule is needed for step 2 above. Change CFStore.removeDeleted according to this rule.;;;","02/May/09 06:53;junrao;Patch v2. Added some description on hinted data gets delivered.;;;","03/May/09 03:13;jbellis;any idea what the purpose of this code in runHints is?  why flush if nothing changed?

            	if(hintedColumnFamily == null)
            	{
                    columnFamilyStore_.forceFlush();
            		return;
            	}
;;;","03/May/09 03:34;jbellis;also, now that we have range queries, it seems that a normal CF would be a better fit for this than a single super CF with keys as supercolumns.  i guess that is a separate issue.;;;","03/May/09 11:27;jbellis;committed the sendMessage fix.  I've reworked the rest substantially in two parts.

01 is just refactoring / cleanup.

02 includes your fix to deleteKey, and also:
 A make hint generation include a real timestamp so we can do meaningful deletes
 B call removeDeleted on the data we read locally to purge tombstones
 C because of (B) any supercolumn w/o subcolumns simply won't exist so we know we can skip re-deleting the endpoint data.  so deleteKey becomes deleteHintedData.
 D because deleted data is not immediately purged, increased the scheduled interval fro 20min to 60 to reduce the load of scanning the hint CF.

(for another ticket: since all this is purely local data and not subject to read repair we should find a way to GC it immediately post-delete.)

how does this look to you?;;;","03/May/09 20:31;hudson;Integrated in Cassandra #57 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/57/])
    make sendMessage only return true after ack by recipient.
patch by Jun Rao; reviewed by jbellis for 
;;;","03/May/09 22:11;jbellis;created CASSANDRA-128 for improvments beyond the scope of 0.3;;;","05/May/09 08:00;junrao;Looked at the new patch. Here are some comments.

1. Move the comments above sendMessage to the beginning of class.

2. There is compilation error in MinorCompactionManager because of the removed HintedHandOff class.

3. Your new code deletes a column using the same timestamp as the column to be deleted. This is a more fundamental question. If a column has a non-delete entry and a deleted entry with the same timestamp, which one wins? I don't think that we can rely on the ordering of the insertion/deletion. This is because the insertion and the deletion can end up in different SSTables. During compaction, FileStruct is sorted only by keys. Therefore, columns from different SSTables with the same key can come in arbitrary order. One solution is to modify CF.addColumn such that a deletion always wins when the column timestamp is the same. Not sure if there is any other implications, though.
;;;","05/May/09 08:07;junrao;4. It's probably worthwhile to make intervalInMins_ in HHM configurable.;;;","05/May/09 23:30;jbellis;patch to make tombstones have higher precedence than non- for same timestamp in Column (and SuperColumn, to be consistent w/ C and CF);;;","05/May/09 23:31;jbellis;(incorporated changes for comments (1) and (2) into my patchset, not bothering resubmitting those unless you really want 'em);;;","05/May/09 23:32;jbellis;noted comment (4) on CASSANDRA-128;;;","06/May/09 00:38;junrao;Comments for the new patch.
1. In CFStore.removeDeleted(), we should add comments to explain how we resolve the conflicts among CF, SC, and C when the timestamps are the same. As time goes, we are likely to forget those decisions that we have made.

Other than that, the patch looks fine to me.
;;;","06/May/09 01:03;jbellis;done and committed.;;;","07/May/09 21:35;hudson;Integrated in Cassandra #63 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/63/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ant javadoc fails on windows,CASSANDRA-2248,12499705,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,norman,norman,norman,25/Feb/11 20:47,16/Apr/19 17:33,22/Mar/23 14:57,25/Feb/11 23:06,0.7.3,,Packaging,,0,,,,,,"When try to run ""ant javadoc"" (or any task that include javadoc) on windows it fails with the error:

Javadoc failed: java.io.IOException: Cannot run program ""c:\Program Files\Java\jdk1.6.0_17\bin\javadoc.exe"": CreateProcess error=87, The parameter is incorrect","windows 7, ant 1.8.2",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Feb/11 20:49;norman;CASSANDRA-2248.diff;https://issues.apache.org/jira/secure/attachment/12471927/CASSANDRA-2248.diff",,,,,,,,,,,,,,1.0,norman,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20524,,,Fri Feb 25 15:37:46 UTC 2011,,,,,,,,,,"0|i0ga8n:",93089,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"25/Feb/11 20:49;norman;This fix the build on windows.;;;","25/Feb/11 21:01;norman;Related to this: https://issues.apache.org/bugzilla/show_bug.cgi?id=41958;;;","25/Feb/11 23:06;jbellis;committed, thanks!;;;","25/Feb/11 23:37;hudson;Integrated in Cassandra-0.7 #322 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/322/])
    fix ant javadoc on Windows
patch by Norman Maurer; reviewed by jbellis for CASSANDRA-2248
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide way to remove nodes from gossip entirely,CASSANDRA-644,12443778,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jaakko,jbellis,jbellis,19/Dec/09 00:18,16/Apr/19 17:33,22/Mar/23 14:57,28/Jan/10 05:19,0.6,,Legacy/Tools,,1,,,,,,"As reported in CASSANDRA-634, ""Now that we're gossiping about dead nodes as well, gossip digest continues to grow without boundary when nodes come and go. This information will never disappear as it will be propagated to new nodes no matter how old and obsolete it is. To counter this, we need some mechanism to (1) either remove dead node from endpointstateinfo or (2) at some point stop to gossip about it, or both.""

This is also seen when using ""fat clients"" that participate in the gossip ring; if a client leaves and does not come back it stays in the gossip forever.  (This can be confusing if the client does start up again, connecting to a _different_ cluster, but the old one notices it is back and starts gossiping to it again!)

I would prefer to leave management of these things explicit; 3 days is long enough that the fat client problem in particular needs another solution, and if it needs another solution then that can become the only solution. :)

So I would be in favor of removeToken clearing out gossip entries, and also adding a command to remove an endpoint from the gossip ring that does not have a token associated with it (like fat clients).  A command to ask ""what are all the known gossip hosts"" would also be useful, since nodeprobe ring only includes nodes w/ tokens.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jan/10 13:01;jaakko;644.patch;https://issues.apache.org/jira/secure/attachment/12430987/644.patch",,,,,,,,,,,,,,1.0,jaakko,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19797,,,Wed Jan 27 21:19:14 UTC 2010,,,,,,,,,,"0|i0g07b:",91463,,,,,Low,,,,,,,,,,,,,,,,,"21/Dec/09 20:35;jaakko;> connecting to a _different_ cluster, but the old one notices it is back and starts gossiping to it again!

Actually this cannot happen since gossip SYN carries cluster ID and only messages from the same cluster are handled.

I think removing fat clients automatically is OK. We could run a check periodically and remove all nodes from gossiper that have been silent for certain period of time, and do not have token associated with it. I'm not sure in what kind of situations this kind of clients are used, but if a node does not participate in storage and has been dead for an hour, there should be no harm in removing it. It will be back in gossip anyway immediately if it reappears. Cleaning up fat clients automatically would prevent gossip data from growing ""rapidly"" if there are many clients coming and going from different IP addresses.

As for ""proper"" storage nodes: These are likely to be more stable than clients, so probably it is best to have removetoken and/or other manual commands to take care of this.
;;;","21/Dec/09 23:59;jbellis;> We could run a check periodically and remove all nodes from gossiper that have been silent for certain period of time, and do not have token associated with it.

that sounds fine.;;;","14/Jan/10 06:47;jbellis;(I originally marked this as Improvement, but isn't decommission/removeToken not cleaning out from gossip a Bug?);;;","14/Jan/10 23:27;jaakko;yeah, that should probably be fixed. Attached patch should do the trick. I'll fix the other part tomorrow.;;;","16/Jan/10 15:22;jaakko;patch attached:

- removeToken now removes node from gossip
- fat clients (nodes without token) are removed from gossip after 1 hour of inactivity
- added justRemovedEndPoints to gossip to prevent removed nodes from reappearing immediately. Removed nodes are kept here for RING_DELAY period, during which time new joins from them are ignored. It takes a while for remove token gossip to propagate to all nodes. During this time some nodes will continue to gossip about the just-now-being-removed-node, while others have already removed it.
- fixed a bug related to removeToken command which allowed node's own token being removed.
;;;","19/Jan/10 00:22;jbellis;shouldn't this also r/m from FD.arrivalSamples?;;;","21/Jan/10 13:01;jaakko;yeah, I decided it is not worth the trouble, but you're right, it is better to remove from FD too. attached new version.
;;;","22/Jan/10 23:37;jbellis;+1

are you comfortable committing this to 0.5 branch? ;;;","23/Jan/10 08:01;jaakko;Before committing to 0.5, it would be good if other people tested this as well.;;;","25/Jan/10 17:27;jaakko;Committed to trunk. Let's wait a while and commit to 0.5 if nothing breaks.;;;","25/Jan/10 20:43;hudson;Integrated in Cassandra #334 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/334/])
    fixed broken test. 
modify removetoken to remove node from gossip. remove fat clients automatically after 1h of inactivity. patch by jaakko, reviewed by jbellis. 
;;;","25/Jan/10 22:06;jbellis;could you also update CHANGES?;;;","28/Jan/10 05:19;jbellis;updated CHANGES and closed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The initial arrive time should not be set to zero,CASSANDRA-14,12419528,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,zhu han,zhu han,26/Mar/09 23:02,16/Apr/19 17:33,22/Mar/23 14:57,23/Apr/09 02:40,0.3,,,,0,,,,,,"In line 253 of src.org.apache.cassandra.gms .FailureDetector, the initial arrive time should not be zero. Otherwise, the failure detector would report the new joining node as dead because the phi is positive infinity when the mean of arrive time is zero. 

A proper value should be set as the initial value. I use half of the gossip period as the initial value,  whose default configuration is 1000ms. ",,,,,,,,,,,,,,,,,,,,,,,1800,1800,,0%,1800,1800,,,,,,,,,,,,,,,,,,,,"23/Apr/09 00:16;jbellis;14-part-2.patch;https://issues.apache.org/jira/secure/attachment/12406148/14-part-2.patch","23/Apr/09 00:09;jbellis;14.patch;https://issues.apache.org/jira/secure/attachment/12406147/14.patch",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19517,,,Wed Apr 22 18:40:56 UTC 2009,,,,,,,,,,"0|i0fwdb:",90842,,,,,Normal,,,,,,,,,,,,,,,,,"10/Apr/09 22:44;jbellis;I have seen this cause problems -- when I restart all the nodes in my test cluster at close to the same time sometimes they will partition even though there is nothing wrong with the network.;;;","10/Apr/09 23:30;avinash.lakshman@gmail.com;Actually this will happen for the initial start maybe for a few seconds. But it will stabilize very very soon. In any system in this nature one could think of this as a barrier synchronization point where one waits for a few seconds for things to stabilize. It takes about less than 10 secs in a 100 node cluster for all this to kinda settle down. In my opinion a non-issue.;;;","11/Apr/09 00:12;jbellis;I'm talking minutes, not seconds.

Is it possible that more nodes ameliorates any problems?;;;","11/Apr/09 01:26;avinash.lakshman@gmail.com;Hmm. That is weird. Because I have small, 5 node test cluster, and it definitely doesn't take minutes. Maybe the recovery is taking time. Nodes do not join the cluster untill recovery is complete.;;;","23/Apr/09 00:09;jbellis;avoid setting the initial arrival time (and hence the mean arrival time until the next value) to zero;;;","23/Apr/09 01:00;jbellis;part 2 cleans up FD by using a Deque instead of a List.;;;","23/Apr/09 02:00;junrao;comment for this patch: arrivalIntervals_.remove(0); in FailureDetector should be arrivalIntervals_.remove();

The patch looks fine to me otherwise.
;;;","23/Apr/09 02:36;jbellis;ooh, very right.  thanks.;;;","23/Apr/09 02:40;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
remove PropertyConfigurator from CassandraDaemon,CASSANDRA-803,12456487,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,jmcconnell,jmcconnell,17/Feb/10 06:40,16/Apr/19 17:33,22/Mar/23 14:57,28/Dec/10 22:37,,,,,0,,,,,,"
In order for users to make use of the EmbeddedCassandraService for unit testing they need to have a dependency declared on log4j.  

It would be nice if we could use the log4j-over-slf4j artifact to bridge this requirement for those of us using slf4j.  

http://www.slf4j.org/legacy.html#log4j-over-slf4j

Currently it errors with the direct usage of the PropertyConfigurator in o.a.c.thrift.CassandraDaemon.",,cw_krebs,dalaro,mpierce,teosoft,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-971,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19870,,,Fri Jul 01 19:58:06 UTC 2011,,,,,,,,,,"0|i0g167:",91620,,,,,Normal,,,,,,,,,,,,,,,,,"28/Aug/10 22:34;cw_krebs;As far I can see, the {{PropertyConfigurator}} is already removed with rev 934505 for CASSANDRA-971 .
Though {{org.apache.cassandra.service.StorageService#setLog4jLevel}} has another dependency to _log4j_, which will probably cause the same problems when using _log4j-over-sl4j_.
I don't know how to cope with this. What's the strategy  regarding the log4j dependencies? 
* Should all log4j dependencies be removed from the cassandra code base and thus in turn {{org.apache.cassandra.service.StorageServiceMBean#setLog4jLevel}} and all of it's implementations
* {{setLog4jLevel}} should be kept, but as some kind of optional operation, just in the case that log4j is present
* Stay completely with  log4j?
 

;;;","28/Dec/10 22:37;jbellis;bq. As far I can see, the PropertyConfigurator is already removed with rev 934505 for CASSANDRA-971

Right. Closing this one as a duplicate.

bq. setLog4jLevel should be kept, but as some kind of optional operation, just in the case that log4j is present

This is what we're going with for now.;;;","24/Apr/11 14:45;teosoft;I see PropertyConfigurator still there, as of version 0.7.4. 

public abstract class AbstractCassandraDaemon implements CassandraDaemon
{
    //Initialize logging in such a way that it checks for config changes every 10 seconds.
    static
    {
        String config = System.getProperty(""log4j.configuration"", ""log4j-server.properties"");
        URL configLocation = null;
        try 
        {
            // try loading from a physical location first.
            configLocation = new URL(config);
        }
        catch (MalformedURLException ex) 
        {
            // load from the classpath.
            configLocation = AbstractCassandraDaemon.class.getClassLoader().getResource(config);
            if (configLocation == null)
                throw new RuntimeException(""Couldn't figure out log4j configuration."");
        }
--->    PropertyConfigurator.configureAndWatch(configLocation.getFile(), 10000);
        org.apache.log4j.Logger.getLogger(AbstractCassandraDaemon.class).info(""Logging initialized"");
    }

    private static Logger logger = LoggerFactory.getLogger(AbstractCassandraDaemon.class);
    
;;;","25/Apr/11 21:29;jbellis;It was added back for CASSANDRA-1525;;;","01/Jul/11 12:06;mpierce;It's a shame to add a hard dep on log4j for this, especially when both dynamically reloading configs and the thing that caused the use of PropertyConfigurator to get back in (CASSANDRA-1525, configuring via JMX) are already handled natively by Logback without any in-code configuration. (http://logback.qos.ch/manual/configuration.html#autoScan and http://logback.qos.ch/manual/jmxConfig.html).

I'm happy to contribute a patch. What about simply making that static initializer block short-circuit if log4j isn't there? It's kind of hackish but it does the job. I'd be happy to rip out all the log4j specific stuff and replace it with slf4j if that patch would be used.;;;","02/Jul/11 03:58;jbellis;bq. I'd be happy to rip out all the log4j specific stuff and replace it with slf4j if that patch would be used.

Sure, as long as the log4j-based defaults continue to work.

Related: CASSANDRA-2383;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Invalid Java identified by JDK7 in StorageProxy.java,CASSANDRA-366,12433113,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,euphoria,euphoria,euphoria,15/Aug/09 03:23,16/Apr/19 17:33,22/Mar/23 14:57,15/Aug/09 04:19,0.4,,,,0,,,,,,"Per http://bugs.sun.com/view_bug.do?bug_id=6182950 the two dispatchMessage() methods in StorageProxy.java should not compile.  JDK6 allows both to coexist, against the spec.  JDK7 fixes this, resulting in a compilation error on StorageProxy.java",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/09 03:24;euphoria;366_v1.diff;https://issues.apache.org/jira/secure/attachment/12416596/366_v1.diff",,,,,,,,,,,,,,1.0,euphoria,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19653,,,Fri Aug 14 20:19:37 UTC 2009,,,,,,,,,,"0|i0fyhz:",91187,,,,,Normal,,,,,,,,,,,,,,,,,"15/Aug/09 03:24;euphoria;Attached patch renames one of the methods. All tests pass on both JDK6 and JDK7 following the patch.;;;","15/Aug/09 04:19;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CommitLog.add doesn't really force to disk,CASSANDRA-182,12425601,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,sandeep_tata,sandeep_tata,16/May/09 00:17,16/Apr/19 17:33,22/Mar/23 14:57,28/Jul/09 10:45,0.4,,,,0,,,,,,CommitLog.add does't really force writes to disk. This could result in acked writes being lost.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jul/09 23:45;jbellis;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-182-handle-incomplete-CL-entries-on-recover.txt;https://issues.apache.org/jira/secure/attachment/12414626/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-182-handle-incomplete-CL-entries-on-recover.txt","27/Jul/09 23:45;jbellis;ASF.LICENSE.NOT.GRANTED--0002-mv-AbstractWriter-to-its-own-top-level-class-and-remov.txt;https://issues.apache.org/jira/secure/attachment/12414627/ASF.LICENSE.NOT.GRANTED--0002-mv-AbstractWriter-to-its-own-top-level-class-and-remov.txt","27/Jul/09 23:45;jbellis;ASF.LICENSE.NOT.GRANTED--0003-naive-fsync-after-each-log-entry.txt;https://issues.apache.org/jira/secure/attachment/12414628/ASF.LICENSE.NOT.GRANTED--0003-naive-fsync-after-each-log-entry.txt","27/Jul/09 23:45;jbellis;ASF.LICENSE.NOT.GRANTED--0004-threadpoolexecutor.txt;https://issues.apache.org/jira/secure/attachment/12414629/ASF.LICENSE.NOT.GRANTED--0004-threadpoolexecutor.txt","27/Jul/09 23:45;jbellis;ASF.LICENSE.NOT.GRANTED--0005-custom-CommitLogExecutorService-that-can-fsync-per-mul.txt;https://issues.apache.org/jira/secure/attachment/12414630/ASF.LICENSE.NOT.GRANTED--0005-custom-CommitLogExecutorService-that-can-fsync-per-mul.txt","27/Jul/09 23:45;jbellis;ASF.LICENSE.NOT.GRANTED--0006-config-options.txt;https://issues.apache.org/jira/secure/attachment/12414631/ASF.LICENSE.NOT.GRANTED--0006-config-options.txt","27/Jul/09 23:45;jbellis;ASF.LICENSE.NOT.GRANTED--0007-arrayblockingqueue.txt;https://issues.apache.org/jira/secure/attachment/12414632/ASF.LICENSE.NOT.GRANTED--0007-arrayblockingqueue.txt","27/Jul/09 23:45;jbellis;ASF.LICENSE.NOT.GRANTED--0008-updates.txt;https://issues.apache.org/jira/secure/attachment/12414633/ASF.LICENSE.NOT.GRANTED--0008-updates.txt","16/May/09 00:41;sandeep_tata;CASSANDRA-182.patch;https://issues.apache.org/jira/secure/attachment/12408264/CASSANDRA-182.patch",,,,,,9.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19586,,,Tue Jul 28 13:29:27 UTC 2009,,,,,,,,,,"0|i0fxdb:",91004,,,,,Normal,,,,,,,,,,,,,,,,,"16/May/09 00:41;sandeep_tata;1. Added a class in SequenceFile called SyncWriter
2. New configuration option CommitLogForceLogs if you want ""safe"" writes. Not turning this on leaves everything else the same as before. All this does is use SyncWriter for the logger.

For trunk and 0.4 we should revisit the options we want to allow on the logs, especially once we re-architect the logger to use batched forces.;;;","16/May/09 01:22;jbellis;The more I look at the commitlog code the more I'm inclined to go with my initial reaction, that we shouldn't mess with this for 0.3... :-|

The FastSync option looks like A&P's attempt to deal with this.  It does call force() on close, for instance.  (Is that really all we need for durability?)  Having two options for syncing, one of which is commented to have issues and the other is virtually entirely untested, doesn't really seem like an improvement to me.

If we did go with ""let's add a third option"" then IMO we should be doing the force in CommitLog not the writer.  updateHeader for instance does a looping write w/ a single writer; i assume only one sync for the entire method is needed, not one sync per write.

For 0.3 I suggest emailing A&P off-list and see if they can clarify why fastsync has problems purging log files.  If it is something simple to fix great.  Otherwise let's note to ""use fastsync for durable writes, but this has the drawback of not purging log files.  fixing this is a priority for 0.4"";;;","16/May/09 02:38;sandeep_tata;FastSync (inappropriately named, IMHO) option tells the CommitLog to use a FastConcurrentWriter. This certainly doesn't force to disk, and therefore is not an option for durable writes.

If we don't add a third option for 0.3 which forces to disk, we simply have *no* durable writes. Even on blocking writes, it is possible that the data has not hit disk in any of the replica's logs.

We have two options:
1. Release as is, and say ""we don't offer durable writes yet""
2. Add an option to force the logs and say ""this is currently a low-performing option and will be improved for 0.4""

We can do the forces in CommitLog (probably a cleaner approach in terms of eliminating unnecessary forces, but updateHeader writes only for CFs encountered for the first time).  The approach in CASSANDRA-182.patch seemed minimally invasive to me :-)

Building a well-tested high-performance logger will take significantly more time. (Just testing the logger for correctness is non-trivial.);;;","16/May/09 03:02;jbellis;FCW does force on close, leading me to believe that for whatever reason A&P felt that was all that was needed to call the option ""Sync.""

Of course I could be reading too much into this.  Always a possibility in ""code archaeology."" :);;;","22/May/09 03:54;jbellis;Assuming we don't hear back from Avinash & Prashant, I suggest that we include a ""known issues"" section in the README for 0.3 noting this problem.  I'm not comfortable with basing the CL on brand-new code.

For 0.4 let's take a closer look at FastSync and either build on that or rip it out and go with the approach in your patch (either in CL or the writer).  I'd like to see this coupled with a CL executor that force()s less than once per mutation if there is a constant stream of writes, similar to postgresql's commit_delay setting (http://www.postgresql.org/docs/8.3/static/runtime-config-wal.html).;;;","22/May/09 22:53;sandeep_tata;I haven't heard back from A&P.
I'm fine with releasing as is and saying logging doesn't work correctly yet and that it'll be fixed soon.;;;","22/May/09 22:54;sandeep_tata;Won't fix in 0.3;;;","22/Jul/09 06:39;jbellis;Are you working on this, Sandeep?  If not I will take it.;;;","22/Jul/09 07:16;sandeep_tata;Nope, I'm not looking at this right now. ;;;","22/Jul/09 09:15;jbellis;confirmed that naive fsync-per-write kills performance -- i'm seeing 1/10 the throughput.

will try the executor approach I mentioned above.;;;","22/Jul/09 14:01;sandeep_tata;I did figure how to avoid using force(true) to flush the metadata in addition to the data (2 disk forces instead of one). If you preallocate the pages for the log file, you need to force(true) only when you have to grab more pages for the log. This would of course require some extra work in the logger code so the end of the log can be tracked without depending on file size/end of file.;;;","22/Jul/09 22:49;jbellis;the executor approach is looking really good, but that's worth keeping in mind down the road.;;;","25/Jul/09 13:35;jbellis;07
    switch to arrayblockingqueue; it's a little cleaner and performance seems unaffected

06
    config options.  increasing write threads to 32 (was 4) allows performance to be mostly decent again, especially if you crank the delay up to 10ms

05
    custom CommitLogExecutorService that can fsync per multiple CL additions

04
    threadpoolexecutor, just using an out-of-the-box executor as a first step

03
    naive fsync-after-each-log-entry.  performance _sucks._

02
    mv AbstractWriter to its own top-level class and remove redundant IFileWriter

01
    handle incomplete CL entries on recover (a fairly important bug fix)
;;;","27/Jul/09 23:45;jbellis;removed old patches, attached rebased ones;;;","28/Jul/09 06:41;junrao;The patch looks good to me. Thanks, Jonathan.
;;;","28/Jul/09 10:45;jbellis;committed;;;","28/Jul/09 21:29;hudson;Integrated in Cassandra #151 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/151/])
    Use arrayblockingqueue in commitlog executor; this cleans up the code a bit (performance is unaffected since the writes and syncs are far more expensive than any queue ops)
patch by jbellis; reviewed by Jun Rao for 
add config options for commitlog syncing
patch by jbellis; reviewed by Jun Rao for 
custom CommitLogExecutorService that can fsync per multiple CL additions
patch by jbellis; reviewed by Jun Rao for 
move log ops to callables on a threadpoolexecutor instead of synchronizing.  this prepares the way to merge multiple add() calls into a single sync.
patch by jbellis; reviewed by Jun Rao for 
naive fsync-after-each-log-entry
patch by jbellis; reviewed by Jun Rao for 
mv AbstractWriter to its own top-level class and remove redundant IFileWriter
patch by jbellis; reviewed by Jun Rao for 
handle incomplete CL entries on recover
patch by jbellis; reviewed by Jun Rao for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
possible NPE in StorageProxy?,CASSANDRA-631,12443176,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,riffraff,riffraff,riffraff,12/Dec/09 20:27,16/Apr/19 17:33,22/Mar/23 14:57,12/Dec/09 22:50,0.5,0.6,,,0,,,,,,"insert() in StorageProxy contains a logging statement that refers to a possibly un-initialized variable
{{{
logger.debug(""insert writing key "" + rm.key() + "" to "" + unhintedMessage.getMessageId() + ""@"" + hintedTarget + "" for "" + target);
}}}

this could happen if getHintedEndpointMap(rm.key(), naturalEndpoints) returns only elements for which target.equals(hintedTarget) returns false, which seems possible to me. 

Looking at the code I get the feeling the reference should probably be to 'hintedMessage', instead of ""unhintedMessage"", if not so an 
assert statement could be appropriate",all,,,,,,,,,,,,,,,,,,,,,,600,600,,0%,600,600,,,,,,,,,,,,,,,,,,,,"12/Dec/09 20:41;riffraff;CASSANDRA-631-big.patch;https://issues.apache.org/jira/secure/attachment/12427820/CASSANDRA-631-big.patch","12/Dec/09 20:41;riffraff;CASSANDRA-631-tiny.patch;https://issues.apache.org/jira/secure/attachment/12427819/CASSANDRA-631-tiny.patch",,,,,,,,,,,,,2.0,riffraff,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19790,,,Thu Dec 17 22:03:03 UTC 2009,,,,,,,,,,"0|i0g04f:",91450,,,,,Normal,,,,,,,,,,,,,,,,,"12/Dec/09 20:41;riffraff;changes the hunhinted ->hinted as per summary. 

The -tiny patch only does that change .

The -big patch also removes other compile warnings from the file (generics, synthetic accessors) and avoids using useless allocations (Collections.max(Arrays.AsList(new ary[a,b])) seems unnecessary when there is only the need to compare two objects and commons-lang already provides a good enough method);;;","12/Dec/09 20:43;riffraff;forgot to say: with both patches I have a tet failure 

    [junit] Testcase: testImportSuperCf(org.apache.cassandra.tools.SSTableImportTest):	Caused an ERROR
    [junit] Invalid localDeleteTime read: -2099059532
    [junit] java.io.IOException: Invalid localDeleteTime read: -2099059532
    [junit] 	at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:368)
    [junit] 	at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:1)
    [junit] 	at org.apache.cassandra.db.filter.SSTableNamesIterator.<init>(SSTableNamesIterator.java:103)
    [junit] 	at org.apache.cassandra.db.filter.NamesQueryFilter.getSSTableColumnIterator(NamesQueryFilter.java:69)
    [junit] 	at org.apache.cassandra.tools.SSTableImportTest.testImportSuperCf(SSTableImportTest.java:63)

at r889834 I also see it without the patches though :);;;","12/Dec/09 22:50;jbellis;applied -tiny to 0.5 and trunk, good catch.

-big removes private from a bunch of variables for no reason, so left that part out, but applied the generics fixes to trunk (although fixing generics only to add in an ObjectUtils call requiring a cast seems a little schizophrenic to me; saving a 2-element array allocation in get_range_slice is like picking up a teaspoon of sand from the seashore :);;;","13/Dec/09 02:30;riffraff;Sorry, I wrote it in the patch comment but I failed to express myself clearly: in my setup I have basically all warnings turned on, and for access of private fields from inner classes there are some performance-related ones (the need for compiler-generated accessor methods) which are not present in case of package visibility. 

It's a silly reason but I just use all possible warnings (+pmd, +findbugs) in my code, and this happen to be in the set, I'll just turn this off if it seems unnecessary. Shall I update the CodeStyle wiki page to point this out?

Same for generics, mostly a warning removal thingy. 
Regarding ObjectUtils, we save an array, a list and an iterator object instantiation, so it's 3 spoons! (that should be probably optimized away with escape analysis anyway :)

Thanks for applying the patch. ;;;","13/Dec/09 04:39;jbellis;Interesting, I didn't know about that.

But if it's actually in a performance sensitive place, wouldn't the JIT inlining will take care of it?;;;","13/Dec/09 17:47;riffraff;you can check it with JAD if you disable the inner class analisys, the code 
{{{
public class C {
  private static int var = 10;
  public static class Inner {
    public int meth(){
      return var;
    }
  }
}
}}}

gives back from the C.class 
{{{
public class C
{

    public C()
    {
    }

    static int access$000()
    {
        return var;
    }

    private static int var = 10;

}

}}}

and for C$Inner.class
{{{
public class C$Inner
{

    public C$Inner()
    {
    }

    public int meth()
    {
        return C.access$000();
    }
}
}}}

I _believe_ this is because inner classes had to be fitted over an existing class format in java 1.1, but I'm not sure. 


I agree that these method are most probably going to be optimized away, as I said I just changed the code because they are in the default warnings set  and I can simply change my warnings setup (and eventually update the CodeStyle wiki so the next person who comes across them will know it).;;;","18/Dec/09 06:03;hudson;Integrated in Cassandra #291 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/291/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tombstoned rows not purged from cache after gcgraceseconds,CASSANDRA-2305,12500984,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,paladin8,paladin8,10/Mar/11 12:39,16/Apr/19 17:33,22/Mar/23 14:57,11/Apr/11 02:50,0.7.5,,,,0,,,,,,"From email to list:

I was wondering if this is the expected behavior of deletes (0.7.0). Let's say I have a 1-node cluster with a single CF which has gc_grace_seconds = 0. The following sequence of operations happens (in the given order):

insert row X with timestamp T
delete row X with timestamp T+1
force flush + compaction
insert row X with timestamp T

My understanding is that the tombstone created by the delete (and row X) will disappear with the flush + compaction which means the last insertion should show up. My experimentation, however, suggests otherwise (the last insertion does not show up).

I believe I have traced this to the fact that the markedForDeleteAt field on the ColumnFamily does not get reset after a compaction (after gc_grace_seconds has passed); is this desirable? I think it introduces an inconsistency in how tombstoned columns work versus tombstoned CFs. Thanks.",,slebresne,,,,,,,,,,,,,,,,,,,,";11/Mar/11 18:26;slebresne;7200",7200,0,7200,100%,7200,0,7200,,,,,,,,,,,,,,,,,,,"11/Mar/11 18:24;slebresne;0001-Compaction-test.patch;https://issues.apache.org/jira/secure/attachment/12473375/0001-Compaction-test.patch","11/Mar/11 18:24;slebresne;0002-Invalidate-row-cache-on-compaction-purge.patch;https://issues.apache.org/jira/secure/attachment/12473376/0002-Invalidate-row-cache-on-compaction-purge.patch","14/Mar/11 22:07;slebresne;2305_2nd_patch.patch;https://issues.apache.org/jira/secure/attachment/12473565/2305_2nd_patch.patch",,,,,,,,,,,,3.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20550,,,Sun Apr 10 23:17:06 UTC 2011,,,,,,,,,,"0|i0galb:",93146,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"10/Mar/11 12:41;paladin8;For a little more info, I think this only happens when you remove an entire row. If you delete specific columns, the tombstones are handled appropriately.;;;","11/Mar/11 02:01;slebresne;This will also happen if you remove all the columns inside the row (even though you didn't issued a row deletion command).

The problem is that when you flush + compact and gcGrace has elapsed, if the row is empty (i.e. all tombstone have been collected), the row itself is collected. This means that when you issue the second wave of inserts, there is no trace whatsoever of the row.

That's why you are not supposed to have a gcGrace too low and why it is highly advised to use the current time as a timestamp. If so, the scenario above will never happen.

Best thing we can do is probably to edit http://wiki.apache.org/cassandra/DistributedDeletes to add that gcGrace should be such that no insert with a timestamp lower that a delete could reach any given node after gcGrace has elapsed.
;;;","11/Mar/11 02:32;jbellis;bq. The problem is that when you flush + compact and gcGrace has elapsed, if the row is empty (i.e. all tombstone have been collected), the row itself is collected. This means that when you issue the second wave of inserts, there is no trace whatsoever of the row.

That's what's supposed to happen, but Jeffrey is saying that is NOT what he observes.;;;","11/Mar/11 02:49;slebresne;Oups, my mistake. I somehow confused myself. I was not able to reproduce though, but I'll try harder tomorrow.;;;","11/Mar/11 18:24;slebresne;I think this is due to row cache. We do not invalidate the row cache when a row is fully collected by compaction.

Jeffrey, can you confirm that you had some row cache enabled when doing your experiments ?

Attaching 2 patch against 0.7. The first one is a unit test showing the failure, the second one is the fix.;;;","11/Mar/11 22:26;jbellis;committed;;;","12/Mar/11 04:57;paladin8;I actually don't have row cache enabled (I just checked cfstats to make sure), so I don't think that's the cause of my problem in particular. Here's some more info that may or may not be correct:

- When I run the compaction, in ColumnFamilyStore.removeDeletedStandard() I see that columns are being removed because of the c.timestamp() <= cf.getMarkedForDeleteAt() condition, which makes sense since I issued a delete on the entire row.
- However, after the compaction, I do the insert, and if I flush/compact again, I still see the columns being removed because of that condition. It seems like the markedForDeleteAt field on the ColumnFamily is persisting across the major compaction which I believe is hiding the newly inserted column.

Also, my initial steps to repro were not correct, which made it hard to figure out the root cause. Here is a proper repro:

- Create a CF with gc_grace_seconds = 0 and no row cache.
- Insert row X, col A with timestamp 0.
- Insert row X, col B with timestamp 2.
- Remove row X with timestamp 1 (expect col A to disappear, col B to stay).
- Wait 1 second.
- Force flush and compaction.
- Insert row X, col A with timestamp 0.
- Read row X, col A (see nothing).

Inserting row X, col B is necessary for this to repro because if all the columns in a row disappear, the ColumnFamily object goes away and the markedForDeleteAt field is reset. Only when a column still exists does the field persist across the compaction. Hope this helps!;;;","12/Mar/11 04:57;paladin8;I believe the cause to be something else (see latest comment).;;;","12/Mar/11 07:07;hudson;Integrated in Cassandra-0.7 #375 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/375/])
    ;;;","12/Mar/11 19:22;slebresne;Ok, I understand what you meant. I've created CASSANDRA-2317 with the fix since we have already committed a patch here. Thanks a lot for the report.;;;","12/Mar/11 19:23;slebresne;Remarking this resolved, the follow up is in CASSANDRA-2317 instead.;;;","14/Mar/11 22:00;slebresne;I'm reopening because the committed patch, while ok, is only a partial fix.;;;","14/Mar/11 22:07;slebresne;The first patch was purging the cache when the full row is expired. But we don't remove expired tombstone from the cache.

Attaching a second patch to purge the cache. This has two purposes: 
  # avoid surprise for client getting tombstones back from query (either sstable2json or rangeSlice) even well after gc_grace and compaction has occured
  # reclaim some memory for row sitting in the cache for a very long time

Note that this patch introduces concurrent deletes, and as such SHOULDN'T be applied to a branch that do not have CASSANDRA-1559 (0.7 and 0.8 have it).

Patch is against 0.7;;;","09/Apr/11 05:47;jbellis;+1;;;","11/Apr/11 02:50;slebresne;Committed to 0.7 (r1090867) and a rebased version to trunk (r1090866);;;","11/Apr/11 03:07;hudson;Integrated in Cassandra-0.7 #429 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/429/])
    Purge tombstone from row cache (0.7 version)
patch by slebresne; reviewed by jbellis for CASSANDRA-2305
;;;","11/Apr/11 07:17;hudson;Integrated in Cassandra #846 (See [https://hudson.apache.org/hudson/job/Cassandra/846/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception thrown when running cassandra.bat,CASSANDRA-1806,12491903,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Duplicate,,jb6,jb6,03/Dec/10 01:10,16/Apr/19 17:33,22/Mar/23 14:57,03/Dec/10 01:29,,,,,0,,,,,,"I've followed the Getting Started instructions for a single node and it worked fine in cassandra-0.7beta2 and beta3 on default configuration. But when trying to do the same for cassandra-0.7rc1 it spews out the following stack trace, when running the cassandra.bat file:
{code}
INFO 20:21:37,771 Starting up server gossip
 INFO 20:21:37,786 switching in a fresh Memtable for LocationInfo at CommitLogContext(file='/var/lib
/cassandra/commitlog\CommitLog-1290885697489.log', position=700)
 INFO 20:21:37,786 Enqueuing flush of Memtable-LocationInfo@29247351(227 bytes, 4 operations)
 INFO 20:21:37,786 Writing Memtable-LocationInfo@29247351(227 bytes, 4 operations)
ERROR 20:21:38,161 Fatal exception in thread Thread[FlushWriter:1,5,main]
java.io.IOError: java.io.IOException: rename failed of D:\var\lib\cassandra\data\system\LocationInfo-e-1-Data.db
        at org.apache.cassandra.io.sstable.SSTableWriter.rename(SSTableWriter.java:214)
        at org.apache.cassandra.io.sstable.SSTableWriter.closeAndOpenReader(SSTableWriter.java:184)
        at org.apache.cassandra.io.sstable.SSTableWriter.closeAndOpenReader(SSTableWriter.java:167)
        at org.apache.cassandra.db.Memtable.writeSortedContents(Memtable.java:161)
        at org.apache.cassandra.db.Memtable.access$000(Memtable.java:49)
        at org.apache.cassandra.db.Memtable$1.runMayThrow(Memtable.java:174)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: rename failed of D:\var\lib\cassandra\data\system\LocationInfo-e-1-Data.db
        at org.apache.cassandra.utils.FBUtilities.renameWithConfirm(FBUtilities.java:359)
        at org.apache.cassandra.io.sstable.SSTableWriter.rename(SSTableWriter.java:210)
        ... 12 more
{code} 

It seems there's a problem, when cassandra is trying to rename the tmp files (e.g. LocationInfo-tmp-e-1-Data.db). It happens in FBUtilities.java:
{code:title=FBUtilities.java|borderStyle=solid}
public static void renameWithConfirm(String tmpFilename, String filename) throws IOException
    {
        if (!new File(tmpFilename).renameTo(new File(filename)))
        {
            throw new IOException(""rename failed of "" + filename);
        }
    }
{code} 
","Windows XP SP3, jdk1.6.0_22",jb6,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20321,,,Thu Dec 02 17:29:38 UTC 2010,,,,,,,,,,"0|i0g7iv:",92649,,,,,Critical,,,,,,,,,,,,,,,,,"03/Dec/10 01:29;jbellis;dupe of CASSANDRA-1790;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
word_count fails in multi-node setup,CASSANDRA-1787,12491563,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,stuhood,jeromatron,jeromatron,30/Nov/10 01:29,16/Apr/19 17:33,22/Mar/23 14:57,30/Nov/10 03:40,0.6.9,0.7.0 rc 2,,,0,,,,,,"(also affects RC1, wasn't in the ""Affects Versions"" list)

from a user list email:

I am trying to run word_count example from contrib directory (0.7 beta
3 and 0.7.0 rc 1).
It works fine in a single-node configuration, but fails with 2+ nodes.

It fails in the assert statement, which caused problems before
(https://issues.apache.org/jira/browse/CASSANDRA-1700).

Here's a simple ring I have and error messages.
---
Address         Status State   Load            Owns    Token

143797990709940316224804537595633718982
127.0.0.2       Up     Normal  40.2 KB         51.38%
61078635599166706937511052402724559481
127.0.0.1       Up     Normal  36.01 KB        48.62%
143797990709940316224804537595633718982
---
[SERVER SIDE]

ERROR 17:39:57,098 Fatal exception in thread Thread[ReadStage:4,5,main]
java.lang.AssertionError:
(143797990709940316224804537595633718982,61078635599166706937511052402724559481]
	at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1273)
	at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:48)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:62)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
---
[CLIENT_SIDE]
java.lang.RuntimeException: org.apache.thrift.TApplicationException:
Internal error processing get_range_slices
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$RowIterator.maybeInit(ColumnFamilyRecordReader.java:277)
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$RowIterator.computeNext(ColumnFamilyRecordReader.java:292)
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$RowIterator.computeNext(ColumnFamilyRecordReader.java:189)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.nextKeyValue(ColumnFamilyRecordReader.java:148)
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:423)
	at org.apache.hadoop.mapreduce.MapContext.nextKeyValue(MapContext.java:67)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:143)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:621)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)
Caused by: org.apache.thrift.TApplicationException: Internal error
processing get_range_slices
	at org.apache.thrift.TApplicationException.read(TApplicationException.java:108)
	at org.apache.cassandra.thrift.Cassandra$Client.recv_get_range_slices(Cassandra.java:724)
	at org.apache.cassandra.thrift.Cassandra$Client.get_range_slices(Cassandra.java:704)
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$RowIterator.maybeInit(ColumnFamilyRecordReader.java:255)
	... 11 more
---

Looks like tokens used in ColumnFamilySplits
(ColumnFamilyInputFormat.java) are on wrapping ranges (left_token >
right_token).
Any ideas how to fix this?",,chrusty,patrik.modesto,shroman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-1781,,,,,,,,,,,"01/Dec/10 09:33;stuhood;1781.txt;https://issues.apache.org/jira/secure/attachment/12465018/1781.txt",,,,,,,,,,,,,,1.0,stuhood,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20315,,,Wed Dec 01 01:33:44 UTC 2010,,,,,,,,,,"0|i0g7ef:",92629,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"30/Nov/10 01:49;jeromatron;Roman:

What version of Cassandra are your server nodes running?  RC1?;;;","30/Nov/10 03:40;jbellis;dupe of CASSANDRA-1781;;;","30/Nov/10 09:12;shroman;Thank you, Jeremy
I am working with 0.7 beta 3, but the same issue is available in rc1.;;;","01/Dec/10 09:33;stuhood;Another missing case post CASSANDRA-1442.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming never makes progress,CASSANDRA-1766,12480607,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,23/Nov/10 04:25,16/Apr/19 17:33,22/Mar/23 14:57,28/Dec/10 06:47,0.6.9,0.7.0,,,0,,,,,,"I have a client that can never complete a bootstrap.  AC finishes, streaming begins.  Stream initiate completes, and the sources wait on the transfer to finish, but progress is never made on any stream.  Nodetool reports streaming is happening, the socket is held open, but nothing happens.",,eonnen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Dec/10 04:02;jbellis;1766-keepalive.txt;https://issues.apache.org/jira/secure/attachment/12466831/1766-keepalive.txt","24/Nov/10 08:25;eonnen;CASSANDRA-1766.patch;https://issues.apache.org/jira/secure/attachment/12460325/CASSANDRA-1766.patch",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20301,,,Mon Dec 27 23:00:45 UTC 2010,,,,,,,,,,"0|i0g79b:",92606,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"24/Nov/10 08:25;eonnen;Not sure it's exactly related but I encountered an issue where a stream failed post AE and was just wedged with the following stack trace:

""STREAM-STAGE:1"" prio=10 tid=0x00007ff2440a5800 nid=0x3c3c in Object.wait() [0x00007ff24a21f000]
   java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        - waiting on <0x00007ff28884fad8> (a org.apache.cassandra.utils.SimpleCondition)
        at java.lang.Object.wait(Object.java:485)
        at org.apache.cassandra.utils.SimpleCondition.await(SimpleCondition.java:38)
        - locked <0x00007ff28884fad8> (a org.apache.cassandra.utils.SimpleCondition)
        at org.apache.cassandra.streaming.StreamOutManager.waitForStreamCompletion(StreamOutManager.java:164)
        at org.apache.cassandra.streaming.StreamOut.transferSSTables(StreamOut.java:138)
        at org.apache.cassandra.service.AntiEntropyService$Differencer$1.runMayThrow(AntiEntropyService.java:511)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

We suspect that this occurred because the destination node was in drain state, although from reading the code it appears that any failed stream where the destination goes away would be susceptible to this issue. In this case, the StreamManager will never unblock making subsequent repairs to any node that was pending transfer impossible.

I've attached a patch that smooths out some possible streaming issues:

* Catches streaming errors. Near as I can tell, if an error occurred during streaming because the remote node went away, it would bubble all the way out of the executor and not even be logged. Worse, it would keep the current pending file wedged and never allow it to be cleared. This patch will remove the failed transfer when an IOException occurs. Could be it should be more general
* Allows for manual purging of pending files to a host via JMX which means un-sticking a wedged transfer no-longer requires a restart of that node. It also unfortunately results in removal of the file which could require anti-compaction again but this was the least painful path through the code.
* Corrects an unlikely but potentially fatal scenario where concurrent mutation/read from the file and fileMap references could result in dirty reads by making them concurrency-safe collections. Only way I could see this happening is if someone were to run repair multiple times in succession while streaming was happening. Unlikely but possible and the effects on unsafe map reads can result in a completely unresponsive JVM.


I'm not entirely sure this is the right thing to do but I though I'd float it out there for review. Whatever the correct fix, I think there needs to be a way to cancel pending streams so that they aren't stuck.;;;","25/Nov/10 01:18;jbellis;Thanks for the patch, Erik.  Moving followup on that to CASSANDRA-1438 to leave this for the ""streaming doesn't start at all"" problem.;;;","23/Dec/10 02:57;brandon.williams;What's happening here in my case, is there is a firewall/vpn between the bootstrapping node and the source.  The source takes a long time to anticompact, and in this time the tcp connection is killed due to being idle by the firewall.  This causes the stream initiate done message to never be received, because OutboundTcpConnection doesn't actually retry, it only buffers.;;;","23/Dec/10 03:01;jbellis;Should we just turn on socket keepalive?;;;","23/Dec/10 03:15;brandon.williams;That would hack around the problem, but the real issue is SID can get lost on the wire and hang the streaming process forever.  For 0.6, maybe keepalive is the least invasive thing to do.;;;","23/Dec/10 04:02;jbellis;keepalive patch against 0.6;;;","24/Dec/10 06:28;brandon.williams;+1 on keepalive;;;","28/Dec/10 06:47;jbellis;committed.

if you want you can open a new ticket for retrying Initiate if the message gets swallowed, as well as recovering if a source is correctly/incorrectly marked dead, but IMO those have very small benefit:effort ratios.;;;","28/Dec/10 07:00;hudson;Integrated in Cassandra-0.6 #36 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/36/])
    enable keepalive on intra-cluster sockets
patch by jbellis; reviewed by brandonwilliams for CASSANDRA-1766
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Review uses of FileStruct to make sure they are using decorated or raw keys correctly,CASSANDRA-67,12422430,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,09/Apr/09 09:41,16/Apr/19 17:33,22/Mar/23 14:57,10/Apr/09 05:41,,,,,0,,,,,,"Jun Rao commented in #58,

The problem is that FileStruct.key_ is referenced directly in 4 places. At least 2 of those places assume key_ to be the real key, instead of decorated key. These 2 places are in
ColumnFamilyStore.doFileAntiCompaction() (key_ is assigned to lastkey, which is used in isKeyInRanges)
ColumnFamilyStore.doFileCompaction()

In the above places, key_ has to be undeocrated first. Also, we need to make key_ private in FileStruct and use getKey() for referencing.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Apr/09 10:06;jbellis;67-v2.patch;https://issues.apache.org/jira/secure/attachment/12405031/67-v2.patch","10/Apr/09 02:22;jbellis;67-v3.patch;https://issues.apache.org/jira/secure/attachment/12405088/67-v3.patch","10/Apr/09 02:36;jbellis;67-v4.patch;https://issues.apache.org/jira/secure/attachment/12405090/67-v4.patch","10/Apr/09 05:23;jbellis;67-v5.patch;https://issues.apache.org/jira/secure/attachment/12405104/67-v5.patch","09/Apr/09 10:00;jbellis;67.patch;https://issues.apache.org/jira/secure/attachment/12405030/67.patch",,,,,,,,,,5.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19532,,,Thu Apr 09 21:41:03 UTC 2009,,,,,,,,,,"0|i0fwo7:",90891,,,,,Normal,,,,,,,,,,,,,,,,,"09/Apr/09 10:00;jbellis;Clean up FileStruct and make it iterable.  This improves the API and will also be necessary for range queries.;;;","09/Apr/09 10:06;jbellis;Undecorate FS.key when calling isKeyInRanges per Jun's findings.

Note that the rest of anticompaction (and compaction) assume they are dealing with decorated keys, so that (and FS.key) are left alone.;;;","10/Apr/09 01:25;junrao;Reviewed this patch. Here are the comments.

1. FileStruct.getNextKey() should throw IOException (instead of RuntimeException) and let callers deal with it.

2. FileStruct.SeekTo() is not used.

3. FileStruct.iterator() gives user the impression that one can open up multiple independent iterators, but it is not.

4. In the new SSTable format, the block indexes are stored at the end of the file. If you encounter a blockindex key, you can be sure that you will never see a real key afterward.
So, need to change what FileStruct.getNextKey() does when incurring blockindex key.
;;;","10/Apr/09 02:22;jbellis;1. Agreed.

2. It's going to be used by the range patch, but okay, to be consistent I will take it out of this one. :)

3. Added comment warning that iterators are not independent.

4. Good catch!  I was just going off the old advance() method and didn't notice that.  So when I get to the blockindex key I will treat it as if it were EOF.

v3 patch attached.;;;","10/Apr/09 02:26;junrao;3. Wouldn't it be better if FileStruct implements iterator interface directly, if there should be only 1 iterator expected on FileStruct?;;;","10/Apr/09 02:36;jbellis;Yes.  v4 attached.

Also, to avoid confusion with the next() method from Iterator, renamed getNextKey() back to advance().;;;","10/Apr/09 03:07;junrao;FileStructIterator seems unnecessary. It seems it's better to fold what's in FileStructIterator to FileStruct itself.;;;","10/Apr/09 03:08;jbellis;Disagree.  It's cleaner to not have it in the main namespace.;;;","10/Apr/09 05:23;jbellis;version w/o iteration code, per IRC comments.;;;","10/Apr/09 05:37;junrao;comments on v5:
1. remove the following unreferenced pacakges in FileStruct
java.util.Iterator
sun.reflect.generics.reflectiveObjects.NotImplementedException

2. occasional triggers the following failure in test. Not always reproducible. Wonder if it's another timing issue here.
   [testng] FAILED: testGetCompactionBuckets
   [testng] java.lang.AssertionError
   [testng]     at org.apache.cassandra.db.ColumnFamilyStoreTest.testGetCompactionBuckets(ColumnFamilyStoreTest.java:289)
   [testng] ... Removed 22 stack frames
   [testng]

Other than the above, the patch looks fine.
;;;","10/Apr/09 05:41;jbellis;buckets test exception is unrelated.  there is a separate ticket open for that.

committed w/ unused imports removed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Data directories not being properly scrubbed,CASSANDRA-1542,12475000,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,gdusbabek,stuhood,stuhood,24/Sep/10 10:57,16/Apr/19 17:33,22/Mar/23 14:57,28/Sep/10 13:43,0.7 beta 2,,,,0,,,,,,"AbstractCassandraDaemon is trying to scrub data directories once the server has already been initialized (CASSANDRA-1477, r997490).

To reproduce, delete a single component of an SSTable and restart.",,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Sep/10 13:39;gdusbabek;ASF.LICENSE.NOT.GRANTED--v2-0001-do-not-intialize-table-instances-when-loading-schema.-.txt;https://issues.apache.org/jira/secure/attachment/12455799/ASF.LICENSE.NOT.GRANTED--v2-0001-do-not-intialize-table-instances-when-loading-schema.-.txt",,,,,,,,,,,,,,1.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20189,,,Tue Sep 28 13:31:31 UTC 2010,,,,,,,,,,"0|i0g5vb:",92381,,stuhood,,stuhood,Low,,,,,,,,,,,,,,,,,"27/Sep/10 14:50;gdusbabek;The only problem I saw is that we were calling Table.open() for each table as part of DD.loadSchemas().  This made it impossible to scrub the data directories *after* we find out about the tables but *before* they are initialized.  We were already calling Table.open() in AbstractCassandraDaemon, so I decided to remove the call from DD.

That being said, stack traces in logs are still alarming for users.  Since the reason behind FNFE is well understood, I decided to modify ACD.setup() to catch it specifically and emit the error sans trace.  FWIW, CFS.scrubDataDirectories() does seem to be doing its job correctly.

This patch addresses those two items.

One question I have is if we should detect missing bloom filters and row indexes and proactively rebuild them.  I can't think of a case where this would happen in real life, so this would end up being a courtesy for people manually moving files around.  We probably shouldn't encourage this.;;;","27/Sep/10 23:55;jbellis;I don't know that ""corrupt sstable"" is really that much less scary than a stack trace. :);;;","28/Sep/10 02:45;stuhood;> One question I have is if we should detect missing bloom filters and row indexes and proactively rebuild them.
Might be a cool feature to add at some point: would probably be more useful for recovery of corrupted filters/indexes than for missing ones.

+1 Looks good.;;;","28/Sep/10 13:43;gdusbabek;Committed with a less intimidating error message. ;;;","28/Sep/10 21:31;hudson;Integrated in Cassandra #549 (See [https://hudson.apache.org/hudson/job/Cassandra/549/])
    do not intialize table instances when loading schema. complain less loudly when there is a missing sstable component. patch by gdusbabek, reviewed by stuhood. CASSANDRA-1542
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Misspelled ColumSort attribute results in deleted commit logs and .db files,CASSANDRA-8,12419164,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,hammer,hammer,22/Mar/09 02:52,16/Apr/19 17:33,22/Mar/23 14:57,25/Mar/09 11:56,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Mar/09 03:54;armchairalligator;column_sort_fix.patch;https://issues.apache.org/jira/secure/attachment/12403375/column_sort_fix.patch",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19512,,,Tue Mar 24 15:47:29 UTC 2009,,,,,,,,,,"0|i0fwbz:",90836,,,,,Normal,,,,,,,,,,,,,,,,,"22/Mar/09 02:56;hammer;From Alexander Staubo:

""""""
I was confused by the meaning of the ""ColumnSort"" attribute in the
config file, and typed something invalid. The result was the following
exception on compaction, when Cassandra starts up:

java.lang.NullPointerException
       at org.apache.cassandra.config.DatabaseDescriptor.getTypeInfo(DatabaseDescriptor.java:729)
       at org.apache.cassandra.db.ColumnIndexer.serialize(ColumnIndexer.java:62)
       at org.apache.cassandra.db.CompactSerializerInvocationHandler.invoke(CompactSerializerInvocationHandler.java:50)
       at $Proxy0.serialize(Unknown Source)
       at org.apache.cassandra.db.Memtable.flushForRandomPartitioner(Memtable.java:461)
       at org.apache.cassandra.db.Memtable.flush(Memtable.java:440)
       at org.apache.cassandra.db.Memtable.forceflush(Memtable.java:279)
       at org.apache.cassandra.db.ColumnFamilyStore.forceFlush(ColumnFamilyStore.java:409)
       at org.apache.cassandra.db.Table.flush(Table.java:857)
       at org.apache.cassandra.db.CommitLog.doRecovery(CommitLog.java:408)
       at org.apache.cassandra.db.CommitLog.recover(CommitLog.java:317)
       at org.apache.cassandra.db.RecoveryManager.recoverEachTable(RecoveryManager.java:90)
       at org.apache.cassandra.db.RecoveryManager.doRecovery(RecoveryManager.java:77)
       at org.apache.cassandra.db.DBManager.<init>(DBManager.java:112)
       at org.apache.cassandra.db.DBManager.instance(DBManager.java:61)
       at org.apache.cassandra.service.StorageService.start(StorageService.java:465)
       at org.apache.cassandra.service.CassandraServer.start(CassandraServer.java:96)
       at org.apache.cassandra.service.CassandraServer.main(CassandraServer.java:1049)

...after which the commit logs were gone, and there were no .db files.
So my database went *poof*.

The attached patch emits a warning if the ColumnSort attribute is
invalid, and uses the appropriate default of ""Time"" in that case.

However, this fixes only one possible failure scenario for
compactions, but not the underlying problem that a failing compaction
hoses the database. In my opinion, failing compactions should never
result in the commit logs being deleted, regardless of the failure
cause.
"""""";;;","22/Mar/09 03:54;armchairalligator;Attached patch to add warning on misconfiguration;;;","24/Mar/09 23:47;avinash.lakshman@gmail.com;This works for now. However a more disciplined approach is required. For now this case is closed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig loadfunc fails with java.io.FileNotFoundException: ...:.../job.jar!/storage-conf.xml,CASSANDRA-1590,12476720,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,tv42,tv42,07/Oct/10 07:05,16/Apr/19 17:33,22/Mar/23 14:57,13/Nov/10 05:51,0.6.9,,,,0,,,,,,"Trying to run the example job from contrib/pig (after fixing it to start at all in the first place; details later) results in this:


2010-10-06 15:43:32,117 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - (Name: Store(hdfs://localhost/tmp/temp-1257182404/tmp1075428643:org.apache.pig.builtin.BinStorage) - 1-60 Operator Key: 1-60)
2010-10-06 15:43:32,164 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.CombinerOptimizer - Choosing to move algebraic foreach to combiner
2010-10-06 15:43:32,224 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size before optimization: 3
2010-10-06 15:43:32,224 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size after optimization: 3
2010-10-06 15:43:32,302 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3
2010-10-06 15:43:40,356 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting up single store job
2010-10-06 15:43:40,450 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 1 map-reduce job(s) waiting for submission.
2010-10-06 15:43:40,457 [Thread-12] WARN  org.apache.hadoop.mapred.JobClient - Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.
2010-10-06 15:43:40,950 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 0% complete
2010-10-06 15:43:41,038 [Thread-12] INFO  org.apache.cassandra.config.DatabaseDescriptor - DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
2010-10-06 15:43:41,211 [Thread-12] WARN  org.apache.cassandra.config.DatabaseDescriptor - KeysCachedFraction is deprecated: use KeysCached instead.
2010-10-06 15:43:41,232 [Thread-12] WARN  org.apache.cassandra.config.DatabaseDescriptor - KeysCachedFraction is deprecated: use KeysCached instead.
2010-10-06 15:43:42,305 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - HadoopJobId: job_201010061447_0008
2010-10-06 15:43:42,305 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - More information at: http://localhost:50030/jobdetails.jsp?jobid=job_201010061447_0008
2010-10-06 15:44:15,025 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 33% complete
2010-10-06 15:44:17,037 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 100% complete
2010-10-06 15:44:17,037 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 1 map reduce job(s) failed!
2010-10-06 15:44:17,067 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Failed!
2010-10-06 15:44:17,199 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2997: Unable to recreate exception from backed error: Error: java.lang.RuntimeException: java.io.FileNotFoundException: /var/lib/hadoop-0.20/cache/hadoop/mapred/local/taskTracker/jobcache/job_201010061447_0008/attempt_201010061447_0008_m_000000_0/work/file:/var/lib/hadoop-0.20/cache/hadoop/mapred/local/taskTracker/jobcache/job_201010061447_0008/jars/job.jar!/storage-conf.xml (No such file or directory)
Details at logfile: /home/tv/casspig/cassandra/contrib/pig/pig_1286405010154.log

Contents of that pig_*.log:


Backend error message
---------------------
Error: java.lang.RuntimeException: java.io.FileNotFoundException: /var/lib/hadoop-0.20/cache/hadoop/mapred/local/taskTracker/jobcache/job_201010061447_0008/attempt_201010061447_0008_m_000000_0/work/file:/var/lib/hadoop-0.20/cache/hadoop/mapred/local/taskTracker/jobcache/job_201010061447_0008/jars/job.jar!/storage-conf.xml (No such file or directory)
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:542)
	at org.apache.cassandra.hadoop.ConfigHelper.getThriftPort(ConfigHelper.java:188)
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$RowIterator.<init>(ColumnFamilyRecordReader.java:118)
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$RowIterator.<init>(ColumnFamilyRecordReader.java:104)
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.initialize(ColumnFamilyRecordReader.java:93)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader.initialize(PigRecordReader.java:133)
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.initialize(MapTask.java:418)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:620)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
	at org.apache.hadoop.mapred.Child.main(Child.java:170)
Caused by: java.io.FileNotFoundException: /var/lib/hadoop-0.20/cache/hadoop/mapred/local/taskTracker/jobcache/job_201010061447_0008/attempt_201010061447_0008_m_000000_0/work/file:/var/lib/hadoop-0.20/cache/hadoop/mapred/local/taskTracker/jobcache/job_201010061447_0008/jars/job.jar!/storage-conf.xml (No such file or directory)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:106)
	at java.io.FileInputStream.<init>(FileInputStream.java:66)
	at sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:70)
	at sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:161)
	at com.sun.org.apache.xerces.internal.impl.XMLEntityManager.setupCurrentEntity(XMLEntityManager.java:653)
	at com.sun.org.apache.xerces.internal.impl.XMLVersionDetector.determineDocVersion(XMLVersionDetector.java:186)
	at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:772)
	at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:737)
	at com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:119)
	at com.sun.org.apache.xerces.internal.parsers.DOMParser.parse(DOMParser.java:235)
	at com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderImpl.parse(DocumentBuilderImpl.java:284)
	at javax.xml.parsers.DocumentBuilder.parse(DocumentBuilder.java:208)
	at org.apache.cassandra.utils.XMLUtils.<init>(XMLUtils.java:43)
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:167)
	... 9 more

Pig Stack Trace
---------------
ERROR 2997: Unable to recreate exception from backed error: Error: java.lang.RuntimeException: java.io.FileNotFoundException: /var/lib/hadoop-0.20/cache/hadoop/mapred/local/taskTracker/jobcache/job_201010061447_0008/attempt_201010061447_0008_m_000000_0/work/file:/var/lib/hadoop-0.20/cache/hadoop/mapred/local/taskTracker/jobcache/job_201010061447_0008/jars/job.jar!/storage-conf.xml (No such file or directory)

org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias topnames
	at org.apache.pig.PigServer.openIterator(PigServer.java:607)
	at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:544)
	at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:241)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:162)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:138)
	at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:75)
	at org.apache.pig.Main.main(Main.java:380)
Caused by: org.apache.pig.backend.executionengine.ExecException: ERROR 2997: Unable to recreate exception from backed error: Error: java.lang.RuntimeException: java.io.FileNotFoundException: /var/lib/hadoop-0.20/cache/hadoop/mapred/local/taskTracker/jobcache/job_201010061447_0008/attempt_201010061447_0008_m_000000_0/work/file:/var/lib/hadoop-0.20/cache/hadoop/mapred/local/taskTracker/jobcache/job_201010061447_0008/jars/job.jar!/storage-conf.xml (No such file or directory)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.Launcher.getErrorMessages(Launcher.java:231)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.Launcher.getStats(Launcher.java:175)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:270)
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.execute(HExecutionEngine.java:308)
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1007)
	at org.apache.pig.PigServer.store(PigServer.java:697)
	at org.apache.pig.PigServer.openIterator(PigServer.java:590)
	... 6 more
================================================================================


I'm attaching a tarball with everything needed to reproduce this, see the run script there.",Ubuntu 10.04 with Hadoop from Cloudera CDH3b2,jeromatron,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Oct/10 07:06;tv42;casspig.tgz;https://issues.apache.org/jira/secure/attachment/12456553/casspig.tgz","12/Nov/10 02:38;tv42;p1590.diff;https://issues.apache.org/jira/secure/attachment/12459368/p1590.diff",,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20210,,,Tue Nov 16 01:01:09 UTC 2010,,,,,,,,,,"0|i0g65r:",92428,,,,,Normal,,,,,,,,,,,,,,,,,"07/Oct/10 07:06;tv42;A script with some auxiliary files to reproduce said problem from scratch. Assumes a working Hadoop installation, pseudo-distributed mode is fine.;;;","14/Oct/10 06:55;jeromatron;It works if you manually build pig and put your storage-conf.xml in the jar, but there is something wrong with how it's trying to find the storage-conf.xml.  It shouldn't matter with 0.7 since it's only using hadoop vars or env vars (CASSANDRA-1322).  However, it needs to be addressed in 0.6.x so that it's more usable.;;;","12/Nov/10 02:38;tv42;This seems to be the right fix; URIs/URLs are loaded from jars, just filenames are not.;;;","13/Nov/10 05:51;brandon.williams;Committed, thanks!;;;","14/Nov/10 20:38;jbellis;Did this make it into 0.6.8?;;;","16/Nov/10 09:01;brandon.williams;Not quite.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-cli cannot connect ,CASSANDRA-1333,12470395,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,arya,arya,29/Jul/10 08:06,16/Apr/19 17:33,22/Mar/23 14:57,29/Jul/10 09:49,0.7 beta 1,,,,0,,,,,,"I cannot connect to any of my nodes using Cassandra-CLI. I think this has happened about 2 weeks ago:

[agoudarzi@cas-test1 bin]$ cassandra-cli --host 10.50.26.132 --port 9160 --debug
Exception retrieving information about the cassandra node, check you have connected to the thrift port.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
	at org.apache.thrift.transport.TFramedTransport.readFrame(TFramedTransport.java:129)
	at org.apache.thrift.transport.TFramedTransport.read(TFramedTransport.java:101)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:369)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:295)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:202)
	at org.apache.cassandra.thrift.Cassandra$Client.recv_describe_cluster_name(Cassandra.java:1117)
	at org.apache.cassandra.thrift.Cassandra$Client.describe_cluster_name(Cassandra.java:1103)
	at org.apache.cassandra.cli.CliMain.connect(CliMain.java:164)
	at org.apache.cassandra.cli.CliMain.main(CliMain.java:255)
Welcome to cassandra CLI.

Type 'help' or '?' for help. Type 'quit' or 'exit' to quit.
[default@unknown] exit                     

However using Thrift PHP Client I have no problem connecting and executing describe_cluster_name().

I have configured Cassandra RPC port and IP as follows:

# The address to bind the Thrift RPC service to
rpc_address: 10.50.26.132
# port for Thrift to listen on
rpc_port: 9160

Steps to Reproduce:
1. Start from a clean setup;
2. Run py_stress to insert some keys and create the default keyspace;
3. Try connecting using cassandra-cli like command above. You'll get the Exception.
","CentOS 5.2
trunk",arya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20086,,,Mon Nov 22 02:07:24 UTC 2010,,,,,,,,,,"0|i0g4f3:",92146,,,,,Normal,,,,,,,,,,,,,,,,,"29/Jul/10 08:17;arya;More Info:

The server log says my client is old! But I am using the latest build from trunc:

ERROR [pool-1-thread-22] 2010-07-28 17:09:54,688 CustomTThreadPoolServer.java (line 175) Thrift error occurred during processing of message.
org.apache.thrift.protocol.TProtocolException: Missing version in readMessageBegin, old client?
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:211)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2587)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
;;;","29/Jul/10 09:49;jbellis;fixed in r980285.  thanks!;;;","21/Nov/10 21:05;jpartogi;Is this broken in 0.7.0-beta3 again?

I am getting this error when trying to connect from CLI:
[default@unknown] connect localhost/9160 
Exception retrieving information about the cassandra node, check you have connected to the thrift port.

Cassandra is running and thrift is bound to port 9160;;;","22/Nov/10 10:07;jbellis;No. Probably you are running server in un-framed mode; cli defaults to trying to connect w/ framed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Upgrade from 0.6.3 to 0.6.5, exception when replay commitlog",CASSANDRA-1492,12473815,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,apache.zli,apache.zli,10/Sep/10 23:28,16/Apr/19 17:33,22/Mar/23 14:57,13/Sep/10 21:30,0.6.6,,,,0,,,,,,"When start cassandra 0.6.5 with commitlog of 0.6.3, got exception when replay the commitlog. 

ERROR 15:14:16,174 Exception encountered during startup.
org.apache.cassandra.db.marshal.MarshalException: invalid UTF8 bytes [-84, 16, 10, -105]
	at org.apache.cassandra.db.marshal.UTF8Type.getString(UTF8Type.java:43)
	at org.apache.cassandra.db.Column.getString(Column.java:215)
	at org.apache.cassandra.db.marshal.AbstractType.getColumnsString(AbstractType.java:85)
	at org.apache.cassandra.db.ColumnFamily.toString(ColumnFamily.java:334)
	at org.apache.commons.lang.ObjectUtils.toString(ObjectUtils.java:241)
	at org.apache.commons.lang.StringUtils.join(StringUtils.java:3073)
	at org.apache.commons.lang.StringUtils.join(StringUtils.java:3133)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:241)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:173)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:114)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:214)
","Linux, jdk 1.6",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Sep/10 12:04;jbellis;1492.txt;https://issues.apache.org/jira/secure/attachment/12454356/1492.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20164,,,Mon Sep 13 13:30:27 UTC 2010,,,,,,,,,,"0|i0g5e7:",92304,,gdusbabek,,gdusbabek,Normal,,,,,,,,,,,,,,,,,"10/Sep/10 23:49;gdusbabek;Commit logs are typically not compatible across minor versions.  You should drain your node before upgrading and restarting it.;;;","11/Sep/10 00:38;apache.zli;Tested again. Drained 0.6.3 commitlog with Cassandra 0.6.3, then start Cassandra 0.6.5 and works. 
Then stop Cassandra 0.6.5 and start Cassandra 0.6.5 again. Same exception. 

I shouldn't remove all data file and recreate a new node with minor version upgrade. Right?

;;;","11/Sep/10 03:27;jbellis;commitlog is supposed to be compatible w/in minor versions.  this looks like one of those ""something is declared UTF8Type, that should be bytestype"" things.  STATUS_CF is UTF8Type in current 0.6, is that a bug?;;;","11/Sep/10 04:52;gdusbabek;Probably. It's a BytesType in trunk.;;;","11/Sep/10 12:04;jbellis;patch to switch to bytestype.

agreed that if you have this in your commitlog already, deleting the commitlog file is the best fix.;;;","13/Sep/10 20:46;gdusbabek;+1;;;","13/Sep/10 21:30;jbellis;committed.

(blowing away the CL isn't a permanent fix if you have a node in your cluster w/ a non-UTF8 IP.  you'll probably need to apply this patch.);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get_range_slice returns multiple copies of each row for ConsistencyLevel > ONE,CASSANDRA-884,12458977,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,omerhj,omerhj,omerhj,13/Mar/10 04:50,16/Apr/19 17:33,22/Mar/23 14:57,20/Mar/10 04:36,0.6,,,,0,,,,,,"I've noticed that both 0.5.1 and 0.6b2 return multiple identical copies of the data stored in my keyspace whenever I make a call to get_range_slice or get_range_slices using
ConsistencyLevel.QUORUM and ReplicationFactor is greater than one.

So with ReplicationFactor set to 2 for my application's KeySpace I get double the number of KeySlices that I expect to get. When using ConsistencyLevel.ONE I get only one KeySlice for each row.

I've seen this happen with Cassandra 0.5.1 and with 0.6 beta 2. The behavior on 0.6 beta 2 is exhibited with both get_range_slice and get_range_slices.

The attached Java program demonstrates the issue for 0.6 beta 2. The program writes a series of single-column rows into the Standard1 table, and then uses get_range_slice to receive a list of all row. The returned number of rows is consistently twice the number of rows written to the database. I wipe out the database completely before running the test.
",4-cluster Gentoo Linux 2.6.18 with a ReplicationFactor of 2,ajslater,johanoskarsson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Mar/10 04:28;omerhj;0001-RangeSliceResponseResolver.patch;https://issues.apache.org/jira/secure/attachment/12438957/0001-RangeSliceResponseResolver.patch","17/Mar/10 04:57;jbellis;884-v2.txt;https://issues.apache.org/jira/secure/attachment/12438962/884-v2.txt","13/Mar/10 04:53;omerhj;TestApp2.java;https://issues.apache.org/jira/secure/attachment/12438639/TestApp2.java",,,,,,,,,,,,3.0,omerhj,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19902,,,Mon May 31 23:29:19 UTC 2010,,,,,,,,,,"0|i0g1o7:",91701,,,,,Normal,,,,,,,,,,,,,,,,,"13/Mar/10 04:53;omerhj;The attached Java program demonstrates that multiple copies of rows are returned by get_range_slice. At least it does here :-);;;","17/Mar/10 04:28;omerhj;I think I have identified the source of my problem. I'm still new to the Cassandra source code so I could have this completely wrong. The patch is for the 0.6 version of org.apache.cassandra.service.RangeSliceResponseResolver.

Removing redundant copies of returned rows appears to happen in the RangeSliceResponseResolver class. This class uses an anonymous innner class that extends ReducingIterator to weed out the duplicates.

ReducingIterator provides a computeNext() method that compares successive items to see if they are duplicates. It does this by comparing the current and previous ('last') items to its isEqual() method.

RangeSliceResponseResolver does not override that isEqual method. That causes Pair<Row, InetAddress> objects to be compared with each other.  The ReducingIterator.isEqual method always returns false, because (1)  Row doesn't specify an equals() method and (2) even if it did, the InetAddresses of rows retrieved from different Cassandra instances would still be different. This causes each row to be seen as unique.

The attached patch repairs this by providing the ReducingIterator derivative in RangeSliceResponseResolver with an isEqual() method that compares the key and cf members of the Row objects. It ignores the InetAddress component of the Pair.

Alternatively I could have added an equals() method to Row which would have simplified the isEqual() method in RangeSliceResponseResolver.java a bit.

;;;","17/Mar/10 04:57;jbellis;Your analysis is spot on, good work there.

I think though that equals should just look at the key: we want to collect (in the ""reduced"" list) all versions of the rows associated with that key, so we can repair any differences.  If we only collect identical versions together then the repair is a no-op.

Attached is a diff that makes this change and adds a missing versionSources.clear() call.;;;","17/Mar/10 04:58;jbellis;Can you test v2?;;;","19/Mar/10 00:05;omerhj;Your v2 patch works for me. The attached TestApp2 now gives back the expected result. Also, all JUnit test cases for my Cassandra-using application now pass. Thanks!
;;;","19/Mar/10 00:25;jbellis;committed, thanks!;;;","20/Mar/10 02:35;omerhj;We've started using the wrapper API I've written for Cassandra for light development here and quite unexpectedly I've started getting NullPointerExceptions whenever a get_range_slices request is performed against a particular CF. The CF in question has a replication factor of 2. get_range_slices still works fine against the Standard1 CF that's exercised by the TestApp2 file included in this ticket. In my storage-conf.xml Standard1 is set up with a replication factor of 3.

Here is the stack trace I'm seeing over and over again, on each of my 4 servers:

ERROR [pool-1-thread-64] 2010-03-19 12:23:30,119 Cassandra.java (line 1440) Internal error processing get_range_slices
java.lang.NullPointerException
        at org.apache.cassandra.service.RangeSliceResponseResolver$2.isEqual(RangeSliceResponseResolver.java:81)
        at org.apache.cassandra.service.RangeSliceResponseResolver$2.isEqual(RangeSliceResponseResolver.java:74)
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:69)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:135)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:130)
        at org.apache.cassandra.service.RangeSliceResponseResolver.resolve(RangeSliceResponseResolver.java:101)
        at org.apache.cassandra.service.RangeSliceResponseResolver.resolve(RangeSliceResponseResolver.java:41)
        at org.apache.cassandra.service.QuorumResponseHandler.get(QuorumResponseHandler.java:86)
        at org.apache.cassandra.service.StorageProxy.getRangeSlice(StorageProxy.java:592)
        at org.apache.cassandra.thrift.CassandraServer.getRangeSlicesInternal(CassandraServer.java:587)
        at org.apache.cassandra.thrift.CassandraServer.get_range_slices(CassandraServer.java:559)
        at org.apache.cassandra.thrift.Cassandra$Processor$get_range_slices.process(Cassandra.java:1432)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:1115)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)

Additional notes:
I'm using revision 924837 of the Cassandra 0.6 branch which includes the patch above. For some reason the line numbers in RangeSliceResponseResolver seem to be off by one in the stack trace, but I'm positive that it's the patched version being used with the isEqual() method that tests only the keys.

I guess this might be fixed by adding a few null checks to that isEqual() method, but perhaps that would just be hiding a problem somewhere else in the code.
;;;","20/Mar/10 02:56;jbellis;Can you add this to isEqual to see which part is null?  You're right, that ""shouldn't happen,"" let's not just band-aid w/ null checks.

                assert o1 != null && o2 != null : ""null pair"";
                assert o1.left != null && o2.left != null : ""null row"";
;;;","20/Mar/10 04:26;omerhj;During my last build an apache-casssandra-0.6-b2.jar somehow made its way into the lib directories, overriding the newer 0.6-b3.jar that I should have been using. I removed the older apache-casssandra-0.6-b2.jar file that contained my older patch, restarted my instances and the problem disappeared. So it looks likely that this was operator error on my part. 




;;;","20/Mar/10 04:36;jbellis;k, marking closed again;;;","01/Jun/10 05:07;ajslater;Please re-open or duplicate.

Testing with 0.6-trunk today:

Reading with CL > ONE returns multiple copies of the same column per key consistent with the replicas queried before return. i.e,  for RC=3, a QUORUM read yields 2 copies and an ALL read returns 3.
This is with pycassa get_range() which is using get_range_slice()

I see the same behavior with 0.6.1 and 0.6.2 debs

If my experience is not unique, anyone using get_range_slice is now deluged with duplicate data.;;;","01/Jun/10 06:16;jbellis;AJ, can you create a new issue with some sample code to reproduce what you are seeing?;;;","01/Jun/10 07:29;ajslater;CASSANDRA-1145 Opened;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Crash during startup: SSTable doesn't handle corrupt (empty) tmp files,CASSANDRA-1904,12494118,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,gdusbabek,tcn,tcn,27/Dec/10 19:01,16/Apr/19 17:33,22/Mar/23 14:57,30/Dec/10 21:57,0.7.0,,,,0,,,,,,"Applies to 0.7rc3 as well, but not yet selectable in Jira.

cassandra stumbles upons empty Data files and crashes during startup rather than ignoring these files:

java.lang.ArithmeticException: / by zero
	at org.apache.cassandra.io.sstable.SSTable.estimateRowsFromIndex(SSTable.java:233)
	at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:284)
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:200)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:225)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:448)
	at org.apache.cassandra.db.ColumnFamilyStore.addIndex(ColumnFamilyStore.java:305)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:246)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:448)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:436)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:138)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:55)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:216)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:134)
Exception encountered during startup.
java.lang.ArithmeticException: / by zero
	at org.apache.cassandra.io.sstable.SSTable.estimateRowsFromIndex(SSTable.java:233)
	at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:284)
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:200)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:225)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:448)
	at org.apache.cassandra.db.ColumnFamilyStore.addIndex(ColumnFamilyStore.java:305)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:246)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:448)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:436)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:138)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:55)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:216)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:134)

The empty Data/Index tmp files were in my case created and left over when I attempted to create a secondary index at runtime which crashed the JVM due to OOM.

SSTable handles IOExceptions so it should be an easy fix: in SSTable.estimateRowsFromIndex() just check for ifile.length() ==ifile.getFilePointer()==keys==0 and throw an IOException.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Dec/10 07:26;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-CFS.scrubDataDirectories-to-include-index-files.txt;https://issues.apache.org/jira/secure/attachment/12467142/ASF.LICENSE.NOT.GRANTED--v1-0001-CFS.scrubDataDirectories-to-include-index-files.txt",,,,,,,,,,,,,,1.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20364,,,Thu Dec 30 13:57:42 UTC 2010,,,,,,,,,,"0|i0g84n:",92747,,,,,Normal,,,,,,,,,,,,,,,,,"30/Dec/10 05:25;gdusbabek;I get a different error, which happens before the error in Timo's stack trace.  I suspect his files weren't really zero-length, but the BRAF just reported them that way.  Either way: CFS.scrubDataDirectories() should take secondary indexes into account.

My forced error:

 INFO [main] SSTableReader.java@154 14:43:44,146 Opening /Users/gary.dusbabek/cass-configs/trunk/data_1/data/Keyspace1/Indexed1.birthdate_idx-f-1
DEBUG [main] SSTableReader.java@164 14:43:44,146 Load statistics for /Users/gary.dusbabek/cass-configs/trunk/data_1/data/Keyspace1/Indexed1.birthdate_idx-f-1
ERROR [main] ColumnFamilyStore.java@234 14:43:44,147 Corrupt sstable /Users/gary.dusbabek/cass-configs/trunk/data_1/data/Keyspace1/Indexed1.birthdate_idx-f-1=[Filter.db, Index.db, Data.db, Statistics.db]; skipped
java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:375)
	at org.apache.cassandra.utils.EstimatedHistogram$EstimatedHistogramSerializer.deserialize(EstimatedHistogram.java:179)
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:166)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:225)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:488)
	at org.apache.cassandra.db.ColumnFamilyStore.addIndex(ColumnFamilyStore.java:345)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:246)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:488)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:476)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:162)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:54)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:240)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:133)
DEBUG [main] SSTableTracker.java@179 14:43:44,148 key cache capacity for Indexed1.birthdate_idx is 200000
DEBUG [main] SSTableTracker.java@190 14:43:44,149 row cache capacity for Indexed1.birthdate_idx is 0;;;","30/Dec/10 06:11;tcn;What is BRAF? Yes, they were zero-length, at least according to ls and eclipse' debugger and I even deleted the files and recreated equally named, empty files manually :) 

IMHO code like

  int foo = 0;

  while(whatever) { whatever }

  bar / foo;

is always broken. A simple additional if-throw-IOException won't hurt and will help to grasp things faster.;;;","30/Dec/10 06:55;gdusbabek;I reproduced the original error by removing the statistics db in my database.  I still think the right approach is to make sure that CFS.scrubDataDirectories() includes index CFs.  

FWIW empty files will cause problems all over the place, not just at that particular spot.  The approach that has worked so far is to 'scrub' the directories of undesired files at startup rather than address all the places in the code where we've assumed the files in the directories are healthy and supposed to be there.;;;","30/Dec/10 07:36;jbellis;bq. I still think the right approach is to make sure that CFS.scrubDataDirectories() includes index CFs. 

+1, recovering from random files being missing isn't something we want to try to handle now.;;;","30/Dec/10 07:40;jbellis;also +1 to the patch;;;","30/Dec/10 21:57;gdusbabek;committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reads (get_column) miss data or return stale values if a memtable is being flushed,CASSANDRA-98,12423628,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,sandeep_tata,sandeep_tata,sandeep_tata,24/Apr/09 06:08,16/Apr/19 17:33,22/Mar/23 14:57,24/Apr/09 08:52,0.3,,,,0,,,,,,"Reads can return missing values (null/exception) or find stale copies of a column if the read happens during an SSTable flush.

The get_column can go in, and not find the data in the current memtable. When it looks in the ""historical"" memtable, if that CF has already been flushed, then  it gets cleared from the historical memtable. As a result, the read looks for the column in older SSTables and finds a stale value (if it exists) or returns with null.

It can be tricky to reproduce this problem, but the reason is pretty easy to see.

While subsequent reads might return the correct value (from disk), this behavior makes it very difficult for apps that expect to ""read your writes"", at least in the absence of failures.",all,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Apr/09 06:12;sandeep_tata;CASSANDRA-98.patch;https://issues.apache.org/jira/secure/attachment/12406291/CASSANDRA-98.patch",,,,,,,,,,,,,,1.0,sandeep_tata,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19549,,,Fri Apr 24 00:52:55 UTC 2009,,,,,,,,,,"0|i0fwv3:",90922,,,,,Normal,,,,,,,,,,,,,,,,,"24/Apr/09 06:12;sandeep_tata;Simple solution: Don't call columnFamily.clear() until the entire ""historical"" memtable has been flushed. This way, you're not stuck in a state where you can't find the data in the memtables and the SSTable is not ready yet.

The impact of this change is that memory cannot get freed up partially while a flush is going on. This is an insignificant penalty.;;;","24/Apr/09 08:52;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FatClient removal causes ConcurrentModificationException,CASSANDRA-757,12455173,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,03/Feb/10 21:33,16/Apr/19 17:33,22/Mar/23 14:57,28/Apr/10 02:36,0.7 beta 1,,,,0,,,,,,"After using a fatclient and killing it, I later receive this ST on all nodes:

 INFO 16:04:58,999 FatClient /10.242.4.13 has been silent for 3600000ms, removing from gossip
ERROR 16:04:58,999 Fatal exception in thread Thread[Timer-1,5,main]
java.lang.RuntimeException: java.util.ConcurrentModificationException
        at org.apache.cassandra.gms.Gossiper$GossipTimerTask.run(Gossiper.java:96)
        at java.util.TimerThread.mainLoop(Timer.java:534)
        at java.util.TimerThread.run(Timer.java:484)
Caused by: java.util.ConcurrentModificationException
        at java.util.Hashtable$Enumerator.next(Hashtable.java:1048)
        at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:382)
        at org.apache.cassandra.gms.Gossiper$GossipTimerTask.run(Gossiper.java:90)
        ... 2 more
","debian lenny amd64 OpenJDK 64-Bit Server VM (build 1.6.0_0-b11, mixed mode)
",brandon.williams,johanoskarsson,kingryan,mojodna,wadey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Apr/10 03:52;brandon.williams;0001_use_concurrent_structures.txt;https://issues.apache.org/jira/secure/attachment/12442616/0001_use_concurrent_structures.txt",,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19852,,,Fri Jul 16 20:40:55 UTC 2010,,,,,,,,,,"0|i0g0vz:",91574,,,,,Low,,,,,,,,,,,,,,,,,"05/Feb/10 03:20;jbellis;The good news is this looks like a pretty harmless heisenbug -- eventually (usually the next timer iteration) no changes will be made to the map during iteration and it will work.;;;","09/Mar/10 06:47;brandon.williams;Patch to avoid concurrent modification.;;;","09/Mar/10 06:53;jbellis;were you able to reproduce before, and not w/ this patch?  because AFAIK using an iterator manually instead of a foreach only helps when the CME is from modifying the collection in the same loop, and you can replace that w/ iter.remove().  otherwise if the CME is happening from another thread modifying stuff you have to move to a Concurrent collection.;;;","09/Mar/10 07:19;jbellis;i think what needs to happen is take all the structures in Gossiper and make them Concurrent equivalents, encapsulate any places where we're returning them to other objects directly, and audit the rest for correctness, because the original author basically ignored threadsafety entirely;;;","20/Apr/10 05:25;brandon.williams;Patch to use concurrent structures and remove synchronization  in the Gossiper.;;;","22/Apr/10 23:08;jbellis;Jaakko hasn't replied so I'll take a stab at reviewing.

Can you rebase?  2 hunks are failing in Gossiper for me.;;;","23/Apr/10 03:52;brandon.williams;Updated w/rebased patch.;;;","28/Apr/10 02:36;jbellis;committed.  also inlined the comparator, replaced the last synchronized methods in EndpointState w/ volatile fields, and removed getSortedApplicationStates.;;;","17/Jul/10 04:35;wadey;This is a more serious bug than originally thought due to CASSANDRA-1289. When this exception gets thrown, it causes GossipTimerTask to stop running until the server is restarted. Because of this, I would recommend a backport to 0.6.x (I'll offer to do the backport as well). ;;;","17/Jul/10 04:40;jbellis;backporting this to 0.6 doesn't work for me, but a hack to log exceptions would be ok.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FBUtilities.hexToBytes() doesn't accommodate odd-length strings.,CASSANDRA-1411,12472038,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,gdusbabek,gdusbabek,gdusbabek,20/Aug/10 04:49,16/Apr/19 17:33,22/Mar/23 14:57,20/Aug/10 05:02,0.7 beta 2,,,,0,,,,,,"This is a problem when a user specifies ByteOrderedPartitioner with an odd-length initial token (like ""0"").",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Aug/10 04:53;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-FBUtilities.hexToBytes-doesn-t-handle-odd-length-strin.txt;https://issues.apache.org/jira/secure/attachment/12452572/ASF.LICENSE.NOT.GRANTED--v1-0001-FBUtilities.hexToBytes-doesn-t-handle-odd-length-strin.txt",,,,,,,,,,,,,,1.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20124,,,Sat Aug 21 11:14:34 UTC 2010,,,,,,,,,,"0|i0g4w7:",92223,,,,,Low,,,,,,,,,,,,,,,,,"20/Aug/10 04:56;jhermes;+1 anti-annoyance patch.;;;","20/Aug/10 23:56;messi;Good ol' well-thought pragmatism? Why sanitize user input and adjust the token when you can hide errors by making core functions more flexible.;;;","21/Aug/10 00:02;gdusbabek;Erroring on odd-lengthed, yet still valid hex values is neither pragmatic or very useful.  hex 0 is dec 0, hex 101 is dec 257, so is hex 0101; let's just handle them.;;;","21/Aug/10 19:14;hudson;Integrated in Cassandra #518 (See [https://hudson.apache.org/hudson/job/Cassandra/518/])
    FBUtilities.hexToBytes doesn't handle odd-length strings. patch by Gary Dusbabek, reviewed by Jon Hermes. CASSANDRA-1411
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Queries on system keyspace over thrift api all fail,CASSANDRA-1649,12478089,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,thepaul,thepaul,23/Oct/10 00:37,16/Apr/19 17:33,22/Mar/23 14:57,23/Oct/10 05:17,0.7 beta 3,,Legacy/CQL,,0,,,,,,"as far as I can tell, any calls to get, get_slice, get_range_slices, get_count, etc on any ColumnFamily in the ""system"" keyspace results in an error like the following:

{noformat}
ERROR 16:29:41,278 Internal error processing get
java.lang.AssertionError: No replica strategy configured for system
        at org.apache.cassandra.service.StorageService.getReplicationStrategy(StorageService.java:315)
        at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:1459)
        at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:1447)
        at org.apache.cassandra.service.StorageService.findSuitableEndpoint(StorageService.java:1493)
        at org.apache.cassandra.service.StorageProxy.weakRead(StorageProxy.java:245)
        at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:224)
        at org.apache.cassandra.thrift.CassandraServer.readColumnFamily(CassandraServer.java:131) 
        at org.apache.cassandra.thrift.CassandraServer.get(CassandraServer.java:324)
        at org.apache.cassandra.thrift.Cassandra$Processor$get.process(Cassandra.java:2655)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2555)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
{noformat}

Might be only when this is done over the thrift api.  I don't even know how to use the avro api or query in any other way.  But at least this sort of thing used to work around a week ago.","Debian Squeeze, cassandra svn HEAD (r1026380)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Oct/10 04:55;thepaul;1649-system-test.txt;https://issues.apache.org/jira/secure/attachment/12457870/1649-system-test.txt","23/Oct/10 02:48;jbellis;1649.txt;https://issues.apache.org/jira/secure/attachment/12457859/1649.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20239,,,Sat Oct 23 12:49:05 UTC 2010,,,,,,,,,,"0|i0g6j3:",92488,,thepaul,,thepaul,Low,,,,,,,,,,,,,,,,,"23/Oct/10 02:48;jbellis;moves strategy creation into Table instantiation so it can't be out of sync;;;","23/Oct/10 04:55;thepaul;An addition to test/system/test_thrift_server.py which makes sure queries to the system keyspace can be made;;;","23/Oct/10 05:17;jbellis;test passes both with and without this patch, so the problem must be subtle, but I still think this patch has a good chance of stopping it.  committed.;;;","23/Oct/10 20:49;hudson;Integrated in Cassandra #574 (See [https://hudson.apache.org/hudson/job/Cassandra/574/])
    move strategy creation into Table instantiation so it can't be out of sync
patch by jbellis; tested by Paul Cannon for CASSANDRA-1649
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reading an empty commit log throw an exception,CASSANDRA-2285,12500768,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,08/Mar/11 22:47,16/Apr/19 17:33,22/Mar/23 14:57,10/Mar/11 04:52,0.7.4,,,,0,,,,,,"Start a one node cluster, shutdown within 10 seconds but after the node is started and the location infos has been flushed. Restart node, you'll get a 'EOFException: unable to seek past the end of the file in read-only mode.'",,,,,,,,,,,,,,,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,,,,,"09/Mar/11 20:49;slebresne;0001-skip-CL-recover-when-fully-data-was-fully-flushed-wi.patch;https://issues.apache.org/jira/secure/attachment/12473123/0001-skip-CL-recover-when-fully-data-was-fully-flushed-wi.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20540,,,Wed Mar 09 20:52:47 UTC 2011,,,,,,,,,,"0|i0gagv:",93126,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"08/Mar/11 22:48;slebresne;Attached patch against 0.7;;;","09/Mar/11 00:11;jbellis;I can't reproduce, probably because Cassandra always does a few writes to system tables post-startup.  Can you make a failing test?;;;","09/Mar/11 00:22;slebresne;Yeah, when I say stop, that may involve killing the node mid-start.;;;","09/Mar/11 20:49;slebresne;
I had of wrong understanding of what was going on. The problem is that we can have a commit log header with a replay position greater than the size of the commit log.

This happens if some data gets flushed before it had time to hit the actual log (thus only in periodic mode). Which in turn can happen because we use a FileOutputStream for the header, which will get sync to disk even if cassandra dies/is killed shortly afterwards (unless this is a system failure).

It's fairly unlikely to happen in real use, but it is fairly easy to reproduce (see description).

Attaching a patch using the correct condition as well as a test unit.
;;;","10/Mar/11 04:52;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
system.test_thrift_server.TestMutations.test_batch_mutate_standard_columns appears to be non deterministic,CASSANDRA-944,12460991,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,mdennis,mdennis,02/Apr/10 11:02,16/Apr/19 17:33,22/Mar/23 14:57,07/Apr/10 05:33,0.7 beta 1,,Legacy/Tools,,0,,,,,,"system.test_thrift_server.TestMutations.test_batch_mutate_standard_columns appears to be non deterministic.  The first time I ran the thrift tests after a clean checkout it failed.  However, it did not fail the ~10 times after that.

{code}
mdennis@mdennis:~/c/cassandra$ nosetests test/system/test_thrift_server.py 
...........E....................................
======================================================================
ERROR: system.test_thrift_server.TestMutations.test_batch_mutate_standard_columns
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/lib/pymodules/python2.6/nose/case.py"", line 183, in runTest
    self.test(*self.arg)
  File ""/home/mdennis/c/cassandra/test/system/test_thrift_server.py"", line 318, in test_batch_mutate_standard_columns
    _assert_column('Keyspace1', column_family, key, 'c1', 'value1')
  File ""/home/mdennis/c/cassandra/test/system/test_thrift_server.py"", line 43, in _assert_column
    raise Exception('expected %s:%s:%s:%s:%s, but was not present' % (keyspace, column_family, key, column, value) )
Exception: expected Keyspace1:Standard1:key_27:c1:value1, but was not present

----------------------------------------------------------------------
Ran 48 tests in 184.700s

FAILED (errors=1)
{code}","xubuntu 9.10
Linux mdennis 2.6.31-20-generic #58-Ubuntu SMP Fri Mar 12 05:23:09 UTC 2010 i686 GNU/Linux
",johanoskarsson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/10 00:19;brandon.williams;944.patch;https://issues.apache.org/jira/secure/attachment/12440920/944.patch",,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19932,,,Wed Apr 07 12:41:54 UTC 2010,,,,,,,,,,"0|i0g21j:",91761,,,,,Normal,,,,,,,,,,,,,,,,,"02/Apr/10 19:22;gdusbabek;I bet you're running a mac.

If you increase the time.sleep(0.1) to a larger value, the problem will go away.  The problem is that the operation being tested is happening at ConsistencyLevel.ZERO and you are feeling the 'eventual' part of 'eventual consistency.'

I wonder if it would be a good idea to make the sleep value a constant and have it set to 0.15 if MacOS is detected.;;;","03/Apr/10 01:24;mdennis;Sorry, not on a mac, on Linux (see environment field for details )

I was thinking more like lowering the sleep, but putting it in a loop with a much higher max wait time (like an entire second).

Assuming the test is trying to verify that eventually the correct data shows up (and is not trying to verify the data shows up in under 0.1 seconds), then I believe something similar to the following would address the issue.

wait = 1s
start = now
verified = false
while not (verified = try_to_verify) and now < start+wait:
  sleep(0.05)

if not verified:
  fail_test


thoughts?
;;;","03/Apr/10 02:39;gdusbabek;That would work.  Maybe something like a assert_within(func, wait) function.  If func doesn't return true within wait, then the test fails.;;;","07/Apr/10 00:19;brandon.williams;Patch to add a loop as described and use it in conjunction with CL.ZERO;;;","07/Apr/10 05:33;jbellis;committed;;;","07/Apr/10 20:41;hudson;Integrated in Cassandra #400 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/400/])
    fix heisenbug in system tests, especially common on OS X.  patch by Brandon Williams; reviewed by jbellis for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AES makes Streaming unhappy,CASSANDRA-1169,12466310,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,gdusbabek,jbellis,jbellis,07/Jun/10 11:41,16/Apr/19 17:33,22/Mar/23 14:57,15/Jun/10 02:40,0.6.3,0.7 beta 1,,,0,,,,,,"Streaming service assumes there will only be one stream from S to T at a time for any nodes S and T.  For the original purpose of node movement, this was a reasonable assumption (any node T can only perform one move at a time) but AES throws off streaming tasks much more frequently than that given the right conditions, which will de-sync the fragile file ordering that Streaming assumes (that T knows which files S is going to send, in what order).  Eventually T is expecting file F1 but S sends a smaller file F2, leading to an infinite loop on T while it waits for F1 to finish, and T waits for S to acknowledge F2, which it never will.

For 0.6 maybe the best solution is for AES to manually wait for one of its streaming tasks to finish, before it allows itself to create another.  For 0.7 it would be nice to make Streaming more robust.  The whole 4-stage-ack process seems very fragile, and poking around in parent objects via inetaddress keys makes reasoning about small pieces impossible b/c of encapsulation violations.",,appodictic,gdusbabek,johanoskarsson,schubertzhang,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jun/10 01:55;gdusbabek;1169-2.txt;https://issues.apache.org/jira/secure/attachment/12447046/1169-2.txt","11/Jun/10 22:33;gdusbabek;1169.txt;https://issues.apache.org/jira/secure/attachment/12446866/1169.txt","09/Jun/10 22:32;appodictic;aes.txt;https://issues.apache.org/jira/secure/attachment/12446693/aes.txt",,,,,,,,,,,,3.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20014,,,Mon Jun 14 18:40:07 UTC 2010,,,,,,,,,,"0|i0g3f3:",91984,,,,,Critical,,,,,,,,,,,,,,,,,"07/Jun/10 20:04;gdusbabek;Is this the root cause of the problems being experienced at Digg and by Lu Ming on the ML?;;;","07/Jun/10 22:17;jbellis;I think so.  That's what prompted this ticket.;;;","08/Jun/10 00:53;lenn0x;As mentioned on IRC, we actually saw T sending to S, while anticompaction was running on S using the exact filename as T. We should probably break streaming out to support `stream/<source>/<sstables>`. ;;;","09/Jun/10 22:19;appodictic;I have this problem as well. I have a 5 node cluster. A simple repair on keyspace1 (which has nothing but some test data) streams never complete.;;;","09/Jun/10 22:32;appodictic;I have upgraded to 6.2 because 6.1 streaming would randomly timeout on me. Now, I am still having issues with move, join, repair. Since I was having so many streaming problems I tuned this up in some logs. Over the past few weeks I have spent a lot of time managing my clusters, I try to do these type of operations in the AM so they are less performance impacting, but I have a very low sucess rate with any move,join,repair. I have a building list of nodes to join and ring management that I keep having to put off due to failures. So anything to make these processes less brittle would be a big big deal. Attached is ooutput.
 ;;;","11/Jun/10 22:33;gdusbabek;Ensures that AES streaming happens on the streaming stage and waits for each transfer to complete.;;;","11/Jun/10 23:56;jbellis;you don't need to do that exception dance with futures, it will throw an exception that happened in the background as a wrapped ExecutionException on get (and all our executors are DebuggableTPE, which makes sure the exception gets logged even if get() is never called)

LGTM otherwise;;;","12/Jun/10 16:21;albert_e;StreamOutManager.waitForStreamCompletion() can't block the AES streaming thread if StreamOutManager.condition is signaled once and StreamOutManager has not been removed from streamManagers map. 

Make StreamOutManager.addFilesToStream() synchronized and block the thread if StreamOutManager.files.size() > 0 may be more efficient.;;;","12/Jun/10 16:29;xluke;I have applied your patch to cassandra.
According to my log on StreamOutManager.addFilesToStream() , the function is still called when StreamOutManager.files.size() > 0 
I think the problem is maybe not fixed yet.;;;","12/Jun/10 18:57;xluke;After I applied the above patch,
StreamOutManager.waitForStreamCompletion()  return immediately and StreamOut.transferSSTables do Not  wait for its streaming tasks to finish

27938- INFO [STREAM-STAGE:1] 2010-06-12 17:08:48,810 StreamOut.java (line 132) Sending a stream initiate message to /121.1.1.1...
27939: INFO [STREAM-STAGE:1] 2010-06-12 17:08:48,810 StreamOut.java (line 137) Waiting for transfer to /121.1.1.1 to complete
27940- INFO [STREAM-STAGE:1] 2010-06-12 17:08:48,810 StreamOut.java (line 141) Done with transfer to /121.1.1.1
27941- INFO [AE-SERVICE-STAGE:1] 2010-06-12 17:08:48,811 AntiEntropyService.java (line 641) Finished streaming repair to /121.1.1.1 for (GroupDataStore,Group)
..................................
27982- INFO [STREAM-STAGE:1] 2010-06-12 17:19:22,066 StreamOut.java (line 132) Sending a stream initiate message to /222.222.2.2 ...
27983: INFO [STREAM-STAGE:1] 2010-06-12 17:19:22,066 StreamOut.java (line 137) Waiting for transfer to /222.222.2.2 to complete
27984- INFO [STREAM-STAGE:1] 2010-06-12 17:19:22,066 StreamOut.java (line 141) Done with transfer to /222.222.2.2
27985- INFO [AE-SERVICE-STAGE:1] 2010-06-12 17:19:22,067 AntiEntropyService.java (line 641) Finished streaming repair to /222.222.2.2 for (GroupChat,Topic)
..................................;;;","15/Jun/10 01:55;gdusbabek;Instruct AES to remove active StreamManager when it's finished and reset the condition in SOM any time files are added so that it is a bit more reentrant.;;;","15/Jun/10 01:56;gdusbabek;albert_e: patch 2 adjusts addFilesToStream to reset the condition so that future waiters do wait.
Lu Ming: I believe you were experiencing that problem.;;;","15/Jun/10 02:04;jbellis;won't removing the active SOM bork things, if another stream to that target is going on?;;;","15/Jun/10 02:20;gdusbabek;SOM.remove() checks for that.  (I'm not saying the code is perfect--it's not--but I don't think removing the SOM is going to mess things up.);;;","15/Jun/10 02:20;stuhood;Rather than an 'endpoint->StreamManager' map, we really should have a 'session_id->StreamManager' map.;;;","15/Jun/10 02:26;gdusbabek;Stu: Right.  But I don't think we want to introduce it in 0.6.  I'm hoping just to get things to the point of working and then fix it all in 0.7. ;;;","15/Jun/10 02:34;jbellis;+1 on this patch and do-what-we-can-in-0.6-and-eviscerate-this-crap-in-0.7 in general;;;","15/Jun/10 02:40;gdusbabek;both patches are committed and merged.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
make jmx count/latency stuff actually useful,CASSANDRA-144,12424751,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,07/May/09 04:20,16/Apr/19 17:33,22/Mar/23 14:57,07/May/09 06:27,,,,,0,,,,,,"i just track those since ""the beginning of time"" it is not very useful for monitoring.  
(say things are fine for days, then something breaks -- you have to overcome the days of fine-ness in the avg before you can see anything wrong.)

option 1: reset the counters after each request for data.  problem: then i can't have multiple collectors (which i don't need atm but might be nice to have).
option 2: keep the last N data points and compute an average on demand, but that's more complicated than just doing a += for each op.

going to go with option 2.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/May/09 05:36;jbellis;144.patch;https://issues.apache.org/jira/secure/attachment/12407396/144.patch",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19565,,,Thu May 07 13:35:06 UTC 2009,,,,,,,,,,"0|i0fx53:",90967,,,,,Normal,,,,,,,,,,,,,,,,,"07/May/09 06:20;urandom;The patch as attached causes test failures, but as you pointed out the on IRC, the following fixes that.

--- a/src/java/org/apache/cassandra/utils/TimedStatsDeque.java
+++ b/src/java/org/apache/cassandra/utils/TimedStatsDeque.java
@@ -19,7 +19,7 @@ public class TimedStatsDeque extends AbstractStatsDeque
     private void purge()
     {
         long now = System.currentTimeMillis();
-        while (deque.peek().timestamp < now - period)
+        while (!deque.isEmpty() && deque.peek().timestamp < now - period)
         {
             deque.remove();
         }

+1;;;","07/May/09 06:27;jbellis;committed w/ above diff;;;","07/May/09 21:35;hudson;Integrated in Cassandra #63 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/63/])
    clean up more ad-hoc timing message and move to mbeans.  add TimedStatsDeque to
simplify tracking load stats over a one-minute window.
patch by jbellis; reviewed by Eric Evans for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ColumnFamilyStore masking IOException from FileUtils as IOError,CASSANDRA-1557,12475385,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,amorton,amorton,29/Sep/10 16:02,16/Apr/19 17:33,22/Mar/23 14:57,08/Oct/10 04:59,0.7 beta 3,,Legacy/CQL,,0,,,,,,"The code in ColumnFamilyStore.snapshot() line 1368 is catching an IOException from the call to FileUtils.createHardLink() and wrapping it in an IOError. However the code in TruncateVerbHandler:56 is looking for the IOException. This can result  in the client not getting a response to a truncate() API call. 

When running on a machine with very low memory I attempted to truncate a CF with few rows, the following error occurred in the logs.

ERROR [MUTATION_STAGE:25] 2010-09-29 16:44:39,341 AbstractCassandraDaemon.java (line 88) Fatal exception in thread Thread[MUTATION_STAGE:25,5,main]
java.io.IOError: java.io.IOException: Cannot run program ""ln"": java.io.IOException: error=12, Cannot allocate memory
        at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1368)
        at org.apache.cassandra.db.ColumnFamilyStore.truncate(ColumnFamilyStore.java:1511)
        at org.apache.cassandra.db.Table.truncate(Table.java:633)
        at org.apache.cassandra.db.TruncateVerbHandler.doVerb(TruncateVerbHandler.java:54)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at javautil.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.IOException: Cannot run program ""ln"": java.io.IOException: error=12, Cannot allocate memory
        at java.lang.ProcessBuilder.start(ProcessBuilder.java:460)
        at org.apache.cassandra.io.util.FileUtils.createHardLinkWithExec(FileUtils.java:263)
        at org.apache.cassandra.io.util.FileUtils.createHardLink(FileUtils.java:229)
        at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1360)
        ... 7 more
Caused by: java.io.IOException: java.io.IOException: error=12, Cannot allocate memory
        at java.lang.UNIXProcess.<init>(UNIXProcess.java:148)
        at java.lang.ProcessImpl.start(ProcessImpl.java:65)
        at java.lang.ProcessBuilder.start(ProcessBuilder.java:453)
        ... 10 more

On the client I got this:

  File ""/tech/home//git_home/trojan/trojan/cassandra/Cassandrapy"", line 846, in truncate
    self.recv_truncate()
  File ""/tech/home//git_home/trojan/trojan/cassandra/Cassandra.py"", line 857, in recv_truncate
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
  File ""/tech/home//git_home/trojan/trojan/thrift/protocol/TBinaryProtocol.py"", line 126, in readMessageBegin
    sz = self.readI32()
<snip>
    chunk = self.read(sz-have)
  File ""/tech/home//git_home/trojan/trojan/thrift/transport/TSocket.py"", line 92, in read
    buff = self.handle.recv(sz)
timeout: timed out",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Oct/10 00:38;jbellis;1557.txt;https://issues.apache.org/jira/secure/attachment/12456133/1557.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20199,,,Thu Oct 07 20:59:42 UTC 2010,,,,,,,,,,"0|i0g5yn:",92396,,amorton,,amorton,Low,,,,,,,,,,,,,,,,,"02/Oct/10 00:38;jbellis;patch allows snapshot IOException to propagate up to TruncateVerbHandler;;;","08/Oct/10 04:59;amorton;Looks good to me.

Tested by throwing an IOException from CFS.snapshot(). TruncateVerbHandler caught it and logged the error, the client returned immediately without an error being raised. 

One small thing I noticed was that ColumnFamilyStore.truncate() wraps all exceptions from forceBlockingFlush() in an IOException..

        try
        {
            forceBlockingFlush();
        }
        catch (Exception e)
        {
            throw new IOException(e);
        }

but all other functions that call forceBlockingFlush (e..g snapshot or addIndex) catch the two exceptions it throws and wrap them differently
            try
            {
                forceBlockingFlush();
            }
            catch (ExecutionException e)
            {
                throw new RuntimeException(e);
            }
            catch (InterruptedException e)
            {
                throw new AssertionError(e);
            }
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ant release target doesn't include all jars in binary tarball,CASSANDRA-850,12458223,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,urandom,johanoskarsson,johanoskarsson,05/Mar/10 18:18,16/Apr/19 17:33,22/Mar/23 14:57,27/Mar/10 01:53,0.6,,,,0,,,,,,"The ant release target doesn't create a complete tarball, the jars in build/lib/jars are not included.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Mar/10 23:53;urandom;ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-850-runtime-dependencies-formerly-handled-by.txt;https://issues.apache.org/jira/secure/attachment/12439786/ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-850-runtime-dependencies-formerly-handled-by.txt","25/Mar/10 23:53;urandom;ASF.LICENSE.NOT.GRANTED--v2-0002-licensing-and-attribution-for-newly-added-jars.txt;https://issues.apache.org/jira/secure/attachment/12439787/ASF.LICENSE.NOT.GRANTED--v2-0002-licensing-and-attribution-for-newly-added-jars.txt","25/Mar/10 23:53;urandom;ASF.LICENSE.NOT.GRANTED--v2-0003-remove-default-runtime-ivy-config.txt;https://issues.apache.org/jira/secure/attachment/12439788/ASF.LICENSE.NOT.GRANTED--v2-0003-remove-default-runtime-ivy-config.txt","25/Mar/10 23:53;urandom;ASF.LICENSE.NOT.GRANTED--v2-0004-remove-build-lib-jars-from-runtime-search-paths.txt;https://issues.apache.org/jira/secure/attachment/12439789/ASF.LICENSE.NOT.GRANTED--v2-0004-remove-build-lib-jars-from-runtime-search-paths.txt","25/Mar/10 23:53;urandom;ASF.LICENSE.NOT.GRANTED--v2-0005-do-not-ship-build.xml-and-ivy.xml-in-binary-dist.txt;https://issues.apache.org/jira/secure/attachment/12439790/ASF.LICENSE.NOT.GRANTED--v2-0005-do-not-ship-build.xml-and-ivy.xml-in-binary-dist.txt","25/Mar/10 23:53;urandom;ASF.LICENSE.NOT.GRANTED--v2-0006-update-release-notes-and-readme-for-ivy-rollback.txt;https://issues.apache.org/jira/secure/attachment/12439791/ASF.LICENSE.NOT.GRANTED--v2-0006-update-release-notes-and-readme-for-ivy-rollback.txt","05/Mar/10 18:21;johanoskarsson;CASSANDRA-850.patch;https://issues.apache.org/jira/secure/attachment/12437991/CASSANDRA-850.patch","26/Mar/10 06:19;urandom;v2-0007-remove-gratuitous-copy-of-junit-from-lib.patch;https://issues.apache.org/jira/secure/attachment/12439832/v2-0007-remove-gratuitous-copy-of-junit-from-lib.patch","26/Mar/10 06:19;urandom;v2-0008-reference-lib-licenses-from-LICENSE.txt.patch;https://issues.apache.org/jira/secure/attachment/12439833/v2-0008-reference-lib-licenses-from-LICENSE.txt.patch",,,,,,9.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19889,,,Mon Mar 29 15:42:30 UTC 2010,,,,,,,,,,"0|i0g1gn:",91667,,,,,Normal,,,,,,,,,,,,,,,,,"05/Mar/10 18:21;johanoskarsson;This patch makes the release target include all the jars;;;","05/Mar/10 21:25;jbellis;My understanding is that this is deliberate, or we'd have to go back to maintaining NOTICE for all those.;;;","05/Mar/10 21:52;johanoskarsson;Isn't the point of a binary release tarball that it should work with minimal effort out of the box? 
Surely the overhead of maintaining the notice file is a tiny bit of extra effort for a small group of developers and of a major benefit to a large group of users? I would agree that it might not be needed in the source version though.;;;","06/Mar/10 01:09;urandom;> Isn't the point of a binary release tarball that it should work with minimal effort out of the box?

The added effort basically boils down to invoking `ant ivy-retrieve' no?

> Surely the overhead of maintaining the notice file is a tiny bit of extra effort for a small group of developers and of a major benefit to a large group of users?

I think ""tedious"" is a better adjective than ""tiny"". Also, despite the very best of intentions, it wasn't being kept up properly (which is a pretty common outcome of tedious manual tasks).

We traded:

* manual dependency management
* the requirement to document license and attribution
* license incompatibility issues (caused by redistribution)

For:

* requiring ant to be installed (which doesn't seem to be too onerous)
* requiring network connectivity
* invoking `ant ivy-retrieve'

I'm not opposed to returning to the past practice of embedding all of the jars, especially if someone is stepping up to do a better job of maintaining this, but I think the changeset needs to revert Ivy, and include the jars and the necessary changes to NOTICE and LICENSE .
;;;","11/Mar/10 00:21;jbellis;Johan mentioned in IRC that http://ant.apache.org/ivy/history/latest-milestone/ivyfile/license.html could allow auto-generating LICENSE and NOTICE.

This is metadata from the maven repo right?  Not something we would maintain ourselves in ivy.xml?;;;","11/Mar/10 00:28;jbellis;For the record, the official description of NOTICE is as follows (from http://www.apache.org/legal/src-headers.html#notice):

   0.  Every Apache distribution should include a NOTICE file in the top directory, along with the standard LICENSE file.
   1. The top of each NOTICE file should include the following text, suitably modified to reflect the product name and year(s) of distribution of the current and past versions of the product:

                Apache [PRODUCT_NAME]
                Copyright [yyyy] The Apache Software Foundation

                This product includes software developed at
                The Apache Software Foundation (http://www.apache.org/).
        

   2. The remainder of the NOTICE file is to be used for required third-party notices. The NOTICE file may also include copyright notices moved from source files submitted to the ASF.

The only official description of LICENSE I found is http://apache.org/dev/apply-license.html#new, which seems to mean that LICENSE should always be the ASL 2.

I'm not sure where the practice of ""include a copy of each dependency's license in LICENSE"" comes from; I couldn't find it documented anywhere.;;;","11/Mar/10 01:01;urandom;> I'm not sure where the practice of ""include a copy of each dependency's license in LICENSE"" comes from; I couldn't find it documented anywhere.

There is obviously a hard requirement to include licensing information for everything with a license, but the requirement to put it all in LICENSE came from individuals on general@incubator and wasn't universally agreed upon.

Some folks thought it was enough to include it, others thought that it should at least be discoverable via LICENSE, and others still were adamant that they be entirely contained with LICENSE. We went with the latter primarily to avoid controversy (we needed release votes).;;;","23/Mar/10 23:26;gdusbabek;Have we reached consensus on this?  If not, I want to offer my opinion.

We started using ivy for several reasons, but the big problem I thought it was solving was avoiding license management.  If we end up having to update LICENSE.txt/NOTICE.txt manually in order to ship a full binary, I prefer to go back to keeping the dependencies in svn.  Let's rid of ivy.;;;","23/Mar/10 23:30;jbellis;+1 getting rid of ivy if we're manually updating license files.;;;","24/Mar/10 01:47;urandom;I suppose we could keep it around for managing build dependencies, stuff that we're unwilling or unable to check into subversion, (rat, cobertura, and maybe parts of antlr). I don't know if that's enough justification; just throwing out the option.;;;","25/Mar/10 07:50;urandom;0001: This patch places a copy of everything that used to be downloaded by the ivy ""default"" conf (i.e. `ant ivy-retrieve'), into lib/ (it probably requires git to apply this patch).

0002: Adds license and attribution info for the new jars, and moves the existing third-party license info from LICENSE to lib/licenses.

0003: Cleans out everything from the ivy ""default"" conf that has since been moved to lib/, and merges the ""build"" and ""qa"" confs to simplify things. Also makes the necessary changes to build.xml so that Ivy is only used to manage build dependencies.

0004: Adjusts jar file search path in scripts and batch files.

0005: Fixes the release target to no longer include build.xml and ivy.xml in the binary dist.

0006: Updates the release notes and readme files.

It would be great to have another set of eyes on this, I can't hardly imagine *not* missing something in all of these changes.;;;","25/Mar/10 11:49;jbellis;I think the commons-logging and httpclient ivy lines are just there for contrib/word_count (i was lazy).  We could totally just r/m those entirely (and any dependencies those had, in turn?) and let people who want to run the contrib example get them manually or w/ ivy.;;;","25/Mar/10 23:56;urandom;commons-{logging,httpclient} have been removed from the v2 patches.;;;","26/Mar/10 05:11;johanoskarsson;+1, looks good to me. 

Couple of thoughts:
* Is there a reason to keep the junit 3.8.1 jar in lib and also use ivy to fetch junit 4.6?
* Due to my lack of legal expertise I will assume the bits in the notice file are sufficient.
* I remember someone voiced the opinion earlier that licenses should be pulled into a single file in the root, can't remember which of license/notice it was. Personally I think that this approach with one license file per jar approach is much more user friendly and surely it must be as valid from a legal standpoint?;;;","26/Mar/10 05:31;jbellis;1. all (?) our junit tests require junit 4, so that should be an easy call. :)
3. that was someone on incubator-general pissing on our release because it wasn't The Way He Liked, there was no legal reason that per-file was invalid brought up in that discussion;;;","26/Mar/10 06:11;urandom;> Is there a reason to keep the junit 3.8.1 jar in lib and also use ivy to fetch junit 4.6? 

We were explicitly pulling in junit 4.6 to use for our unit tests, and implicitly pulling in junit 3.8.1 as a dependency for at least one of those other jars in lib/ (Ivy would resolve that to 4.6 only). So that explains why it is the way it is, though I do suspect that the jar(s) that declared a dependency on junit did so in error, (or we simply don't exercise those parts of the lib(s)). 

The unit tests, system tests and cassandra-cli (one of those aforementioned jars is jline, used by the cli) all seem to work after removing lib/junit-3.8.1.jar , so it's probably safe to leave it out.

> Due to my lack of legal expertise I will assume the bits in the notice file are sufficient. 

I'm not sure it would satisfy everyone on the incubator list, but it is correct with respect to the legal requirements, at least according to my understanding of the respective licenses (actually, it errs on the side of caution wherever I wasn't 100% sure).

> I remember someone voiced the opinion earlier that licenses should be pulled into a single file in the root

That was something else that came up on the incubator list, and TTBOMK there wasn't consensus for this either. I can find no such requirement in my reading of the actual licenses. I can however append a sentence or two to LICENSE that refers to lib/license for third-party libs, (I think that would satisfy that camp).;;;","26/Mar/10 06:20;urandom;0007: removes junit 3.8.1 and the corresponding license file

0008: adds a reference to lib/licenses from LICENSE.txt;;;","27/Mar/10 06:41;mike.javorski;Sorry.. I'm a bit late to the party (and did so because of the 6MB delta download git just pulled).  Wouldn't managing the ivy.xml and the NOTICES file in concert meet the ASF legal requirements? You could run ivy-retrieve as part of the dist process and eliminate the requirement for ant and ivy for the end user, and still get the benefits on the development side.

On a related note (and I realize this doesn't effect the official SVN users),  git users will get a nice jump in repo size every time there is a lib change with this route. Shouldn't be a regular occurrence, but if it's easily avoided, why not make it ""nicer""

I am happy to take a crack at a patch to this effect if there is interest.

[Edit:] looks like the first patch of this issue would have done that, so I guess no patch needed from me;;;","29/Mar/10 23:42;urandom;Ivy dependency retrieval is a dynamic process, I really don't see how you could make any guarantees that an ivy.xml and NOTICE/LICENSE pair maintained in lock-step would remain in syn.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hinted HandOff doesn't work for SuperColumnFamilies,CASSANDRA-491,12438149,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,sammy.yu,sammy.yu,15/Oct/09 09:47,16/Apr/19 17:33,22/Mar/23 14:57,21/Oct/09 01:45,0.5,,,,0,,,,,,"I was combing through our logs and noticed the following error which seems to indicate that hinted handoffs doesn't work for Super Column families.

ERROR [HINTED-HANDOFF-POOL:25] 2009-10-14 00:11:58,723 DebuggableThreadPoolExecutor.java (line 127) Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.UnsupportedOperationException: This operation is not supported for Super Columns.
        at org.apache.cassandra.db.HintedHandOffManager$1.run(HintedHandOffManager.java:250)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.UnsupportedOperationException: This operation is not supported for Super Columns.
        at org.apache.cassandra.db.SuperColumn.timestamp(SuperColumn.java:136)
        at org.apache.cassandra.db.HintedHandOffManager.deliverAllHints(HintedHandOffManager.java:180)
        at org.apache.cassandra.db.HintedHandOffManager.access$000(HintedHandOffManager.java:52)
        at org.apache.cassandra.db.HintedHandOffManager$1.run(HintedHandOffManager.java:246)
        ... 3 more
ERROR [HINTED-HANDOFF-POOL:25] 2009-10-14 00:11:58,723 CassandraDaemon.java (line 71) Fatal exception in thread Thread[HINTED-HANDOFF-POOL:25,5,main]
java.lang.RuntimeException: java.lang.UnsupportedOperationException: This operation is not supported for Super Columns.
        at org.apache.cassandra.db.HintedHandOffManager$1.run(HintedHandOffManager.java:250)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.UnsupportedOperationException: This operation is not supported for Super Columns.
        at org.apache.cassandra.db.SuperColumn.timestamp(SuperColumn.java:136)
        at org.apache.cassandra.db.HintedHandOffManager.deliverAllHints(HintedHandOffManager.java:180)
        at org.apache.cassandra.db.HintedHandOffManager.access$000(HintedHandOffManager.java:52)
        at org.apache.cassandra.db.HintedHandOffManager$1.run(HintedHandOffManager.java:246)
        ... 3 more",,hammer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Oct/09 01:02;jbellis;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-491-fix-HH-of-tombstones.txt;https://issues.apache.org/jira/secure/attachment/12422372/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-491-fix-HH-of-tombstones.txt","17/Oct/09 01:02;jbellis;ASF.LICENSE.NOT.GRANTED--0002-Avoid-using-non-existent-supercolumn-timestamp-in-dele.txt;https://issues.apache.org/jira/secure/attachment/12422373/ASF.LICENSE.NOT.GRANTED--0002-Avoid-using-non-existent-supercolumn-timestamp-in-dele.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19718,,,Wed Oct 21 12:34:22 UTC 2009,,,,,,,,,,"0|i0fz9b:",91310,,,,,Normal,,,,,,,,,,,,,,,,,"17/Oct/09 01:03;jbellis;02
    Avoid using non-existent supercolumn timestamp in delete op.  Better conceptual integrity to delete w/ correct system time anyway

01
    fix HH of tombstones

this should fix the problem w/o having to manually clear out old data.;;;","20/Oct/09 23:27;junrao;So the only way to cleanup hinted data is to manually run cleanup compaction from nodeprode? We need to document that in the header of HHM.

Other than that, the patch looks good to me.;;;","20/Oct/09 23:43;jbellis;patch includes this in HHM:

 * HHM never deletes the row from Application tables; there is no way to distinguish that
 * from hinted tombstones!  instead, rely on cleanup compactions to remove data
 * that doesn't belong on this node.

How would you suggest clarifying that?;;;","21/Oct/09 01:33;junrao;Add this ""cleanup compactions have to be manually started through nodeprobe on each node"".;;;","21/Oct/09 01:45;jbellis;done, and committed;;;","21/Oct/09 20:34;hudson;Integrated in Cassandra #234 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/234/])
    Avoid using non-existent supercolumn timestamp in delete op.  Better conceptual integrity to delete w/ correct system time anyway.
patch by jbellis; reviewed by junrao for 
fix HH of tombstones.  patch by jbellis; reviewed by junrao for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTable cleanup killed by IllegalStateException,CASSANDRA-1458,12473181,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,cgist,cgist,03/Sep/10 03:51,16/Apr/19 17:33,22/Mar/23 14:57,08/Sep/10 02:33,0.6.6,0.7 beta 2,,,0,,,,,,"Compacted SSTables were not being deleted even after a forced GC. The following stack traces were observed:

ERROR [SSTABLE-CLEANUP-TIMER] 2010-09-01 15:54:07,254 CassandraDaemon.java (line 85) Uncaught exception in thread Thread[SSTABLE-CLEANUP-TIMER,5,main]
java.lang.IllegalStateException: Task already scheduled or cancelled
        at java.util.Timer.sched(Timer.java:380)
        at java.util.Timer.schedule(Timer.java:192)
        at org.apache.cassandra.io.sstable.SSTableDeletingReference$CleanupTask.run(SSTableDeletingReference.java:86)
        at java.util.TimerThread.mainLoop(Timer.java:534)
        at java.util.TimerThread.run(Timer.java:484)

ERROR [SSTABLE-DELETER] 2010-09-01 16:20:22,587 CassandraDaemon.java (line 85) Uncaught exception in thread Thread[SSTABLE-DELETER,5,main]
java.lang.IllegalStateException: Timer already cancelled.
        at java.util.Timer.sched(Timer.java:376)
        at java.util.Timer.schedule(Timer.java:192)
        at org.apache.cassandra.io.sstable.SSTableDeletingReference.cleanup(SSTableDeletingReference.java:70)
        at org.apache.cassandra.io.sstable.SSTableReader$1$1.run(SSTableReader.java:85)
        at java.lang.Thread.run(Thread.java:636)

If the SSTableDeletingReference$CleanupTask cannot delete a file, it reschedules itself for later. TimerTasks (which CleanupTask subclasses) are intended to be scheduled only once and will cause an IllegalStateException in the timer when it tries to schedule itself again. The exception causes timer to effectively cancel itself and the next attempt to schedule a task will cause an IllegalStateException in the SSTABLE-DELETER.

It appears this could be fixed by scheduling a new CleanupTask instead of the same one that failed (SSTableDeletingReference.java:86).","trunk from 2010-08-31
Linux 2.6.18-164.2.1.el5.plus #1 SMP x86_64
OpenJDK 64-Bit Server VM (build 1.6.0-b09)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Sep/10 22:41;jbellis;1458.txt;https://issues.apache.org/jira/secure/attachment/12454019/1458.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20151,,,Tue Sep 07 18:33:33 UTC 2010,,,,,,,,,,"0|i0g56n:",92270,,stuhood,,stuhood,Low,,,,,,,,,,,,,,,,,"03/Sep/10 06:27;jbellis;what was causing the original failure to delete?;;;","03/Sep/10 06:49;jbellis;I suspect retrying the delete is simply bogus -- if there is a race condition that would make the delete fail, we need to fix that; I can't think of another reason delete would succeed later after failing initially.

btw, are you running Windows?;;;","03/Sep/10 10:14;cgist;The cause of the failed delete is unknown. I was running Linux as in the updated environment.;;;","07/Sep/10 22:41;jbellis;patch attached;;;","07/Sep/10 22:44;jbellis;(patch is against 0.7; I will backport to 0.6 as well);;;","08/Sep/10 02:33;jbellis;+1'd by Stu in IRC. committed to 0.6 and trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BufferUnderflowExceptions,CASSANDRA-1513,12474434,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,wr0ngway,wr0ngway,18/Sep/10 03:48,16/Apr/19 17:33,22/Mar/23 14:57,28/Dec/10 23:49,,,,,0,,,,,,"Seeing a number of these in my log when running a trunk build from 9/11/2010
No idea how to duplicate it, hopefully you can make sense of it from the stack trace

ERROR [MUTATION_STAGE:19] 2010-09-14 02:24:50,704 DebuggableThreadPoolExecutor.
java (line 102) Error in ThreadPoolExecutorjava.nio.BufferUnderflowException
        at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:145)
        at java.nio.ByteBuffer.get(ByteBuffer.java:692)
        at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:62)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)

","Ubuntu 10.04.1, 1.6.0_18-b18",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20175,,,Tue Dec 28 15:49:45 UTC 2010,,,,,,,,,,"0|i0g5iv:",92325,,,,,Normal,,,,,,,,,,,,,,,,,"18/Sep/10 04:09;wr0ngway;Actually, I can get these pretty regularly (in my cluster anyway) - quite a few happen on some nodes when I restart a specific node
;;;","28/Dec/10 23:49;jbellis;believe this was one of our thrift 0.5 upgrade bugs.  should be fixed in latest 0.7 rc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bloom filters have much higher false-positive rate than expected,CASSANDRA-68,12422480,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,09/Apr/09 23:13,16/Apr/19 17:33,22/Mar/23 14:57,18/Apr/09 04:18,0.3,,,,0,,,,,,Gory details: http://spyced.blogspot.com/2009/01/all-you-ever-wanted-to-know-about.html,,kimtea,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Apr/09 05:54;jbellis;0001-r-m-unused-code-including-entire-CountingBloomFilte.patch;https://issues.apache.org/jira/secure/attachment/12405113/0001-r-m-unused-code-including-entire-CountingBloomFilte.patch","10/Apr/09 05:54;jbellis;0002-replace-JenkinsHash-w-MurmurHash.-its-hash-distrib.patch;https://issues.apache.org/jira/secure/attachment/12405114/0002-replace-JenkinsHash-w-MurmurHash.-its-hash-distrib.patch","10/Apr/09 05:55;jbellis;0003-rename-BloomFilter.fill-add.patch;https://issues.apache.org/jira/secure/attachment/12405115/0003-rename-BloomFilter.fill-add.patch","10/Apr/09 05:56;jbellis;0004-rewrite-bloom-filters-to-use-murmur-hash-and-combina.patch;https://issues.apache.org/jira/secure/attachment/12405116/0004-rewrite-bloom-filters-to-use-murmur-hash-and-combina.patch","11/Apr/09 20:38;jbellis;0004-v3.patch;https://issues.apache.org/jira/secure/attachment/12405230/0004-v3.patch","10/Apr/09 10:19;jbellis;0004a-tests.patch;https://issues.apache.org/jira/secure/attachment/12405132/0004a-tests.patch","10/Apr/09 10:19;jbellis;0004b-code.patch;https://issues.apache.org/jira/secure/attachment/12405133/0004b-code.patch","11/Apr/09 20:46;jbellis;0005-switch-back-to-old-hash-generation-code-to-demonstra.patch;https://issues.apache.org/jira/secure/attachment/12405232/0005-switch-back-to-old-hash-generation-code-to-demonstra.patch","11/Apr/09 07:13;sandeep_tata;fp_test_for_old_code.patch;https://issues.apache.org/jira/secure/attachment/12405205/fp_test_for_old_code.patch","11/Apr/09 07:37;sandeep_tata;fp_test_for_old_code_v2.patch;https://issues.apache.org/jira/secure/attachment/12405208/fp_test_for_old_code_v2.patch","11/Apr/09 20:49;jbellis;words.gz;https://issues.apache.org/jira/secure/attachment/12405233/words.gz",,,,11.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19533,,,Fri Apr 17 20:18:46 UTC 2009,,,,,,,,,,"0|i0fwof:",90892,,,,,Normal,,,,,,,,,,,,,,,,,"10/Apr/09 05:54;jbellis;1. r/m unused code, including entire CountingBloomFilter;;;","10/Apr/09 05:54;jbellis;2. replace JenkinsHash w/ MurmurHash.  its hash distribution is just as good, and it's faster;;;","10/Apr/09 05:55;jbellis;3. rename BloomFilter.fill -> add;;;","10/Apr/09 05:56;jbellis;4. rewrite bloom filters to use murmur hash and combinatorics to generate better hash values.  (again: see blog post if you want details of approaches that were tried and rejected.)  add test suite.;;;","10/Apr/09 09:51;sandeep_tata;Modify tests to catch FileNotFound for ""/usr/share/dict/words"" and not run those tests per conversation on irc.
;;;","10/Apr/09 10:24;jbellis;Made words test optional.

Also split 0004 changes into tests and code, but I don't think that's going to be too useful.  If you want to test the old hash functions the easiest thing is probably to modify Filter.getHashBuckets to use the old hash functions instead.

But I remember that you will see from 50% to 200% more FP than you should.  Sorry I don't have the code anymore.  (Lost in a git rebase, apparently.);;;","10/Apr/09 14:35;sandeep_tata;+1

This is a large patch, but the code is structured nicely so it shouldn't be too hard for others to take a look.

I have 2 suggestions for changes:

1. Can you add short high-level comments describing the design of the Filter, BloomFilter, CountingBloomFilter classes so others who want to take a look at the patch get the idea quickly ?

2. Since this is a somewhat large patch, I'd wait a couple of days to check if people have questions:
a) the new bloomfilter is any faster/slower than the old one
b) they have some tests comparing false-positive rates of this vs old one.




;;;","10/Apr/09 15:05;sandeep_tata;The regression to watch for is if the bloomfilters sizes are significantly different. I'm guessing not : Jonathan -- do you have any estimates?;;;","10/Apr/09 20:35;jbellis;I commented getHashBuckets which is the core of the change.  Those who are not familiar with bloom filters in general are referred to wikipedia. :)

Normal BloomFilter size is going to be the same as the old; both are based on a BitSet which is about as efficient as you can get.

CountingBloomFilter size is going to be half the size of the old since the old uses a full byte per bucket and this uses a half byte.  (If you reach a count of 15 your filter is way too small to be useful anyway; there is no reason to allow a count of 255.);;;","10/Apr/09 22:10;jbellis;To answer the question from IRC:

The new hash generator is increasingly faster than the old as more hashes are needed.  But the real win is in lower false positives, meaning the user has to do that much less of the far more expensive operations that it's trying to avoid by using a bloom filter in the first place.;;;","11/Apr/09 03:25;avinash.lakshman@gmail.com;What's the problem here again? Not sure I follow? What is the fundamental reason for any change here?;;;","11/Apr/09 03:36;avinash.lakshman@gmail.com;The reason I ask is the following:

(1) The Bloom Calculations table is straight out of some paper. I cannot quite recall now.
(2) THe Counting Bloom Filter is left there because I think it could still be used for certain purposes. So I wouldn't want to get rid of it too soon.
(3) We use 8 bits/element and 5 hash function to get the false positive rate as indicated by the paper in its table which is captured in the Bloom Calculations.

So how are we concluding which hash is faster based on what scientific evidence? I am not quite sure I follow the rest of the comments w.r.t. the justification of this patch.;;;","11/Apr/09 03:46;jbellis;Avinash, 

1. BC is not an issue.  The numbers there are fine.
2. Yes, patch 4 adds back a new CBF that uses half the memory (by using a half byte per bucket instead of a full byte).
3. the problem is the hash functions themselves.  to elaborate, the old code used a hardcoded list of hash functions which were NOT independent -- for 6 or less hash functions, these functions were only actually generating 2 distinct hash values, so the false positive rate was far far higher than would have been expected.  After extensive comparsions with Jenkins, Murmur, and SHA-based approaches, it now generates hashes using Murmur as a base and then combinatorics as described here: http://www.eecs.harvard.edu/~kirsch/pubs/bbbf/esa06.pdf

We're concluding it's faster based on the two weeks of testing I did. :)  But the real win is getting to the theoretically predicted false positive rate instead of much higher.

;;;","11/Apr/09 05:26;sandeep_tata;What we need now is some code so we can reproduce the false-positive rate comparison :-);;;","11/Apr/09 05:50;jbellis;Fine.  Here is a simple test that demonstrates the problem on the old code that you could have written in about a minute:

public class BloomFilterTest
{
    @Test
    public void testHashes() {
        Set<Integer> hashResults = new HashSet<Integer>();
        for (ISimpleHash hash : BloomFilter.hashLibrary_) {
            hashResults.add(hash.hash(""1""));
        }
        assert hashResults.size() == BloomFilter.hashLibrary_.size() : hashResults.size();
    }
}

This shows that the old hash functions only generate 5 unique values (from 11 functions).  A little inspection will show that only 2 of those are in the first six, as I mentioned above.

If that doesn't tell you ""oh yeah the false positive rate will be the roof"" then you need to read the wikipedia page on Bloom filters. :);;;","11/Apr/09 07:13;sandeep_tata;Patch to run false positive tests on the old code -- based completely on Jonathan's unit tests that test the new bloomfilters.;;;","11/Apr/09 07:37;sandeep_tata;If you want to compare the FP rates for before and after, first apply fp_test_for_old_code_v2.patch. The tests in BloomFilterTest print out the # false positives.

Roll back those patches and apply Jonathan's 1-4 to use the new bloomfilter. 

(v2 of patch uses 10000 elements to be directly comparable to Jonathan's patches, v1 changed this value to 100000);;;","11/Apr/09 20:46;jbellis;Attached 0004-v3, which cleans out printlns and adds the FP ratio to the assertionerror.

Attached 0005, which switches to the old hash generation code on top of the new Filter + tests framework.

Several small tests always fail because of hash collisions where there should not be any.  For our purposes these are noise except to confirm that the hash generation is broken.

False positives for testWords is the big red fail.  17% more FP than expected at MAX_FAILURE_RATE. of 0.1.  Increase MFP to force more hases to be used and this goes up:  22% at 0.01, and 66% at 0.001.  It would keep going up but we're maxing out the precalculated probabilities in BloomCalculations here.  (A FP of 0.001 is _not_ all that low for a bloom filter, btw, but for what we are using it for so far it is probably adequate.)

Note also that testManyRandom fails because the old code is limited to 11 hashes.  The new code can generate as many as needed.

;;;","11/Apr/09 20:49;jbellis;here is the /usr/share/dict//words I'm using (gzipped).  it's the one from ubuntu 8.10 wamerican package.
;;;","12/Apr/09 07:30;sandeep_tata;I ran some more tests, here's what I found:

Old code:
Test                                             MAX_FP Actual FP
FalsePositivesInt/Random     0.1            0.142
FalsePositivesInt/Random     0.01         (pass)
FalsePositivesInt/Random     0.001       (pass)
Words                                         0.1            0.15
Words                                         0.01          (pass)
Words                                         0.001        0.0013

New code:

FalsePositivesInt/Random     0.1           (pass)
FalsePositivesInt/Random     0.01         (pass)
FalsePositivesInt/Random     0.001       (pass)
Words                                         0.1            (pass)
Words                                         0.01          (pass)
Words                                         0.001        (pass)

The old bloomfilter certainly reports up to 50% more than expected false positive rates for some cases. The new bloomfilter is more predictable, it always passes.

On my machine, some quick-n-crude tests show that the new bloom-filter is about 4x slower. (I tested at FP rate = 0.01) . When you take into account the fact that the penalty for a false positive at least 3 orders of magnitude more expensive than the actual hash calculation (an FP usually means you'll end up hitting disk unnecessarily), it makes sense to use it even when you set the FP rate to 0.001. It is even more useful at higher rates.;;;","13/Apr/09 23:15;jbellis;Prashant wrote on the mailing list:

""The results are a bit counter intuitive here I would have expected it to be faster with the same FP rate but   I am not sure why it is slower if you are just using a couple of hash functions and using double hashing...  I am sorry I haven't looked at the test code but have you tried it with large strings as keys ? e.g 128 byte keys , also with Longs.""

I replied:

""Murmur is a higher-quality hash and takes more operations to achieve its better key distribution.  But since the new implementation always uses two calls to Murmur no matter how many hashes are needed it is virtually constant time.  The random strings generated are 128 bytes."";;;","13/Apr/09 23:16;jbellis;Any further questions / comments / objections before we commit?;;;","16/Apr/09 04:23;jbellis;One other point that is probably obvious: this change is incompatible with old serialized bloomfilters since the hash values for a given key necessarily change.;;;","18/Apr/09 04:18;jbellis;committed.  (33 breaks binary compatibility anyway, so might as well throw this one in too.);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
a decommissioned node (ip) joining ring again,CASSANDRA-543,12440490,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,jaakko,jaakko,12/Nov/09 21:09,16/Apr/19 17:33,22/Mar/23 14:57,13/Nov/09 04:07,0.5,,,,0,,,,,,"Don't know the exact cause yet (will continue to test tomorrow), but seems there's something wrong with maintaining tokenMetadata under certain circumnstances:

(1) 8-node cluster all nodes have some data
(2) decommission one of them
(3) stop the decommissioned node, clear its data, restart node

When the 3rd step is repeated, I usually get the following exception on the second or third restart on other nodes in the cluster

java.lang.IllegalArgumentException: value already present: /192.168.0.108
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:116)
	at com.google.common.collect.StandardBiMap.putInBothMaps(StandardBiMap.java:106)
	at com.google.common.collect.StandardBiMap.put(StandardBiMap.java:91)
	at com.google.common.collect.HashBiMap.put(HashBiMap.java:82)
	at org.apache.cassandra.locator.TokenMetadata.update(TokenMetadata.java:91)
	at org.apache.cassandra.service.StorageService.updateForeignToken(StorageService.java:188)
	at org.apache.cassandra.service.StorageService.onChange(StorageService.java:444)
	at org.apache.cassandra.service.StorageService.onJoin(StorageService.java:536)
	at org.apache.cassandra.gms.Gossiper.handleNewJoin(Gossiper.java:558)
	at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:591)
	at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(Gossiper.java:972)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)

The exact sequence leading to this exception might not have anything to do with decommission and/or restarts, but no time to test more today.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19746,,,Thu Nov 12 20:07:41 UTC 2009,,,,,,,,,,"0|i0fzkv:",91362,,,,,Normal,,,,,,,,,,,,,,,,,"13/Nov/09 04:07;jbellis;yeah, this is the same as the bug in CASSANDRA-541;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gossip should handle 'dead' states,CASSANDRA-2496,12504582,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,19/Apr/11 02:57,16/Apr/19 17:33,22/Mar/23 14:57,26/Jul/11 02:58,0.8.3,,,,1,,,,,,"For background, see CASSANDRA-2371",,jeromatron,mauzhang,scode,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-957,,,"01/Jun/11 09:39;brandon.williams;0001-Rework-token-removal-process.txt;https://issues.apache.org/jira/secure/attachment/12481028/0001-Rework-token-removal-process.txt","01/Jun/11 08:28;brandon.williams;0002-add-2115-back.txt;https://issues.apache.org/jira/secure/attachment/12481019/0002-add-2115-back.txt","15/Jul/11 04:54;thepaul;0003-update-gossip-related-comments.patch.txt;https://issues.apache.org/jira/secure/attachment/12486503/0003-update-gossip-related-comments.patch.txt","15/Jul/11 04:56;thepaul;0004-do-REMOVING_TOKEN-REMOVED_TOKEN.patch.txt;https://issues.apache.org/jira/secure/attachment/12486504/0004-do-REMOVING_TOKEN-REMOVED_TOKEN.patch.txt","15/Jul/11 04:57;thepaul;0005-drain-self-if-removetoken-d-elsewhere.patch.txt;https://issues.apache.org/jira/secure/attachment/12486505/0005-drain-self-if-removetoken-d-elsewhere.patch.txt","22/Jul/11 04:09;thepaul;0006-acknowledge-unexpected-repl-fins.patch.txt;https://issues.apache.org/jira/secure/attachment/12487346/0006-acknowledge-unexpected-repl-fins.patch.txt","23/Jul/11 08:20;brandon.williams;0007-Always-update-epstate-timestamps-when-the-node-is-al.patch;https://issues.apache.org/jira/secure/attachment/12487575/0007-Always-update-epstate-timestamps-when-the-node-is-al.patch","23/Jul/11 08:20;brandon.williams;0008-only-handleStateRemoving-if-the-node-is-a-member.patch;https://issues.apache.org/jira/secure/attachment/12487576/0008-only-handleStateRemoving-if-the-node-is-a-member.patch",,,,,,,8.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20655,,,Mon Jul 25 19:43:56 UTC 2011,,,,,,,,,,"0|i0anvr:",60142,,thepaul,,thepaul,Normal,,,,,,,,,,,,,,,,,"01/Jun/11 08:08;brandon.williams;The first patch allows gossip to track dead states and completely changes how removetoken works, since it is the problem with keeping dead state around, and currently broken in many scenarios.  The second patch adds the previously reverted CASSANDRA-2115 back now that removetoken is more resilient.;;;","02/Jun/11 00:02;brandon.williams;Some explanation of what changed and why it was necessary:

Consider nodes A through D. D is partitioned, and C is dead and needs to be removed. A removetoken will be issued to A for this.

In the current way we do things, A will modify it's own state by appending information to its status indicating that it will be removing C. B will see this, re-replicate as needed, then report to A that is is done. The problem however, is that since A modified its own state, A is also free to wipe that state out, either by restarting, or simple remove another token, because there's only space for one. If A reboots and then D's partition heals, D will never know C was removed. Worse, it will still have state for C that neither A nor B do, and so it will repopulate the ring with C again.

This patch changes this by instead having A sleep for RING_DELAY to make sure the generation for C is stable, and then it modifies C's state to indicate it is being removed, just as if C itself had done this. It also appends some extra state to indicate that A will be the removal coordinator. The others nodes see this, re-replicate and report back to A, which then modifies C's state once more to indicate it is completely removed. At this point, it doesn't matter if A dies completely and D's partition heals, since the state is stored in C's gossip information. If A reboots, it will be able to get the correct state information from B, or any other node.

If A fails while the other nodes are re-replicating, a new removetoken can be started elsewhere, or in the case of other replicas being down preventing removetoken from completing, a removetoken force will remove the node and then repair can be run to restore the replica count.;;;","15/Jul/11 01:40;brandon.williams;I see two more things to be done with this patch.  First, when re-replicating nodes report back to the removal coordinator, if the coordinator has restarted it won't understand them, and they will infinitely loop retrying the confirmation.  Second, since we're holding dead states, we need to make sure that bootstrapping/moving nodes can take over these dead tokens.;;;","15/Jul/11 04:54;thepaul;These small patches build on the others.

0003-update-gossip-related-comments.patch.txt: updates gossip-related comments derp derp.;;;","15/Jul/11 04:56;thepaul;0004-do-REMOVING_TOKEN-REMOVED_TOKEN.patch.txt: use REMOVED_TOKEN instead of STATUS_LEFT (would probably be ok either way, but otherwise, the REMOVED_TOKEN state would not be used). Seems this is more the way it was intended.;;;","15/Jul/11 04:57;thepaul;0005-drain-self-if-removetoken-d-elsewhere.patch.txt : when node X was partitioned and removetoken'd but then it shows up again, it should shut itself down, rather than becoming a zombie;;;","15/Jul/11 04:58;thepaul;I'll see what I can do to test the ""infinitely loop retrying the confirmation"" and ""bootstrapping/moving nodes can take over these dead tokens"" situations.;;;","21/Jul/11 07:48;thepaul;Ok, nodes do indeed infinitely retry the replication confirmation in some cases, but it appears it's not just when the former removal coordinator has restarted in the interim- it seems to be when the removetoken is reissued to another, new removal coordinator. In this case, I get this traceback every 10 seconds:

{noformat}
ERROR [MiscStage:9] 2011-07-20 23:42:06,599 AbstractCassandraDaemon.java (line 113) Fatal exception in thread Thread[MiscStage:9,5,main]
java.lang.AssertionError
        at org.apache.cassandra.service.StorageService.confirmReplication(StorageService.java:2088)
        at org.apache.cassandra.streaming.ReplicationFinishedVerbHandler.doVerb(ReplicationFinishedVerbHandler.java:38)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}

I'll look into this.

Second, it seems that moving/joining nodes can take over the removed token fine, once the removetoken is complete. I haven't tried having a node take over the removed token while the removal is ongoing- I assume we can just document that that probably isn't a great idea?;;;","22/Jul/11 03:50;thepaul;0006-acknowledge-unexpected-repl-fins.patch.txt: don't assert and drop the message when we see an unexpected REPLICATION_FINISHED. Ack it instead, so the sender doesn't continually retry.;;;","22/Jul/11 04:09;thepaul;0006-acknowledge-unexpected-repl-fins.patch.txt (updated): also log at info when acknowledging the unexpected messages;;;","22/Jul/11 04:52;thepaul;ok, +1 with these patches.;;;","23/Jul/11 08:20;brandon.williams;0007 handles problems when a node has been down longer than aVeryLongTime, and ensures that we advertise the new token states long enough.

0008 makes sure that SS only get involved with removal if the token is a member.;;;","26/Jul/11 02:33;thepaul;+1;;;","26/Jul/11 02:58;brandon.williams;Committed.;;;","26/Jul/11 03:43;hudson;Integrated in Cassandra-0.8 #238 (See [https://builds.apache.org/job/Cassandra-0.8/238/])
    Gossip handles dead states, token removal actually works, gossip states
are held for aVeryLongTime.
Patch by brandonwilliams and Paul Cannon, reviewed by Paul Cannon for
CASSANDRA-2496.

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1150847
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/Gossiper.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/VersionedValue.java
* /cassandra/branches/cassandra-0.8/NEWS.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/HeartBeatState.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageService.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/net/MessagingService.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/ApplicationState.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nodetool ring crashes when no schema is loaded,CASSANDRA-1286,12469470,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,17/Jul/10 01:04,16/Apr/19 17:33,22/Mar/23 14:57,03/Aug/10 04:22,0.7 beta 1,,Tool/nodetool,,1,,,,,,"Nodetool ring uses SP.getRangeToEndpointMap(null) that tries to retrieve the first non system keyspace.
Hences it crashes (with a IndexOutOfBoundsException) if no schema is loaded.

Should we return a nice little error message or make it work even no schema is loaded ?",,arya,brandon.williams,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Aug/10 23:07;slebresne;1286-Don-t-use-ranges-to-print-the-ring-with-nodetool.patch;https://issues.apache.org/jira/secure/attachment/12451044/1286-Don-t-use-ranges-to-print-the-ring-with-nodetool.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20058,,,Wed Aug 04 13:25:35 UTC 2010,,,,,,,,,,"0|i0g44v:",92100,,,,,Low,,,,,,,,,,,,,,,,,"01/Aug/10 04:18;brandon.williams;FYI: CASSANDRA-1291 is also suffering from a problem with SP.getRangeToEndpointMap(null) when the RF is greater than 1.;;;","01/Aug/10 10:12;dopsun;This becomes a real problem for Cassandra 0.7, because by default, Cassandra 0.7 does not load any key space, and means that cannot use the nodetool to check the newly setup cluster.;;;","02/Aug/10 20:25;gdusbabek;We should be able to get ring data by directly consulting TokenMetadata (no need to collect range information which is what the keyspace argument gives us).;;;","02/Aug/10 23:07;slebresne;Attaching a patch that follows Gary suggestion. It simply exposes the
map of tokens -> endpoint (including the boostrapping ones).

This fixes the ring when no schema is loaded at least.;;;","03/Aug/10 03:30;gdusbabek;Should we deprecate StorageServiceMBean.getPendingRangeToEndpointMap()?;;;","03/Aug/10 03:47;jbellis;No, that's there because people want to use it to connect their clients to the ""right"" machines directly.;;;","03/Aug/10 04:21;gdusbabek;+1 committed.;;;","04/Aug/10 21:25;hudson;Integrated in Cassandra #509 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/509/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Build does not handle ANTLR generated code properly,CASSANDRA-40,12421816,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,permellqvist,permellqvist,permellqvist,02/Apr/09 04:46,16/Apr/19 17:33,22/Mar/23 14:57,03/Apr/09 03:17,,,,,0,,,,,,"The default ant target does not trigger generation of code from Cli.g and Cql.g
The clean target does not remove generated files
The targets gen-cli-grammar and gen-cql-grammar do not reference current locations of Cli.g and Cql.g
Generated files are commited in svn repository (by mistake?)",,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,"02/Apr/09 04:50;permellqvist;diff.txt;https://issues.apache.org/jira/secure/attachment/12404376/diff.txt",,,,,,,,,,,,,,1.0,permellqvist,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19525,,,Thu Apr 02 19:17:11 UTC 2009,,,,,,,,,,"0|i0dxav:",79325,,,,,Low,,,,,,,,,,,,,,,,,"02/Apr/09 04:50;permellqvist;Patch (generated by 'svn diff build.xml' on revision 760988)
Fixes mentioned issues with build.xml
Removing ANTLR-generated files from svn remains;;;","02/Apr/09 04:55;jbellis;What does this do to Eclipse users?  Can you tell eclipse ""just build it with ant?""

(I don't use Eclipse but I think most others do.);;;","02/Apr/09 16:21;permellqvist;Eclipse has ant integration (so does IntelliJ and as far as I know NetBeans).
To trigger ant from eclipse http://help.eclipse.org/help33/index.jsp?topic=/org.eclipse.platform.doc.user/gettingStarted/qs-92_project_builders.htm

For Eclipse users that do not want to configure ant integration, running the gen- tasks once after checkout should be enough. After that the normal eclipse build cycle should work;;;","02/Apr/09 20:27;jbellis;IMO we should move generated code out of the main src/ tree too but as I have said before I am not a java build guru.  What's the best practice in these situations?;;;","02/Apr/09 20:57;jbellis;I get errors running `ant clean build` after applying this patch:

gen-cli-grammar:
     [echo] Building Grammar /home/jonathan/projects/cassandra-new-svn/src/org/apache/cassandra/cli/Cli.g  ....
     [java] Exception in thread ""main"" java.lang.NoClassDefFoundError: org/antlr/stringtemplate/StringTemplateErrorListener
     [java] 	at org.antlr.Tool.main(Tool.java:67)
     [java] Caused by: java.lang.ClassNotFoundException: org.antlr.stringtemplate.StringTemplateErrorListener
     [java] 	at java.net.URLClassLoader$1.run(URLClassLoader.java:200)
     [java] 	at java.security.AccessController.doPrivileged(Native Method)
     [java] 	at java.net.URLClassLoader.findClass(URLClassLoader.java:188)
     [java] 	at java.lang.ClassLoader.loadClass(ClassLoader.java:307)
     [java] 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
     [java] 	at java.lang.ClassLoader.loadClass(ClassLoader.java:252)
     [java] 	at java.lang.ClassLoader.loadClassInternal(ClassLoader.java:320)
     [java] 	... 1 more
     [java] Java Result: 1
;;;","02/Apr/09 23:38;junrao;I think it's convenient to include antlr-generated java files in the source tree. Not everybody will be modifying the CQL grammar. In a similar way, we already include thrift-generated java code in the source tree.;;;","02/Apr/09 23:49;jbellis;That's because Thrift is, frankly, a bitch to build.

Building the antlr files should be easy since we just ship the relevant jars.  Or it should be. :)  I'm sure Per can fix the problem I'm seeing.;;;","03/Apr/09 02:10;permellqvist;Jonathan - the NoClassDefFoundError is a separate issue (you would get the same problem even without the patch if you delete the generated sources).
I had to put some jars from an older revision back in the lib directory to run the gen-tasks

stringtemplate-3.0.jar
antlr-2.7.7.jar

They are both referenced in build.xml but were removed from the lib directory at some point.;;;","03/Apr/09 03:17;jbellis;applied;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTable generation clash during compaction,CASSANDRA-418,12434696,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,sammy.yu,sammy.yu,03/Sep/09 09:51,16/Apr/19 17:33,22/Mar/23 14:57,07/Sep/09 22:50,0.4,,,,0,,,,,,"We found that one of our node started getting timeouts for get_slice.  Looking further we found that the CFS.ssTables_ references a SStable doesn't exist on the file system.

Walking down the log we see that the sstable in question 6038 is being compacted onto itself (in terms of filename file wise it is written to -tmp):
system.log.2009-09-01: INFO [MINOR-COMPACTION-POOL:1] 2009-09-01 23:50:07,553 ColumnFamilyStore.java (line 1067) Compacting 
[/mnt/var/cassandra/data/Digg/FriendActions-6037-Data.db,/mnt/var/cassandra/data/Digg/FriendActions-6038-Data.db,/mnt/var/cassandra/data/Digg/
FriendActions-6040-Data.db,/mnt/var/cassandra/data/Digg/FriendActions-6042-Data.db]
system.log.2009-09-01: INFO [MINOR-COMPACTION-POOL:1] 2009-09-01 23:51:43,727 ColumnFamilyStore.java (line 1209) Compacted to
/mnt/var/cassandra/data/Digg/FriendActions-6038-Data.db.  0/1010269806 bytes for 9482/9373 keys read/written.  Time: 96173ms.

It appears the generation number is generated by looking at the lowest number in the list of files to be compacted and adding 1.  In this scenario it is 6037+1=6038.
The code in CFS.doFileCompaction will remove the key and add the key back and remove the key again, hence the error we were seeing.

Should the generation number be generated via another way or should we update doFileCompaction to be smarter?

",,hammer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Sep/09 10:56;sammy.yu;0001-CASSANDRA-418-Use-monotonically-increasing-generatio.patch;https://issues.apache.org/jira/secure/attachment/12418464/0001-CASSANDRA-418-Use-monotonically-increasing-generatio.patch","03/Sep/09 21:51;sammy.yu;0002-CASSANDRA-418-Use-monotonically-increasing-generatio.patch;https://issues.apache.org/jira/secure/attachment/12418515/0002-CASSANDRA-418-Use-monotonically-increasing-generatio.patch","04/Sep/09 22:44;jbellis;418-2.patch;https://issues.apache.org/jira/secure/attachment/12418630/418-2.patch",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19680,,,Tue Sep 08 12:35:49 UTC 2009,,,,,,,,,,"0|i0fyt3:",91237,,,,,Normal,,,,,,,,,,,,,,,,,"03/Sep/09 10:09;jbellis;the compaction code relies on the bucketizer to keep files of the same compaction-count (a bucket of sstables that have been compacted twice, one of sstables that have been compacted 3 times) so that you are never compacting sstables of consecutive generations -- all will have even numbers, or all odd.  something has broken that invariant.

rather than try to band-aid the bucketizer i think making the generation-generator more robust is the way to go.  this seems like a flimsy property to try to preserve.

my vote would be to simplify: just pick the next monotonically increasing int any time we need a new tmp sstable file, whether for flush, compaction, or bootstrap.  I.e. via CFS.getTempSSTableFileName, without the extra increment.

the reason historically that FB tried to be fancy is, they were trying to optimize away reading older sstables at all if the data being queried was found in a newer one.  the ""only new sstables get a number from the atomic int, and the compactions fit in between"" was to preserve this.  (then you sort on the generation number and higher ones are always newer.)

but that can't work (see CASSANDRA-223) so we always do a full merge across all sstables now.  so we can simplify this safely.;;;","03/Sep/09 10:56;sammy.yu;Use monotonically increase generation number for newly compacted sstable.
;;;","03/Sep/09 10:57;lenn0x;A note on the ML might be needed, with this bug it looks like we are going to have to dump our old data and re-import since we don't have a 100% way of figuring out what data is missing across the cluster.;;;","03/Sep/09 10:57;sammy.yu;Should we also change CFS.getTempSSTableFileName to just increment fileIndexGenerator_ once?
;;;","03/Sep/09 20:40;jbellis;> Should we also change CFS.getTempSSTableFileName to just increment fileIndexGenerator_ once? 

right, that's what i meant by ""w/o the extra increment."";;;","03/Sep/09 21:51;sammy.yu;Self contained patch that now increment the generation number by one
;;;","03/Sep/09 23:04;jbellis;(also affects version 0.3 for the record);;;","03/Sep/09 23:11;jbellis;committed;;;","04/Sep/09 22:44;jbellis;missed some code.  this cleans up inaccurate comments and remaining double-increment code;;;","05/Sep/09 01:57;sammy.yu;+1 looks good;;;","07/Sep/09 22:50;jbellis;committed to 0.4 and 0.5;;;","08/Sep/09 20:35;hudson;Integrated in Cassandra #191 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/191/])
    clean up inaccurate comments; remaining double-increment code.
patch by jbellis; reviewed by Sammy Yu for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConcurrentModificationException when updating column family metadata,CASSANDRA-1736,12479813,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,bterm,bterm,13/Nov/10 01:31,16/Apr/19 17:33,22/Mar/23 14:57,23/Nov/10 23:26,0.7.0 rc 1,,,,0,,,,,,"From cli
> update column family Tweet with column_metadata=[{column_name:state, validation_class:UTF8Type}]
> set Tweet [x][state] = TX
> get Tweet where state = TX
No index columns present
> update column family Tweet with column_metadata=[{column_name:state, index_type:0, validation_class:UTF8Type}]
null
> list Tweet
java.net.SocketException: Broken pipe


ERROR [MigrationStage:1] 2010-11-12 09:12:28,618 AbstractCassandraDaemon.java (line 90) Fatal exception in thread Thread[Migra$
java.util.ConcurrentModificationException
        at java.util.HashMap$HashIterator.nextEntry(HashMap.java:793)
        at java.util.HashMap$KeyIterator.next(HashMap.java:828)
        at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1495)
        at org.apache.cassandra.db.migration.UpdateColumnFamily.beforeApplyModels(UpdateColumnFamily.java:76)
        at org.apache.cassandra.db.migration.Migration.apply(Migration.java:109)
        at org.apache.cassandra.thrift.CassandraServer$2.call(CassandraServer.java:672)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
ERROR [pool-1-thread-5] 2010-11-12 09:12:28,636 CustomTThreadPoolServer.java (line 175) Thrift error occurred during processin$
org.apache.thrift.protocol.TProtocolException: Required field 'why' was not present! Struct: InvalidRequestException(why:null)
        at org.apache.cassandra.thrift.InvalidRequestException.validate(InvalidRequestException.java:340)
        at org.apache.cassandra.thrift.InvalidRequestException.write(InvalidRequestException.java:309)
        at org.apache.cassandra.thrift.Cassandra$system_update_column_family_result.write(Cassandra.java:26764)
        at org.apache.cassandra.thrift.Cassandra$Processor$system_update_column_family.process(Cassandra.java:3605)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2555)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Nov/10 10:41;jbellis;1736-v2.txt;https://issues.apache.org/jira/secure/attachment/12459872/1736-v2.txt","15/Nov/10 03:33;jbellis;1736.txt;https://issues.apache.org/jira/secure/attachment/12459568/1736.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20282,,,Sat Nov 20 03:12:00 UTC 2010,,,,,,,,,,"0|i0g72n:",92576,,gdusbabek,,gdusbabek,Normal,,,,,,,,,,,,,,,,,"13/Nov/10 11:06;jbellis;This is a regression caused by treating the compaction marker as a component (CASSANDRA-1471, CASSANDRA-1544).  The CME comes when compaction modifies the components set while snapshot is iterating over them; since snapshot begins by flushing, this is actually fairly likely to happen (especially with non-JNA snapshots, i.e., slow ones).

IMO the best fix to this would be to stop treating the compaction marker as a component and make the components set Unmodifiable to close off that avenue of bugs in the future.;;;","13/Nov/10 11:15;jbellis;One reason I don't think compaction marker belongs in components is that as this bug highlights, we could end up with the marker as part of a snapshot.  Which would cause a more subtle bug.  Here is the order of events:

{code}
My CFS has sstables A B C.
Flush introduces sstable D.
We begin compacting A B C D.
We begin iterating A B C D for snapshot.
During the iteration, compaction finishes producing sstable E.  A B C D are marked compacted.
snapshot finishes, with (say) C D marked compacted.
{code}

Now, the sstable tracker guarantees that we see a consistent view of the sstables -- we will either exactly one of  {A B C D} or {E}.  But by mixing the compaction marker in as a component we now have a snapshot that implies that A and B were live but C and D were compacted, and if we take that snapshot as-is and promote it to live data, when we restart Cassandra will purge C and D since they were marked compacted.

We could band-aid this in a number of ways but I think the less fragile approach is to treat the compaction marker as something separate from components.;;;","15/Nov/10 03:33;jbellis;patch along the above lines;;;","16/Nov/10 02:39;stuhood;This patch adds the isCompacted method, but it isn't called anywhere, which raises the question: who might call that method in the future, or care that an SSTable is compacted?

Also, SSTable.delete() will fail if the SSTable is not marked compacted, for instance if the second branch of the 'if' in CFStore is triggered.;;;","16/Nov/10 03:49;jbellis;bq. This patch adds the isCompacted method, but it isn't called anywhere

I will remove it.

bq. SSTable.delete() will fail if the SSTable is not marked compacted

If you're referring to 

{code}
            FileUtils.delete(desc.filenameFor(Component.COMPACTED_MARKER));
{code}

I changed that to not use deleteWithConfirm for exactly that reason.;;;","18/Nov/10 10:37;jbellis;v2 removes boolean and replaces with assert that we're not instantiating a compacted SSTableReader;;;","20/Nov/10 01:19;gdusbabek;+1;;;","20/Nov/10 04:05;hudson;Integrated in Cassandra-0.7 #19 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/19/])
    fix race between snapshot andcompaction
patch by jbellis; reviewed by gdusbabek for CASSANDRA-1736
;;;","20/Nov/10 11:12;hudson;Integrated in Cassandra-0.7 #22 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/22/])
    avoid attempting to delete compacted sstables twice on restart
patch by jbellis to fix regression introduced by CASSANDRA-1736
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NoSuchElement exception on node which is streaming a repair,CASSANDRA-2316,12501231,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,alienth,alienth,12/Mar/11 15:12,16/Apr/19 17:33,22/Mar/23 14:57,19/Apr/11 07:04,0.7.5,,,,0,repair,,,,,"Running latest SVN snapshot of 0.7.

When I ran a repair on a node, that node's neighbor threw the following exception. Let me know what other info could be helpful.

{code}
 INFO 23:43:44,358 Streaming to /10.251.166.15
ERROR 23:50:21,321 Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.util.NoSuchElementException
        at com.google.common.collect.AbstractIterator.next(AbstractIterator.java:146)
        at org.apache.cassandra.service.AntiEntropyService$Validator.add(AntiEntropyService.java:366)
        at org.apache.cassandra.db.CompactionManager.doValidationCompaction(CompactionManager.java:825)
        at org.apache.cassandra.db.CompactionManager.access$800(CompactionManager.java:56)
        at org.apache.cassandra.db.CompactionManager$6.call(CompactionManager.java:358)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
{code}",,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-2324,,,,,,"19/Mar/11 03:55;jbellis;2316-assert.txt;https://issues.apache.org/jira/secure/attachment/12474026/2316-assert.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20557,,,Tue Apr 19 14:04:20 UTC 2011,,,,,,,,,,"0|i0ganr:",93157,,stuhood,,stuhood,Normal,,,,,,,,,,,,,,,,,"15/Mar/11 00:45;jbellis;The loop in validator.add apparently assumes that _some_ range will contain any given row. But

- ranges is supposed to be ""invalid"" ranges not all ranges
- comments in validator.add say it is called for each row in the CF
- so any rows that are not part of an ""invalid"" range will cause this exception

So either my superficial understanding of what ""invalid"" ranges are is broken, or the comments are wrong, or I'm surprised we're not hitting this a lot more frequently.;;;","15/Mar/11 00:47;jbellis;Looks like this dates back to 0.6.;;;","18/Mar/11 03:36;stuhood;""Invalid"" ranges in the tree are ranges that need to be hashed. The idea was that the tree could be persisted between repair sessions, and ranges would be invalidated as writes arrived: then the validation compaction would only need to compact invalid ranges of the tree.

In the current implementation, the tree will only contain invalid ranges, since it is being created from scratch for every repair.;;;","18/Mar/11 03:39;stuhood;I wonder if this is a keys-out-of-order problem?;;;","18/Mar/11 10:39;jbellis;Since we iterate over each key in the CF, order shouldn't actually matter should it?;;;","18/Mar/11 16:04;stuhood;Order matters, because there will be up to 2^16 invalid ranges. If keys arrive out of order we will consume ranges that should have contained keys, possibly leading us to consume all invalid ranges.

Either way, an assert that keys are arriving in order would be handy here.;;;","19/Mar/11 03:55;jbellis;proposed assert attached;;;","19/Apr/11 06:12;stuhood;+1 For the assert.;;;","19/Apr/11 22:04;hudson;Integrated in Cassandra-0.7 #447 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/447/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
a few insert operations failed while bootstrapping,CASSANDRA-731,12446281,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jaakko,david.pan,david.pan,22/Jan/10 10:42,16/Apr/19 17:33,22/Mar/23 14:57,10/Feb/10 23:57,0.5,,,,0,,,,,,"I inserted 10000 key/value while bootstrapping and found 2 insert operations failed.

DEBUG [pool-1-thread-63] 2010-01-20 17:01:57,033 StorageProxy.java (line 225) insert writing key 15530 to 10981@/10.81.37.65
ERROR [pool-1-thread-46] 2010-01-20 17:01:57,033 Cassandra.java (line 1064) Internal error processing insert
java.lang.AssertionError
at org.apache.cassandra.locator.AbstractReplicationStrategy.getHintedMapForEndpoints(AbstractReplicationStrategy.java:157)
at org.apache.cassandra.locator.AbstractReplicationStrategy.getHintedEndpoints(AbstractReplicationStrategy.java:76)
at org.apache.cassandra.service.StorageService.getHintedEndpointMap(StorageService.java:1178)
at org.apache.cassandra.service.StorageProxy.insertBlocking(StorageProxy.java:169)
at org.apache.cassandra.service.CassandraServer.doInsert(CassandraServer.java:466)
at org.apache.cassandra.service.CassandraServer.insert(CassandraServer.java:417)
at org.apache.cassandra.service.Cassandra$Processor$insert.process(Cassandra.java:1056)
at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:817)
at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
at java.lang.Thread.run(Thread.java:619)
ERROR [pool-1-thread-44] 2010-01-20 17:01:57,033 Cassandra.java (line 1064) Internal error processing insert
java.lang.AssertionError
at org.apache.cassandra.locator.AbstractReplicationStrategy.getHintedMapForEndpoints(AbstractReplicationStrategy.java:157)
at org.apache.cassandra.locator.AbstractReplicationStrategy.getHintedEndpoints(AbstractReplicationStrategy.java:76)
at org.apache.cassandra.service.StorageService.getHintedEndpointMap(StorageService.java:1178)
at org.apache.cassandra.service.StorageProxy.insertBlocking(StorageProxy.java:169)
at org.apache.cassandra.service.CassandraServer.doInsert(CassandraServer.java:466)
at org.apache.cassandra.service.CassandraServer.insert(CassandraServer.java:417)
at org.apache.cassandra.service.Cassandra$Processor$insert.process(Cassandra.java:1056)
at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:817)
at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
at java.lang.Thread.run(Thread.java:619)


I traced the code and found the following assertion failed :
/* org.apache.cassandra.locator.AbstractReplicationStrategy.getHintedMapForEndpoints(Collection<InetAddress>) */
assert map.size() == targets.size(); 

The following reasons caused this issue:
1) targets is a list , not a map, as a result there may be some duplicated IP.
2) The following codes are not atomic :
org.apache.cassandra.service.StorageService.handleStateNormal(InetAddress, String)
        tokenMetadata_.updateNormalToken(token, endPoint);
        calculatePendingRanges();

 That's to say the IP may be both in the naturalEndpoints and pendingRanges.

eg : 
targets is IPa, IPb, IPc, IPa; (size = 4)
then, the map will be IPa, IPb, IPc. (size = 3)
as a result, assert failed.







",,david.pan,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,"22/Jan/10 10:49;david.pan;731-throw_internal_exception_while_bootstrapping.patch;https://issues.apache.org/jira/secure/attachment/12431087/731-throw_internal_exception_while_bootstrapping.patch","22/Jan/10 10:56;david.pan;the new log after patch.txt;https://issues.apache.org/jira/secure/attachment/12431088/the+new+log+after+patch.txt",,,,,,,,,,,,,2.0,jaakko,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19842,,,Wed Feb 10 15:57:03 UTC 2010,,,,,,,,,,"0|i0g0q7:",91548,,,,,Normal,,,,,,,,,,,,,,,,,"22/Jan/10 10:49;david.pan;This is a simple way to deal with this problem, but it's not the final solution.
I think the best way is to make the modification of TokenMetadata atomic, but that needs a big change both in the TokenMetadata and StorageService.
Consider that the pendingRange is needed in writing, bootstrapping and leaving only, this simple modification looks like ok at current time.;;;","22/Jan/10 10:56;david.pan;After patch, I add a 500ms sleep between "" tokenMetadata_.updateNormalToken(token, endPoint);"" and "" calculatePendingRanges();"" to make it easy to repeat the problem.
Through the log, you can see it :-);;;","28/Jan/10 09:41;jaakko;Yeah, you're right. I'll have a look at this.;;;","09/Feb/10 17:31;jaakko;This assert might fail for the reason David said, but it might also fail because targets includes a node with pending ranges that has died (for instance did not complete bootstrap). 

It does not matter if a node is in pending ranges and normal token map simultaneously, as that time is very short and may only result in a write going to too many nodes once in a while. Gossip propagation is far from being instant, so writes go to wrong places routinely anyway when nodes move. It's a much smaller thing than locking whole metadata for the duration of calculations. 

I think removing the assert is enough. It should be removed in any case, since it is checking a condition that is no longer valid. I don't see any other problem in this part.;;;","10/Feb/10 23:57;jbellis;+1 Jaakko's diagnosis.  Committed assertion removal to 0.5 and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compactions might remove tombstones without removing the actual data,CASSANDRA-604,12442530,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,rrabah,rrabah,05/Dec/09 08:12,16/Apr/19 17:33,22/Mar/23 14:57,09/Dec/09 02:45,0.5,,,,0,,,,,,"I was looking at the code for compaction, and noticed that when we are doing compactions during the normal course of
Cassandra, we call:

           for (List<SSTableReader> sstables :
getCompactionBuckets(ssTables_, 50L * 1024L * 1024L))
           {
               if (sstables.size() < minThreshold)
               {
                   continue;
               }
               other wise docompactions...

where getCompactionBuckets puts in buckets very small files, or files
that are 0.5-1.5 of each other's sizes. It will only compact those if
they are >= minimum threshold which is 4 by default.
So far so good. Now how about this scenario, I have an old entry that
I inserted long time ago and that was compacted into a 75MB file.
There are fewer 75MB files than 4. I do many deletes, and I end with 4
extra sstable files filled with tombstones, each about 300 MB large.
These 4 files are compacted together and in the compaction code, if
the tombstone is there we don't copy it over to the new file. Now
since we did not compact the 75MB files, but we compacted the
tombstone files, that leaves us with the tombstone gone, but
the data still intact in the 75MB file. If we compacted all the
files together I don't think that would be a problem, but since we
only compact 4, this potentially leaves data not cleaned.",Cent-OS,hammer,johanoskarsson,rrabah,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Dec/09 14:58;jbellis;604.patch;https://issues.apache.org/jira/secure/attachment/12427304/604.patch",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19775,,,Wed Dec 09 12:42:41 UTC 2009,,,,,,,,,,"0|i0fzyf:",91423,,,,,Normal,,,,,,,,,,,,,,,,,"07/Dec/09 02:04;rrabah;Thinking about it some more, there is another case when data can be lost. In the above case the file containing the tombstone was compacted by itself before the data file.
The second case is that the file containing the data is compacted by itself before the tombstone is compacted. 

So in both cases, it seems like the only viable solution I can think of, is to only remove the tombstones when every single SSTable file for the column family is compacted (I.E. major compaction). Otherwise, the tombstone should stick around.

Does that make sense?;;;","07/Dec/09 05:36;jbellis;I don't follow.

All the tombstone does is suppress earlier versions.  And all compaction does is write out a new sstable containing only the newest version.  So compacting data w/o the tombstone will either yield (1) a single version that is newer than the tombstone, in which case the tombstone will be ignored on reads, or (2) a single version older than the tombstone, in which case it will still be supressed.  Either way correctness is preserved.;;;","07/Dec/09 08:33;rrabah;What I meant was there are 2 orders of compactions that might happen that would lead to the clean up of tombstones but not data:

Case 1) I described first, data is in sstable 1, tombstone in sstable 2. sstables 2-5 are compacted producing sstable 6, and sstable 6 has no tombstone. That leaves us with sstable 1 data, sstable 6 no tombstone --> Probably bad. In this case tombstones are compacted before the data is ever compacted.

Case 2) data is in sstable 1, tombstone is in sstable 5. sstable 1-4 get compacted to sstable 6 that has data, so now we have sstable 5 tombstone, sstable 6 data. sstable 5,7,8,9 are compacted producing sstable 10. sstable 6 data, sstable 10 no tombstone --> Probably bad.  In this case, data was compacted first, then tombstones. 

Both cases can probably be fixed in the same manner, but just wanted to point out all scenarios which I can think of that can cause this problem. 


;;;","07/Dec/09 08:55;jbellis;That makes sense.  I agree that only GCing tombstones during major compactions (or, other compactions that happen to include all sstables) is the easiest fix.;;;","08/Dec/09 14:58;jbellis;patch to only GC when compacting all sstables, as outlined above.

Ramzi to review?;;;","09/Dec/09 02:33;rrabah;Looks great. I will test the patch with a higher volume of inserts and deletes and let you know how the test goes.;;;","09/Dec/09 02:45;jbellis;committed.  let us know if your testing uncovers any problems.;;;","09/Dec/09 20:42;hudson;Integrated in Cassandra #282 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/282/])
    only GC when compacting all sstables, to avoid situations where the data a tombstone is intended to supress is in an sstable that is not part of the compaction set.
patch by jbellis; reviewed by Ramzi Rabah for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.lang.NegativeArraySizeException being thrown for large column names,CASSANDRA-460,12436674,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,tjake,tjake,27/Sep/09 10:07,16/Apr/19 17:33,22/Mar/23 14:57,29/Sep/09 01:47,0.4,0.5,,,0,,,,,,"When inserting large columns I'm getting this stacktrace in the cassandra log:

ERROR [ROW-MUTATION-STAGE:3] 2009-09-26 18:57:02,589 DebuggableThreadPoolExecutor.java (line 110) Error in ThreadPoolExecutor
java.lang.NegativeArraySizeException
        at org.apache.cassandra.db.ColumnSerializer.readName(ColumnSerializer.java:46)
        at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:345)
        at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:313)
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:88)
        at org.apache.cassandra.db.RowMutationSerializer.defreezeTheMaps(RowMutation.java:313)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:323)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:276)
        at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:59)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:39)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
ERROR [ROW-MUTATION-STAGE:3] 2009-09-26 18:57:02,589 CassandraDaemon.java (line 71) Fatal exception in thread Thread[ROW-MUTATION-STAGE:3,5,main]
java.lang.NegativeArraySizeException
        at org.apache.cassandra.db.ColumnSerializer.readName(ColumnSerializer.java:46)
        at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:345)
        at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:313)
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:88)
        at org.apache.cassandra.db.RowMutationSerializer.defreezeTheMaps(RowMutation.java:313)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:323)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:276)
        at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:59)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:39)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
 INFO [ROW-MUTATION-STAGE:90] 2009-09-26 18:57:03,183 ColumnFamilyStore.java (line 367) TermVectors has reached its threshold; switching in a fresh Memtable
 INFO [R",linux,hbadenes,herchu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Sep/09 23:55;jbellis;460-2.patch;https://issues.apache.org/jira/secure/attachment/12420707/460-2.patch","27/Sep/09 11:16;jbellis;460.patch;https://issues.apache.org/jira/secure/attachment/12420645/460.patch",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19699,,,Tue Sep 29 12:34:59 UTC 2009,,,,,,,,,,"0|i0fz2n:",91280,,,,,Normal,,,,,,,,,,,,,,,,,"27/Sep/09 11:10;tjake;The exception is reproducible with this column name:

lopadotemachoselachogaleokranioleipsanodrimhypotrimmatosilphioparaomelitokatakechymenokichlepikossyphophattoperisteralektryonoptekephalliokigklopeleiolagoiosiraiobaphetraganopterygon;;;","27/Sep/09 11:28;tjake;+1 fixed my issue;;;","28/Sep/09 23:55;jbellis;Committed first patch, which fixes the internal name serialization for column names longer than 128 bytes but less than 64k.

This one adds sanity checking to provide nice error messages if a name is larger than 64k.;;;","29/Sep/09 01:23;sammy.yu;+1 460-2.patch
;;;","29/Sep/09 01:47;jbellis;committed to 0.4 and 0.5;;;","29/Sep/09 20:34;hudson;Integrated in Cassandra #212 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/212/])
    fix bit math on column name read
patch by jbellis; tested by T Jake Luciani for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-cli doesn't handle non-string column names well,CASSANDRA-1701,12478959,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jancona,jancona,jancona,03/Nov/10 13:21,16/Apr/19 17:33,22/Mar/23 14:57,04/Nov/10 00:42,,,,,0,,,,,,"cassandra-cli has several bugs when using column names that aren't strings. Attached is a patch that updates CliTest to show the problems and fixes CliClient by properly converting non-string column and sub-column values passed to the GET, SET and COUNT commands.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Nov/10 13:22;jancona;cassandra-cli-non-string-column-names.patch;https://issues.apache.org/jira/secure/attachment/12458714/cassandra-cli-non-string-column-names.patch",,,,,,,,,,,,,,1.0,jancona,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20266,,,Wed Nov 03 16:42:01 UTC 2010,,,,,,,,,,"0|i0g6uv:",92541,,xedin,,xedin,Normal,,,,,,,,,,,,,,,,,"03/Nov/10 22:58;xedin;This looks good to me, all .getBytes() replaced with columnNameAsByteArray methods which is how it should be + tests added.;;;","04/Nov/10 00:42;jbellis;committed.  thanks, Jim!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
loadbalance operation never completes on a 3 node cluster,CASSANDRA-1221,12467688,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,gdusbabek,gdusbabek,gdusbabek,23/Jun/10 20:39,16/Apr/19 17:33,22/Mar/23 14:57,21/Jul/10 01:12,0.6.4,,,,1,,,,,,"Arya Goudarzi reports:

Please confirm if this is an issue and should be reported or I am doing something wrong. I could not find anything relevant on JIRA:

Playing with 0.7 nightly (today's build), I setup a 3 node cluster this way:

 - Added one node;
 - Loaded default schema with RF 1 from YAML using JMX;
 - Loaded 2M keys using py_stress;
 - Bootstrapped a second node;
 - Cleaned up the first node;
 - Bootstrapped a third node;
 - Cleaned up the second node;

I got the following ring:

Address       Status     Load          Range                                      Ring
                                      154293670372423273273390365393543806425
10.50.26.132  Up         518.63 MB     69164917636305877859094619660693892452     |<--|
10.50.26.134  Up         234.8 MB      111685517405103688771527967027648896391    |   |
10.50.26.133  Up         235.26 MB     154293670372423273273390365393543806425    |-->|

Now I ran:

nodetool --host 10.50.26.132 loadbalance

It's been going for a while. I checked the streams

nodetool --host 10.50.26.134 streams
Mode: Normal
Not sending any streams.
Streaming from: /10.50.26.132
  Keyspace1: /var/lib/cassandra/data/Keyspace1/Standard1-tmp-d-3-Data.db/[(0,22206096), (22206096,27271682)]
  Keyspace1: /var/lib/cassandra/data/Keyspace1/Standard1-tmp-d-4-Data.db/[(0,15180462), (15180462,18656982)]
  Keyspace1: /var/lib/cassandra/data/Keyspace1/Standard1-tmp-d-5-Data.db/[(0,353139829), (353139829,433883659)]
  Keyspace1: /var/lib/cassandra/data/Keyspace1/Standard1-tmp-d-6-Data.db/[(0,366336059), (366336059,450095320)]

nodetool --host 10.50.26.132 streams
Mode: Leaving: streaming data to other nodes
Streaming to: /10.50.26.134
  /var/lib/cassandra/data/Keyspace1/Standard1-d-48-Data.db/[(0,366336059), (366336059,450095320)]
Not receiving any streams.

These have been going for the past 2 hours.

I see in the logs of the node with 134 IP address and I saw this:

INFO [GOSSIP_STAGE:1] 2010-06-22 16:30:54,679 StorageService.java (line 603) Will not change my token ownership to /10.50.26.132

So, to my understanding from wikis loadbalance supposed to decommission and re-bootstrap again by sending its tokens to other nodes and then bootstrap again. It's been stuck in streaming for the past 2 hours and the size of ring has not changed. The log in the first node says it has started streaming for the past hours:

INFO [STREAM-STAGE:1] 2010-06-22 16:35:56,255 StreamOut.java (line 72) Beginning transfer process to /10.50.26.134 for ranges (154293670372423273273390365393543806425,69164917636305877859094619660693892452]
 INFO [STREAM-STAGE:1] 2010-06-22 16:35:56,255 StreamOut.java (line 82) Flushing memtables for Keyspace1...
 INFO [STREAM-STAGE:1] 2010-06-22 16:35:56,266 StreamOut.java (line 128) Stream context metadata [/var/lib/cassandra/data/Keyspace1/Standard1-d-48-Data.db/[(0,366336059), (366336059,450095320)]] 1 sstables.
 INFO [STREAM-STAGE:1] 2010-06-22 16:35:56,267 StreamOut.java (line 135) Sending a stream initiate message to /10.50.26.134 ...
 INFO [STREAM-STAGE:1] 2010-06-22 16:35:56,267 StreamOut.java (line 140) Waiting for transfer to /10.50.26.134 to complete
 INFO [FLUSH-TIMER] 2010-06-22 17:36:53,370 ColumnFamilyStore.java (line 359) LocationInfo has reached its threshold; switching in a fresh Memtable at CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1277249454413.log', position=720)
 INFO [FLUSH-TIMER] 2010-06-22 17:36:53,370 ColumnFamilyStore.java (line 622) Enqueuing flush of Memtable(LocationInfo)@1637794189
 INFO [FLUSH-WRITER-POOL:1] 2010-06-22 17:36:53,370 Memtable.java (line 149) Writing Memtable(LocationInfo)@1637794189
 INFO [FLUSH-WRITER-POOL:1] 2010-06-22 17:36:53,528 Memtable.java (line 163) Completed flushing /var/lib/cassandra/data/system/LocationInfo-d-9-Data.db
 INFO [MEMTABLE-POST-FLUSHER:1] 2010-06-22 17:36:53,529 ColumnFamilyStore.java (line 374) Discarding 1000


Nothing more after this line.

Am I doing something wrong?",,anty,arya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jul/10 23:25;gdusbabek;0.6-conviction-fix.diff;https://issues.apache.org/jira/secure/attachment/12449928/0.6-conviction-fix.diff","20/Jul/10 19:39;gdusbabek;0001-Gossiper-and-FD-never-called-MS.convict-to-shut-down.patch;https://issues.apache.org/jira/secure/attachment/12449920/0001-Gossiper-and-FD-never-called-MS.convict-to-shut-down.patch","15/Jul/10 08:24;arya;system1.log;https://issues.apache.org/jira/secure/attachment/12449507/system1.log","15/Jul/10 08:24;arya;system2.log;https://issues.apache.org/jira/secure/attachment/12449508/system2.log","15/Jul/10 08:24;arya;system3.log;https://issues.apache.org/jira/secure/attachment/12449509/system3.log",,,,,,,,,,5.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20038,,,Wed Jul 21 12:50:25 UTC 2010,,,,,,,,,,"0|i0g3qn:",92036,,,,,Normal,,,,,,,,,,,,,,,,,"14/Jul/10 08:05;arya;Hi Gary,

I was able to reproduce this using today's nightly build. This time i used a smaller data set (500000 keys) and I got the following:

[agoudarzi@cas-test3 scripts]$ nodetool --host 10.50.26.132 ring   
Address         Status State   Load            Token                                       
                                       160348796167900510561059505917619274541    
10.50.26.134    Up     Normal  116.98 MB       32717880524093094169411234083126184860      
10.50.26.132    Up     Leaving 58.58 MB        75101027859180840627831025901565139619      
10.50.26.133    Up     Normal  117.09 MB       160348796167900510561059505917619274541    

[agoudarzi@cas-test3 scripts]$ nodetool --host 10.50.26.132 streams
Mode: Leaving: streaming data to other nodes
Streaming to: /10.50.26.133
   /var/lib/cassandra/data/Keyspace1/Standard1-d-17-Data.db/[(0,54080834)]
Not receiving any streams.
[agoudarzi@cas-test3 scripts]$ nodetool --host 10.50.26.133 streams
Mode: Normal
Not sending any streams.
Not receiving any streams.

From the logs of 10.50.26.132 it seams that it tried to tell 10.50.26.133 to claim its stream:

INFO [STREAM-STAGE:1] 2010-07-13 16:50:35,994 StreamOut.java (line 135) Sending a stream initiate message to /10.50.26.133 ...
INFO [STREAM-STAGE:1] 2010-07-13 16:50:35,994 StreamOut.java (line 140) Waiting for transfer to /10.50.26.133 to complete

But nothing in 133's log acknowledges the receipt of the request from 132 and as you see above it shows that it is getting no streams and this has been going for the past hour or so.

-Arya

;;;","15/Jul/10 05:50;gdusbabek;Arya,  can you supply the nodetool commands you are using that constitute ""cleanup""?  I've tried a few times now and can't get the failure you describe.  In your latest test was .132 the second or third node booted?;;;","15/Jul/10 06:55;arya;132 is node1
133 is node2
134 is node3

Give me some time and I'll regenerate all the commands for you in details. ;;;","15/Jul/10 08:20;arya;Gary, I missed one thing (Step 9-13) where I took a node down and insert. I tested this without step 9-13 and loadbalance worked. Not sure why step 9-13 changes everything. Here are the full production steps (I am also attaching the logs from all 3 nodes if it helps):

This is the ring topology I discuss here:

Node1 10.50.26.132 (Hostname: cas-test1)
Node2 10.50.26.133 (Hostname: cas-test2)
Node3 10.50.26.134 (Hostname: cas-test3)

This run is using today's nightly built from a clean setup.

Step 1: Startup Node1

[agoudarzi@cas-test1 ~]$ sudo /etc/init.d/cassandra start

Step 2: LoadSchemadFromYAML

I go to JConsole and call the function from o.a.c.service StorageService MBeans

Step 3: I insert 500000 keys into Standard1 CF using py_stress

$ python stress.py --num-keys 500000 --threads 8 --nodes 10.50.26.132 --keep-going --operation insert
Keyspace already exists.
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
62455,6245,6245,0.00128478354559,10
121893,5943,5943,0.00134767375398,21
184298,6240,6240,0.00128336335573,31
248124,6382,6382,0.00124898537112,42
297205,4908,4908,0.00163303957852,52
340338,4313,4313,0.00189026848124,63
380233,3989,3989,0.00203818801591,73
444452,6421,6421,0.00124198496903,84
500000,5554,5554,0.00114441244599,93

Step 3: Let's Take a Look at Ring

[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.132 ring
Address         Status State   Load            Token                                       
10.50.26.132    Up     Normal  206.05 MB       139380634429053457983268837561452509806     

Step 4: Bootstrap Node 2 into cluster

[agoudarzi@cas-test2 ~]$ sudo /etc/init.d/cassandra start

Step 5: Check the ring
[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.132 ring
Address         Status State   Load            Token                                       
                                       139380634429053457983268837561452509806    
10.50.26.133    Up     Joining 5.84 KB         54081521187303805945240848606999860232      
10.50.26.132    Up     Normal  206.05 MB       139380634429053457983268837561452509806     

Step 6: Check the streams on Node 1

[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.132 streams
Mode: Normal
Streaming to: /10.50.26.133
   /var/lib/cassandra/data/Keyspace1/Standard1-d-5-Data.db/[(0,34658183), (89260032,109057810)]
   /var/lib/cassandra/data/Keyspace1/Standard1-d-6-Data.db/[(0,8746823), (22272929,27264363)]
   /var/lib/cassandra/data/Keyspace1/Standard1-d-8-Data.db/[(0,8749389), (22336617,27264253)]
   /var/lib/cassandra/data/Keyspace1/Standard1-d-9-Data.db/[(0,8235190), (21174782,25822054)]
   /var/lib/cassandra/data/Keyspace1/Standard1-d-7-Data.db/[(0,8642472), (22333239,27264347)]
Not receiving any streams.

Step 7: Check the ring from Node1 and Node2 and make sure they agree

[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.132 ring
Address         Status State   Load            Token                                       
                                       139380634429053457983268837561452509806    
10.50.26.133    Up     Normal  116.94 MB       54081521187303805945240848606999860232      
10.50.26.132    Up     Normal  233.93 MB       139380634429053457983268837561452509806     
[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.133 ring
Address         Status State   Load            Token                                       
                                       139380634429053457983268837561452509806    
10.50.26.133    Up     Normal  116.94 MB       54081521187303805945240848606999860232      
10.50.26.132    Up     Normal  233.93 MB       139380634429053457983268837561452509806

Step 8: Cleanup Node1

[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.132 cleanup

Step 9: Check the ring agreement again

[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.132 ring
Address         Status State   Load            Token                                       
                                       139380634429053457983268837561452509806    
10.50.26.133    Up     Normal  116.94 MB       54081521187303805945240848606999860232      
10.50.26.132    Up     Normal  117 MB          139380634429053457983268837561452509806     
[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.133 ring
Address         Status State   Load            Token                                       
                                       139380634429053457983268837561452509806    
10.50.26.133    Up     Normal  116.94 MB       54081521187303805945240848606999860232      
10.50.26.132    Up     Normal  117 MB          139380634429053457983268837561452509806     

Step 9: Let's kill Node 2

[agoudarzi@cas-test2 ~]$ sudo /etc/init.d/cassandra stop 

Step 10: Check the ring on Node 1

[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.132 ring
Address         Status State   Load            Token                                       
                                       139380634429053457983268837561452509806    
10.50.26.133    Down   Normal  116.94 MB       54081521187303805945240848606999860232      
10.50.26.132    Up     Normal  117 MB          139380634429053457983268837561452509806

Step 11: Let's try to insert 500000 more keys expecting lots of unavailable exceptions ad the only replica for some keys is dead and py_stress does not use CLevel.ANY or ZERO

$ python stress.py --num-keys 500000 --threads 8 --nodes 10.50.26.132 --keep-going --operation insert

Keyspace already exists.
UnavailableException()
UnavailableException()
UnavailableException()
....
...
..
.
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
500000,2922,2922,0.000816067281446,67

Step 12: Check the ring

[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.132 ring
Address         Status State   Load            Token                                       
                                       139380634429053457983268837561452509806    
10.50.26.133    Down   Normal  116.94 MB       54081521187303805945240848606999860232      
10.50.26.132    Up     Normal  205.35 MB       139380634429053457983268837561452509806     

Node 1 got more data as expected

Step 13: Bring up Node 2 again

[agoudarzi@cas-test2 ~]$ sudo /etc/init.d/cassandra start

Step 14: Check the Ring

[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.132 ring
Address         Status State   Load            Token                                       
                                       139380634429053457983268837561452509806    
10.50.26.133    Up     Normal  116.94 MB       54081521187303805945240848606999860232      
10.50.26.132    Up     Normal  205.35 MB       139380634429053457983268837561452509806     
[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.133 ring
Address         Status State   Load            Token                                       
                                       139380634429053457983268837561452509806    
10.50.26.133    Up     Normal  116.94 MB       54081521187303805945240848606999860232      
10.50.26.132    Up     Normal  205.35 MB       139380634429053457983268837561452509806

Step 15: Bootstrap Node 3

[agoudarzi@cas-test3 ~]$ sudo /etc/init.d/cassandra start

Step 11: Check Ring 

[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.132 ring
Address         Status State   Load            Token                                       
                                       139380634429053457983268837561452509806    
10.50.26.133    Up     Normal  116.94 MB       54081521187303805945240848606999860232      
10.50.26.134    Up     Normal  116.68 MB       96565427321648203609592911083606603165      
10.50.26.132    Up     Normal  234 MB          139380634429053457983268837561452509806     
[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.133 ring
Address         Status State   Load            Token                                       
                                       139380634429053457983268837561452509806    
10.50.26.133    Up     Normal  116.94 MB       54081521187303805945240848606999860232      
10.50.26.134    Up     Normal  116.68 MB       96565427321648203609592911083606603165      
10.50.26.132    Up     Normal  234 MB          139380634429053457983268837561452509806     
[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.134 ring
Address         Status State   Load            Token                                       
                                       139380634429053457983268837561452509806    
10.50.26.133    Up     Normal  116.94 MB       54081521187303805945240848606999860232      
10.50.26.134    Up     Normal  116.68 MB       96565427321648203609592911083606603165      
10.50.26.132    Up     Normal  234 MB          139380634429053457983268837561452509806     

Step 12: Cleanup Node 1 (132)

[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.132 cleanup

Step 13: Check Ring

[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.132 ring
Address         Status State   Load            Token                                       
                                       139380634429053457983268837561452509806    
10.50.26.133    Up     Normal  116.94 MB       54081521187303805945240848606999860232      
10.50.26.134    Up     Normal  116.68 MB       96565427321648203609592911083606603165      
10.50.26.132    Up     Normal  58.89 MB        139380634429053457983268837561452509806     
[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.133 ring
Address         Status State   Load            Token                                       
                                       139380634429053457983268837561452509806    
10.50.26.133    Up     Normal  116.94 MB       54081521187303805945240848606999860232      
10.50.26.134    Up     Normal  116.68 MB       96565427321648203609592911083606603165      
10.50.26.132    Up     Normal  58.89 MB        139380634429053457983268837561452509806     
[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.134 ring
Address         Status State   Load            Token                                       
                                       139380634429053457983268837561452509806    
10.50.26.133    Up     Normal  116.94 MB       54081521187303805945240848606999860232      
10.50.26.134    Up     Normal  116.68 MB       96565427321648203609592911083606603165      
10.50.26.132    Up     Normal  58.89 MB        139380634429053457983268837561452509806     

Looks as expected. Node 1 (132) has the least load. Let's loadbalance it.

Step 14: Loadbalance Node 1

[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.132 loadbalance &
[1] 27457

Step 15: Check Ring

[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.132 ring
Address         Status State   Load            Token                                       
                                       139380634429053457983268837561452509806    
10.50.26.133    Up     Normal  116.94 MB       54081521187303805945240848606999860232      
10.50.26.134    Up     Normal  116.68 MB       96565427321648203609592911083606603165      
10.50.26.132    Up     Leaving 58.89 MB        139380634429053457983268837561452509806     
[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.133 ring
Address         Status State   Load            Token                                       
                                       139380634429053457983268837561452509806    
10.50.26.133    Up     Normal  116.94 MB       54081521187303805945240848606999860232      
10.50.26.134    Up     Normal  116.68 MB       96565427321648203609592911083606603165      
10.50.26.132    Up     Leaving 58.89 MB        139380634429053457983268837561452509806     
[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.134 ring
Address         Status State   Load            Token                                       
                                       139380634429053457983268837561452509806    
10.50.26.133    Up     Normal  116.94 MB       54081521187303805945240848606999860232      
10.50.26.134    Up     Normal  116.68 MB       96565427321648203609592911083606603165      
10.50.26.132    Up     Leaving 58.89 MB        139380634429053457983268837561452509806     

Step 16: Check the Streams

[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.132 streams
Mode: Leaving: streaming data to other nodes
Streaming to: /10.50.26.133
   /var/lib/cassandra/data/Keyspace1/Standard1-d-17-Data.db/[(0,54278564)]
Not receiving any streams.
[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.133 streams
Mode: Normal
Not sending any streams.
Not receiving any streams.

PROBLEM:
Notice 132 says I am streaming to 133 but 133 says ""Not receiving any streams!""  


;;;","15/Jul/10 08:24;arya;Cassandra System Logs for node 1-3;;;","16/Jul/10 04:30;gdusbabek;Thanks Arya. I can reproduce this.  Now just to fix it.;;;","20/Jul/10 19:48;gdusbabek;Several problems on this ticket.
1. MessaingService implemented IFailureDetector and was in charge of shutting down TCP connections during a partition.  However, it was never added to the listeners in FD.  This meant that MS.convict() was never getting called.
2. Under the right conditions (I still don't understand this fully), java sockets still give every indication they are connected even though the host on the other end is down.  A single write will succeed, even though no bytes are sent.  Seriously.  In our case the single write was a StreamInitiateMessage that we then wait forever to be acked the the [dead] remote host.

My solution was to use Gossiper.convict, which calls SS.onDead, to call MS.convict().  I don't think it makes sense to have MS implement IFailureDetector since we treat Gossiper as authoritative with respect to node alive-ness.

I'll spend some time checking this morning, but I suspect we have the same situation in 0.6.;;;","20/Jul/10 23:25;gdusbabek;patch for 0.6.  I couldn't get stress.py to work in my branch, but the same problem should be present.  All tests pass with this patch.;;;","21/Jul/10 00:17;jbellis;+1

(but fix brace placement in onDead please);;;","21/Jul/10 20:50;hudson;Integrated in Cassandra #496 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/496/])
    failure detection wasn't closing sockets. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1221
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SlicePredicate doesn't support the java.lang.Object.equals() method contract,CASSANDRA-1775,12480813,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Duplicate,,igor.demydenko,igor.demydenko,24/Nov/10 23:44,16/Apr/19 17:33,22/Mar/23 14:57,25/Nov/10 06:18,,,Legacy/CQL,,0,,,,,,"SlicePredicate doesn't support the java.lang.Object.equals() method contract.

Execute following test:
{code:title=SlicePredicateTest.java}
public class SlicePredicateTest {
    private static final String ENCODING = ""UTF-8"";
    private static final String COLUMN_NAME = ""ColumnName"";

    @Test
    public void testEquals() throws Exception {

        SlicePredicate predicate = new SlicePredicate();
        predicate.setColumn_names(Arrays.asList(COLUMN_NAME.getBytes(ENCODING)));

        SlicePredicate predicate2 = new SlicePredicate();
        predicate2.setColumn_names(Arrays.asList(COLUMN_NAME.getBytes(ENCODING)));

        SlicePredicate predicate3 = new SlicePredicate();
        predicate3.setColumn_names(Arrays.asList(COLUMN_NAME.getBytes(ENCODING)));

        assertFalse(predicate.equals(null)); // not null
        assertTrue(predicate.equals(predicate)); // reflexivity
        assertTrue(predicate.equals(predicate2) && predicate2.equals(predicate)); // symmetry
        assertTrue(predicate.equals(predicate2) && predicate2.equals(predicate3) 
                && predicate.equals(predicate3)); // transitivity
    }
}
{code}","apcahe cassandra 0.6.6
lib thrift r917130
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20307,,,Wed Nov 24 22:18:59 UTC 2010,,,,,,,,,,"0|i0g7bj:",92616,,,,,Low,,,,,,,,,,,,,,,,,"25/Nov/10 00:16;jbellis;do we ever actually call .equals on SlicePredicate?;;;","25/Nov/10 05:08;igor.demydenko;Yes, we can.
For example:
In unit tests, when Cassandra.Client is mocked by EasyMock and we expect the SlicePredicate, that isn't created in test method (it is created in tested method).

Also it affects the hashCode() too.
SlicePredicate is used in Deletion,
Deletion is used in Mutation.
So if we want to accumulate Mutation into the HashSet (e.g. to filter reiterate Mutations).
In the following code we have created 3 Deletion (Mutation) for the same column and have added they to the HashSet().
The expected Set size must be 1 (because it is the same (equals) operations), but actual result is 3.
IMHO it looks strange.
 
{code}
        SlicePredicate predicate = new SlicePredicate();
        predicate.setColumn_names(Arrays.asList(COLUMN_NAME.getBytes(ENCODING)));
        Deletion deletion = new Deletion();
        deletion.setPredicate(predicate);
        deletion.setTimestamp(timestamp);
        Mutation mutation = new Mutation();
        mutation.setDeletion(deletion);

        SlicePredicate predicate2 = new SlicePredicate();
        predicate2.setColumn_names(Arrays.asList(COLUMN_NAME.getBytes(ENCODING)));
        Deletion deletion2 = new Deletion();
        deletion2.setPredicate(predicate2);
        deletion2.setTimestamp(timestamp);
        Mutation mutation2 = new Mutation();
        mutation2.setDeletion(deletion2);

        
        SlicePredicate predicate3 = new SlicePredicate();
        predicate3.setColumn_names(Arrays.asList(COLUMN_NAME.getBytes(ENCODING)));
        Deletion deletion3 = new Deletion();
        deletion3.setPredicate(predicate3);
        deletion3.setTimestamp(timestamp);
        Mutation mutation3 = new Mutation();
        mutation3.setDeletion(deletion3);

        Set<Mutation> mutations = new HashSet<Mutation>(Arrays.asList(mutation, mutation2, mutation3));
        assertEquals(mutations.size(), 1);
{code};;;","25/Nov/10 05:31;jbellis;I think this is caused by THRIFT-226, so it should be fixed in Cassandra 0.7 beta3 where we upgraded to Thrift 0.5 release.;;;","25/Nov/10 05:41;igor.demydenko;Thanks,
Looks like truth :) 
I haven't found this issue, so I have created the new issue.

Can I close the bug?

Thanks;;;","25/Nov/10 06:18;jbellis;Closed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StringTokenizer throws NoSuchElementException,CASSANDRA-486,12437926,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,elubow,elubow,13/Oct/09 09:40,16/Apr/19 17:33,22/Mar/23 14:57,13/Oct/09 23:48,0.4,,,,0,,,,,,"---- cf format ----
<ColumnFamily CompareWith=""UTF8Type"" Name=""Users"" />
    'bar@baz.com': {
        email: 'bar@baz.com',
        person_id: '789',
        send_dates_2009-09-30: '2245',
        send_dates_2009-10-01: '2246',
    },
---- Relevant log lines ----
DEBUG - insertBlocking writing key schmidtcaroline71@yahoo.com to 3489502@[127.0.0.1:7000]
DEBUG - Applying RowMutation(table='Mailings', key='schmidtcaroline71@yahoo.com', modifications=[ColumnFamily(Users [email,person_id,send_dates_2009-09-30,])])
DEBUG - RowMutation(table='Mailings', key='schmidtcaroline71@yahoo.com', modifications=[ColumnFamily(User/Users-46-Data.db   : 914432
DEBUG - index size for bloom filter calc for file  : /nfsassets/mailings/db_tests/cassandra/data/Mailings/Users-48-Data.db   : 949632
DEBUG - index size for bloom filter calc for file  : /nfsassets/mailings/db_tests/cassandra/data/Mailings/Users-50-Data.db   : 984832
DEBUG - index size for bloom filter calc for file  : /nfsassets/mailings/db_tests/cassandra/data/Mailings/Users-52-Data.db   : 1020032
DEBUG - index size for bloom filter calc for file  : /nfsassets/mailings/db_tests/cassandra/data/Mailings/Users-54-Data.db   : 1055232
DEBUG - index size for bloom filter calc for file  : /nfsassets/mailings/db_tests/cassandra/data/Mailings/Users-56-Data.db   : 1090432
DEBUG - index size for bloom filter calc for file  : /nfsassets/mailings/db_tests/cassandra/data/Mailings/Users-58-Data.db   : 1125632
DEBUG - index size for bloom filter calc for file  : /nfsassets/mailings/db_tests/cassandra/data/Mailings/Users-60-Data.db   : 1160832
DEBUG - index size for bloom filter calc for file  : /nfsassets/mailings/db_tests/cassandra/data/Mailings/Users-62-Data.db   : 1196032
DEBUG - index size for bloom filter calc for file  : /nfsassets/mailings/db_tests/cassandra/data/Mailings/Users-64-Data.db   : 1231232
DEBUG - Expected bloom filter size : 1231232
ERROR - Error in executor futuretask
java.util.concurrent.ExecutionException: java.util.NoSuchElementException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
        at java.util.concurrent.FutureTask.get(FutureTask.java:111)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.logFutureExceptions(DebuggableThreadPoolExecutor.java:95)
        at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor.afterExecute(DebuggableScheduledThreadPoolExecutor.java:50)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1118)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.util.NoSuchElementException
        at java.util.StringTokenizer.nextToken(StringTokenizer.java:349)
        at org.apache.cassandra.dht.RandomPartitioner$1.compare(RandomPartitioner.java:56)
        at org.apache.cassandra.dht.RandomPartitioner$1.compare(RandomPartitioner.java:40)
        at org.apache.cassandra.io.FileStruct.compareTo(FileStruct.java:80)
        at org.apache.cassandra.io.FileStruct.compareTo(FileStruct.java:33)
        at java.util.PriorityQueue.siftUpComparable(PriorityQueue.java:599)
        at java.util.PriorityQueue.siftUp(PriorityQueue.java:591)
        at java.util.PriorityQueue.offer(PriorityQueue.java:291)
        at java.util.PriorityQueue.add(PriorityQueue.java:268)
        at org.apache.cassandra.db.ColumnFamilyStore.doFileCompaction(ColumnFamilyStore.java:1113)
        at org.apache.cassandra.db.ColumnFamilyStore.doCompaction(ColumnFamilyStore.java:689)
        at org.apache.cassandra.db.MinorCompactionManager$1.call(MinorCompactionManager.java:165)
        at org.apache.cassandra.db.MinorCompactionManager$1.call(MinorCompactionManager.java:162)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:165)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        ... 2 more
","[root@db5 ~]# java -version
java version ""1.6.0""
OpenJDK  Runtime Environment (build 1.6.0-b09)
OpenJDK 64-Bit Server VM (build 1.6.0-b09, mixed mode)
[root@db5 ~]# uname -a
Linux db5.shermanstravelmedia.com 2.6.18-92.1.18.el5 #1 SMP Wed Nov 12 09:19:49 EST 2008 x86_64 x86_64 x86_64 GNU/Linux",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Oct/09 10:51;jbellis;486.patch;https://issues.apache.org/jira/secure/attachment/12421944/486.patch","13/Oct/09 09:53;elubow;master.txt;https://issues.apache.org/jira/secure/attachment/12421937/master.txt","13/Oct/09 09:53;elubow;part_cas_load.pl;https://issues.apache.org/jira/secure/attachment/12421938/part_cas_load.pl",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19715,,,Tue Oct 13 15:48:03 UTC 2009,,,,,,,,,,"0|i0fz87:",91305,,,,,Low,,,,,,,,,,,,,,,,,"13/Oct/09 09:53;elubow;The master.txt file is an example of the master.txt files that are normally used.  They generally have between 3 and 4 million lines all in that format.  A new one is generated every day.  The part_cas_load.pl Perl script loads up the master.txt line by line and creates an entry per user.;;;","13/Oct/09 10:51;jbellis;patch that prevents empty keys from being inserted, which is the root cause of the problem;;;","13/Oct/09 23:48;jbellis;committed to 0.4 and trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE aborts streaming operations for keyspaces with hyphens ('-') in their names,CASSANDRA-1377,12471303,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,gdusbabek,bhoyt,bhoyt,11/Aug/10 22:54,16/Apr/19 17:33,22/Mar/23 14:57,13/Aug/10 05:40,0.6.5,0.7 beta 2,,,0,,,,,,"When streaming starts for operations such as repair or bootstrap, it will fail due to an NPE if they rows are in a keyspace that has a hyphen in its name.  One workaround for this issue would be to not use keyspace names containing hyphens.  It would be even nicer if streaming worked for keyspace names with hyphens, since keyspaces named like that seem to be fine in all other ways.

To reproduce:
 1. With a multi-node ring, load up a keyspace with a hyphen in its name
 2. Add some data to that keyspace
 3. nodetool repair

Expected results:
Repair operations complete normally

Actual results:
Repair operations don't complete normally.  The stacktrace below is correlated with the repair request.  

 INFO [AE-SERVICE-STAGE:1] 2010-06-30 14:11:29,744 AntiEntropyService.java (line 619) Performing streaming repair of 1 ranges to /10.255.0.20 for (my-keyspace,AColumnFamily)
ERROR [MESSAGE-DESERIALIZER-POOL:1] 2010-06-30 14:11:30,034 DebuggableThreadPoolExecutor.java (line 101) Error in ThreadPoolExecutor
java.lang.NullPointerException
        at org.apache.cassandra.streaming.StreamInitiateVerbHandler.getNewNames(StreamInitiateVerbHandler.java:154)
        at org.apache.cassandra.streaming.StreamInitiateVerbHandler.doVerb(StreamInitiateVerbHandler.java:76)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:40)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)",,eonnen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Aug/10 20:59;gdusbabek;1377-0.6-check-for-dashes.txt;https://issues.apache.org/jira/secure/attachment/12452925/1377-0.6-check-for-dashes.txt","13/Aug/10 01:03;gdusbabek;1377-0.6.txt;https://issues.apache.org/jira/secure/attachment/12451925/1377-0.6.txt","13/Aug/10 01:02;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-disallow-invalid-ks-cf-names.txt;https://issues.apache.org/jira/secure/attachment/12451923/ASF.LICENSE.NOT.GRANTED--v1-0001-disallow-invalid-ks-cf-names.txt","24/Aug/10 11:02;eonnen;CAS-1377-1.patch;https://issues.apache.org/jira/secure/attachment/12452887/CAS-1377-1.patch","24/Aug/10 11:42;eonnen;CAS-1377-2.patch;https://issues.apache.org/jira/secure/attachment/12452889/CAS-1377-2.patch","24/Aug/10 05:51;eonnen;CAS-1377.patch;https://issues.apache.org/jira/secure/attachment/12452863/CAS-1377.patch",,,,,,,,,6.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20110,,,Tue Aug 24 13:40:09 UTC 2010,,,,,,,,,,"0|i0g4on:",92189,,,,,Normal,,,,,,,,,,,,,,,,,"12/Aug/10 10:40;gdusbabek;We currently disallow hyphens in CF names.  It sounds like we need to disallow it in KS names too.;;;","12/Aug/10 13:15;jbellis;I vote for disallowing everything but \w characters - [a-zA-Z0-9_] ;;;","13/Aug/10 01:03;gdusbabek;0001 is for trunk, 1377-0.6 is for 0.6.;;;","13/Aug/10 02:36;jbellis;+1;;;","14/Aug/10 20:48;hudson;Integrated in Cassandra #514 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/514/])
    disallow invalid ks+cf names. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1377
;;;","21/Aug/10 19:14;hudson;Integrated in Cassandra #518 (See [https://hudson.apache.org/hudson/job/Cassandra/518/])
    CHANGES.txt and NEWS.txt update explaining the ramifications of CASSANDRA-1377
;;;","24/Aug/10 03:00;gdusbabek;eonnen in #cassandra has made a compelling argument that committing this to 0.6 breaks some backwards compatibility.  I am inclined to agree if the amount of work to handle dashes during streaming is trivial.;;;","24/Aug/10 03:11;jbellis;We really need some ""reserved"" characters in keyspace/CF names.  You can make a case that restricting them to \w is going too far in the other direction, but hyphens have always been reserved, and letting them pass in keyspaces was definitely a bug that was going to bite us (see: this issue).;;;","24/Aug/10 03:15;jbellis;(One of the reasons I'd like to restrict to \w is that makes us not have to deal with people reporting bugs from Thrift strings being utf8-encoded in some languages and not in others.);;;","24/Aug/10 05:48;eonnen;We have a multi-tenant deployment hosting multiple customers, each with multiple deployments of our upstream software (think test/prod). For each customer deployment, we've had a UUID identifying the instance since before the dawn of time, or at least since before Cassandra :)

Up until this change, using a keyspace-UUID mapping worked perfectly, especially after set_keyspace was added to the lower-level client API which allowed us to have pools for a given customer with different customers able to have different throughput to a point.

In hopes of getting this relaxed just a bit to allow ""-"", I've attached a fix for the bug in 0.6.0 tested with a keyspace name of ""e610eed7-c6be-449b-ad2c-562f35d75528"" which is a Type4 UUID. If you can broaden the limit here just a bit, we'll be good. I don't think it's all that unrealistic that users will want to have a keyspace correspond to real artifacts in their system (although I don't feel the same about CF names). This would at least broaden things just enough to allow UUIDs at that level.

While I understand the need to limit what goes into the name of the keyspace and why, it's too restrictive for my needs and I'll argue against it at least and try and patch my way out of it as long as you'll listen.

Happy to do the same for 0.7.0 if you're receptive.;;;","24/Aug/10 05:51;eonnen;Fixes NPE with ""-"" in Keyspace names, re-allows using them at the DatabaseDescriptor.;;;","24/Aug/10 10:08;jbellis;committed Erik's patch to 0.6 and trunk.

I still think we need to make some explicit rules about reserved characters but I agree that 0.6.5 is not the place to make that change.;;;","24/Aug/10 11:02;eonnen;This patch lightens the restriction on KS names to allow ""-"" in addition to \\w bringing in line the functionality w/ the 0.6 patch submitted earlier.;;;","24/Aug/10 11:05;eonnen;Thanks Jonathan. I added one additional patch to relax things slightly in 0.7 trunk. Tested with a nodetool loadbalance and nodetool move with a two node ring and both worked fine. Lots of changes in the streaming code between 0.6 and 0.7 and it looks like the broken code didn't move forward into 0.7 near as I can tell in reading though it.;;;","24/Aug/10 11:42;eonnen;Forgot to attach test in CAS-1377-1.patch;;;","24/Aug/10 20:59;gdusbabek;Puts hyphen check back into CF definition.;;;","24/Aug/10 21:37;jbellis;+1 hyphens-in-CF check;;;","24/Aug/10 21:40;gdusbabek;committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception while recovering commitlog when debug logging enabled,CASSANDRA-1274,12469178,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,mdennis,johanoskarsson,johanoskarsson,14/Jul/10 01:10,16/Apr/19 17:33,22/Mar/23 14:57,27/Jul/10 20:46,0.6.4,,,,0,,,,,,"On a cluster with debug logging enabled the commit log fails to recover on start. An UTF8 exception is thrown when trying to toString a column from the system column family LocationInfo. That CF is using UTF8Type but I suspect the column name in this specific case is a byte representation of an ip address, and as such not a valid UTF8 string. That column is most perhaps created in SystemTable line 74.

Full exception stack trace:
ERROR [main] 2010-07-13 11:03:17,050 AbstractCassandraDaemon.java (line 107) Exception encountered during startup.
org.apache.cassandra.db.marshal.MarshalException: invalid UTF8 bytes [10, -48, 40, -124]
        at org.apache.cassandra.db.marshal.UTF8Type.getString(UTF8Type.java:43)
        at org.apache.cassandra.db.Column.getString(Column.java:200)
        at org.apache.cassandra.db.marshal.AbstractType.getColumnsString(AbstractType.java:85)
        at org.apache.cassandra.db.ColumnFamily.toString(ColumnFamily.java:393)
        at org.apache.commons.lang.ObjectUtils.toString(ObjectUtils.java:241)
        at org.apache.commons.lang.StringUtils.join(StringUtils.java:3073)
        at org.apache.commons.lang.StringUtils.join(StringUtils.java:3133)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:250)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:171)
        at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:120)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:90)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:221)
",,mojodna,rschildmeijer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jul/10 13:38;mdennis;1274-0.6.patch;https://issues.apache.org/jira/secure/attachment/12450563/1274-0.6.patch","27/Jul/10 13:38;mdennis;1274-trunk.patch;https://issues.apache.org/jira/secure/attachment/12450564/1274-trunk.patch",,,,,,,,,,,,,2.0,mdennis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20055,,,Tue Aug 03 20:46:20 UTC 2010,,,,,,,,,,"0|i0g427:",92088,,,,,Low,,,,,,,,,,,,,,,,,"14/Jul/10 01:24;jbellis;in 0.6?;;;","14/Jul/10 01:41;johanoskarsson;This was in trunk from yesterday. It's a modified version, but none of that code should have been touched.;;;","24/Jul/10 09:38;brandon.williams;Jon Hermes presented a theory that you have to be using an IP that has at least one quad over 128, which won't be met by a localhost cluster.  I can reliably reproduce on a real cluster, even in 0.6.;;;","27/Jul/10 13:35;mdennis;Jon Hermes is correct, it has to be invalid UTF8 bytes to trigger this;;;","27/Jul/10 13:38;mdennis;one line patches to change it to BytesType

Once the value is in the log in .6 the node will need to be started witout DEBUG logging to clear out the hints after which the log level can be turned back to DEBUG.

;;;","27/Jul/10 20:46;jbellis;committed;;;","04/Aug/10 00:21;brandon.williams;Just clearing the CL won't help, because the HH cf already exists, so it's not recreated.  Then the problem continues to occur.;;;","04/Aug/10 00:38;jbellis;Isn't the HH CF ""created"" on server startup in DD?  Or did that change in 0.7 too?;;;","04/Aug/10 04:46;mdennis;The meta data is created statically in CfMetaData;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DatacenterEndPointSnitch does not work with RackAwareStrategy,CASSANDRA-988,12462209,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,erickt,erickt,16/Apr/10 12:48,16/Apr/19 17:33,22/Mar/23 14:57,17/Apr/10 01:46,,,,,0,,,,,,"RackAwareStrategy explicitly tests for the snitch to be a subclass of EndPointSnitch, but DatacenterEndPointSnitch doesn't subclass from it, but rather AbstractEndPointSnitch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Apr/10 12:52;erickt;ASF.LICENSE.NOT.GRANTED--0001-Rename-AbstractEndpointSnitch-to-AbstractEndPointSni.patch;https://issues.apache.org/jira/secure/attachment/12441919/ASF.LICENSE.NOT.GRANTED--0001-Rename-AbstractEndpointSnitch-to-AbstractEndPointSni.patch","16/Apr/10 12:52;erickt;ASF.LICENSE.NOT.GRANTED--0002-Fix-RackAwareStrategy-to-work-with-DatacenterEndPoin.patch;https://issues.apache.org/jira/secure/attachment/12441920/ASF.LICENSE.NOT.GRANTED--0002-Fix-RackAwareStrategy-to-work-with-DatacenterEndPoin.patch",,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19944,,,Fri Apr 16 17:46:58 UTC 2010,,,,,,,,,,"0|i0g2bb:",91805,,,,,Normal,,,,,,,,,,,,,,,,,"16/Apr/10 12:52;erickt;These depend on #984 being committed. The first patch is a simple rename of AbstractEndpointSnitch to AbstractEndPointSnitch to be consistent with the other locator files. The second changes the RackAwareStrategy to test for subclasses of AbstractEndPointSnitch instead of EndPointSnitch. Not sure if there's a more appropriate way to fix this though.;;;","16/Apr/10 13:08;erickt;It looks like things aren't completely fixed yet. I'm now getting this assertion on a couple of my nodes:

java.lang.AssertionError
	at org.apache.cassandra.locator.TokenMetadata.removeEndpoint(TokenMetadata.java:192)
	at org.apache.cassandra.locator.TokenMetadata.cloneAfterAllLeft(TokenMetadata.java:296)
	at org.apache.cassandra.service.StorageService.calculatePendingRanges(StorageService.java:725)
	at org.apache.cassandra.service.StorageService.calculatePendingRanges(StorageService.java:703)
	at org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:571)
	at org.apache.cassandra.service.StorageService.onChange(StorageService.java:514)
	at org.apache.cassandra.service.StorageService.onJoin(StorageService.java:881)
	at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:588)
	at org.apache.cassandra.gms.Gossiper.handleNewJoin(Gossiper.java:563)
	at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:630)
	at org.apache.cassandra.gms.Gossiper$GossipDigestAck2VerbHandler.doVerb(Gossiper.java:1016)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:41)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
;;;","16/Apr/10 13:26;erickt;That exception seems to be coming from a race condition. It occurred when I started the 10 nodes all at the same time. I repeated this while starting one node at a time, and they all came up cleanly.;;;","17/Apr/10 01:46;jbellis;Closing in favor of CASSANDRA-994 which describes changes that address this problem but cut a little deeper.

I think your TokenMetadata onJoin problem is separate from snitch activity and needs a separate ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
exceptions after cleanup,CASSANDRA-1922,12494314,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,31/Dec/10 01:29,16/Apr/19 17:33,22/Mar/23 14:57,31/Dec/10 02:20,0.7.0,,,,0,,,,,,"It looks like CASSANDRA-1916 may have introduced a regression.  After running a cleanup, I get the following exception when trying to read:

{noformat}

ERROR 17:25:23,574 Fatal exception in thread Thread[ReadStage:99,5,main]
java.lang.AssertionError: skipping negative bytes is illegal: -1393754107
        at org.apache.cassandra.io.util.MappedFileDataInput.skipBytes(MappedFileDataInput.java:96)
        at org.apache.cassandra.io.sstable.IndexHelper.skipBloomFilter(IndexHelper.java:50)
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.<init>(SimpleSliceReader.java:56)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.createReader(SSTableSliceIterator.java:91)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:67)
        at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:68)
        at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:80)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1215)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1107)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1077)
        at org.apache.cassandra.db.Table.getRow(Table.java:384)
        at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:63)
        at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:68)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:63)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Dec/10 02:00;jbellis;1922.txt;https://issues.apache.org/jira/secure/attachment/12467174/1922.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20371,,,Thu Dec 30 19:37:30 UTC 2010,,,,,,,,,,"0|i0g88f:",92764,,,,,Normal,,,,,,,,,,,,,,,,,"31/Dec/10 02:00;jbellis;Patch includes test to reproduce the problem and a (one-line) fix in CompactionManager.  Also includes comments on AbstractCompactionIterator as to what each method is supposed to include, to make it easier to avoid this mistake in the future.;;;","31/Dec/10 02:20;brandon.williams;Committed with unused import removed.;;;","31/Dec/10 03:37;hudson;Integrated in Cassandra-0.7 #137 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/137/])
    Fix CompactionManager regression from CASSANDRA-1916 and add a better
test and more docs to prevent in the future.
Patch by jbellis, reviewed by brandonwilliams for CASSANDRA-1922
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
system tests fail straingely if an instance is running,CASSANDRA-282,12429827,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,euphoria,urandom,urandom,09/Jul/09 01:10,16/Apr/19 17:33,22/Mar/23 14:57,10/Jul/09 01:40,0.4,,Legacy/Tools,,0,,,,,,"The system tests blow up spectacularly if there is another instance of cassandra running (and it can be difficult to tell that this is why). Having the harness start its instance using alternate ports would prevent this from happening in some cases, but something should be done to test for instances left over from failed tests, and at least produce clearer error messages.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jul/09 06:49;euphoria;282-v1.diff;https://issues.apache.org/jira/secure/attachment/12412931/282-v1.diff","10/Jul/09 01:06;urandom;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-282-stop-drop-roll-if-uncleanly-shutdown.txt;https://issues.apache.org/jira/secure/attachment/12413036/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-282-stop-drop-roll-if-uncleanly-shutdown.txt",,,,,,,,,,,,,2.0,euphoria,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19618,,,Fri Jul 10 12:35:56 UTC 2009,,,,,,,,,,"0|i0fxzb:",91103,,,,,Normal,,,,,,,,,,,,,,,,,"09/Jul/09 06:49;euphoria;This patch shifts all the relevant ports by 10, so they are still predictable.  It also sets the timeout for a new client connection to 10 seconds, because 20 seconds gets really unreasonable if you are trying to debug things, and I can't imagine the regression test server taking longer than 10 seconds to come up.;;;","10/Jul/09 01:09;urandom;+1 282-v1.diff

0001-CASSANDRA-282-stop-drop-roll-if-uncleanly-shutdown.txt is meant to be applied in addition to Michael's (282-v1.diff), and  causes the tests to fail with a (more )useful message if a previous run failed to cleanup.;;;","10/Jul/09 01:32;euphoria;+1 on 0001-CASSANDRA-282-stop-drop-roll-if-uncleanly-shutdown.txt since it actually caught the error it was supposed to on my system, even though I didn't expect it.;;;","10/Jul/09 01:40;urandom;committed.;;;","10/Jul/09 20:35;hudson;Integrated in Cassandra #133 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/133/])
    abort system tests if previous run shutdown uncleanly

Patch by eevans; reviewed by Michael Greene for 
move ports used in system tests; shorten timeout

Patch by Michael Greene; reviewed by eevans for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CassandraStorage for pig checks for environment variable on mappers/reducers, but it should only need to be set on the machine launching pig.",CASSANDRA-2310,12501053,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,eldondev,eldondev,11/Mar/11 03:14,16/Apr/19 17:33,22/Mar/23 14:57,11/Mar/11 04:47,0.7.4,,,,0,,,,,,"Only error out if necessary pig settings have not previously been set in job config. CassandraStorage checks for environment variables on mappers/reducers, but it should only need to be set on the machine launching the pig jobs.",,eldondev,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Mar/11 03:16;eldondev;0001-Dont-fail-if-configs-already-set.patch;https://issues.apache.org/jira/secure/attachment/12473312/0001-Dont-fail-if-configs-already-set.patch",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20553,,,Fri Mar 11 03:47:16 UTC 2011,,,,,,,,,,"0|i0gamf:",93151,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"11/Mar/11 03:15;eldondev;If the environment variable is set, it wins. If not, and it's already stored in the job configuration, do nothing. If it doesn't exist at all, fail.;;;","11/Mar/11 03:30;jeromatron;+1 - works where before it would error out in mapreduce mode (post storefunc). I did suggest maybe unifying the conditions and using an else or something to make it more concise, but that's a trivial thing.;;;","11/Mar/11 04:47;brandon.williams;Committed, thanks!;;;","11/Mar/11 11:47;hudson;Integrated in Cassandra-0.7 #373 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/373/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
multiple seeds (only when seed count = node count?) can cause cluster partition,CASSANDRA-150,12424825,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jaakko,jbellis,jbellis,08/May/09 00:13,16/Apr/19 17:33,22/Mar/23 14:57,26/Nov/09 10:16,0.5,,,,0,,,,,,happens fairly frequently on my test cluster of 5 nodes.  (i normally restart all nodes at once when updating the code.  haven't tested w/ restarting one machine at a time.),,cagatayk,johanoskarsson,mauzhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Nov/09 20:58;jaakko;150.patch;https://issues.apache.org/jira/secure/attachment/12426096/150.patch",,,,,,,,,,,,,,1.0,jaakko,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19569,,,Thu Nov 26 12:34:02 UTC 2009,,,,,,,,,,"0|i0fx6f:",90973,,,,,Low,,,,,,,,,,,,,,,,,"08/May/09 00:21;jbellis;daishi's ""Unable to find a live Endpoint we might be out of live nodes""  bug was almost certainly caused by the same thing.  (he has a 3-node cluster.)

for both of us switching to a single seed has fixed the issue for now.  (but ultimately we do want to support multiple seeds for redundancy.);;;","24/Nov/09 21:59;jaakko;Network partition may happen if (1) cluster size is at least four nodes, (2) all nodes are seeds and (3) at least two nodes boot ""simultaneously"".

Gossiping cycle works as follows:
(i) gossip to random live node
(ii) gossip to random unreachable node
(iii) if the node gossiped to at (i) was not seed, gossip to random seed

Suppose there are four nodes in the cluster: nodeA, nodeB, nodeC and nodeD, all of them seeds. Suppose they are all brought online at the same time. Following event sequence leads to partition:

(1) nodeA comes online. No live nodes (and no unreachable either, of course), so gossip to random seed. Let's suppose nodeA chooses nodeB. It sends nodeB gossip.
(2) nodeB gets nodeA's gossip and marks it live. It sends its own gossip, and since it has a live node (nodeA), it sends gossip according to gossip's first rule. nodeA is seed, so no gossip is sent to random seed at (iii).
(3) nodeC comes online. It has not seen other live nodes yet, so it will gossip to random seed. Let's suppose it chooses nodeD.
(4) nodeD comes online and sees nodeC's gossip. Since it now has a live node, it will send nodeC gossip according to the first rule. Since nodeC is seed, again no gossip is sent to random seed.

(there are other sequences as well, but basic idea is the same)

Now all nodes know of one live node, so they will always send gossip according to the first rule. Since this node is seed, they will never send gossip to random seed according to rule three. This will prevent them from finding rest of the cluster. One non-seed node will break this loop, as gossip sent to it will trigger gossip to random seed.

While investigating this, I noticed we might have caused some harm to scalability of gossip mechanism when we added two new application states for node movement. I'll fix this bug tomorrow when checking if there is a problem.
;;;","24/Nov/09 22:32;jbellis;That makes total sense.  Nice work!;;;","25/Nov/09 20:58;jaakko;Added extra condition to send gossip to random seed also if liveEndpoints.size + unreachableEndpoints.size is less than seeds.size. This will cause us to send same gossip to same seed twice occasionally (when the seed is live, but we have not yet seen enough nodes), but I think this is OK, as this is quite special case and will go away as soon as we've seen enough nodes.

Another option would be to add extra parameter excludeThisNode to sendGossip and not send gossip if random returns that address, but IMHO this option is messy and gains very little.
;;;","26/Nov/09 07:25;jbellis;I don't think this quite works -- e.g. the guy on the mailing list with 3 seeds in a 4 node cluster.  I think we have to make the check ""have we seen all the seeds yet"" rather than ""have we seen as many nodes as there are seeds"";;;","26/Nov/09 09:33;jaakko;This kind of partition cannot happen if there are less than four seeds or there is even a single non-seed node. There must be enough seeds to form at least two separate network closures of at least two seeds each. If there has been a problem with 3/4 cluster, it must be different from this as there are two preconditions that are not met.

Gossip rule #1 sends gossip to a live node and rule #3 sends to a random seed if the node in #1 was not seed. If there is even a single non-seed node, it will trigger gossip to a random seed every time gossip is sent to it. Eventually this will break the network closures. What the patch basically does is it aggressively searches for seeds as long as it has found at least as many nodes as there are seeds. It does not matter even if this does not include all seeds, as that means there are non-seeds in liveEndpoints, which triggers search for random seed every time gossip is sent to it. So basically this is just to help Gossiper to get started, not to find all seeds. Whether it finds all seeds or at least one non-seed does not matter, it can continue from there.

Now of course the ""correct"" checks for this condition would be to on each gossip round check (1) whether liveEndpoints and unreachableEndpoints include all seeds or (2) if liveEndpoints includes at least one non-seed. However, putting these checks on the normal execution path only for the sake of one special case does not appeal to me, so decided to add this simple check instead.

Now that I think of it, there is one extremely special case that still could cause a partition: cluster of 4 seeds and 2 non-seeds. First 2 seeds and 2 non-seeds come online -> everybody is happy as cluster size is the same as number of seeds. Now both seeds go down, and then the other two seeds come up. Again everybody is happy. Now suppose the two non-seeds go down, and after that the two original seeds come up simultaneously, and happen to choose each other from the list of random seeds. In this case all seeds will send gossip only to the other seed, as they have 2 nodes in unreachableEndpoint, which makes the total number of seen nodes equal number of seeds. To avoid this, we might relax the condition a bit and send gossip to a seed if number of liveEndpoints is less than seeds (that is, ignore unreachableEndpoints). This modification would take care of the scenario above, but don't know if it is worth the trouble. If either of the non-seeds recovers (or one of the seeds goes down), this deadlock will be broken.
;;;","26/Nov/09 09:50;jaakko;It might indeed be better to check only if liveEndpoints.size < seeds.size (do not count unreachableEndpoints). This will cause a bit more unnecessary gossip to seeds in some special cases, but is perhaps better approach. Have to think about this a bit still.;;;","26/Nov/09 10:12;jbellis;I see, so to make the less-strong check be enough when seeds.size <= live node count we reason that:

either all the live nodes are seeds, in which case non-seeds that come online will introduce themselves to a member of the ring by definition, and become known in turn,

or there is at least one non-seed node in the list, in which case eventually someone will gossip to it, and then do a gossip to a random seed from the existing clause in the if statement.

> It might indeed be better to check only if liveEndpoints.size < seeds.size

yes, let's go with this.  better to do a little extra gossiping in corner cases than risk indefinite partitions.;;;","26/Nov/09 10:16;jbellis;committed as described above;;;","26/Nov/09 20:34;hudson;Integrated in Cassandra #269 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/269/])
    send extra gossip to random seed as long as there are less nodes alive than seed nodes configured
patch by Jaakko Laine; reviewed by jbellis for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""java.net.ConnectException: Connection timed out"" in MESSAGE-STREAMING-POOL:1",CASSANDRA-1019,12462907,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,stuhood,btoddb,btoddb,24/Apr/10 05:12,16/Apr/19 17:33,22/Mar/23 14:57,27/May/10 03:00,0.6.3,0.7 beta 1,,,0,,,,,,"after doing a nodetool repair on a node in my cluster, i see the following exception on 4 out of the 7 nodes.  replication factor is 3.  no compactions happening.  no client traffic to the cluster.  nodetool streams (on one of the nodes not repaired) shows the following which is not ever increasing:

Mode: Normal
Streaming to: /192.168.132.117
   /data/cassandra-data/data/UdsProfiles/stream/UdsProfiles-43-Data.db 0/523088443
Not receiving any streams.


in addition those same four nodes all show AE-SERVICE-STAGE with pending
work, and been showing this for several hours now. each node in the
cluster has less than 2gb, so it should be finished by now.

here is the exception:

2010-04-23 10:08:43,416 ERROR [MESSAGE-STREAMING-POOL:1]
[DebuggableThreadPoolExecutor.java:101] Error in ThreadPoolExecutor
java.lang.RuntimeException: java.net.ConnectException: Connection timed out
at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:619)
Caused by: java.net.ConnectException: Connection timed out
at sun.nio.ch.Net.connect(Native Method)
at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:507)
at org.apache.cassandra.net.FileStreamTask.runMayThrow(FileStreamTask.java:60)
at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
... 3 more
2010-04-23 10:08:43,417 ERROR [MESSAGE-STREAMING-POOL:1]
[CassandraDaemon.java:78] Fatal exception in thread
Thread[MESSAGE-STREAMING-POOL:1,5,main]
java.lang.RuntimeException: java.net.ConnectException: Connection timed out
at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:619)
Caused by: java.net.ConnectException: Connection timed out
at sun.nio.ch.Net.connect(Native Method)
at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:507)
at org.apache.cassandra.net.FileStreamTask.runMayThrow(FileStreamTask.java:60)
at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
... 3 more

",,anty,gdusbabek,mojodna,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/May/10 04:50;stuhood;1019-for-0.6-0001-Add-exponentially-backed-off-retry-to-FileStreamTask.patch;https://issues.apache.org/jira/secure/attachment/12445382/1019-for-0.6-0001-Add-exponentially-backed-off-retry-to-FileStreamTask.patch","25/May/10 04:50;stuhood;1019-for-trunk-0001-Rename-CompletedFileStatus-to-FileStatus-to-indicate.patch;https://issues.apache.org/jira/secure/attachment/12445383/1019-for-trunk-0001-Rename-CompletedFileStatus-to-FileStatus-to-indicate.patch","25/May/10 04:50;stuhood;1019-for-trunk-0002-Rename-StreamCompletionHandler-to-FileStatusHandler-.patch;https://issues.apache.org/jira/secure/attachment/12445384/1019-for-trunk-0002-Rename-StreamCompletionHandler-to-FileStatusHandler-.patch","25/May/10 04:50;stuhood;1019-for-trunk-0003-Rename-StreamCompletionAction-to-Action-and-change-d.patch;https://issues.apache.org/jira/secure/attachment/12445385/1019-for-trunk-0003-Rename-StreamCompletionAction-to-Action-and-change-d.patch","25/May/10 04:52;stuhood;1019-for-trunk-0004-Add-exponentially-backed-off-retry-to-FileStreamTask.patch;https://issues.apache.org/jira/secure/attachment/12445386/1019-for-trunk-0004-Add-exponentially-backed-off-retry-to-FileStreamTask.patch",,,,,,,,,,5.0,stuhood,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19958,,,Thu May 27 12:47:28 UTC 2010,,,,,,,,,,"0|i0g2i7:",91836,,,,,Normal,,,,,,,,,,,,,,,,,"25/Apr/10 13:19;jbellis;Stu, could you add some ""retry N (3?  10?) times before aborting the stream"" logic?;;;","26/Apr/10 05:34;stuhood;I should be able to tackle this Tuesday or Wednesday.;;;","27/Apr/10 00:15;btoddb;some more info that may assist.  we have just purchased new machines for our test cluster and we are having lots of trouble with the NICs going down.  this causes an extremely long timeout situation and could have been the catalyst for this problem.

this situation does cause the cluster to behave very poorly because the connection takes several minutes to timeout.  this type of situation makes me want the ability to manually take a node out of the cluster and prevent nodes from gossiping to it.  is this something that has been talked about?;;;","25/May/10 04:50;stuhood;Adds retries with exponential backoff to the connect step in FileStreamTask.;;;","25/May/10 04:52;stuhood;...and a set that does the same thing for trunk. 1019-for-0.6-0001 should be the same as 1019-for-trunk-0004.

The rest of the set performs some cleanups in the Streaming package to give things more appropriate names and document them a little better.;;;","26/May/10 23:41;gdusbabek;+1 on the 0.6 change.  Re trunk: would it make sense to rename FileStatus.STREAM -> FileStatus.RESTREAM?;;;","27/May/10 00:12;stuhood;> Re trunk: would it make sense to rename FileStatus.STREAM -> FileStatus.RESTREAM?
I debated doing that, but since the FileStatus object exists for the lifetime of the transfer it needs to have an initial Action/status that indicates it is streaming.;;;","27/May/10 20:47;hudson;Integrated in Cassandra #447 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/447/])
    rename StreamCompletionAction to Action. patch by stuhood, reviwed by gdusbabek. CASSANDRA-1019
rename StreamCompletionHandler to FileStatusHandler. patch by stuhood, reviwed by gdusbabek. CASSANDRA-1019
rename CompletedFileStatus to FileStatus. patch by stuhood, reviwed by gdusbabek. CASSANDRA-1019
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hinted Handoff and schema race,CASSANDRA-2083,12497250,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,01/Feb/11 03:57,16/Apr/19 17:33,22/Mar/23 14:57,04/Feb/11 07:34,0.7.1,,,,0,,,,,,"If a node is down while a keyspace/cf is created and then data is inserted into the CF causing other nodes to hint, when the down node recovers it will lose some hints until the schema propagates:

{noformat}
ERROR 19:59:28,264 Error in row mutation
org.apache.cassandra.db.UnserializableColumnFamilyException: Couldn't find cfId=1000
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:117)
        at org.apache.cassandra.db.RowMutation$RowMutationSerializer.deserialize(RowMutation.java:377)
        at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:50)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:70)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
 INFO 19:59:28,356 Applying migration 28e2e7a4-2d74-11e0-9b6b-cdc89135952c
{noformat}",,gdusbabek,,,,,,,,,,,,,,,,,,,,,28800,28800,,0%,28800,28800,,,,,,,,,,,,,,,,,,,,"04/Feb/11 01:36;jbellis;2083-v3.txt;https://issues.apache.org/jira/secure/attachment/12470163/2083-v3.txt","04/Feb/11 04:07;brandon.williams;2083-v4.txt;https://issues.apache.org/jira/secure/attachment/12470180/2083-v4.txt","02/Feb/11 06:46;brandon.williams;2083.txt;https://issues.apache.org/jira/secure/attachment/12469992/2083.txt","03/Feb/11 05:10;brandon.williams;2083v2.txt;https://issues.apache.org/jira/secure/attachment/12470070/2083v2.txt",,,,,,,,,,,4.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20433,,,Fri Feb 04 00:04:40 UTC 2011,,,,,,,,,,"0|i0g97j:",92922,,gdusbabek,,gdusbabek,Low,,,,,,,,,,,,,,,,,"02/Feb/11 06:46;brandon.williams;Patch to wait up to RING_DELAY for schema alignment before delivering hints.  Also ensures that we keep schema up to date in gossip.;;;","02/Feb/11 22:25;jbellis;gossip propagation is laggy enough that we've traditionally used the special version rpc call for schema version checks instead.  we should probably keep doing that here.;;;","03/Feb/11 05:10;brandon.williams;v2 builds on v1, in that it keeps schema up to date in gossip.  In HHOM we first check if the schema matches in gossip so we can avoid the rpc in the common case of no schema change, then fall back to waiting up to RING_DELAY for agreement via rpc before delivering hints.;;;","03/Feb/11 05:18;jbellis;So...  now that I think about it more, maybe gossip lag is a GOOD thing here: w/ the RPC, everyone will start hammering the recovered nodes w/ replay at the same time.  W/ gossip it will likely be staggered.;;;","03/Feb/11 05:22;brandon.williams;Interesting point, although schema will probably match 99% of the time and they'll replay at the same time anyway.;;;","04/Feb/11 01:34;jbellis;v3 only uses gossip to check for agreement, and adds a random sleep from 0 to 60 seconds if schemas agree to stagger delivery a little.;;;","04/Feb/11 01:59;brandon.williams;Committed with the debug line indicating the check is in progress bumped to info.;;;","04/Feb/11 02:31;brandon.williams;Reverted.  This has problems, the least of which is passing tests.;;;","04/Feb/11 04:07;brandon.williams;v4 is similar to v3, but fixes migration announcement in gossip correctly by splitting the active announcement (via rpc) from the passive announcement (via gossip), increases logging to indicate it is sleeping to stagger the hints, and makes sure the host is still alive after the sleep before beginning delivery.;;;","04/Feb/11 04:16;hudson;Integrated in Cassandra-0.7 #243 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/243/])
    ;;;","04/Feb/11 07:28;gdusbabek;+1;;;","04/Feb/11 07:34;brandon.williams;Committed.;;;","04/Feb/11 08:04;hudson;Integrated in Cassandra-0.7 #245 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/245/])
    Fix race between HH and schema changes.
Patch by brandonwilliams, reviewed by gdusbabek for CASSANDRA-2083
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stress.java -k doesn't keep going,CASSANDRA-1973,12495398,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,brandon.williams,brandon.williams,13/Jan/11 02:27,16/Apr/19 17:33,22/Mar/23 14:57,19/Jan/11 04:22,0.7.1,,,,0,,,,,,"stress.java's -k option doesn't work correctly.  In the face of many errors, it ends up printing 'null' a bunch and then exiting.",,,,,,,,,,,,,,,,,,,,,,";16/Jan/11 21:25;xedin;3600",3600,0,3600,100%,3600,0,3600,,,,,,,,,,,,,,,,,,,"19/Jan/11 04:10;xedin;CASSANDRA-1973-v2.patch;https://issues.apache.org/jira/secure/attachment/12468677/CASSANDRA-1973-v2.patch","16/Jan/11 21:24;xedin;CASSANDRA-1973.patch;https://issues.apache.org/jira/secure/attachment/12468490/CASSANDRA-1973.patch",,,,,,,,,,,,,2.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20387,,,Tue Jan 18 21:28:25 UTC 2011,,,,,,,,,,"0|i0g8jb:",92813,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"13/Jan/11 05:28;xedin;Can't reproduce issue on the latest stress build it just prints Key XXX not found. and then exits after all -n keys are done.

Tried on empty db to read 100000 keys - standard and super (on both Standard and Super CF).

Can you please provide a better description how to reproduce this issue?;;;","13/Jan/11 05:30;brandon.williams;I was trying to insert a node to death.  As death approached, stress.java died.;;;","13/Jan/11 05:33;xedin;Ok gotcha, I will take a look at this. Thank you!;;;","19/Jan/11 03:48;brandon.williams;Better, but it looks like the exception isn't trickling up:

Error while inserting key 4686646 - null
Error while inserting key 4697766 - null
Error while inserting key 4297775 - null
Error while inserting key 0609162 - null
Error while inserting key 0003898 - null
Error while inserting key 2719911 - null
Error while inserting key 4308726 - null
Error while inserting key 4453292 - null
Error while inserting key 4203275 - null
Error while inserting key 2625669 - null
Error while inserting key 1342402 - null
Error while inserting key 1247940 - null

I'm pretty sure these were TimeoutExceptions so it'd be nice to have that reflected.;;;","19/Jan/11 03:51;xedin;I can make it write exception class too e.g, ""Error while inserting key 1247940 - (ExceptionClass): null"", what do you think?;;;","19/Jan/11 04:22;brandon.williams;v2 looks good, committed.  Thanks!;;;","19/Jan/11 05:28;hudson;Integrated in Cassandra-0.7 #172 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/172/])
    Improve stress.java error handling.
Patch by Pavel Yaskevich, reviewed by brandonwilliams for CASSANDRA-1973
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disallow KS definition with RF > # of nodes,CASSANDRA-1310,12469974,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,zznate,arya,arya,23/Jul/10 09:15,16/Apr/19 17:33,22/Mar/23 14:57,29/Jul/10 23:59,0.7 beta 1,,,,0,,,,,,"Cassandra 0.7 allows user to create Keyspaces with Replication Factor >  number of endpoints causing in java.lang.IllegalStateException: replication factor (2) exceeds number of endpoints (1) exception in nodetool and Internal Errors on Thrift making the node useless.

Steps to Reproduce:

From a clean setup of Cassandra:
1. Start a single node out of cluster of 3. This means my configuration has the other two nodes in the seeds list, but have not restarted them yet;
2. Use Thrift API (I am using PHP) and create a Keyspace with replication factor 2;
3. The command executes with no exception or error;
4. Now try writing to it, you will get TException with Internal Error message;
5. Try nodetool ring and you will get Exception:

Exception in thread ""main"" java.lang.IllegalStateException: replication factor (2) exceeds number of endpoints (1)
	at org.apache.cassandra.locator.RackUnawareStrategy.calculateNaturalEndpoints(RackUnawareStrategy.java:61)
	at org.apache.cassandra.locator.AbstractReplicationStrategy.getNaturalEndpoints(AbstractReplicationStrategy.java:87)
	at org.apache.cassandra.service.StorageService.constructRangeToEndpointMap(StorageService.java:536)
	at org.apache.cassandra.service.StorageService.getRangeToAddressMap(StorageService.java:522)
	at org.apache.cassandra.service.StorageService.getRangeToEndpointMap(StorageService.java:496)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:111)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:45)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:226)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:251)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:857)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:795)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1449)
	at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:90)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1284)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1382)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:807)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
	at sun.rmi.transport.Transport$1.run(Transport.java:177)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)

Expected:

1. Either step 3 should not let you create the KS with RF 2 and 1 node in ring, or there should be a peaceful way for Cassandra to recover from IllegalStateException and replicate once other nodes become available.","CentOS 5.1
Trunc July 22nd",zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Jul/10 06:49;zznate;1310-v2.txt;https://issues.apache.org/jira/secure/attachment/12450765/1310-v2.txt","27/Jul/10 02:02;zznate;trunk-1310.txt;https://issues.apache.org/jira/secure/attachment/12450497/trunk-1310.txt",,,,,,,,,,,,,2.0,zznate,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20074,,,Fri Jul 30 13:37:48 UTC 2010,,,,,,,,,,"0|i0g4a7:",92124,,,,,Normal,,,,,,,,,,,,,,,,,"26/Jul/10 16:34;zznate;Patch validates on RF being less than the number of nodes and that the KsDef.name and CsDef.keyspace match when providing a KsDef to system_add_keyspace on CassandraServer;;;","27/Jul/10 02:02;zznate;Removed thrift validation method as nothing else really needs to validate on KsDef ;;;","27/Jul/10 22:48;gdusbabek;I think we should allow the creation of keyspaces even when there aren't enough nodes to support the specified replication factor.  For example, it should be allowable for an operator to bring up a single node in a new cluster, define all the keyspaces and then start bringing up new nodes to populate the cluster.

Throwing IllegalStateException is a special case that should probably be handled in NodeTool (gently output the error string and die).;;;","28/Jul/10 00:32;zznate;That makes sense to me. I would like to have a check on CsDef.keyspace = KsDef.name though. I hit that in a test case and it cause a really obtuse NPE. ;;;","29/Jul/10 05:14;gdusbabek;Nate: feel free to resubmit the patch for keyspace checks.  Also, having nodetool print just the error (not the whole stack trace) might be more user-friendly.;;;","29/Jul/10 06:49;zznate;Just validates CsDef vs. KsDef have the same keyspace. Includes NodeCmd patch that prints an error msg instead of the stack trace.;;;","29/Jul/10 23:59;gdusbabek;committed.;;;","30/Jul/10 21:37;hudson;Integrated in Cassandra #504 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/504/])
    print a more friendly error when printRing gets IllegalState. Patch by Nate McCall, reviewed by Gary Dusbabek. CASSANDRA-1310
check for ks/cf keyspace name agreement. Patch by Nate McCall, reviewed by Gary Dusbabek. CASSANDRA-1310
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException in CacheWriter.saveCache(),CASSANDRA-2416,12503379,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,skamio,skamio,05/Apr/11 17:28,16/Apr/19 17:33,22/Mar/23 14:57,19/Apr/11 00:56,0.7.5,,,,0,,,,,,"I've seen NullPointerException of CacheWriter in our cluster (replication 3).

ERROR [CompactionExecutor:1] 2011-04-05 09:57:42,968 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.utils.ByteBufferUtil.writeWithLength(ByteBufferUtil.java:275)
        at org.apache.cassandra.io.sstable.CacheWriter.saveCache(CacheWriter.java:84)
        at org.apache.cassandra.db.CompactionManager$10.runMayThrow(CompactionManager.java:960)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 6 more
",linux,cburroughs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Apr/11 00:20;jbellis;2416-v2.txt;https://issues.apache.org/jira/secure/attachment/12476624/2416-v2.txt","09/Apr/11 05:23;jbellis;2416.txt;https://issues.apache.org/jira/secure/attachment/12475841/2416.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20610,,,Mon Apr 18 18:07:44 UTC 2011,,,,,,,,,,"0|i0gb8v:",93252,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"09/Apr/11 05:23;jbellis;Patch to avoid storing DecoratedKeys with null key (which are generated by getRangeSlice) in the key cache.;;;","09/Apr/11 05:41;slebresne;Why not do that in cacheKey directly to be sure we don't miss places ? ;;;","19/Apr/11 00:20;jbellis;Because attempting to cache a DK that is really just a token is a semantic error that we shouldn't just paper over.  v2 adds an assert to make that clear.;;;","19/Apr/11 00:46;slebresne;+1;;;","19/Apr/11 00:56;jbellis;committed;;;","19/Apr/11 02:07;hudson;Integrated in Cassandra-0.7 #439 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/439/])
    avoid caching token-only decoratedkeys
patch by jbellis; reviewed by slebresne for CASSANDRA-2416
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InvalidRequestException(why='') returned from system_add_keyspace when strategy_class not found,CASSANDRA-1556,12475384,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,amorton,amorton,amorton,29/Sep/10 15:50,16/Apr/19 17:33,22/Mar/23 14:57,07/Oct/10 02:47,0.7 beta 3,,Legacy/CQL,,0,,,,,,"In thrift/CassandraServer system_add_keyspace() the strategy_class string from the KsDef is used to load a class. The ClassNotFoundError is then caught and used to build an InvalidRequestException. If the strategy_class is missing or empty, the error returned to the client is 

(python)
InvalidRequestException: InvalidRequestException(why='')

or 

InvalidRequestException: InvalidRequestException(why='foo')",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Oct/10 10:20;amorton;1556-amorton.txt;https://issues.apache.org/jira/secure/attachment/12456463/1556-amorton.txt","02/Oct/10 05:29;jbellis;1556.txt;https://issues.apache.org/jira/secure/attachment/12456160/1556.txt",,,,,,,,,,,,,2.0,amorton,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20198,,,Tue Oct 12 14:03:56 UTC 2010,,,,,,,,,,"0|i0g5yf:",92395,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"02/Oct/10 05:29;jbellis;Odd that the CNFE would not include a message.  Does this patch work better?;;;","06/Oct/10 10:11;amorton;I noticed system_update_keyspace uses FBUtilities.<T>classForName() which prints out pretty error messages, e.g. 

InvalidRequestException(why=""Unable to find keyspace replication strategy class 'InvalidStrategyClass': is the CLASSPATH set correctly?"")

Have created a patch to use that in both the thrift and avro CassandraServer, will upload.;;;","06/Oct/10 10:20;amorton;modified CassandraServer for thrift and avro to use FBUtilities.classForName() to get the strategy class for system_add_keyspace, was already doing it for system_update_keyspace;;;","07/Oct/10 02:47;jbellis;committed, thanks!;;;","12/Oct/10 22:03;hudson;Integrated in Cassandra #563 (See [https://hudson.apache.org/hudson/job/Cassandra/563/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix RemoveSuperColumnTest,CASSANDRA-683,12445060,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Duplicate,,jbellis,jbellis,09/Jan/10 03:01,16/Apr/19 17:33,22/Mar/23 14:57,15/Jan/10 04:15,,,,,0,,,,,,test fails intermittently.  Probably it is a fragile test but may be a real bug.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19818,,,Thu Jan 14 20:15:06 UTC 2010,,,,,,,,,,"0|i0g0fr:",91501,,,,,Low,,,,,,,,,,,,,,,,,"15/Jan/10 04:15;jbellis;this is the same regression caused by CASSANDRA-658 fixed by patch 3 in CASSANDRA-689;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Startup can fail if DNS lookup fails for seed node,CASSANDRA-1697,12478906,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,appodictic,appodictic,03/Nov/10 00:21,16/Apr/19 17:33,22/Mar/23 14:57,03/Nov/10 01:13,0.6.7,0.7.0 rc 1,,,0,,,,,,"This might fall into one of those WONT FIX scenarios, since not many things are going to work well with flaky DNS. This has only happened to me once, but I someone might be interested in the stack trace. In this case cdbsd01.hadoop.pvt is one of my seed nodes.

{noformat}
 INFO [main] 2010-11-02 11:52:00,141 CLibrary.java (line 43) JNA not found. Native methods will be disabled.
 INFO [main] 2010-11-02 11:52:00,421 DatabaseDescriptor.java (line 246) DiskAccessMode ismmap, indexAccessMode is mmap
ERROR [main] 2010-11-02 11:52:25,591 CassandraDaemon.java (line 232) Exception encountered during startup.
java.lang.ExceptionInInitializerError
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:72)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:214)
Caused by: java.lang.RuntimeException: java.net.UnknownHostException: cdbsd01.hadoop.pvt
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:551)
	... 2 more
Caused by: java.net.UnknownHostException: cdbsd01.hadoop.pvt
	at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method)
	at java.net.InetAddress$1.lookupAllHostAddr(InetAddress.java:850)
	at java.net.InetAddress.getAddressFromNameService(InetAddress.java:1201)
	at java.net.InetAddress.getAllByName0(InetAddress.java:1154)
	at java.net.InetAddress.getAllByName(InetAddress.java:1084)
	at java.net.InetAddress.getAllByName(InetAddress.java:1020)
	at java.net.InetAddress.getByName(InetAddress.java:970)
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:540)
	... 2 more
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Nov/10 00:36;jbellis;1697.txt;https://issues.apache.org/jira/secure/attachment/12458646/1697.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20264,,,Tue Nov 02 17:13:26 UTC 2010,,,,,,,,,,"0|i0g6tz:",92537,,gdusbabek,,gdusbabek,Low,,,,,,,,,,,,,,,,,"03/Nov/10 00:31;jbellis;Yeah, sorry -- this is why we recommend using IP addresses instead of hostnames for seeds.;;;","03/Nov/10 00:36;jbellis;Patch to generate a more human-friendly error message attached.;;;","03/Nov/10 01:06;gdusbabek;+1;;;","03/Nov/10 01:13;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Debian source package is unavailable on www.apache.org/dist,CASSANDRA-2375,12502244,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,eyealike,eyealike,24/Mar/11 09:03,16/Apr/19 17:33,22/Mar/23 14:57,25/Mar/11 01:01,,,Packaging,,0,,,,,,"We are repackaging the Debian packages for in-house use with minor modifications. To do so we need the source package, but as of today I seem to be unable to install it. Apt-get fails with a 403 when trying to download the orig tarball: 

{noformat}
# apt-get source cassandra=0.6.12
Reading package lists...
Building dependency tree...
Reading state information...
NOTICE: 'cassandra' packaging is maintained in the 'Svn' version control system at:
https://svn.apache.org/repos/asf/cassandra/trunk
Need to get 8,433kB of source archives.
Get:1 http://www.apache.org/dist/cassandra/debian/ 06x/main cassandra 0.6.12 (dsc) [1,796B]
Err http://www.apache.org/dist/cassandra/debian/ 06x/main cassandra 0.6.12 (tar)
  403  Forbidden
Get:2 http://www.apache.org/dist/cassandra/debian/ 06x/main cassandra 0.6.12 (diff) [20B]
Failed to fetch http://www.apache.org/dist/cassandra/debian/pool/main/c/cassandra/cassandra_0.6.12.orig.tar.gz  403  Forbidden
Fetched 1,816B in 0s (43.8kB/s)
E: Failed to fetch some archives.
{noformat}

From looking at mirror indexes the orig. tarball seems to be symlinked to the release tarball in a different directory. Maybe Apache is configured to not follow symlinks?

0.7.x is probably also affected but I didn't check.",Linux vsp08 2.6.32-21-server #32-Ubuntu SMP Fri Apr 16 09:17:34 UTC 2010 x86_64 GNU/Linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20588,,,Thu Mar 24 17:01:55 UTC 2011,,,,,,,,,,"0|i0gb0n:",93215,,,,,Normal,,,,,,,,,,,,,,,,,"24/Mar/11 09:29;jbellis;see CASSANDRA-2370;;;","25/Mar/11 00:53;eyealike;Sorry not a dupe. CASSANDRA-2370 is about 

http://www.apache.org/dist/cassandra/debian/dists/unstable

being missing. This issue is about 

http://www.apache.org/dist/cassandra/debian/pool/main/c/cassandra/cassandra_0.6.12.orig.tar.gz

giving a 403 due to being a symlink.;;;","25/Mar/11 01:01;brandon.williams;Yes, the problem is symlinks with both.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some kinds of membership changes will still cause overcounts,CASSANDRA-1961,12495200,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,stuhood,stuhood,11/Jan/11 10:28,16/Apr/19 17:33,22/Mar/23 14:57,12/Jan/11 07:14,0.8 beta 1,,,,0,,,,,,"Assume replicas A, B, C, and a joining node D, where D is joining between A and B. The join process will remove C from the replica set, but C will still be holding counts for those replicas (unless cleanup is run, which we can't safely assume). If a second membership change occurs such that any of A, B or D leave the ring, C will be acting as a new member of the replica set, but it will still be holding its old counts.

The join will:
 * BOOTSTRAP - D will bootstrap from the nearest replica, possibly C (but not necessarily)

The leave will either: 
 * UNBOOTSTRAP - D will send to C
 * RESTORE_REPLICA_COUNT - since D is assumed dead, C will stream from the nearest replica

Only the AES stream task performs fixups of counters: in all other cases I think we assume that 'nodetool cleanup' has run, so it is possible to overcount.",,johanoskarsson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-1938,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20383,,,Tue Jan 11 21:02:24 UTC 2011,,,,,,,,,,"0|i0g8gn:",92801,,,,,Normal,,,,,,,,,,,,,,,,,"11/Jan/11 10:43;stuhood;CASSANDRA-1938 could potentially be a solution to this problem.;;;","12/Jan/11 05:02;slebresne;Yes, I do intend to fix this in CASSANDRA-1938 (by renewing the node uuid each time the node range grows).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hudson is failing every night on NameSortTests,CASSANDRA-1045,12463554,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,gdusbabek,gdusbabek,gdusbabek,03/May/10 20:51,16/Apr/19 17:33,22/Mar/23 14:57,03/May/10 21:33,0.7 beta 1,,,,0,hudson_can_bite_me,,,,,http://hudson.zones.apache.org/hudson/job/Cassandra/,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/May/10 20:58;gdusbabek;0001-test-timeout-to-60s.patch;https://issues.apache.org/jira/secure/attachment/12443462/0001-test-timeout-to-60s.patch",,,,,,,,,,,,,,1.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19972,,,Mon May 03 13:33:18 UTC 2010,,,,,,,,,,"0|i0g2nz:",91862,,,,,Normal,,,,,,,,,,,,,,,,,"03/May/10 20:58;gdusbabek;I went ahead and committed this in hopes that it solves the problem.

;;;","03/May/10 21:33;gdusbabek;Dupes CASSANDRA-1044;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
register AES verbs at SS start,CASSANDRA-717,12445948,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,20/Jan/10 01:15,16/Apr/19 17:33,22/Mar/23 14:57,22/Jan/10 05:22,0.6,,,,0,,,,,,"the reason we do all registration in one place is it prevents bugs like this one

ERROR - Error in ThreadPoolExecutor
java.lang.AssertionError: unknown verb TREE-RESPONSE-VERB
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
ERROR - Fatal exception in thread Thread[AE-SERVICE-STAGE:1,5,main]
java.lang.AssertionError: unknown verb TREE-RESPONSE-VERB
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
",,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Jul/10 01:28;messi;0003-EnumMap-StorageService.Verb.patch.txt;https://issues.apache.org/jira/secure/attachment/12449369/0003-EnumMap-StorageService.Verb.patch.txt","20/Jan/10 03:45;jbellis;ASF.LICENSE.NOT.GRANTED--0001-mv-tree-and-gossip-verb-registration-into-StorageServi.txt;https://issues.apache.org/jira/secure/attachment/12430788/ASF.LICENSE.NOT.GRANTED--0001-mv-tree-and-gossip-verb-registration-into-StorageServi.txt","20/Jan/10 03:45;jbellis;ASF.LICENSE.NOT.GRANTED--0002-convert-verbs-to-enums.txt;https://issues.apache.org/jira/secure/attachment/12430789/ASF.LICENSE.NOT.GRANTED--0002-convert-verbs-to-enums.txt",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19833,,,Wed Jul 14 13:56:17 UTC 2010,,,,,,,,,,"0|i0g0n3:",91534,,,,,Low,,,,,,,,,,,,,,,,,"20/Jan/10 04:59;stuhood;+1... it's an improvement, although if we can find a good excuse to give the components of SS (AES, Gossiper, Streaming) an abstract base class and lifecycle, that would probably be ideal.;;;","22/Jan/10 05:22;jbellis;rebased and committed;;;","22/Jan/10 20:36;hudson;Integrated in Cassandra #331 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/331/])
    convert verbs to enums
patch by jbellis; reviewed by Stu Hood for 
mv tree and gossip verb registration into StorageService
patch by jbellis; reviewed by Stu Hood for 
;;;","14/Jul/10 01:28;messi;Use EnumMap for Map<StorageService.Verb, IVerbHandler>.;;;","14/Jul/10 03:46;jbellis;committed;;;","14/Jul/10 21:56;hudson;Integrated in Cassandra #491 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/491/])
    Use EnumMap for verbHandlers.  patch by Folke Behrens; reviewed by jbellis for CASSANDRA-717
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
multiget_slice() calls using TBinaryProtocolAccelerated always take up to the TSocket->recvTimeout before returning results,CASSANDRA-1199,12467123,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,arya,arya,17/Jun/10 04:56,16/Apr/19 17:33,22/Mar/23 14:57,17/Jun/10 05:07,,,,,0,,,,,,"I am comparing the following 
   - Reading of 100 SuperColumns in 1 SCF row using multiget_slice() with 1 key and 1 column name in 100 loop iterations
   - Reading of 100 SuperColumns in 1 SCF row using multiget_slice() with 1 key and 100 column names in a single call

I always get a consistent result and that is the single call takes more time then 100 calls. After some investigation, it seamed that the time it took to execute multiget_slice with 100 columns is always close to the TSocket->recvTimeout, Increasing the recvTimeout results that call to take that much time before retuning. After digged into TSocket->read (TSocket.php line 261) and looking at some of the meta data of fread, it seams that none of the buffer chunks get the eof flag=1. And the stream waits till timeout has reached. 

This only happens if TBinaryProtocolAccelerated (thrift_protocol.so) is used. 

I have attached my code to reproduce this issue. You can set the timeouts to see how it affects the read call in multiget_slice.

Please investigate and move to Thrift if not a Cassandra interface issue.","CentOS 5.2
Cassandra Nightly Build June 11th
Thrift Trunc
Using TBinaryProtocolAccelerated only!
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jun/10 04:59;arya;multiget_slice.php;https://issues.apache.org/jira/secure/attachment/12447270/multiget_slice.php",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20032,,,Wed Jun 16 21:10:30 UTC 2010,,,,,,,,,,"0|i0g3lr:",92014,,,,,Normal,,,,,,,,,,,,,,,,,"17/Jun/10 04:59;arya;You can change $hosts in this script to include a cluster of nodes or a single machine as you like to. Note that replication factor is the same as number of hosts. For me the max hosts I have in my cluster are 3 so using consistency levels 1 is ok, but you might want to change that if you're adding more than 3 nodes into the list.;;;","17/Jun/10 05:01;arya;This a timing example for the above scenario with TSocket's default timeouts:

100 Sequential Writes took: 0.4047749042511 seconds;
100 Sequential Reads took: 0.16357207298279 seconds;
100 Batch Read took: 0.77017998695374 seconds;;;;","17/Jun/10 05:03;arya;Just a clarification on the issue title, when I mistakenly used always. In my scenario, it is always happening when fetching multiple column names from one row of SCF.;;;","17/Jun/10 05:07;jbellis;This is the same as THRIFT-788.;;;","17/Jun/10 05:10;jbellis;(But your analysis of the problem seems more thorough, you should add your investigation of TSocket->read over there);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not allow Cassandra to start if filesystem is in read-only mode.,CASSANDRA-2117,12497777,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Duplicate,lenn0x,lenn0x,lenn0x,06/Feb/11 14:05,16/Apr/19 17:33,22/Mar/23 14:57,26/Jan/13 04:37,,,,,0,,,,,,"If the underlying filesystem of commit log drive or data drives is in read-only mode, do not allow startup.",,cburroughs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,lenn0x,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20451,,,Fri Jan 25 20:37:39 UTC 2013,,,,,,,,,,"0|i0g9f3:",92956,,,,,Low,,,,,,,,,,,,,,,,,"26/Jan/13 04:37;jbellis;Belive disk_failure_policy: stop will do this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
batch_mutate can't insert a super column that has ever insert and was removed.,CASSANDRA-991,12462227,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,seamine,seamine,16/Apr/10 17:01,16/Apr/19 17:33,22/Mar/23 14:57,17/Apr/10 00:22,,,,,0,,,,,,"I called the cassandra client api  batch_mutate to insert a row that row key is '1' into cassandra.  Then , In cassandra command-line, use ""del"" command to remove this row..Then, called the api  to insert a row that row key is '1' again,  it execute successfully,  There is no exception was throwed . But ,  while i use ""get"" command to get the row in cassandra command-line,I can't get anything, command-line response ""Returned 0 results."",  why??? is this a bug??",Windows XP Professional SP2 + Eclipse 3.4.2  + cassandra 0.6.0 + thrift 0.2.0,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19945,,,Fri Apr 16 16:22:00 UTC 2010,,,,,,,,,,"0|i0g2bz:",91808,,,,,Normal,,,,,,,,,,,,,,,,,"17/Apr/10 00:22;jbellis;sounds like a duplicate of CASSANDRA-920, which is fixed for 0.6.1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"nodetool move caused the moved node to drop itself from 'nodetool ring' output; others think it's 'joining'",CASSANDRA-1840,12492917,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Duplicate,,scode,scode,10/Dec/10 18:01,16/Apr/19 17:33,22/Mar/23 14:57,10/Dec/10 21:25,,,,,0,,,,,,"I have a test cluster with three nodes on a very recent 0.7 (last few days branch). It has very little data in it (so maybe timing can be an issue given how fast operations complete). It was otherwise healthy; nodetool ring was consistent on all nodes and I had just run some compactions and 'repair' commands on all nodes repeatedly.

I had a single client doing some reads/writes of single columns; nothing extreme (low load).

When I did a 'nodetool move' the node exited the ring, stopped responding to thrift RPC, entered the ring again, and started accepting RPC requests via thrift. It reports in the log that it is joined.

However, at this point 'nodetool ring' on the node I moved does *not* show its own location in the ring, and other nodes show it as 'joining' (with the new token, not the old token). I will include nodetool ring output and log output below.

The situation was un-wedged by restarting the node that I had moved. After it started and a few seconds passed, nodetool ring looked correct on the node in question and other nodes now reported it as 'up' rather than 'joining'.

Moved node said post-move (.61 in the below pastes is the node that I moved):

Address         Status State   Load            Owns    Token                                       
                                                       110288156320304836825416347816186393502     
78.31.15.204    Up     Normal  224.34 KB       61.44%  44678687293344048155696022135861768368      
193.182.3.229   Up     Normal  251.84 KB       38.56%  110288156320304836825416347816186393502     

And the other two:


Address         Status State   Load            Owns    Token                                       
                                                       164957594472845753490452447750528540018     
78.31.15.204    Up     Normal  224.34 KB       29.31%  44678687293344048155696022135861768368      
193.182.3.229   Up     Normal  251.84 KB       38.56%  110288156320304836825416347816186393502     
193.182.3.61    Up     Joining 194.76 KB       32.13%  164957594472845753490452447750528540018     

Address         Status State   Load            Owns    Token                                       
                                                       164957594472845753490452447750528540018     

78.31.15.204    Up     Normal  
224.34 KB       29.31%  44678687293344048155696022135861768368      
193.182.3.229   Up     Normal  251.84 KB       38.56%  110288156320304836825416347816186393502     
193.182.3.61    Up     Joining 194.76 KB       32.13%  164957594472845753490452447750528540018     

I'll try reproducing a few times, and also merge latest 0.7.

Here is some system log output from the node that got moved; it looks good to me:

 INFO [RMI TCP Connection(32)-193.182.3.61] 2010-12-10 09:38:17,560 StorageService.java (line 455) Leaving: sleeping 30000 ms for pending range setup
 INFO [RMI TCP Connection(32)-193.182.3.61] 2010-12-10 09:38:47,564 StorageService.java (line 455) Leaving: streaming data to other nodes
 INFO [StreamStage:1] 2010-12-10 09:38:47,566 StreamOut.java (line 75) Beginning transfer to /78.31.15.204
 INFO [StreamStage:1] 2010-12-10 09:38:47,566 StreamOut.java (line 98) Flushing memtables for KeyspaceSlask...
 INFO [StreamStage:1] 2010-12-10 09:38:47,567 ColumnFamilyStore.java (line 639) switching in a fresh Memtable for KeyValue at CommitLogContext(file='/var/lib/spotify-cassandra/slask/commitlog/CommitLog-1291973418600.log', position=2573610)
 INFO [StreamStage:1] 2010-12-10 09:38:47,567 ColumnFamilyStore.java (line 943) Enqueuing flush of Memtable-KeyValue@1131602880(370711 bytes, 5533 operations)
 INFO [FlushWriter:1] 2010-12-10 09:38:47,567 Memtable.java (line 155) Writing Memtable-KeyValue@1131602880(370711 bytes, 5533 operations)
 INFO [FlushWriter:1] 2010-12-10 09:38:47,599 Memtable.java (line 162) Completed flushing /var/lib/spotify-cassandra/slask/data/KeyspaceSlask/KeyValue-e-68-Data.db (15042 bytes)
 INFO [StreamStage:1] 2010-12-10 09:38:47,601 StreamOut.java (line 171) Stream context metadata [/var/lib/spotify-cassandra/slask/data/KeyspaceSlask/KeyValue-e-67-Data.db/(0,10094)
         progress=0/10094 - 0%, /var/lib/spotify-cassandra/slask/data/KeyspaceSlask/KeyValue-e-68-Data.db/(0,10094)
         progress=0/10094 - 0%], 2 sstables.
 INFO [StreamStage:1] 2010-12-10 09:38:47,601 StreamOutSession.java (line 175) Streaming to /78.31.15.204
 INFO [StreamStage:1] 2010-12-10 09:38:47,601 StreamOut.java (line 75) Beginning transfer to /193.182.3.229
 INFO [StreamStage:1] 2010-12-10 09:38:47,602 StreamOut.java (line 98) Flushing memtables for KeyspaceSlask...
 INFO [StreamStage:1] 2010-12-10 09:38:47,602 ColumnFamilyStore.java (line 639) switching in a fresh Memtable for KeyValue at CommitLogContext(file='/var/lib/spotify-cassandra/slask/commitlog/CommitLog-1291973418600.log', position=2573755)
 INFO [StreamStage:1] 2010-12-10 09:38:47,602 ColumnFamilyStore.java (line 943) Enqueuing flush of Memtable-KeyValue@1894688899(67 bytes, 1 operations)
 INFO [FlushWriter:1] 2010-12-10 09:38:47,604 Memtable.java (line 155) Writing Memtable-KeyValue@1894688899(67 bytes, 1 operations)
 INFO [FlushWriter:1] 2010-12-10 09:38:47,635 Memtable.java (line 162) Completed flushing /var/lib/spotify-cassandra/slask/data/KeyspaceSlask/KeyValue-e-69-Data.db (198 bytes)
 INFO [StreamStage:1] 2010-12-10 09:38:47,637 StreamOut.java (line 171) Stream context metadata [/var/lib/spotify-cassandra/slask/data/KeyspaceSlask/KeyValue-e-67-Data.db/(10094,15042)
         progress=0/4948 - 0%, /var/lib/spotify-cassandra/slask/data/KeyspaceSlask/KeyValue-e-68-Data.db/(10094,15042)
         progress=0/4948 - 0%], 3 sstables.
 INFO [StreamStage:1] 2010-12-10 09:38:47,637 StreamOutSession.java (line 175) Streaming to /193.182.3.229
 INFO [RMI TCP Connection(32)-193.182.3.61] 2010-12-10 09:38:49,734 StorageService.java (line 1682) re-bootstrapping to new token 164957594472845753490452447750528540018
 INFO [RMI TCP Connection(32)-193.182.3.61] 2010-12-10 09:38:49,735 ColumnFamilyStore.java (line 639) switching in a fresh Memtable for LocationInfo at CommitLogContext(file='/var/lib/spotify-cassandra/slask/commitlog/CommitLog-1291973418600.log', position=2576080)
 INFO [RMI TCP Connection(32)-193.182.3.61] 2010-12-10 09:38:49,736 ColumnFamilyStore.java (line 943) Enqueuing flush of Memtable-LocationInfo@578162504(53 bytes, 2 operations)
 INFO [FlushWriter:1] 2010-12-10 09:38:49,737 Memtable.java (line 155) Writing Memtable-LocationInfo@578162504(53 bytes, 2 operations)
 INFO [FlushWriter:1] 2010-12-10 09:38:49,865 Memtable.java (line 162) Completed flushing /var/lib/spotify-cassandra/slask/data/system/LocationInfo-e-17-Data.db (301 bytes)
 INFO [RMI TCP Connection(32)-193.182.3.61] 2010-12-10 09:38:49,866 StorageService.java (line 455) Joining: sleeping 30000 ms for pending range setup
 INFO [RMI TCP Connection(32)-193.182.3.61] 2010-12-10 09:39:19,866 StorageService.java (line 455) Bootstrapping
 INFO [CompactionExecutor:1] 2010-12-10 09:39:19,960 SSTableReader.java (line 170) Opening /var/lib/spotify-cassandra/slask/data/KeyspaceSlask/KeyValue-e-70
 INFO [CompactionExecutor:1] 2010-12-10 09:39:19,981 SSTableReader.java (line 170) Opening /var/lib/spotify-cassandra/slask/data/KeyspaceSlask/KeyValue-e-71
 INFO [Thread-71] 2010-12-10 09:39:19,983 StreamInSession.java (line 160) Finished streaming session 258244662990017 from /193.182.3.229
 INFO [RMI TCP Connection(32)-193.182.3.61] 2010-12-10 09:39:19,983 StorageService.java (line 249) Bootstrap/move completed! Now serving reads.

So based on the output the node certainly claims to have finished the entire bootstrapping procedure and started serving reads (which it is; reads are working against it).
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20338,,,Fri Dec 10 13:25:21 UTC 2010,,,,,,,,,,"0|i0g7qf:",92683,,,,,Low,,,,,,,,,,,,,,,,,"10/Dec/10 18:14;scode;I could reproduce it consistently. I tried once per node; in each case a restart was required. Still same after stopping all nodes and starting them from scratch.

I will update to today's 0.7 branch and re-try.;;;","10/Dec/10 18:51;scode;I am not able to reproduce with latest 0.7, though I'm not sure which change is expected to fix this problem so I'm not marking resolved yet until someone else does it or weighs in.;;;","10/Dec/10 21:25;jbellis;believe this was fixed in CASSANDRA-1829;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Insert SC (via batch_mutate), Delete, insert again - result is empty",CASSANDRA-1098,12464647,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,mishail,mishail,17/May/10 14:16,16/Apr/19 17:33,22/Mar/23 14:57,17/May/10 20:21,,,,,0,,,,,,"I'm not sure whether it's a bug or I just don't fully understand the API
However from my point it looks very similar to CASSANDRA-703.

Scenario:

# Insert the Supercolumn via batch_mutate
# Using get_slice check that result IS NOT empty
# Delete it via remove
# Using get_slice check that result IS empty
# Insert again
# Using get_slice check that result IS NOT empty

I observe the empty list after 2nd insertion.  What's wrong?",Windows XP,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/May/10 14:20;mishail;BugTest.java;https://issues.apache.org/jira/secure/attachment/12444639/BugTest.java",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19992,,,Mon May 17 12:21:00 UTC 2010,,,,,,,,,,"0|i0g2zj:",91914,,,,,Normal,,,,,,,,,,,,,,,,,"17/May/10 14:20;mishail;Test-case.

It fails with ""Empty result after 2nd insertion"" during the 1st execution, and then it fails ""Empty result after 1st insertion"" on all consequential executions;;;","17/May/10 20:21;jbellis;dupe of CASSANDRA-1063 (fixed for 0.6.2);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Not all column families are created,CASSANDRA-1038,12463375,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,gdusbabek,brandon.williams,brandon.williams,30/Apr/10 05:17,16/Apr/19 17:33,22/Mar/23 14:57,05/May/10 23:35,0.7 beta 1,,,,0,,,,,,"It seems that not all column families will be created via system_add_keyspace in some cases.  To reproduce:

Run stress.py (with CASSANDRA-1033) inserts (I used 1M) against standard columns.  During this run, both Standard1 and Super1 will be created.

Run stress.py again, this time against super columns.  Due to CASSANDRA-1036 no errors will be visible to the client but can be observed in the log.

You can switch the order and stress supers first, in which case Standard1 will not exist.  If you call describe_keyspace on Keyspace1, it will show both CFs even though only one will work.
",,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/May/10 06:22;gdusbabek;0001-restrict-how-and-when-CFMetaData-objects-are-added-t.patch;https://issues.apache.org/jira/secure/attachment/12443512/0001-restrict-how-and-when-CFMetaData-objects-are-added-t.patch",,,,,,,,,,,,,,1.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19968,,,Wed May 05 15:35:49 UTC 2010,,,,,,,,,,"0|i0g2mf:",91855,,,,,Normal,,,,,,,,,,,,,,,,,"04/May/10 06:22;gdusbabek;The problem was that when the keyspace creation was reattempted, identical CFMetaData objects were created that obliterated the older, correct cfids.

I could have put a check in the private CFMetaData constructor that threw an exception, but decided I'd rather have explicit control over cfIdMap instead.;;;","05/May/10 23:24;jbellis;+1;;;","05/May/10 23:35;gdusbabek;I forgot to include the ticket id in the commit msg, so hudson isn't going to adorn us with the svn rev.  It's r941349 for the curious.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Commitlog segments don't get deleted,CASSANDRA-459,12436668,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,27/Sep/09 02:48,16/Apr/19 17:33,22/Mar/23 14:57,29/Sep/09 11:49,0.4,0.5,,,0,,,,,,"System table is not created with a periodic flush, so any update there (such as storing token info) can prevent commitlog segments from being deleted.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Sep/09 04:19;jbellis;ASF.LICENSE.NOT.GRANTED--0001-logging.txt;https://issues.apache.org/jira/secure/attachment/12420727/ASF.LICENSE.NOT.GRANTED--0001-logging.txt","29/Sep/09 04:19;jbellis;ASF.LICENSE.NOT.GRANTED--0002-CASSANDRA-459-flush-locationinfo-every-minute-and-hint.txt;https://issues.apache.org/jira/secure/attachment/12420728/ASF.LICENSE.NOT.GRANTED--0002-CASSANDRA-459-flush-locationinfo-every-minute-and-hint.txt","29/Sep/09 04:19;jbellis;ASF.LICENSE.NOT.GRANTED--0003-cleanup-don-t-preserve-dirty-bits-from-older-replay-s.txt;https://issues.apache.org/jira/secure/attachment/12420729/ASF.LICENSE.NOT.GRANTED--0003-cleanup-don-t-preserve-dirty-bits-from-older-replay-s.txt",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19698,,,Tue Sep 29 03:49:21 UTC 2009,,,,,,,,,,"0|i0fz2f:",91279,,,,,Normal,,,,,,,,,,,,,,,,,"27/Sep/09 02:49;jbellis;01 fixes the bug
02 adds a bunch of debug logging to commitlog purging so you can see which CFs are blocking the discard
;;;","27/Sep/09 05:24;teodor;I started a test to check that for whole night. ;;;","27/Sep/09 06:20;teodor;I'm very sorry, but patch could not be applied cleanly for cassandra-0.4:
% cat /spool/home/teodor/tmp/0001-CASSANDRA-459-flush-system-table-every-five-minutes.txt /spool/home/teodor/tmp/0002-logging.txt  | patch -p1 -C
....
Patching file src/java/org/apache/cassandra/db/CommitLog.java using Plan A...
Hunk #1 succeeded at 97 (offset 1 line).
Hunk #2 succeeded at 463 (offset -1 lines).
Hunk #3 succeeded at 514 (offset 1 line).
Hunk #4 failed at 532.
1 out of 4 hunks failed--saving rejects to src/java/org/apache/cassandra/db/CommitLog.java.rej
....

% cat src/java/org/apache/cassandra/db/CommitLog.java.rej
***************
*** 516,521 ****
                  }
                  else
                  {
                      BufferedRandomAccessFile logWriter = CommitLog.createWriter(oldFile);
                      writeCommitLogHeader(logWriter, oldCommitLogHeader.toByteArray());
                      logWriter.close();
--- 532,539 ----
                  }
                  else
                  {
+                     if (logger_.isDebugEnabled())
+                         logger_.debug(""Not safe to delete commit log "" + oldFile + ""; dirty is "" + oldCommitLogHeader.dirtyString());
                      BufferedRandomAccessFile logWriter = CommitLog.createWriter(oldFile);
                      writeCommitLogHeader(logWriter, oldCommitLogHeader.toByteArray());
                      logWriter.close();

It seems to me  that is not a stopper  to test. 
;;;","27/Sep/09 06:34;jbellis;Yes, 02 doesn't need to apply to 0.4;;;","27/Sep/09 13:59;teodor;Patch doesn't change anything :(   I use the same test script as for CASSANDRA-458.
Immediately after test start:
% ls -l /spool/cassandra/commitlog /spool/cassandra/data/Keyspace1
/spool/cassandra/commitlog:
total 784
-rw-r--r--  1 teodor  wheel  786432 Sep 27 02:42 CommitLog-1254004919295.log

/spool/cassandra/data/Keyspace1:
total 0

After night
% ls -l /spool/cassandra/commitlog /spool/cassandra/data/Keyspace1
/spool/cassandra/commitlog:
total 3856720
-rw-r--r--  1 teodor  wheel  134217971 Sep 27 09:52 CommitLog-1254004919295.log
-rw-r--r--  1 teodor  wheel  134217839 Sep 27 09:52 CommitLog-1254005826443.log
-rw-r--r--  1 teodor  wheel  134218376 Sep 27 09:52 CommitLog-1254006732843.log
-rw-r--r--  1 teodor  wheel  134217734 Sep 27 09:52 CommitLog-1254007676088.log
-rw-r--r--  1 teodor  wheel  134218324 Sep 27 09:52 CommitLog-1254008589671.log
-rw-r--r--  1 teodor  wheel  134218501 Sep 27 09:52 CommitLog-1254009457777.log
-rw-r--r--  1 teodor  wheel  134217841 Sep 27 09:52 CommitLog-1254010329142.log
-rw-r--r--  1 teodor  wheel  134218312 Sep 27 09:52 CommitLog-1254011183824.log
-rw-r--r--  1 teodor  wheel  134217795 Sep 27 09:52 CommitLog-1254012066380.log
-rw-r--r--  1 teodor  wheel  134218031 Sep 27 09:52 CommitLog-1254012920007.log
-rw-r--r--  1 teodor  wheel  134217764 Sep 27 09:52 CommitLog-1254013812856.log
-rw-r--r--  1 teodor  wheel  134217888 Sep 27 09:52 CommitLog-1254014728895.log
-rw-r--r--  1 teodor  wheel  134217842 Sep 27 09:52 CommitLog-1254015574082.log
-rw-r--r--  1 teodor  wheel  134217861 Sep 27 09:52 CommitLog-1254016456859.log
-rw-r--r--  1 teodor  wheel  134218257 Sep 27 09:52 CommitLog-1254017313996.log
-rw-r--r--  1 teodor  wheel  134217874 Sep 27 09:52 CommitLog-1254018190196.log
-rw-r--r--  1 teodor  wheel  134218228 Sep 27 09:52 CommitLog-1254019039153.log
-rw-r--r--  1 teodor  wheel  134217768 Sep 27 09:52 CommitLog-1254019901588.log
-rw-r--r--  1 teodor  wheel  134218072 Sep 27 09:52 CommitLog-1254020850481.log
-rw-r--r--  1 teodor  wheel  134218463 Sep 27 09:52 CommitLog-1254021705210.log
-rw-r--r--  1 teodor  wheel  134217787 Sep 27 09:52 CommitLog-1254022584617.log
-rw-r--r--  1 teodor  wheel  134217817 Sep 27 09:52 CommitLog-1254023445022.log
-rw-r--r--  1 teodor  wheel  134218548 Sep 27 09:52 CommitLog-1254024325493.log
-rw-r--r--  1 teodor  wheel  134217870 Sep 27 09:52 CommitLog-1254025178129.log
-rw-r--r--  1 teodor  wheel  134217763 Sep 27 09:52 CommitLog-1254026031303.log
-rw-r--r--  1 teodor  wheel  134217782 Sep 27 09:52 CommitLog-1254026990171.log
-rw-r--r--  1 teodor  wheel  134217781 Sep 27 09:52 CommitLog-1254027844609.log
-rw-r--r--  1 teodor  wheel  134217742 Sep 27 09:52 CommitLog-1254028740042.log
-rw-r--r--  1 teodor  wheel  134217810 Sep 27 09:52 CommitLog-1254029593664.log
-rw-r--r--  1 teodor  wheel   54067200 Sep 27 09:53 CommitLog-1254030447553.log

/spool/cassandra/data/Keyspace1:
total 785468
-rw-r--r--  1 teodor  wheel  215470950 Sep 27 08:48 Standard1-372-Data.db
-rw-r--r--  1 teodor  wheel    3833605 Sep 27 08:48 Standard1-372-Filter.db
-rw-r--r--  1 teodor  wheel   88353365 Sep 27 08:48 Standard1-372-Index.db
-rw-r--r--  1 teodor  wheel  204226037 Sep 27 09:18 Standard1-408-Data.db
-rw-r--r--  1 teodor  wheel     812005 Sep 27 09:18 Standard1-408-Filter.db
-rw-r--r--  1 teodor  wheel   32812386 Sep 27 09:18 Standard1-408-Index.db
-rw-r--r--  1 teodor  wheel   10467605 Sep 27 09:51 Standard1-444-Data.db
-rw-r--r--  1 teodor  wheel      30349 Sep 27 09:51 Standard1-444-Filter.db
-rw-r--r--  1 teodor  wheel    1442137 Sep 27 09:51 Standard1-444-Index.db
-rw-r--r--  1 teodor  wheel  201238111 Sep 27 09:53 Standard1-445-Data.db
-rw-r--r--  1 teodor  wheel     795205 Sep 27 09:53 Standard1-445-Filter.db
-rw-r--r--  1 teodor  wheel   32237630 Sep 27 09:53 Standard1-445-Index.db
-rw-r--r--  1 teodor  wheel   10458480 Sep 27 09:52 Standard1-446-Data.db
-rw-r--r--  1 teodor  wheel      30261 Sep 27 09:52 Standard1-446-Filter.db
-rw-r--r--  1 teodor  wheel    1438054 Sep 27 09:52 Standard1-446-Index.db

The single problem in logs is:
 WARN [MINOR-COMPACTION-POOL:1] 2009-09-27 09:41:59,422 ColumnFamilyStore.java (line 1034) Nothing to compact (all files empty or corrupt)
 WARN [MINOR-COMPACTION-POOL:1] 2009-09-27 03:41:59,303 ColumnFamilyStore.java (line 1034) Nothing to compact (all files empty or corrupt)
 WARN [MINOR-COMPACTION-POOL:1] 2009-09-27 04:41:59,234 ColumnFamilyStore.java (line 1034) Nothing to compact (all files empty or corrupt)
 WARN [MINOR-COMPACTION-POOL:1] 2009-09-27 05:41:59,355 ColumnFamilyStore.java (line 1034) Nothing to compact (all files empty or corrupt)
 WARN [MINOR-COMPACTION-POOL:1] 2009-09-27 06:41:59,386 ColumnFamilyStore.java (line 1034) Nothing to compact (all files empty or corrupt)
 WARN [MINOR-COMPACTION-POOL:1] 2009-09-27 07:41:59,392 ColumnFamilyStore.java (line 1034) Nothing to compact (all files empty or corrupt)
 WARN [MINOR-COMPACTION-POOL:1] 2009-09-27 08:48:58,819 ColumnFamilyStore.java (line 1034) Nothing to compact (all files empty or corrupt)

Log fragment of one of that warnings:
 INFO [MINOR-COMPACTION-POOL:1] 2009-09-27 09:41:53,827 ColumnFamilyStore.java (line 1155) Compacted to /spool/cassandra/data/Keyspace1/Standard1-434-Data.db.  0/67462071 bytes for 111274/107527 keys read/written.  Time: 31884ms.
 INFO [PERIODIC-FLUSHER-POOL:1] 2009-09-27 09:41:59,033 ColumnFamilyStore.java (line 367) Standard1 has reached its threshold; switching in a fresh Memtable
 INFO [PERIODIC-FLUSHER-POOL:1] 2009-09-27 09:41:59,033 ColumnFamilyStore.java (line 1178) Enqueuing flush of Memtable(Standard1)@8993320
 INFO [MEMTABLE-FLUSHER-POOL:1] 2009-09-27 09:41:59,033 Memtable.java (line 186) Flushing Memtable(Standard1)@8993320
 INFO [MINOR-COMPACTION-POOL:1] 2009-09-27 09:41:59,422 ColumnFamilyStore.java (line 1013) Compacting []
 WARN [MINOR-COMPACTION-POOL:1] 2009-09-27 09:41:59,422 ColumnFamilyStore.java (line 1034) Nothing to compact (all files empty or corrupt)
 INFO [MEMTABLE-FLUSHER-POOL:1] 2009-09-27 09:42:06,148 Memtable.java (line 220) Completed flushing Memtable(Standard1)@8993320

;;;","27/Sep/09 21:04;jbellis;Are you on the latest 0.4 branch before applying the patch?  It needs both r819004 and this one to work.;;;","27/Sep/09 23:59;teodor;Yes, sure.
Another run (r819316 + 0001 patch), just for rechek:
% ls -l /spool/cassandra/commitlog /spool/cassandra/data/Keyspace1 /spool/cassandra/data/system
/spool/cassandra/commitlog:
total 44144
-rw-r--r--  1 teodor  wheel  45154304 Sep 27 18:32 CommitLog-1254061669607.log

/spool/cassandra/data/Keyspace1:
total 35130
-rw-r--r--  1 teodor  wheel  10475393 Sep 27 18:29 Standard1-1-Data.db
-rw-r--r--  1 teodor  wheel     30365 Sep 27 18:29 Standard1-1-Filter.db
-rw-r--r--  1 teodor  wheel   1443066 Sep 27 18:29 Standard1-1-Index.db
-rw-r--r--  1 teodor  wheel  10472280 Sep 27 18:30 Standard1-2-Data.db
-rw-r--r--  1 teodor  wheel     30189 Sep 27 18:30 Standard1-2-Filter.db
-rw-r--r--  1 teodor  wheel   1434966 Sep 27 18:30 Standard1-2-Index.db
-rw-r--r--  1 teodor  wheel  10455997 Sep 27 18:32 Standard1-3-Data.db
-rw-r--r--  1 teodor  wheel     30293 Sep 27 18:32 Standard1-3-Filter.db
-rw-r--r--  1 teodor  wheel   1439401 Sep 27 18:32 Standard1-3-Index.db

/spool/cassandra/data/system:
total 6
-rw-r--r--  1 teodor  wheel  255 Sep 27 18:32 LocationInfo-1-Data.db
-rw-r--r--  1 teodor  wheel   85 Sep 27 18:32 LocationInfo-1-Filter.db
-rw-r--r--  1 teodor  wheel   50 Sep 27 18:32 LocationInfo-1-Index.db

After  hour and a half:
% ls -l /spool/cassandra/commitlog /spool/cassandra/data/Keyspace1 /spool/cassandra/data/system
/spool/cassandra/commitlog:
total 813840
-rw-r--r--  1 teodor  wheel  134217982 Sep 27 19:57 CommitLog-1254061669607.log
-rw-r--r--  1 teodor  wheel  134217875 Sep 27 19:57 CommitLog-1254062560798.log
-rw-r--r--  1 teodor  wheel  134217761 Sep 27 19:57 CommitLog-1254063429524.log
-rw-r--r--  1 teodor  wheel  134217809 Sep 27 19:57 CommitLog-1254064318004.log
-rw-r--r--  1 teodor  wheel  134217785 Sep 27 19:57 CommitLog-1254065174349.log
-rw-r--r--  1 teodor  wheel  134217849 Sep 27 19:57 CommitLog-1254066038411.log
-rw-r--r--  1 teodor  wheel   27459584 Sep 27 19:58 CommitLog-1254066927001.log

/spool/cassandra/data/Keyspace1:
total 612880
-rw-r--r--  1 teodor  wheel  203500680 Sep 27 19:07 Standard1-38-Data.db
-rw-r--r--  1 teodor  wheel     806965 Sep 27 19:07 Standard1-38-Filter.db
-rw-r--r--  1 teodor  wheel   32636516 Sep 27 19:07 Standard1-38-Index.db
-rw-r--r--  1 teodor  wheel  202494151 Sep 27 19:42 Standard1-75-Data.db
-rw-r--r--  1 teodor  wheel     801685 Sep 27 19:42 Standard1-75-Filter.db
-rw-r--r--  1 teodor  wheel   32441650 Sep 27 19:42 Standard1-75-Index.db
-rw-r--r--  1 teodor  wheel   67331570 Sep 27 19:49 Standard1-83-Data.db
-rw-r--r--  1 teodor  wheel     209845 Sep 27 19:49 Standard1-83-Filter.db
-rw-r--r--  1 teodor  wheel    9598920 Sep 27 19:49 Standard1-83-Index.db
-rw-r--r--  1 teodor  wheel   67375520 Sep 27 19:57 Standard1-92-Data.db
-rw-r--r--  1 teodor  wheel     209605 Sep 27 19:57 Standard1-92-Filter.db
-rw-r--r--  1 teodor  wheel    9579322 Sep 27 19:57 Standard1-92-Index.db

/spool/cassandra/data/system:
total 6
-rw-r--r--  1 teodor  wheel  255 Sep 27 18:32 LocationInfo-1-Data.db
-rw-r--r--  1 teodor  wheel   85 Sep 27 18:32 LocationInfo-1-Filter.db
-rw-r--r--  1 teodor  wheel   50 Sep 27 18:32 LocationInfo-1-Index.db
;;;","28/Sep/09 00:03;jbellis;Could you try trunk + 0001 ?;;;","28/Sep/09 01:28;teodor;Sure, results:

% ls -l /spool/cassandra/commitlog /spool/cassandra/data/Keyspace1 /spool/cassandra/data/system
/spool/cassandra/commitlog:
total 89344
-rw-r--r--  1 teodor  wheel  91422720 Sep 27 20:36 CommitLog-1254068803560.log

/spool/cassandra/data/Keyspace1:
total 78400
-rw-r--r--  1 teodor  wheel  70339417 Sep 27 20:36 Standard1-9-Data.db
-rw-r--r--  1 teodor  wheel    210085 Sep 27 20:36 Standard1-9-Filter.db
-rw-r--r--  1 teodor  wheel   9601776 Sep 27 20:36 Standard1-9-Index.db

/spool/cassandra/data/system:
total 6
-rw-r--r--  1 teodor  wheel  255 Sep 27 20:31 LocationInfo-1-Data.db
-rw-r--r--  1 teodor  wheel   85 Sep 27 20:31 LocationInfo-1-Filter.db
-rw-r--r--  1 teodor  wheel   50 Sep 27 20:31 LocationInfo-1-Index.db

After hour:
% ls -l /spool/cassandra/commitlog /spool/cassandra/data/Keyspace1 /spool/cassandra/data/system
/spool/cassandra/commitlog:
total 564560
-rw-r--r--  1 teodor  wheel  134218167 Sep 27 21:27 CommitLog-1254068803560.log
-rw-r--r--  1 teodor  wheel  134218321 Sep 27 21:27 CommitLog-1254069672194.log
-rw-r--r--  1 teodor  wheel  134218547 Sep 27 21:27 CommitLog-1254070532837.log
-rw-r--r--  1 teodor  wheel  134217848 Sep 27 21:27 CommitLog-1254071417379.log
-rw-r--r--  1 teodor  wheel   40793246 Sep 27 21:28 CommitLog-1254072254525.log

/spool/cassandra/data/Keyspace1:
total 487238
-rw-r--r--  1 teodor  wheel  245938744 Sep 27 21:05 Standard1-37-Data.db
-rw-r--r--  1 teodor  wheel     807205 Sep 27 21:05 Standard1-37-Filter.db
-rw-r--r--  1 teodor  wheel   32638599 Sep 27 21:05 Standard1-37-Index.db
-rw-r--r--  1 teodor  wheel   70395458 Sep 27 21:13 Standard1-46-Data.db
-rw-r--r--  1 teodor  wheel     210325 Sep 27 21:13 Standard1-46-Filter.db
-rw-r--r--  1 teodor  wheel    9612077 Sep 27 21:13 Standard1-46-Index.db
-rw-r--r--  1 teodor  wheel   41011329 Sep 27 21:18 Standard1-51-Data.db
-rw-r--r--  1 teodor  wheel     122245 Sep 27 21:18 Standard1-51-Filter.db
-rw-r--r--  1 teodor  wheel    5619283 Sep 27 21:18 Standard1-51-Index.db
-rw-r--r--  1 teodor  wheel   70296583 Sep 27 21:27 Standard1-60-Data.db
-rw-r--r--  1 teodor  wheel     210085 Sep 27 21:27 Standard1-60-Filter.db
-rw-r--r--  1 teodor  wheel    9592964 Sep 27 21:27 Standard1-60-Index.db
-rw-r--r--  1 teodor  wheel   10444198 Sep 27 21:27 Standard1-61-Data.db
-rw-r--r--  1 teodor  wheel      30253 Sep 27 21:27 Standard1-61-Filter.db
-rw-r--r--  1 teodor  wheel    1437846 Sep 27 21:27 Standard1-61-Index.db

/spool/cassandra/data/system:
total 6
-rw-r--r--  1 teodor  wheel  255 Sep 27 20:31 LocationInfo-1-Data.db
-rw-r--r--  1 teodor  wheel   85 Sep 27 20:31 LocationInfo-1-Filter.db
-rw-r--r--  1 teodor  wheel   50 Sep 27 20:31 LocationInfo-1-Index.db


;;;","29/Sep/09 04:22;jbellis;New patches attached.

What was happening is that when a ColumnFamily got marked dirty in the commitlog, all subsequent commitlog segments would also have that CF marked dirty, even after it was flushed.  When flush happened it could clear out old segments, but if the CF never had any more updates (like the LocationInfo CF in the system keyspace) then log segments would pile up.

This makes log segments only treat a CF as dirty if it actually has data written to it for that CF.;;;","29/Sep/09 06:24;junrao;Patch looks good to me. +1;;;","29/Sep/09 11:49;jbellis;committed w/ one more bug fix (r819823) to 0.4 and trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConcurrentModificationException during QuorumResponseHandler,CASSANDRA-864,12458566,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,btoddb,btoddb,10/Mar/10 02:20,16/Apr/19 17:33,22/Mar/23 14:57,11/Mar/10 01:18,0.6,,,,0,,,,,,"using cassandra-0.6.0-beta2/


2010-03-09 09:17:26,827 ERROR [pool-1-thread-675] [Cassandra.java:1166] Internal error processing get
java.util.ConcurrentModificationException
        at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
        at java.util.AbstractList$Itr.next(AbstractList.java:343)
        at org.apache.cassandra.service.QuorumResponseHandler.get(QuorumResponseHandler.java:68)
        at org.apache.cassandra.service.StorageProxy.strongRead(StorageProxy.java:470)
        at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:401)
        at org.apache.cassandra.thrift.CassandraServer.readColumnFamily(CassandraServer.java:101)
        at org.apache.cassandra.thrift.CassandraServer.multigetInternal(CassandraServer.java:309)
        at org.apache.cassandra.thrift.CassandraServer.get(CassandraServer.java:274)
        at org.apache.cassandra.thrift.Cassandra$Processor$get.process(Cassandra.java:1156)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:1114)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619) ",,david.pan,rschildmeijer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/10 10:02;jbellis;864-v2.txt;https://issues.apache.org/jira/secure/attachment/12438352/864-v2.txt","10/Mar/10 06:55;jbellis;864.txt;https://issues.apache.org/jira/secure/attachment/12438339/864.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19894,,,Wed Mar 10 17:18:47 UTC 2010,,,,,,,,,,"0|i0g1jr:",91681,,,,,Low,,,,,,,,,,,,,,,,,"10/Mar/10 02:54;jbellis;What's happening is, we're relying on the synchronization done by the caller of response() to make us thread-safe.  And this works fine _if_ responses arrive before the timeout; then the assumption that responses are never added after the wait in get completes holds true (since no responses may be added after the signal is fired).

But if the request times out, then the signal has not actually been fired, and a response may arrive during iteration of the messages and cause a CME, as seen here.;;;","10/Mar/10 04:18;jbellis;Fix for IAsyncCallback concurrency problems.

I've also removed a bunch of ""if (condition.isSignaled()) return;"" statements that were premature optimizations at best and possibly counterproductive to performance.  Remember that after a successful signal, we remove the callback entry in the finally {} block, so this was only useful for callbacks that arrived (a) after the signal but (b) before the callback was removed.;;;","10/Mar/10 05:02;rschildmeijer;+1;;;","10/Mar/10 06:55;jbellis;Heh, I went to commit and couldn't get the patch to apply to 0.6.  Then I realized I'd only submitted part of the patch (forgot to squash in my local git repo).

Full patch attached.;;;","10/Mar/10 10:02;jbellis;fix IllegalStateException: Queue full;;;","10/Mar/10 15:06;rschildmeijer;Reviewed once more. Still +1 :);;;","10/Mar/10 21:54;gdusbabek;Was changing the for-loop to 'if (iter.hasNext())' in QRH.get() intentional?  It seems like a while-loop is required to keep the same semantics as the old code.;;;","10/Mar/10 21:58;jbellis;I believe the responses all have the same messageid; the for loop was a clunkier way of expressing ""remove the handler, but only if we actually got a response with which to grab the id"";;;","10/Mar/10 22:27;gdusbabek;+1;;;","11/Mar/10 00:55;jbellis;actually ""responses all have the same messageid"" is only true for QRH, not WRH.  I will change both of them back to the way they were before.;;;","11/Mar/10 01:18;jbellis;committed w/ that change;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL: Batch Updates: some consistency levels not working,CASSANDRA-2566,12505240,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,cdaw,cdaw,27/Apr/11 04:00,16/Apr/19 17:33,22/Mar/23 14:57,17/May/11 06:01,0.8.0,,Legacy/CQL,,0,cql,,,,,"Testing the batch updates, and running into some issues with different consistency levels

+*Summary*+
* UNTESTED: CONSISTENCY ANY
* PASS: CONSISTENCY  ONE
* PASS: CONSISTENCY  QUORUM
* PASS: CONSISTENCY  ALL
* CQL ERROR: CONSISTENCY  LOCAL_QUORUM
* CQL ERROR: CONSISTENCY  EACH_QUORUM

 
+*Test Setup*+
{code}
CREATE KEYSPACE cqldb with strategy_class =  'org.apache.cassandra.locator.SimpleStrategy'  
and strategy_options:replication_factor=1;

use cqldb;

CREATE COLUMNFAMILY users (KEY varchar PRIMARY KEY, password varchar, gender varchar, 
session_token varchar, state varchar, birth_year bigint);

INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user1', 'ch@ngem3', 'f', 'CA', '1971');
INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user2', 'ch@ngem3', 'f', 'CA', '1972');
INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user3', 'ch@ngem3', 'f', 'CA', '1973');
{code}


+*Bug Details*+

*CONSISTENCY LOCAL_QUORUM*
{code}
BEGIN BATCH USING CONSISTENCY  LOCAL_QUORUM
UPDATE users SET state = 'UT' WHERE KEY = 'user1';
UPDATE users SET state = 'UT' WHERE KEY = 'user2';
UPDATE users SET state = 'UT' WHERE KEY = 'user3';
APPLY BATCH

cqlsh>  Bad Request: line 1:31 mismatched input 'LOCAL_QUORUM' expecting K_LEVEL
{code}

*CONSISTENCY EACH_QUORUM*
{code}
BEGIN BATCH USING CONSISTENCY  EACH_QUORUM
UPDATE users SET state = 'TX' WHERE KEY = 'user1';
UPDATE users SET state = 'TX' WHERE KEY = 'user2';
UPDATE users SET state = 'TX' WHERE KEY = 'user3';
APPLY BATCH

cqlsh> Bad Request: line 1:31 mismatched input 'EACH_QUORUM' expecting K_LEVEL
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/May/11 05:56;xedin;CASSANDRA-2566.patch;https://issues.apache.org/jira/secure/attachment/12479391/CASSANDRA-2566.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20700,,,Mon May 16 22:30:27 UTC 2011,,,,,,,,,,"0|i0gc4n:",93395,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"17/May/11 04:58;jbellis;Updated CQL CL documentation and edited issue description.
;;;","17/May/11 05:56;xedin;Added missing consistency levels to CQL grammar.;;;","17/May/11 06:01;jbellis;committed (also removed obsolete DCQUORUM* values);;;","17/May/11 06:30;hudson;Integrated in Cassandra-0.8 #111 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/111/])
    update cql consistency levels
patch by pyaskevich; reviewed by jbellis for CASSANDRA-2566
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
making cassandra-cli friendlier to scripts,CASSANDRA-1340,12470478,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,urandom,urandom,urandom,30/Jul/10 06:43,16/Apr/19 17:33,22/Mar/23 14:57,31/Jul/10 05:07,0.7 beta 1,,Legacy/Tools,,0,,,,,,"It is currently possible (and useful) to execute bulk commands with cassandra-cli using shell redirection. However, there is  no mechanism for handling errors, and the same output seen in an interactive session is echoed to the terminal.

The patch that follows accepts a new argument, (--batch), which:
* disables initialization of the history file
* suppresses output (stdout only)
* exits on error with status 2 for invalid syntax, 4 for invalid requests, and 8 for everything else.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Jul/10 06:44;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-1340.-cassandra-cli-batch-processing-mode.txt;https://issues.apache.org/jira/secure/attachment/12450870/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-1340.-cassandra-cli-batch-processing-mode.txt",,,,,,,,,,,,,,1.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20088,,,Sat Jul 31 12:51:02 UTC 2010,,,,,,,,,,"0|i0g4gf:",92152,,,,,Low,,,,,,,,,,,,,,,,,"30/Jul/10 07:37;jbellis;+1;;;","31/Jul/10 05:07;urandom;committed.;;;","31/Jul/10 20:51;hudson;Integrated in Cassandra #505 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/505/])
    CASSANDRA-1340. cassandra-cli: batch processing mode

Passing --batch on the command line causes normal output to be suppressed,
and errors to be fatal. Useful for executing bulk commands in a script ala:

   bin/cassandra-cli < commands.txt

Patch by eevans for CASSANDRA-1340
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DynamicEndPointSnitch backport not functional,CASSANDRA-1543,12475015,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,wouterdebie,wouterdebie,24/Sep/10 16:54,16/Apr/19 17:33,22/Mar/23 14:57,24/Sep/10 22:16,,,,,0,,,,,,"There seems to something really wrong with the DynamicEndpointSnitch in cassandra 0.6.5. There are several issues with it:

1) DatabaseDescriptor.readTablesFromXml() uses Boolean.getBoolean() with the value of System.getProperty(""cassandra.dynamic_snitch""). However, Boolean.getBoolean() also calls System.getProperty(). The result is that it executes a System.getProperty(""true"") or System.getProperty(""false"").
2) The RackAwareStrategy expects an instance of EndPointSnitch and will throw a RuntimeException if it's not. However, the DynamicEndpointSnitch wraps an EndPointSnitch, but extends AbstractEndpointSnitch. RackUnawareStrategy doesn't have this problem.
3) Since the DynamicEndpointSnitch registers an MBean, but is instantiated for every KeySpace, it breaks on trying to reregister the MBean.

I've attached a patch that solves problem 1 and 2, but not 3.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Sep/10 16:55;wouterdebie;0.6.5_dynamic_snitch_partial.patch;https://issues.apache.org/jira/secure/attachment/12455475/0.6.5_dynamic_snitch_partial.patch",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20190,,,Fri Sep 24 14:16:10 UTC 2010,,,,,,,,,,"0|i0g5vj:",92382,,,,,Normal,,,,,,,,,,,,,,,,,"24/Sep/10 22:16;jbellis;1) fixed based on your comments in CASSANDRA-981 (I read my mail in FIFO order :)
2) as also noted on 981, fixed in CASSANDRA-1429
3) raised in CASSANDRA-1448, left unresolved.  the ""right"" fix is to have a single global snitch which was done in 0.7.  open to workarounds for 0.6 but they all seem fairly unpleasant.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnsupportedOperationException: Overflow in bytesPastMark(..),CASSANDRA-2297,12500876,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,muga_nishizawa,muga_nishizawa,09/Mar/11 19:41,16/Apr/19 17:33,22/Mar/23 14:57,12/Mar/11 02:32,0.7.4,,,,0,,,,,,"I hit the following exception on a row that was more than 60GB.  
The row has column families of super column type.

This problem is discussed by the following thread.  
http://www.mail-archive.com/dev@cassandra.apache.org/msg01881.html

{code}
ERROR [HintedHandoff:1] 2011-02-26 18:49:35,708 DebuggableThreadPoolExecutor.java (line 103) Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.UnsupportedOperationException: Overflow: 2147484294
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.UnsupportedOperationException: Overflow: 2147484294
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.bytesPastMark(BufferedRandomAccessFile.java:477)
        at org.apache.cassandra.db.columniterator.IndexedSliceReader$IndexedBlockFetcher.getNextBlock(IndexedSliceReader.java:179)
        at org.apache.cassandra.db.columniterator.IndexedSliceReader.computeNext(IndexedSliceReader.java:120)
        at org.apache.cassandra.db.columniterator.IndexedSliceReader.computeNext(IndexedSliceReader.java:1)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.hasNext(SSTableSliceIterator.java:108)
        at org.apache.commons.collections.iterators.CollatingIterator.set(CollatingIterator.java:283)
        at org.apache.commons.collections.iterators.CollatingIterator.least(CollatingIterator.java:326)
        at org.apache.commons.collections.iterators.CollatingIterator.next(CollatingIterator.java:230)
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:68)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
        at org.apache.cassandra.db.filter.SliceQueryFilter.collectReducedColumns(SliceQueryFilter.java:118)
        at org.apache.cassandra.db.filter.QueryFilter.collectCollatedColumns(QueryFilter.java:142)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1290)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1167)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1095)
        at org.apache.cassandra.db.HintedHandOffManager.sendMessage(HintedHandOffManager.java:138)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:313)
        at org.apache.cassandra.db.HintedHandOffManager.access$1(HintedHandOffManager.java:262)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:391)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
{code}","Java 1.6.0_23, CentOS 5.5 (64bit)",,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,"11/Mar/11 01:07;jbellis;2297.txt;https://issues.apache.org/jira/secure/attachment/12473291/2297.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20547,,,Fri Mar 11 23:07:02 UTC 2011,,,,,,,,,,"0|i0gajj:",93138,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"11/Mar/11 01:07;jbellis;fix bytesPastMark to return long.

Note that if your row index is > 2GB then you should probably increase column_index_size_in_kb to 256 or higher.  I've created CASSANDRA-2308 to expose row index size so this can be tuned more accurately.;;;","12/Mar/11 02:16;slebresne;It breaks BufferedRandomAccessFileTest.
But +1 on the patch otherwise.;;;","12/Mar/11 02:32;jbellis;committed w/ removal of obsolete BRAFTest code;;;","12/Mar/11 07:07;hudson;Integrated in Cassandra-0.7 #375 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/375/])
    fix HH delivery when column index is larger than 2GB
patch by jbellis; reviewed by slebresne for CASSANDRA-2297
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Start up of 0.8-beta1 on Ubuntu,CASSANDRA-2549,12505024,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,selam,drewbroadley,drewbroadley,23/Apr/11 10:09,16/Apr/19 17:33,22/Mar/23 14:57,02/May/11 23:19,0.8.0 beta 2,,Packaging,,0,start,,,,,"root@home:/home/drew# cassandra -f
 INFO 14:06:03,261 Logging initialized
 INFO 14:06:03,323 Heap size: 1543831552/1543831552
 INFO 14:06:03,332 JNA not found. Native methods will be disabled.
 INFO 14:06:03,379 Loading settings from file:/etc/cassandra/cassandra.yaml
 INFO 14:06:03,899 DiskAccessMode 'auto' determined to be standard, indexAccessMode is standard
ERROR 14:06:04,028 Exception encountered during startup.
java.lang.NoClassDefFoundError: org/apache/cassandra/thrift/UnavailableException
	at java.lang.Class.getDeclaredMethods0(Native Method)
	at java.lang.Class.privateGetDeclaredMethods(Class.java:2444)
	at java.lang.Class.privateGetPublicMethods(Class.java:2564)
	at java.lang.Class.getMethods(Class.java:1427)
	at com.sun.jmx.mbeanserver.MBeanAnalyzer.initMaps(MBeanAnalyzer.java:126)
	at com.sun.jmx.mbeanserver.MBeanAnalyzer.<init>(MBeanAnalyzer.java:116)
	at com.sun.jmx.mbeanserver.MBeanAnalyzer.analyzer(MBeanAnalyzer.java:104)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.getAnalyzer(StandardMBeanIntrospector.java:66)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.getPerInterface(MBeanIntrospector.java:181)
	at com.sun.jmx.mbeanserver.MBeanSupport.<init>(MBeanSupport.java:136)
	at com.sun.jmx.mbeanserver.StandardMBeanSupport.<init>(StandardMBeanSupport.java:64)
	at com.sun.jmx.mbeanserver.Introspector.makeDynamicMBean(Introspector.java:174)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:936)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:330)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:516)
	at org.apache.cassandra.service.StorageService.<init>(StorageService.java:231)
	at org.apache.cassandra.service.StorageService.<clinit>(StorageService.java:171)
	at org.apache.cassandra.locator.DynamicEndpointSnitch.<init>(DynamicEndpointSnitch.java:78)
	at org.apache.cassandra.config.DatabaseDescriptor.createEndpointSnitch(DatabaseDescriptor.java:429)
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:294)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:98)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:314)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)
Caused by: java.lang.ClassNotFoundException: org.apache.cassandra.thrift.UnavailableException
	at java.net.URLClassLoader$1.run(URLClassLoader.java:217)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:205)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:321)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:294)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:266)
	... 23 more
Exception encountered during startup.
java.lang.NoClassDefFoundError: org/apache/cassandra/thrift/UnavailableException
	at java.lang.Class.getDeclaredMethods0(Native Method)
	at java.lang.Class.privateGetDeclaredMethods(Class.java:2444)
	at java.lang.Class.privateGetPublicMethods(Class.java:2564)
	at java.lang.Class.getMethods(Class.java:1427)
	at com.sun.jmx.mbeanserver.MBeanAnalyzer.initMaps(MBeanAnalyzer.java:126)
	at com.sun.jmx.mbeanserver.MBeanAnalyzer.<init>(MBeanAnalyzer.java:116)
	at com.sun.jmx.mbeanserver.MBeanAnalyzer.analyzer(MBeanAnalyzer.java:104)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.getAnalyzer(StandardMBeanIntrospector.java:66)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.getPerInterface(MBeanIntrospector.java:181)
	at com.sun.jmx.mbeanserver.MBeanSupport.<init>(MBeanSupport.java:136)
	at com.sun.jmx.mbeanserver.StandardMBeanSupport.<init>(StandardMBeanSupport.java:64)
	at com.sun.jmx.mbeanserver.Introspector.makeDynamicMBean(Introspector.java:174)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:936)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:330)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:516)
	at org.apache.cassandra.service.StorageService.<init>(StorageService.java:231)
	at org.apache.cassandra.service.StorageService.<clinit>(StorageService.java:171)
	at org.apache.cassandra.locator.DynamicEndpointSnitch.<init>(DynamicEndpointSnitch.java:78)
	at org.apache.cassandra.config.DatabaseDescriptor.createEndpointSnitch(DatabaseDescriptor.java:429)
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:294)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:98)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:314)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)
Caused by: java.lang.ClassNotFoundException: org.apache.cassandra.thrift.UnavailableException
	at java.net.URLClassLoader$1.run(URLClassLoader.java:217)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:205)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:321)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:294)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:266)
	... 23 more
","Linux home.broadley.org.nz 2.6.32-29-generic-pae #58-Ubuntu SMP Fri Feb 11 19:15:25 UTC 2011 i686 GNU/Linux
",selam,thepaul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Apr/11 04:07;selam;cassandra-0.8.0beta1-debian-package.patch;https://issues.apache.org/jira/secure/attachment/12477225/cassandra-0.8.0beta1-debian-package.patch","25/Apr/11 18:41;selam;cassandra_multiple_package_v2.patch;https://issues.apache.org/jira/secure/attachment/12477292/cassandra_multiple_package_v2.patch","26/Apr/11 23:27;selam;cassandra_multiple_package_v3.patch;https://issues.apache.org/jira/secure/attachment/12477420/cassandra_multiple_package_v3.patch",,,,,,,,,,,,3.0,selam,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20691,,,Mon May 02 19:54:49 UTC 2011,,,,,,,,,,"0|i0gc0v:",93378,,urandom,,urandom,Normal,,,,,,,,,,,,,,,,,"24/Apr/11 04:06;selam;deb package doesn't contain apache-cassandra-thrift-${VERSION}.jar and apache-cassandra-cql-*.jar 

My patch does not change package version or release number of deb package.;;;","24/Apr/11 08:51;prystupa;Timu, can your patch be applied to the currently latest binary 0.7.4 distribution?;;;","24/Apr/11 10:31;jbellis;No, there is only one jar in 0.7.;;;","25/Apr/11 00:32;urandom;The cql jar is not a dependency here.  If it needs to be packaged, it's something that would get its own package.;;;","25/Apr/11 09:26;jbellis;But from the stacktrace, the thrift jar is a dependency, no?;;;","25/Apr/11 10:36;urandom;Yes, the thrift jar is a dependency, the cql jar is not, the patch adds both.;;;","25/Apr/11 18:41;selam;my second patch generates multiple binary package.
this packages for: libthrift-java, cassandra, cassandra-thrift, cassandra-cql

cassandra and cassandra-cql depends cassandra-thrift.
cassandra-thrift depends libthrift-java

All package versions points to 0.8.0 but for Cql it must be 1.0.0 and for libthrift-java it must be 0.6. i working on for fix this, but i guess deb packaging system doesn't allowed to do this. another solution create separate debian directory for each package, but i guess this is not acceptable.

i build new packages from using this patch and i install 2 nodes without problem. 

patch name: cassandra_multiple_package_v2.patch;;;","26/Apr/11 14:44;drewbroadley;Thanks for the patch, this was enough to get things going again.

Would it be possible to create a separate 0.8 & 0.7 package as I wasn't wanting to jump to 0.8.;;;","26/Apr/11 23:27;selam;sorry,  i was remove some jars from cassandra package and added in dependencies, then after i remove dependencies but i forget to add package install dir. so i believe that patch is final patch. ;;;","27/Apr/11 02:08;urandom;I probably shouldn't have said ""If it needs to be packaged, it's something that would get its own package."", because I don't think it should (not yet at least, and not like this).

For one thing, the cql jar depends on more than the apache-cassandra-thrift jar; I'm not sure what your process was for testing, but it won't work like this.

Secondly, those dependencies (the ones between jars) will eventually be sorted, but then you're faced with the dependency on Thrift.  Our bundling of a private copy of Thrift is already something that brings shame to my ancestors, breaking that out into its own package is going to end in someone being reincarnated as a lemur, or a muskrat, or as the exhaust system of a 1997 Land Rover.  I'll have no part in it.;;;","27/Apr/11 07:32;urandom;+1(ish) to cassandra-0.8.0beta1-debian-package.patch;;;","02/May/11 23:19;jbellis;committed cassandra-0.8.0beta1-debian-package.patch;;;","03/May/11 03:54;hudson;Integrated in Cassandra-0.8 #58 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/58/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Need to close files in loadBloomFilter and loadIndexFile,CASSANDRA-533,12440111,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,tim freeman,tim freeman,tim freeman,08/Nov/09 06:29,16/Apr/19 17:33,22/Mar/23 14:57,08/Nov/09 07:22,0.4,0.5,,,0,,,,,,"When starting Cassandra on a Windows system, I intermittently see errors like:

DEBUG - Expected bloom filter size : 2560
DEBUG - collecting Generation:false:4@9
DEBUG - collecting Token:false:16@0
INFO - Saved Token found: 25027551081353517716727338628156823602
ERROR - Error in ThreadPoolExecutor
java.util.concurrent.ExecutionException: java.io.IOException: Failed to delete LocationInfo-8-Index.db
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:8
6)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.IOException: Failed to delete LocationInfo-8-Index.db
        at org.apache.cassandra.utils.FileUtils.deleteWithConfirm(FileUtils.java:55)
        at org.apache.cassandra.io.SSTableReader.delete(SSTableReader.java:269)
        at org.apache.cassandra.db.ColumnFamilyStore.doFileCompaction(ColumnFamilyStore.java:1145)
        at org.apache.cassandra.db.ColumnFamilyStore.doCompaction(ColumnFamilyStore.java:686)
        at org.apache.cassandra.db.MinorCompactionManager$1.call(MinorCompactionManager.java:166)
        at org.apache.cassandra.db.MinorCompactionManager$1.call(MinorCompactionManager.java:163)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        ... 2 more
DEBUG - Starting to listen on 127.0.0.1:7001
DEBUG - Binding thrift service to localhost:9160

The problem here is that the files are not closed immediately after they are read, and on Windows you can't delete an open file.  In general, failing to close a file causes more subtle problems on a Unix system.  You can run out of file descriptors if files are opened faster than the garbage collector closes them, and you can run out of disk space if the total data in the opened-but-deleted files is enough to fill the disk.

I see two places to fix, SSTableReader.loadIndexFile and SSTableReader.loadBloomFilter.  Here are revised definitions for those two methods:

    private void loadBloomFilter() throws IOException
    {
        DataInputStream stream = new DataInputStream(new FileInputStream(filterFilename()));
        try {
        	bf = BloomFilter.serializer().deserialize(stream);
        } finally {
        	stream.close();
        }
    }

    private void loadIndexFile() throws IOException
    {
        BufferedRandomAccessFile input = new BufferedRandomAccessFile(indexFilename(), ""r"");
        try {
        	indexPositions = new ArrayList<KeyPosition>();

        	int i = 0;
        	long indexSize = input.length();
        	while (true)
        	{
        		long indexPosition = input.getFilePointer();
        		if (indexPosition == indexSize)
        		{
        			break;
        		}
        		String decoratedKey = input.readUTF();
        		input.readLong();
        		if (i++ % INDEX_INTERVAL == 0)
        		{
        			indexPositions.add(new KeyPosition(decoratedKey, indexPosition));
        		}
        	}
        } finally {
        	input.close();
        }
    }   

I have not yet tested those changes, but they look desirable even if they don't fix the symptom I'm experiencing.  I did not search the code for other places that files are opened but not closed.
","Vista, Cassandra 0.4.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,tim freeman,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19741,,,Sat Nov 07 23:22:44 UTC 2009,,,,,,,,,,"0|i0fzin:",91352,,,,,Normal,,,,,,,,,,,,,,,,,"08/Nov/09 06:47;tim freeman;The fix did get rid of the stacktrace;;;","08/Nov/09 07:22;jbellis;committed to 0.4 branch; will merge to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE from PrecompactedRow,CASSANDRA-2528,12504909,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,cywjackson,cywjackson,22/Apr/11 01:55,16/Apr/19 17:33,22/Mar/23 14:57,22/Apr/11 06:48,0.8.0 beta 2,,,,0,,,,,,"received a NPE from trunk (0.8) on PrecompactedRow:

ERROR [CompactionExecutor:2] 2011-04-21 17:21:31,610 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[CompactionExecutor:2,1,main]
java.lang.NullPointerException
        at org.apache.cassandra.io.PrecompactedRow.<init>(PrecompactedRow.java:86)
        at org.apache.cassandra.io.CompactionIterator.getCompactedRow(CompactionIterator.java:167)
        at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:124)
        at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:44)
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:74)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
        at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
        at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:553)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:146)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:112)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)


size of data in /var/lib/cassandra is 11G on this, but there is also report that 1.7G also see the same.

data was previously populated from 0.7.4 cassandra

added debug logging, not sure how much this help (this is logged before the exception.)

 INFO [CompactionExecutor:2] 2011-04-21 17:21:31,588 CompactionManager.java (line 534) Compacting Major: [SSTableReader(path='/var/lib/cassandra/data/cfs/inode.path-f-10-Data.db'), SSTableReader(path='/var/lib/cassandra/data/cfs/inode.path-f-7-Data.db'), SSTableReader(path='/var/lib/cassandra/data/cfs/inode.path-f-6-Data.db'), SSTableReader(path='/var/lib/cassandra/data/cfs/inode.path-f-8-Data.db'), SSTableReader(path='/var/lib/cassandra/data/cfs/inode.path-f-9-Data.db')]
DEBUG [CompactionExecutor:2] 2011-04-21 17:21:31,588 SSTableReader.java (line 132) index size for bloom filter calc for file  : /var/lib/cassandra/data/cfs/inode.path-f-10-Data.db   : 256
DEBUG [CompactionExecutor:2] 2011-04-21 17:21:31,588 SSTableReader.java (line 132) index size for bloom filter calc for file  : /var/lib/cassandra/data/cfs/inode.path-f-7-Data.db   : 512
DEBUG [CompactionExecutor:2] 2011-04-21 17:21:31,588 SSTableReader.java (line 132) index size for bloom filter calc for file  : /var/lib/cassandra/data/cfs/inode.path-f-6-Data.db   : 768
DEBUG [CompactionExecutor:2] 2011-04-21 17:21:31,589 SSTableReader.java (line 132) index size for bloom filter calc for file  : /var/lib/cassandra/data/cfs/inode.path-f-8-Data.db   : 1024
DEBUG [CompactionExecutor:2] 2011-04-21 17:21:31,589 SSTableReader.java (line 132) index size for bloom filter calc for file  : /var/lib/cassandra/data/cfs/inode.path-f-9-Data.db   : 1280
 INFO [CompactionExecutor:2] 2011-04-21 17:21:31,609 CompactionIterator.java (line 185) Major@1181554512(cfs, inode.path, 523/10895) now compacting at 16777 bytes/ms.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Apr/11 05:04;jbellis;2528.txt;https://issues.apache.org/jira/secure/attachment/12477037/2528.txt","22/Apr/11 03:18;jbellis;2528.txt;https://issues.apache.org/jira/secure/attachment/12477027/2528.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20676,,,Thu Apr 21 23:18:50 UTC 2011,,,,,,,,,,"0|i0gbw7:",93357,,cywjackson,,cywjackson,Normal,,,,,,,,,,,,,,,,,"22/Apr/11 02:29;patricioe;I'm seeing the same stacktrace when writing 1GB of data. ;;;","22/Apr/11 03:18;jbellis;can you try w/ this patch?  it adds additional asserts to narrow down where the error is;;;","22/Apr/11 03:38;cywjackson;sure, here is the Assertion Error, the null is on metadata:
;;;","22/Apr/11 03:38;cywjackson;DEBUG [CompactionExecutor:2] 2011-04-21 19:31:46,797 PrecompactedRow.java (line 85) debugging controoler true
DEBUG [CompactionExecutor:2] 2011-04-21 19:31:46,798 PrecompactedRow.java (line 87) debugging compactedCF: ColumnFamily(<anonymous> [3636363663643736663936393536343639653762653339643735306363376439:false:0@1303340825329,])
 INFO [CompactionExecutor:2] 2011-04-21 19:31:46,799 CompactionIterator.java (line 185) Major@31583366(cfs, inode.path, 772/11720) now compacting at 8388 bytes/ms.
ERROR [CompactionExecutor:2] 2011-04-21 19:31:46,850 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[CompactionExecutor:2,1,main]
java.lang.AssertionError
    at org.apache.cassandra.io.PrecompactedRow.<init>(PrecompactedRow.java:91)
    at org.apache.cassandra.io.CompactionIterator.getCompactedRow(CompactionIterator.java:167)
    at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:124)
    at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:44)
    at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:74)
    at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
    at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
    at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
    at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
    at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:553)
    at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:146)
    at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:112)
    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662);;;","22/Apr/11 03:39;cywjackson;{code:title=PrepcompactedRow.java}
    85          logger.debug(String.format(""debugging controoler %b"", controller.shouldPurge(key)));
    86          compactedCf = controller.shouldPurge(key) ? ColumnFamilyStore.removeDeleted(cf, controller.gcBefore) : cf;
    87          logger.debug(String.format(""debugging compactedCF: %s"", compactedCf.toString()));
    88          //if (compactedCf != null && compactedCf.metadata().getDefaultValidator().isCommutative())
    89          if (compactedCf != null)
    90          {
    91                  assert compactedCf.metadata() != null;
    92                  assert compactedCf.metadata().getDefaultValidator() != null;
    93                  if (compactedCf.metadata().getDefaultValidator().isCommutative())
    94                  {
    95                          CounterColumn.removeOldShards(compactedCf, controller.gcBefore);
    96                  }
{code};;;","22/Apr/11 03:40;cywjackson;$ grep java.lang.AssertionError -A3 /var/log/cassandra/system.log 
{noformat}

java.lang.AssertionError
        at org.apache.cassandra.io.PrecompactedRow.<init>(PrecompactedRow.java:91)
        at org.apache.cassandra.io.CompactionIterator.getCompactedRow(CompactionIterator.java:167)
        at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:124)
--
java.lang.AssertionError
        at org.apache.cassandra.io.PrecompactedRow.<init>(PrecompactedRow.java:91)
        at org.apache.cassandra.io.CompactionIterator.getCompactedRow(CompactionIterator.java:167)
        at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:124)
{noformat};;;","22/Apr/11 05:04;jbellis;Patch to preserve the CFMetaData object used to construct the ColumnFamily, instead of looking it up from the registry (which does not contain metadata for index CFs).;;;","22/Apr/11 06:38;cywjackson;the fix does address the NPE as now all CF has metadata.;;;","22/Apr/11 06:48;jbellis;committed;;;","22/Apr/11 07:18;hudson;Integrated in Cassandra-0.8 #32 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/32/])
    fix NPE compacting index CFs
patch by jbellis; reviewed by Jackson Chung for CASSANDRA-2528
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mmap segment underflow when building secondary index after joining ring,CASSANDRA-2315,12501211,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,mdennis,mdennis,12/Mar/11 06:03,16/Apr/19 17:33,22/Mar/23 14:57,30/Apr/11 03:11,,,Feature/2i Index,,0,,,,,,"there was a 7 node ring (N0-N6) that was upgraded to 0.7.3
all the data was scrubbed
7 additional nodes were added (N7-N13) with no problems
some compactions ran, some things were restarted (but there was no read/write load)
after a while 7 more nodes (N14-N20) were added
all joined successfully except one
that node was pulling data from one of the nodes in the N7-N13 set (i.e. a node that got all it's data from the original nodes) 
the EC2 instance that was running was completely destroyed, a new instance created and the bootstrap attempted again but still failed

{code}
INFO [Thread-65] 2011-03-10 17:16:59,448 ColumnFamilyStore.java (line 377) Index build of service_id, complete
INFO [Thread-65] 2011-03-10 17:16:59,449 ColumnFamilyStore.java (line 359) Submitting index build of service_id, for data in SSTableReader(path='/mnt/cassandra/data/BackupifyMetadata/GoogleDocsMetadata-f-1-Data.db')
INFO [Thread-75] 2011-03-10 17:17:45,757 ColumnFamilyStore.java (line 377) Index build of service_id, complete
INFO [Thread-75] 2011-03-10 17:17:45,767 ColumnFamilyStore.java (line 359) Submitting index build of service_id, for data in SSTableReader(path='/mnt/cassandra/data/BackupifyMetadata/GoogleDocsMetadata-f-2-Data.db'), SSTableReader(path='/mnt/cassandra/data/BackupifyMetadata/GoogleDocsMetadata-f-3-Data.db'), SSTableReader(path='/mnt/cassandra/data/BackupifyMetadata/GoogleDocsMetadata-f-4-Data.db'), SSTableReader(path='/mnt/cassandra/data/BackupifyMetadata/GoogleDocsMetadata-f-5-Data.db')
INFO [ScheduledTasks:1] 2011-03-10 17:17:46,550 ColumnFamilyStore.java (line 1020) Enqueuing flush of Memtable-GoogleDocsMetadata.736572766963655f6964@1078361799(353352 bytes, 4539 operations)
INFO [FlushWriter:1] 2011-03-10 17:17:46,552 Memtable.java (line 157) Writing Memtable-GoogleDocsMetadata.736572766963655f6964@1078361799(353352 bytes, 4539 operations)
INFO [FlushWriter:1] 2011-03-10 17:17:46,736 Memtable.java (line 164) Completed flushing /mnt/cassandra/data/BackupifyMetadata/GoogleDocsMetadata.736572766963655f6964-f-1-Data.db (483139 bytes)
INFO [Thread-65] 2011-03-10 17:18:04,278 ColumnFamilyStore.java (line 377) Index build of service_id, complete
INFO [Thread-65] 2011-03-10 17:18:04,279 ColumnFamilyStore.java (line 359) Submitting index build of service_id, for data in SSTableReader(path='/mnt/cassandra/data/BackupifyMetadata/BloggerMetadata-f-1-Data.db')
ERROR [Thread-75] 2011-03-10 17:18:21,781 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[Thread-75,5,main]
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: mmap segment underflow; remaining is 43239499 but 1768777055 requested
        at org.apache.cassandra.db.ColumnFamilyStore.buildSecondaryIndexes(ColumnFamilyStore.java:375)
        at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:159)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:63)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:91)
Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError: mmap segment underflow; remaining is 43239499 but 1768777055 requested
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.db.ColumnFamilyStore.buildSecondaryIndexes(ColumnFamilyStore.java:365)
        ... 3 more
Caused by: java.lang.AssertionError: mmap segment underflow; remaining is 43239499 but 1768777055 requested
        at org.apache.cassandra.io.util.MappedFileDataInput.readBytes(MappedFileDataInput.java:119)
        at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:310)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithLength(ByteBufferUtil.java:267)
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:94)
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:35)
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:79)
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:40)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.hasNext(SSTableSliceIterator.java:108)
        at org.apache.commons.collections.iterators.CollatingIterator.anyHasNext(CollatingIterator.java:364)
        at org.apache.commons.collections.iterators.CollatingIterator.hasNext(CollatingIterator.java:217)
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:55)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
        at org.apache.cassandra.db.filter.SliceQueryFilter.collectReducedColumns(SliceQueryFilter.java:118)
        at org.apache.cassandra.db.filter.QueryFilter.collectCollatedColumns(QueryFilter.java:142)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1340)
        at org.apache.cassandra.db.ColumnFamilyStore.cacheRow(ColumnFamilyStore.java:1158)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1224)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1145)
        at org.apache.cassandra.db.Table.readCurrentIndexedColumns(Table.java:459)
        at org.apache.cassandra.db.Table.access$200(Table.java:56)
        at org.apache.cassandra.db.Table$IndexBuilder.build(Table.java:573)
        at org.apache.cassandra.db.CompactionManager$8.run(CompactionManager.java:892)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
ERROR [CompactionExecutor:1] 2011-03-10 17:18:21,785 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.AssertionError: mmap segment underflow; remaining is 43239499 but 1768777055 requested
        at org.apache.cassandra.io.util.MappedFileDataInput.readBytes(MappedFileDataInput.java:119)
        at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:310)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithLength(ByteBufferUtil.java:267)
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:94)
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:35)
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:79)
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:40)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.hasNext(SSTableSliceIterator.java:108)
        at org.apache.commons.collections.iterators.CollatingIterator.anyHasNext(CollatingIterator.java:364)
        at org.apache.commons.collections.iterators.CollatingIterator.hasNext(CollatingIterator.java:217)
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:55)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
        at org.apache.cassandra.db.filter.SliceQueryFilter.collectReducedColumns(SliceQueryFilter.java:118)
        at org.apache.cassandra.db.filter.QueryFilter.collectCollatedColumns(QueryFilter.java:142)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1340)
        at org.apache.cassandra.db.ColumnFamilyStore.cacheRow(ColumnFamilyStore.java:1158)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1224)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1145)
        at org.apache.cassandra.db.Table.readCurrentIndexedColumns(Table.java:459)
        at org.apache.cassandra.db.Table.access$200(Table.java:56)
        at org.apache.cassandra.db.Table$IndexBuilder.build(Table.java:573)
        at org.apache.cassandra.db.CompactionManager$8.run(CompactionManager.java:892)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code}
",,stuhood,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20556,,,Fri Apr 29 19:11:18 UTC 2011,,,,,,,,,,"0|i0ganj:",93156,,,,,Normal,,,,,,,,,,,,,,,,,"30/Apr/11 03:11;jbellis;I believe this is fixed by CASSANDRA-2283;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CommitLogTest and RecoveryManager2Test is failing in trunk,CASSANDRA-1318,12470134,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,mdennis,mdennis,mdennis,26/Jul/10 14:00,16/Apr/19 17:33,22/Mar/23 14:57,27/Jul/10 12:03,0.7 beta 1,,,,0,,,,,,"{code}
test:
     [echo] running unit tests
    [junit] WARNING: multiple versions of ant detected in path for junit 
    [junit]          jar:file:/usr/share/ant/lib/ant.jar!/org/apache/tools/ant/Project.class
    [junit]      and jar:file:/home/mdennis/mdev/trunkclean/build/lib/jars/ant-1.6.5.jar!/org/apache/tools/ant/Project.class
    [junit] Testsuite: org.apache.cassandra.db.CommitLogTest
    [junit] Tests run: 12, Failures: 1, Errors: 0, Time elapsed: 0.937 sec
    [junit] 
    [junit] Testcase: testCleanup(org.apache.cassandra.db.CommitLogTest):	FAILED
    [junit] null
    [junit] junit.framework.AssertionFailedError
    [junit] 	at org.apache.cassandra.db.CommitLogTest.testCleanup(CommitLogTest.java:66)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.CommitLogTest FAILED

BUILD FAILED
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jul/10 11:59;mdennis;1318-trunk.patch;https://issues.apache.org/jira/secure/attachment/12450556/1318-trunk.patch",,,,,,,,,,,,,,1.0,mdennis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20079,,,Tue Jul 27 13:41:07 UTC 2010,,,,,,,,,,"0|i0g4br:",92131,,,,,Low,,,,,,,,,,,,,,,,,"26/Jul/10 14:04;mdennis;It appears that when we started keeping persistent stats (CASSANDRA-1155) we ended up writing more than the half the segment size chosen by the test to the commit log causing it to fail.;;;","26/Jul/10 21:08;jbellis;wouldn't adding a flush to the system keyspace be a less fragile fix?;;;","26/Jul/10 21:47;jbellis;RecoveryManager2Test is also failing from the same 1155 commit, may be something similar;;;","27/Jul/10 01:42;mdennis;RecoveryManager2Test is nonde;;;","27/Jul/10 11:38;mdennis;Adding a flush to CommitLogTest doesn't help because forceBlockingFlush() eventually calls discardCompletedSegmentsInternal() which re-dirties the column family in case a write happens concurrently with the flush (e.g. CommitLog:411).;;;","27/Jul/10 12:03;jbellis;committed, minus the CLH commented-out code;;;","27/Jul/10 21:41;hudson;Integrated in Cassandra #501 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/501/])
    fix commitlog tests post-1135.  patch by mdennis; reviewed by jbellis for CASSANDRA-1318
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception when dropping keyspace,CASSANDRA-1444,12472883,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,drevell,drevell,31/Aug/10 05:02,16/Apr/19 17:33,22/Mar/23 14:57,03/Sep/10 22:15,0.7 beta 2,,,,0,,,,,,"I have a cluster of 4 nodes that are started with empty commitlog and data directories. Using the Thrift interface, I add a keyspace, wait for check_schema_agreement() to converge, then drop the keyspace. The drop fails with:

InvalidRequestException(why='java.util.concurrent.ExecutionException: java.lang.NullPointerException').

Nothing appears in cassandra.log. I'm running at log level DEBUG.

I am using the Python thrift bindings with no intervening software.

A python unittest that reproduces the problem is at http://pastie.org/1127539

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20142,,,Fri Sep 03 14:15:04 UTC 2010,,,,,,,,,,"0|i0g53j:",92256,,,,,Normal,,,,,,,,,,,,,,,,,"02/Sep/10 22:20;jbellis;can you still reproduce w/ latest trunk?  (i.e., after CASSANDRA-1406)

are multiple nodes required to reproduce?;;;","03/Sep/10 07:13;drevell;The same test passes now in rev 992142 (with minor modifications for new Thrift interface, http://pastie.org/1134921 ).

Hooray!;;;","03/Sep/10 22:15;jbellis;Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
64MB transferTo chunk size is too large for win32,CASSANDRA-793,12456328,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Duplicate,,jbellis,jbellis,15/Feb/10 23:13,16/Apr/19 17:33,22/Mar/23 14:57,15/Feb/10 23:35,0.5,,,,0,,,,,,"As reported by ruslan usifov on cassandra-user, he sees exceptions like

java.io.IOException: Невозможно выполнить операцию на сокете, т.к. буфер слишком мал или очередь переполнена
    at sun.nio.ch.SocketDispatcher.write0(Native Method)
    at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:33)
    at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:104)
    at sun.nio.ch.IOUtil.write(IOUtil.java:60)
    at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:334)
    at sun.nio.ch.FileChannelImpl.transferToTrustedChannel(FileChannelImpl.java:449)
    at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:520)
    at org.apache.cassandra.net.FileStreamTask.stream(FileStreamTask.java:95)

this appears to translate to ""insufficient buffer space.""

He also reports that reducing CHUNK_SIZE to 32MB fixes the problem.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Feb/10 23:14;jbellis;793.txt;https://issues.apache.org/jira/secure/attachment/12435874/793.txt",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19864,,,Mon Feb 15 15:35:45 UTC 2010,,,,,,,,,,"0|i0g13z:",91610,,,,,Low,,,,,,,,,,,,,,,,,"15/Feb/10 23:35;jbellis;dupes CASSANDRA-795;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra leaks FDs,CASSANDRA-283,12429861,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,ieure,ieure,09/Jul/09 05:34,16/Apr/19 17:33,22/Mar/23 14:57,14/Jul/09 02:56,0.4,,,,0,,,,,,"Cassandra leaks file descriptors like crazy. I started getting these errors after a few hours of uptime:

java.lang.RuntimeException: java.io.FileNotFoundException: /var/cassandra/data/Digg-Items-2-Data.db (Too many open files)
	at org.apache.cassandra.service.CassandraServer.readColumnFamily(CassandraServer.java:84)
	at org.apache.cassandra.service.CassandraServer.get_slice(CassandraServer.java:181)
	at org.apache.cassandra.service.Cassandra$Processor$get_slice.process(Cassandra.java:859)
	at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:817)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:252)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.FileNotFoundException: /var/cassandra/data/Digg-Items-2-Data.db (Too many open files)
	at java.io.RandomAccessFile.open(Native Method)
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
	at org.apache.cassandra.io.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:141)
	at org.apache.cassandra.io.SequenceFile$BufferReader.init(SequenceFile.java:811)
	at org.apache.cassandra.io.SequenceFile$Reader.<init>(SequenceFile.java:743)
	at org.apache.cassandra.io.SequenceFile$BufferReader.<init>(SequenceFile.java:805)
	at org.apache.cassandra.io.SequenceFile$ColumnGroupReader.<init>(SequenceFile.java:248)
	at org.apache.cassandra.io.SSTableReader.getColumnGroupReader(SSTableReader.java:346)
	at org.apache.cassandra.db.SSTableColumnIterator.<init>(ColumnIterator.java:61)
	at org.apache.cassandra.db.ColumnFamilyStore.getSliceFrom(ColumnFamilyStore.java:1589)
	at org.apache.cassandra.db.Table.getRow(Table.java:596)
	at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:60)
	at org.apache.cassandra.service.StorageProxy.weakReadLocal(StorageProxy.java:600)
	at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:303)
	at org.apache.cassandra.service.CassandraServer.readColumnFamily(CassandraServer.java:80)

I have an open file limit of 1024. Examining the lsof output for Cassandra shows 975 FDs for the same file: /var/cassandra/data/Digg-Items-2-Data.db

Clearly, these FDs are leaking somewhere.","Debian Etch, J2SE 1.6.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19619,,,Mon Jul 13 18:56:21 UTC 2009,,,,,,,,,,"0|i0fxzj:",91104,,,,,Normal,,,,,,,,,,,,,,,,,"10/Jul/09 23:31;junrao;There exists code in get_slice() that closes all files it opens at the end. Could you describe your scenario a bit more? Are you just issuing get_slice calls? How many requests were issued?;;;","10/Jul/09 23:37;jbellis;Let's re-test after committing CASSANDRA-287 so there's only one code path to debug. :);;;","10/Jul/09 23:43;jbellis;Actually now that I think of it I did fix a FD leak in 287 already.  In the code

               iter = filter.getSSTableColumnIterator(sstable);
               if (iter.hasNext())
               {
                   returnCF.delete(iter.getColumnFamily());
                   iterators.add(iter);
               }
               else
               {
                   iter.close();
               }

the ""else"" clause didn't exist before, and since it wasn't added to the list of iterators it wouldn't get closed at the end.;;;","11/Jul/09 23:01;jbellis;CASSANDRA-287 is committed.

Ian, do you still see leaking?;;;","14/Jul/09 02:18;ieure;We're not seeing this right now, though I suspect the issue still exists somewhere.

We determined that the problem was caused because Cassandra corrupted it's data files in some way which caused it to leak FDs when calling get_slice. After removing the data files, we couldn't reproduce the issue anymore.

So there's the issue with the initial corruption, which we haven't reproduced, and the issue with the FDs, which we might be able to reproduce at some point if the corruption can be reproduced.
;;;","14/Jul/09 02:56;jbellis;I'm going to close this then because CASSANDRA-287 fixes the most obvious FD leak.  Feel free to reopen if you have any more problems.

(Did you save a copy of the bad sstable files?);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
service.SerializationsTest failes under cobertura,CASSANDRA-2258,12500103,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,gdusbabek,jbellis,jbellis,02/Mar/11 05:39,16/Apr/19 17:33,22/Mar/23 14:57,15/Mar/11 21:07,0.7.5,,Legacy/Testing,,0,,,,,,"ant codecoverage -Dtest.name=SerializationsTest gives

{noformat}
    [junit] Testcase: testTreeResponseRead(org.apache.cassandra.service.SerializationsTest):	Caused an ERROR
    [junit] java.io.InvalidClassException: org.apache.cassandra.dht.BigIntegerToken; local class incompatible: stream classdesc serialVersionUID = -5833589141319293006, local class serialVersionUID = 2280189098581028124
    [junit] java.lang.RuntimeException: java.io.InvalidClassException: org.apache.cassandra.dht.BigIntegerToken; local class incompatible: stream classdesc serialVersionUID = -5833589141319293006, local class serialVersionUID = 2280189098581028124
    [junit] 	at org.apache.cassandra.service.AntiEntropyService$TreeResponseVerbHandler.deserialize(AntiEntropyService.java:634)
    [junit] 	at org.apache.cassandra.service.SerializationsTest.testTreeResponseRead(SerializationsTest.java:90)
    [junit] Caused by: java.io.InvalidClassException: org.apache.cassandra.dht.BigIntegerToken; local class incompatible: stream classdesc serialVersionUID = -5833589141319293006, local class serialVersionUID = 2280189098581028124
    [junit] 	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:562)
    [junit] 	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1582)
    [junit] 	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1495)
    [junit] 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1731)
    [junit] 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1328)
    [junit] 	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1946)
    [junit] 	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1870)
    [junit] 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1752)
    [junit] 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1328)
    [junit] 	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1946)
    [junit] 	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1870)
    [junit] 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1752)
    [junit] 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1328)
    [junit] 	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1946)
    [junit] 	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1870)
    [junit] 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1752)
    [junit] 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1328)
    [junit] 	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1946)
    [junit] 	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1870)
    [junit] 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1752)
    [junit] 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1328)
    [junit] 	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1946)
    [junit] 	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1870)
    [junit] 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1752)
    [junit] 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1328)
    [junit] 	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:350)
    [junit] 	at org.apache.cassandra.service.AntiEntropyService$TreeResponseVerbHandler.deserialize(AntiEntropyService.java:630)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Mar/11 05:02;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-do-not-instrument-the-Token-classes.txt;https://issues.apache.org/jira/secure/attachment/12473614/ASF.LICENSE.NOT.GRANTED--v1-0001-do-not-instrument-the-Token-classes.txt","15/Mar/11 04:53;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-fix-cobertura-brokenness-by-forcing-serialVersionUIDs.txt;https://issues.apache.org/jira/secure/attachment/12473610/ASF.LICENSE.NOT.GRANTED--v1-0001-fix-cobertura-brokenness-by-forcing-serialVersionUIDs.txt",,,,,,,,,,,,,2.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20531,,,Wed Mar 16 00:31:14 UTC 2011,,,,,,,,,,"0|i0gaav:",93099,,,,,Low,,,,,,,,,,,,,,,,,"15/Mar/11 05:04;gdusbabek;Several problems here:
1.  AntiEntropyService is using serialized java objects.
2.  Cobertura doesn't preserve serialVersionUID during instrumentation.

I've attached two possible fixes, neither of them very impressive.

The first forces the serialVersionUID for all Token descendants to 1L.  This is bad because it breaks wire compatibility between $THIS_VERSION and 0.7.x (see CASSANDRA-1015 for why we shouldn't allow this).

The second disables instrumenting *Token.class during the instrumentation phase.  Upshot is that we don't get code coverage reports for those classes.;;;","15/Mar/11 06:10;jbellis;bq. The second disables instrumenting *Token.class during the instrumentation phase. Upshot is that we don't get code coverage reports for those classes.

+1 this approach if you add a comment to build.xml for posterity :);;;","15/Mar/11 21:07;gdusbabek;committed;;;","16/Mar/11 00:46;hudson;Integrated in Cassandra-0.7 #382 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/382/])
    ;;;","16/Mar/11 08:31;hudson;Integrated in Cassandra #781 (See [https://hudson.apache.org/hudson/job/Cassandra/781/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DatacenterShardStrategy nodetool ring command shows incorrect/duplicate nodes,CASSANDRA-1304,12469732,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,jigneshdhruv,jigneshdhruv,21/Jul/10 00:55,16/Apr/19 17:33,22/Mar/23 14:57,21/Jul/10 04:47,,,,,0,,,,,,"Hello,

We are planning to use DatacenterShardStrategy for our application. In cassandra-rack.properties file we define the IP address and the RACK information for each machine and in datacenters.properties we define how many copies needs to be created in each datacenter.

When we load the data, correct copies gets created in each datacenter however when we run the ring command using nodeTool, we do not see all the nodes in the cluster.
Say we have 2 DC's and each DC has 5 machines, and one data copy in each datacenter. Now when we run the ring command we see 7 machines from DC1 and 3 from DC2. i.e. same node from DC1 gets displayed twice or thrice with a different key address.

I believe this is eventually creating problem as the data is not balanced properly within a datacenter.

Also at times when we run nodeTool-loadBalance data gets moved from one datacenter to another instead of being balanced within the same datacenter.

Can you please look into this issue?

Thanks,
Jignesh",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20071,,,Tue Jul 20 20:47:38 UTC 2010,,,,,,,,,,"0|i0g48v:",92118,,,,,Normal,,,,,,,,,,,,,,,,,"21/Jul/10 04:47;jbellis;I believe this is the same as CASSANDRA-1291, assuming you're using trunk.  (If you're not using trunk, you shouldn't be using DatacenterShardStrategy.);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Database gets corrupted?,CASSANDRA-2197,12499073,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,osmadja,osmadja,19/Feb/11 01:49,16/Apr/19 17:33,22/Mar/23 14:57,19/Feb/11 01:59,,,,,0,,,,,,"When starting cassandra server, I get the following exception :


ERROR 15:39:17,946 Exception encountered during startup.
java.io.EOFException
	at java.io.DataInputStream.readFully(DataInputStream.java:180)
	at org.apache.cassandra.utils.FBUtilities.readShortByteArray(FBUtilities.java:314)
	at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:66)
	at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:364)
	at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:313)
	at org.apache.cassandra.db.ColumnFamilySerializer.deserializeColumns(ColumnFamilySerializer.java:129)
	at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:120)
	at org.apache.cassandra.db.RowMutationSerializer.defreezeTheMaps(RowMutation.java:385)
	at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:395)
	at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:353)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:283)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:194)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:143)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:55)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:217)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:134)

And the server does not start :-(. 

I don't know what to do.
Thanks,
Olivier
",Mac Os X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20497,,,Fri Feb 18 18:08:27 UTC 2011,,,,,,,,,,"0|i0g9wv:",93036,,,,,Normal,,,,,,,,,,,,,,,,,"19/Feb/11 01:51;osmadja;Here is the full server log :

Last login: Fri Feb 18 15:06:14 on console
olivier-smadjas-macbook-pro:~ OlivierSmadja$ cd Apps/apache-cassandra-0.7.0/bin/olivier-smadjas-macbook-pro:bin OlivierSmadja$ sh cassandra
olivier-smadjas-macbook-pro:bin OlivierSmadja$  INFO 15:39:14,145 Heap size: 1069416448/1069416448
 INFO 15:39:14,166 JNA not found. Native methods will be disabled.
 INFO 15:39:14,184 Loading settings from file:/Users/olivier/Apps/apache-cassandra-0.7.0/conf/cassandra.yaml
 INFO 15:39:14,461 DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
 INFO 15:39:14,731 Creating new commitlog segment /Users/olivier/Data/Cassandra_prod/commitlog/CommitLog-1298050754731.log
 INFO 15:39:14,944 Deleted /Users/olivier/Data/Cassandra_prod/data/system/IndexInfo-e-26
 INFO 15:39:14,945 Deleted /Users/olivier/Data/Cassandra_prod/data/system/IndexInfo-e-28
 INFO 15:39:14,946 Deleted /Users/olivier/Data/Cassandra_prod/data/system/IndexInfo-e-23
 INFO 15:39:14,946 Deleted /Users/olivier/Data/Cassandra_prod/data/system/IndexInfo-e-18
 INFO 15:39:15,000 Deleted /Users/olivier/Data/Cassandra_prod/data/system/IndexInfo-e-36
 INFO 15:39:15,001 Deleted /Users/olivier/Data/Cassandra_prod/data/system/IndexInfo-e-30
 INFO 15:39:15,002 Deleted /Users/olivier/Data/Cassandra_prod/data/system/IndexInfo-e-24
 INFO 15:39:15,002 Deleted /Users/olivier/Data/Cassandra_prod/data/system/IndexInfo-e-31
 INFO 15:39:15,011 Deleted /Users/olivier/Data/Cassandra_prod/data/system/IndexInfo-e-17
 INFO 15:39:15,012 Deleted /Users/olivier/Data/Cassandra_prod/data/system/IndexInfo-e-25
 INFO 15:39:15,012 Deleted /Users/olivier/Data/Cassandra_prod/data/system/IndexInfo-e-34
 INFO 15:39:15,015 Deleted /Users/olivier/Data/Cassandra_prod/data/system/IndexInfo-e-20
 INFO 15:39:15,017 Deleted /Users/olivier/Data/Cassandra_prod/data/system/IndexInfo-e-33
 INFO 15:39:15,021 Deleted /Users/olivier/Data/Cassandra_prod/data/system/IndexInfo-e-27
 INFO 15:39:15,022 Deleted /Users/olivier/Data/Cassandra_prod/data/system/IndexInfo-e-22
 INFO 15:39:15,023 Deleted /Users/olivier/Data/Cassandra_prod/data/system/IndexInfo-e-19
 INFO 15:39:15,024 Deleted /Users/olivier/Data/Cassandra_prod/data/system/IndexInfo-e-32
 INFO 15:39:15,025 Deleted /Users/olivier/Data/Cassandra_prod/data/system/IndexInfo-e-21
 INFO 15:39:15,025 Deleted /Users/olivier/Data/Cassandra_prod/data/system/IndexInfo-e-35
 INFO 15:39:15,026 Deleted /Users/olivier/Data/Cassandra_prod/data/system/IndexInfo-e-29
 INFO 15:39:15,040 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-86
 INFO 15:39:15,041 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-79
 INFO 15:39:15,042 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-92
 INFO 15:39:15,043 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-116
 INFO 15:39:15,051 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-61
 INFO 15:39:15,052 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-64
 INFO 15:39:15,052 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-62
 INFO 15:39:15,056 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-63
 INFO 15:39:15,057 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-67
 INFO 15:39:15,058 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-85
 INFO 15:39:15,059 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-106
 INFO 15:39:15,059 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-69
 INFO 15:39:15,060 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-104
 INFO 15:39:15,061 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-78
 INFO 15:39:15,062 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-83
 INFO 15:39:15,062 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-68
 INFO 15:39:15,063 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-84
 INFO 15:39:15,063 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-80
 INFO 15:39:15,064 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-93
 INFO 15:39:15,065 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-105
 INFO 15:39:15,065 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-107
 INFO 15:39:15,068 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-94
 INFO 15:39:15,070 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-74
 INFO 15:39:15,081 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-81
 INFO 15:39:15,081 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-72
 INFO 15:39:15,082 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-87
 INFO 15:39:15,284 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-58
 INFO 15:39:15,630 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-100
 INFO 15:39:15,632 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-95
 INFO 15:39:15,633 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-73
 INFO 15:39:15,634 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-77
 INFO 15:39:15,635 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-98
 INFO 15:39:15,636 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-101
 INFO 15:39:15,637 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-70
 INFO 15:39:15,640 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-97
 INFO 15:39:15,642 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-76
 INFO 15:39:15,643 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-88
 INFO 15:39:15,644 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-57
 INFO 15:39:15,644 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-60
 INFO 15:39:15,645 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-96
 INFO 15:39:15,646 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-103
 INFO 15:39:15,646 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-75
 INFO 15:39:15,647 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-90
 INFO 15:39:15,648 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-91
 INFO 15:39:15,648 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-102
 INFO 15:39:15,650 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-89
 INFO 15:39:15,650 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-71
 INFO 15:39:15,660 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-59
 INFO 15:39:15,661 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-111
 INFO 15:39:15,662 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-110
 INFO 15:39:15,662 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-108
 INFO 15:39:15,673 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-99
 INFO 15:39:15,674 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-109
 INFO 15:39:15,675 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-82
 INFO 15:39:15,676 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-119
 INFO 15:39:15,677 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-112
 INFO 15:39:15,677 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-113
 INFO 15:39:15,678 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-120
 INFO 15:39:15,678 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-65
 INFO 15:39:15,679 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-117
 INFO 15:39:15,680 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-114
 INFO 15:39:15,680 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-118
 INFO 15:39:15,681 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-115
 INFO 15:39:15,682 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-66
 INFO 15:39:15,694 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-107
 INFO 15:39:15,694 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-76
 INFO 15:39:15,695 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-73
 INFO 15:39:15,696 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-118
 INFO 15:39:15,697 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-78
 INFO 15:39:15,697 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-104
 INFO 15:39:15,698 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-72
 INFO 15:39:15,699 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-77
 INFO 15:39:15,700 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-79
 INFO 15:39:15,701 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-105
 INFO 15:39:15,701 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-70
 INFO 15:39:15,702 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-75
 INFO 15:39:15,702 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-81
 INFO 15:39:15,703 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-106
 INFO 15:39:15,916 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-61
 INFO 15:39:15,918 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-117
 INFO 15:39:15,920 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-74
 INFO 15:39:15,921 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-116
 INFO 15:39:15,922 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-71
 INFO 15:39:15,923 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-86
 INFO 15:39:15,924 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-80
 INFO 15:39:15,926 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-59
 INFO 15:39:15,927 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-98
 INFO 15:39:15,928 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-92
 INFO 15:39:15,929 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-60
 INFO 15:39:15,930 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-62
 INFO 15:39:15,931 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-97
 INFO 15:39:15,932 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-91
 INFO 15:39:15,933 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-57
 INFO 15:39:15,934 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-95
 INFO 15:39:15,934 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-93
 INFO 15:39:15,935 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-103
 INFO 15:39:15,936 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-58
 INFO 15:39:15,936 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-63
 INFO 15:39:15,937 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-96
 INFO 15:39:15,938 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-94
 INFO 15:39:15,939 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-64
 INFO 15:39:15,939 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-89
 INFO 15:39:15,940 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-119
 INFO 15:39:15,940 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-65
 INFO 15:39:15,941 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-99
 INFO 15:39:15,942 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-100
 INFO 15:39:15,943 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-68
 INFO 15:39:15,943 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-90
 INFO 15:39:15,944 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-120
 INFO 15:39:15,945 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-67
 INFO 15:39:15,945 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-102
 INFO 15:39:15,946 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-111
 INFO 15:39:15,947 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-88
 INFO 15:39:15,947 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-66
 INFO 15:39:15,948 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-87
 INFO 15:39:15,949 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-101
 INFO 15:39:15,950 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-115
 INFO 15:39:15,950 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-85
 INFO 15:39:15,951 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-110
 INFO 15:39:15,952 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-112
 INFO 15:39:15,953 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-84
 INFO 15:39:15,953 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-113
 INFO 15:39:15,954 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-114
 INFO 15:39:15,956 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-108
 INFO 15:39:15,957 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-82
 INFO 15:39:15,957 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-109
 INFO 15:39:15,958 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-69
 INFO 15:39:15,959 Deleted /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-83
 INFO 15:39:15,975 Deleted /Users/olivier/Data/Cassandra_prod/data/system/LocationInfo-e-7
 INFO 15:39:15,976 Deleted /Users/olivier/Data/Cassandra_prod/data/system/LocationInfo-e-8
 INFO 15:39:15,986 Deleted /Users/olivier/Data/Cassandra_prod/data/system/LocationInfo-e-5
 INFO 15:39:15,987 Deleted /Users/olivier/Data/Cassandra_prod/data/system/LocationInfo-e-6
 INFO 15:39:16,136 reading saved cache /Users/olivier/Data/Cassandra_prod/saved_caches/system-IndexInfo-KeyCache
 INFO 15:39:16,185 Opening /Users/olivier/Data/Cassandra_prod/data/system/IndexInfo-e-39
 INFO 15:39:16,278 Opening /Users/olivier/Data/Cassandra_prod/data/system/IndexInfo-e-37
 INFO 15:39:16,286 Opening /Users/olivier/Data/Cassandra_prod/data/system/IndexInfo-e-38
 INFO 15:39:16,306 reading saved cache /Users/olivier/Data/Cassandra_prod/saved_caches/system-Schema-KeyCache
 INFO 15:39:16,309 Opening /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-123
 INFO 15:39:16,338 Opening /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-122
 INFO 15:39:16,354 Opening /Users/olivier/Data/Cassandra_prod/data/system/Schema-e-121
 INFO 15:39:16,375 reading saved cache /Users/olivier/Data/Cassandra_prod/saved_caches/system-Migrations-KeyCache
 INFO 15:39:16,377 Opening /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-123
 INFO 15:39:16,383 Opening /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-122
 INFO 15:39:16,390 Opening /Users/olivier/Data/Cassandra_prod/data/system/Migrations-e-121
 INFO 15:39:16,402 reading saved cache /Users/olivier/Data/Cassandra_prod/saved_caches/system-LocationInfo-KeyCache
 INFO 15:39:16,404 Opening /Users/olivier/Data/Cassandra_prod/data/system/LocationInfo-e-10
 INFO 15:39:16,415 Opening /Users/olivier/Data/Cassandra_prod/data/system/LocationInfo-e-9
 INFO 15:39:16,424 reading saved cache /Users/olivier/Data/Cassandra_prod/saved_caches/system-HintsColumnFamily-KeyCache
 INFO 15:39:16,605 Loading schema version 88d4b8cf-3b11-11e0-8feb-e700f669bcfc
 WARN 15:39:17,099 Schema definitions were defined both locally and in cassandra.yaml. Definitions in cassandra.yaml were ignored.
 INFO 15:39:17,139 Deleted /Users/olivier/Data/Cassandra_prod/data/tapix_test/LastQuote-e-1
 INFO 15:39:17,167 Deleted /Users/olivier/Data/Cassandra_prod/data/tapix_test/User-e-1
 INFO 15:39:17,176 Deleted /Users/olivier/Data/Cassandra_prod/data/tapix_test/User.index_user_email-e-1
 INFO 15:39:17,193 Deleted /Users/olivier/Data/Cassandra_prod/data/tapix_test/UserStreamInfo-e-1
 INFO 15:39:17,199 Deleted /Users/olivier/Data/Cassandra_prod/data/tapix_test/TapixConfig-e-1
 INFO 15:39:17,205 Deleted /Users/olivier/Data/Cassandra_prod/data/tapix_test/PendingInvitation-e-1
 INFO 15:39:17,224 Deleted /Users/olivier/Data/Cassandra_prod/data/tapix_test/PendingInvitation.index_invitation_user_creator_oid-e-1
 INFO 15:39:17,226 Deleted /Users/olivier/Data/Cassandra_prod/data/tapix_test/PendingInvitation.index_invitation_invitation_email-e-1
 INFO 15:39:17,230 Deleted /Users/olivier/Data/Cassandra_prod/data/tapix_test/PendingInvitation.index_invitation_external_key-e-1
 INFO 15:39:17,247 Deleted /Users/olivier/Data/Cassandra_prod/data/tapix_test/CompanyArea-e-1
 INFO 15:39:17,250 Deleted /Users/olivier/Data/Cassandra_prod/data/tapix_test/Log-e-1
 INFO 15:39:17,264 Deleted /Users/olivier/Data/Cassandra_prod/data/tapix_test/CompanySymbol-e-1
 INFO 15:39:17,266 Deleted /Users/olivier/Data/Cassandra_prod/data/tapix_test/Stream-e-1
 INFO 15:39:17,268 Deleted /Users/olivier/Data/Cassandra_prod/data/tapix_test/Stream.index_stream_owner_user_oid-e-1
 INFO 15:39:17,275 Deleted /Users/olivier/Data/Cassandra_prod/data/tapix_test/UserInfo-e-1
 INFO 15:39:17,280 Deleted /Users/olivier/Data/Cassandra_prod/data/tapix_test/Company-e-1
 INFO 15:39:17,283 Deleted /Users/olivier/Data/Cassandra_prod/data/tapix_test/Marker-e-1
 INFO 15:39:17,284 Deleted /Users/olivier/Data/Cassandra_prod/data/tapix_test/Marker.index_markerk_user_oid-e-1
 INFO 15:39:17,285 Deleted /Users/olivier/Data/Cassandra_prod/data/tapix_test/Marker.index_marker_object_id-e-1
 INFO 15:39:17,287 Deleted /Users/olivier/Data/Cassandra_prod/data/tapix_test/Marker.index_marker_marker_type-e-1
 INFO 15:39:17,492 reading saved cache /Users/olivier/Data/Cassandra_prod/saved_caches/test-Posts-KeyCache
 INFO 15:39:17,498 Opening /Users/olivier/Data/Cassandra_prod/data/test/Posts-e-1
 INFO 15:39:17,561 Opening /Users/olivier/Data/Cassandra_prod/data/test/StreamElement-e-2
 INFO 15:39:17,574 Opening /Users/olivier/Data/Cassandra_prod/data/test/StreamElement-e-1
 INFO 15:39:17,596 reading saved cache /Users/olivier/Data/Cassandra_prod/saved_caches/test-Stream-KeyCache
 INFO 15:39:17,677 Replaying /Users/olivier/Data/Cassandra_prod/commitlog/CommitLog-1297976161889.log
 INFO 15:39:17,945 Finished reading /Users/olivier/Data/Cassandra_prod/commitlog/CommitLog-1297976161889.log
ERROR 15:39:17,946 Exception encountered during startup.
java.io.EOFException
	at java.io.DataInputStream.readFully(DataInputStream.java:180)
	at org.apache.cassandra.utils.FBUtilities.readShortByteArray(FBUtilities.java:314)
	at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:66)
	at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:364)
	at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:313)
	at org.apache.cassandra.db.ColumnFamilySerializer.deserializeColumns(ColumnFamilySerializer.java:129)
	at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:120)
	at org.apache.cassandra.db.RowMutationSerializer.defreezeTheMaps(RowMutation.java:385)
	at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:395)
	at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:353)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:283)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:194)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:143)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:55)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:217)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:134)
Exception encountered during startup.
java.io.EOFException
	at java.io.DataInputStream.readFully(DataInputStream.java:180)
	at org.apache.cassandra.utils.FBUtilities.readShortByteArray(FBUtilities.java:314)
	at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:66)
	at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:364)
	at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:313)
	at org.apache.cassandra.db.ColumnFamilySerializer.deserializeColumns(ColumnFamilySerializer.java:129)
	at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:120)
	at org.apache.cassandra.db.RowMutationSerializer.defreezeTheMaps(RowMutation.java:385)
	at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:395)
	at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:353)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:283)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:194)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:143)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:55)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:217)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:134)
olivier-smadjas-macbook-pro:bin OlivierSmadja$ 


;;;","19/Feb/11 01:59;jbellis;This looks like CASSANDRA-2128. Upgrade to 0.7.2.;;;","19/Feb/11 02:08;osmadja;Ok. Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
replace fastsync,CASSANDRA-39,12421811,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,jbellis,jbellis,02/Apr/09 04:30,16/Apr/19 17:33,22/Mar/23 14:57,22/May/09 03:09,,,,,0,,,,,,"Problems have been reported with fastsync: http://groups.google.com/group/cassandra-user/browse_thread/thread/c21c8e933d1e2af1/1a65f5625a7390d5?hl=en&lnk=gst&q=fastsync#1a65f5625a7390d5

Avinash says that it works for them in testing but he would not trust it in production.

He also says he is planning to replace it with something else in the near future.  This is a reminder to get more details.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19524,,,Thu May 21 19:09:35 UTC 2009,,,,,,,,,,"0|i0fwif:",90865,,,,,Normal,,,,,,,,,,,,,,,,,"22/May/09 03:09;jbellis;superceded by CASSANDRA-182;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
File descriptors to sstables not closed (even for sstables that have been deleted due to compaction),CASSANDRA-1858,12493145,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,blanquer,blanquer,14/Dec/10 08:56,16/Apr/19 17:33,22/Mar/23 14:57,14/Dec/10 10:33,,,,,0,,,,,,"The Cassandra process doesn't let go of filedescriptors to sstables.
It was initially spotted due to the fact that the disk utilization was really high, while the data dirs of sstables held much less (nicely compacted) data.
Performing an ""lsof"" lists the Cassandra process having basically all open FD's pointing to lots and lots of deleted files. In fact, I saw the ""-1-Data.db"" in the list...which tells me that not even the first sstable was let go.

Restarting the cassandra process obviously gets rid of the references. I've also manually triggered a GC to try to make sure compaction is completed...to no avail. Holding FD's for a high-throughput write servers is a big problem since it accumulates a huge amount of disk space due to compactions..etc...so it's very easy to run out of space.


There's nothing special in this setup, so I think it'd be easily reproducible. I get this in a:
* 4-node cluster, 1 Keyspace, 1 CF, RF=2
* the cluster has basically been stood up, and blasted with inserts (now it has 200GB, 50 each)...but it doesn't get reads (I'm performing just loading data tests).
* While the loading is happening, one can observe that the process keeps compacting and creating new sstables...but never lets the FD's go.
* even if I trigger compactions and cleanups...the disk util only continues to go up...i.e., all FD's are still open, plus the compated sstables are added.
* at a particular node I see about 11 fd's for current sstables, and 1250 fd's pointing to deleted sstables.

",Ubuntu 8.04 with Java 1.6.0_22-b04,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20345,,,Tue Dec 14 02:33:31 UTC 2010,,,,,,,,,,"0|i0g7u7:",92700,,,,,Normal,,,,,,,,,,,,,,,,,"14/Dec/10 09:01;mdennis;possibly fixed by cassandra-1495 ?;;;","14/Dec/10 10:33;jbellis;if he saw it in rc1, it's probably CASSANDRA-1790 (fixed in rc2);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Node not responding, bringing down cluster, marked as up",CASSANDRA-2543,12504977,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,tbritz,tbritz,22/Apr/11 21:05,16/Apr/19 17:33,22/Mar/23 14:57,29/Apr/11 02:23,,,,,0,,,,,,"I have one node which constantly hangs and brings done the entire cluster (not giving any answers).
If I restart the node, the node will hang after a certain number of time. I have no indication

It's marked as up when executing the nodetool ring command.
Executing the ring command on the node itself (without any traffic on the cluster) takes at least 2 minutes to execute. The node takes about 50%-100% of cpu over all cpus.


Netstats doesn't list anything interesting:

/software/cassandra/bin/nodetool -h localhost netstats
Mode: Normal
Not sending any streams.
Not receiving any streams.
Pool Name                    Active   Pending      Completed
Commands                        n/a         0          51064
Responses                       n/a         0         530479


I attached the jstack of the node. There are no indications that the node has faulty hardware. 



/usr/bin/java -ea -XX:+UseThreadPriorities -XX:ThreadPriorityPolicy=42 -Xms5254M -Xmx5254M -Xmn400M -XX:+HeapDumpOnOutOfMemoryError -Xss128k -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:SurvivorRatio=8 -XX:MaxTenuringThreshold=1 -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -Djava.net.preferIPv4Stack=true -Dcom.sun.management.jmxremote.port=8080 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Dlog4j.configuration=log4j-server.properties -Dlog4j.defaultInitOverride=true -Dcassandra-foreground=yes -cp /software/cassandra/bin/../conf:/software/cassandra/bin/../build/classes:/software/cassandra/bin/../lib/antlr-3.1.3.jar:/software/cassandra/bin/../lib/apache-cassandra-0.7.4.jar:/software/cassandra/bin/../lib/avro-1.4.0-fixes.jar:/software/cassandra/bin/../lib/avro-1.4.0-sources-fixes.jar:/software/cassandra/bin/../lib/commons-cli-1.1.jar:/software/cassandra/bin/../lib/commons-codec-1.2.jar:/software/cassandra/bin/../lib/commons-collections-3.2.1.jar:/software/cassandra/bin/../lib/commons-lang-2.4.jar:/software/cassandra/bin/../lib/concurrentlinkedhashmap-lru-1.1.jar:/software/cassandra/bin/../lib/guava-r05.jar:/software/cassandra/bin/../lib/high-scale-lib.jar:/software/cassandra/bin/../lib/jackson-core-asl-1.4.0.jar:/software/cassandra/bin/../lib/jackson-mapper-asl-1.4.0.jar:/software/cassandra/bin/../lib/jetty-6.1.21.jar:/software/cassandra/bin/../lib/jetty-util-6.1.21.jar:/software/cassandra/bin/../lib/jline-0.9.94.jar:/software/cassandra/bin/../lib/json-simple-1.1.jar:/software/cassandra/bin/../lib/jug-2.0.0.jar:/software/cassandra/bin/../lib/libthrift-0.5.jar:/software/cassandra/bin/../lib/log4j-1.2.16.jar:/software/cassandra/bin/../lib/servlet-api-2.5-20081211.jar:/software/cassandra/bin/../lib/slf4j-api-1.6.1.jar:/software/cassandra/bin/../lib/slf4j-log4j12-1.6.1.jar:/software/cassandra/bin/../lib/snakeyaml-1.6.jar org.apache.cassandra.thrift.CassandraDaemon




",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Apr/11 21:05;tbritz;jstack;https://issues.apache.org/jira/secure/attachment/12477102/jstack",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20686,,,Thu Apr 28 18:23:55 UTC 2011,,,,,,,,,,"0|i0gbzj:",93372,,,,,Normal,,,,,,,,,,,,,,,,,"22/Apr/11 21:07;tbritz;/software/cassandra/bin/nodetool -h localhost info
199
Gossip active    : true
Load             : 161.12 GB
Generation No    : 1303472783
Uptime (seconds) : 5024
Heap Memory (MB) : 3656.35 / 5214.00

As mentioned, restarting the node will put the node into this state again after some time.


;;;","22/Apr/11 21:22;jbellis;sounds like your node is OOMing

enable gc logging and check gc thread CPU usage (http://publib.boulder.ibm.com/infocenter/javasdk/tools/index.jsp?topic=/com.ibm.java.doc.igaa/_1vg0001475cb4a-1190e2e0f74-8000_1007.html)

re ""bringing down cluster"", are you sure dynamic snitch is enabled?;;;","22/Apr/11 21:54;scode;If you're running out of memory with CMS there should be a constant fallback to full GC, which is single-threaded (with CMS). That's inconsistent with 50-100% cpu across all cores (typically you check top and it's immediately obvious that it's at almost exactly 100% and that tells you it's full gc).

Are you definitely spinning across multiple cores constantly, or does it flap between one core and multiple cores?;;;","22/Apr/11 22:18;tbritz;It flaps between one core and multiple cores. There was one thread taking 100% cpu, and the other ones flapping as you said.

How can the gc run constantly if there are still 1.5 Gig free according to nodetool info and I did shut down our application, so there shouldn't be any requests (or just a very few read requests) at all? But the localhost info heap info changed constantly (also grew close to the limit).

There also many small tables which haven't yet been compacted.


dynamic snitch is indeed disabled (because of data locality), but I will enable it and then bring the node to life again.;;;","22/Apr/11 22:26;jbellis;bq. How can the gc run constantly if there are still 1.5 Gig free 

could be fragmentation.  could be it's not gc.  i told you how to find out for sure earlier. :)

bq. dynamic snitch is indeed disabled (because of data locality)

you can have best of both worlds with dynamic snitch + badness threshold.;;;","22/Apr/11 22:45;tbritz;Ok thanks.

I will try above to make sure it's the GC thread taking up 100% cpu (which I suspect).

The GC indeed ran indeed very very often: (grepping the old log file)

 INFO [ScheduledTasks:1] 2011-04-22 15:07:24,742 GCInspector.java (line 128) GC for ConcurrentMarkSweep: 3133 ms, 1537221688 reclaimed leaving 3830570400 used; max is 5605687296
 INFO [ScheduledTasks:1] 2011-04-22 15:07:28,978 GCInspector.java (line 128) GC for ConcurrentMarkSweep: 2901 ms, 2172093472 reclaimed leaving 3219903248 used; max is 5605687296
 INFO [ScheduledTasks:1] 2011-04-22 15:07:33,946 GCInspector.java (line 128) GC for ConcurrentMarkSweep: 3138 ms, 1715631768 reclaimed leaving 3581355456 used; max is 5605687296
 INFO [ScheduledTasks:1] 2011-04-22 15:07:38,318 GCInspector.java (line 128) GC for ConcurrentMarkSweep: 2860 ms, 2467501736 reclaimed leaving 2874209912 used; max is 5605687296
 INFO [ScheduledTasks:1] 2011-04-22 15:07:43,466 GCInspector.java (line 128) GC for ConcurrentMarkSweep: 3059 ms, 1491300856 reclaimed leaving 3875493760 used; max is 5605687296
 INFO [ScheduledTasks:1] 2011-04-22 15:07:47,529 GCInspector.java (line 128) GC for ConcurrentMarkSweep: 2780 ms, 2521501408 reclaimed leaving 2824239464 used; max is 5605687296
 INFO [ScheduledTasks:1] 2011-04-22 15:07:52,983 GCInspector.java (line 128) GC for ConcurrentMarkSweep: 3330 ms, 1287324312 reclaimed leaving 3964465048 used; max is 5605687296
 INFO [ScheduledTasks:1] 2011-04-22 15:07:57,147 GCInspector.java (line 128) GC for ConcurrentMarkSweep: 3071 ms, 1950115232 reclaimed leaving 3337420088 used; max is 5605687296
 INFO [ScheduledTasks:1] 2011-04-22 15:08:01,785 GCInspector.java (line 128) GC for ConcurrentMarkSweep: 3069 ms, 1748108592 reclaimed leaving 3444685288 used; max is 5605687296
 INFO [ScheduledTasks:1] 2011-04-22 15:08:06,397 GCInspector.java (line 128) GC for ConcurrentMarkSweep: 3011 ms, 1779092920 reclaimed leaving 3637964728 used; max is 5605687296
 INFO [ScheduledTasks:1] 2011-04-22 15:08:10,715 GCInspector.java (line 128) GC for ConcurrentMarkSweep: 2950 ms, 2042754088 reclaimed leaving 3223819496 used; max is 5605687296
 INFO [ScheduledTasks:1] 2011-04-22 15:08:15,528 GCInspector.java (line 128) GC for ConcurrentMarkSweep: 3217 ms, 956021816 reclaimed leaving 4371838768 used; max is 5605687296
 INFO [ScheduledTasks:1] 2011-04-22 15:08:19,380 GCInspector.java (line 128) GC for ConcurrentMarkSweep: 3007 ms, 2216336016 reclaimed leaving 3213552904 used; max is 5605687296
 INFO [ScheduledTasks:1] 2011-04-22 15:08:24,154 GCInspector.java (line 128) GC for ConcurrentMarkSweep: 2973 ms, 1964499424 reclaimed leaving 3287557528 used; max is 5605687296
 INFO [ScheduledTasks:1] 2011-04-22 15:08:29,088 GCInspector.java (line 128) GC for ConcurrentMarkSweep: 2988 ms, 1964657952 reclaimed leaving 3418170312 used; max is 5605687296


It still interest me why the node didn't recover even though there were nearly no requests at all from our application.

;;;","22/Apr/11 22:58;jbellis;bq. It still interest me why the node didn't recover even though there were nearly no requests at all from our application

the two big users of memory are usually memtables and cache and neither one gets freed when requests stop. (until memtable_flush_after_mins is reached...  and flush needs memory to work so if you are GC storming hard enough it may not make progress even then.);;;","23/Apr/11 01:46;tbritz;I stopped all java programs and restarted the cluster and increased the heap size to 5.5 gig. All my table have a memtable limit of 32MB and I only have about 20 tables with 2 column families in each table.

Executing a manual compactation or just waiting will return the following error:

ERROR [ReadStage:24] 2011-04-22 19:30:58,330 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[ReadStage:24,5,main]
java.lang.OutOfMemoryError: Java heap space
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:123)
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:108)
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:93)
        at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:74)
        at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:548)
        at org.apache.cassandra.db.RowIteratorFactory.getIterator(RowIteratorFactory.java:95)
        at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1425)
        at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:49)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

One table has 778 intermediate tables and won't compact. (Each table has about the size of the memtable flush limit). Only the first one are biggers (e.g 5x 19 GB)

-rw-r--r-- 1 root root  18M 2011-04-22 17:07 table_userentries-f-42142-Data.db
-rw-r--r-- 1 root root  17M 2011-04-22 17:10 table_userentries-f-42143-Data.db
-rw-r--r-- 1 root root  17M 2011-04-22 17:12 table_userentries-f-42144-Data.db
-rw-r--r-- 1 root root  17M 2011-04-22 17:15 table_userentries-f-42145-Data.db
-rw-r--r-- 1 root root  17M 2011-04-22 17:17 table_userentries-f-42146-Data.db
-rw-r--r-- 1 root root  17M 2011-04-22 17:20 table_userentries-f-42147-Data.db
-rw-r--r-- 1 root root  17M 2011-04-22 17:23 table_userentries-f-42148-Data.db
-rw-r--r-- 1 root root  17M 2011-04-22 17:26 table_userentries-f-42149-Data.db
-rw-r--r-- 1 root root  13M 2011-04-22 17:33 table_userentries-f-42150-Data.db
-rw-r--r-- 1 root root  17M 2011-04-22 17:37 table_userentries-f-42152-Data.db
-rw-r--r-- 1 root root  17M 2011-04-22 17:38 table_userentries-f-42153-Data.db
-rw-r--r-- 1 root root  17M 2011-04-22 17:39 table_userentries-f-42154-Data.db
-rw-r--r-- 1 root root  17M 2011-04-22 17:40 table_userentries-f-42155-Data.db
-rw-r--r-- 1 root root  17M 2011-04-22 17:42 table_userentries-f-42156-Data.db
-rw-r--r-- 1 root root  17M 2011-04-22 17:43 table_userentries-f-42157-Data.db
-rw-r--r-- 1 root root  17M 2011-04-22 17:44 table_userentries-f-42158-Data.db
-rw-r--r-- 1 root root  17M 2011-04-22 17:45 table_userentries-f-42159-Data.db
-rw-r--r-- 1 root root 6.7M 2011-04-22 19:18 table_userentries-f-42160-Data.db
-rw-r--r-- 1 root root 528M 2011-04-22 19:19 table_userentries-f-42161-Data.db



It's certainly something related to compacting. 
There are log file entries related to cassandra compacting other tables (Compacting table_....). But this table never shows up (even not when I trigger a manual compaction on that table).



;;;","24/Apr/11 04:42;jbellis;Sure sounds like a classic ""my memtables and/or caches are too large"" symptom to me. Note that the stack trace has nothing to do with compaction and is in fact OOMing trying to allocate a 256KB read buffer.

bq. All my table have a memtable limit of 32MB

Remember that the memtable throughput value is the *serialized* size, in-memory size is typically 8x to 12x that. So back of the envelope math is that you're in trouble if you haven't tuned the operations threshold down a lot.;;;","27/Apr/11 20:46;tbritz;I simply removed the table, and everything works fine again, even at smaller memory footprints.

I did backup the files, so I will try to create test case with only one table. But it will take some time.

I did tune the memtable_throughput value and did get the OOM exception even with thrift & gossip disabled. 

It's similar to http://cassandra-user-incubator-apache-org.3065146.n2.nabble.com/OOM-on-heavy-write-load-td6296984.html.  I also wrote lot's of small entries (about 30-50KB per Entry).


;;;","27/Apr/11 23:02;jbellis;bq. I did tune the memtable_throughput value 

Not to belabor the point, but ""remember that the memtable throughput value is the serialized size"" means that if you're relying on tuning throughput alone you'll have to drop it to ~5MB with as many CFs as you have.

(This is improved in 0.8; see memtable_total_space_in_mb option from CASSANDRA-2006.);;;","29/Apr/11 02:13;tbritz;You can close thisissue. I think it's a duplicate of CASSANDRA-2463 causing the Garbage collector to run every few seconds.
I updated to 0.7.5 and everything runs much smoother (I have also seen no timeoutexceptions in hector anymore).
;;;","29/Apr/11 02:23;jbellis;Sounds good. Thanks for helping troubleshoot; I'm glad 0.7.5 is working well.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Decommission fails after original bootstrap,CASSANDRA-1738,12479830,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,tjake,mbulman,mbulman,13/Nov/10 04:46,16/Apr/19 17:33,22/Mar/23 14:57,19/Nov/10 21:15,0.7.0 rc 1,,,,0,,,,,,"When first standing up a cluster, and trying to decommission (what appears to be) any non-seed node, an error occurs.  The node being targeted will log ""DECOMMISSIONING"", but fail with the error below.  Restarting seems to fix.  See steps and error below:

2-node cluster:
start node0 + start node1 + decom node1 -> error
start node0 + start node1 + decom node0 -> success
start node0 + start node1 + restart node1 + decom node1 -> success

3-node cluster:
start node0 + start node1 + start node2 + decom node2 -> error
start node0 + start node1 + start node2 + decom node1 -> error
start node0 + start node1 + start node2 + decom node0 -> success
start node0 + start node1 + start node2 + restart node2 + decom node2 -> success

Error:

/usr/src/cassandra/branches/cassandra-0.7# bin/nodetool -h localhost decommission
Exception in thread ""main"" java.lang.AssertionError
	at org.apache.cassandra.service.StorageService.getLocalToken(StorageService.java:1128)
	at org.apache.cassandra.service.StorageService.startLeaving(StorageService.java:1529)
	at org.apache.cassandra.service.StorageService.decommission(StorageService.java:1548)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:111)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:45)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:226)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:251)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:857)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:795)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1450)
	at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:90)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1285)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1383)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:807)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
	at sun.rmi.transport.Transport$1.run(Transport.java:177)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
",,mbulman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,tjake,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20283,,,Fri Nov 19 02:19:04 UTC 2010,,,,,,,,,,"0|i0g733:",92578,,,,,Normal,,,,,,,,,,,,,,,,,"13/Nov/10 09:48;jbellis;autobootstrap=false in all cases?;;;","13/Nov/10 22:01;mbulman;autobootstrap=true for node1 + 2;;;","19/Nov/10 10:19;tjake;This is a dup of CASSANDRA-1732;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Duplicate nodes showing on the ring when starting 3 nodes on empty cluster,CASSANDRA-1341,12470479,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,arya,arya,30/Jul/10 07:20,16/Apr/19 17:33,22/Mar/23 14:57,30/Jul/10 07:38,,,,,0,,,,,,"When bringing a cluster of 3 nodes up, if user does not boot up the nodes serially and wait for bootstrap to finish on each before restarting the next one, the cluster may end up having duplicate nodes showing on the ring.

Steps to Reproduce:
1. Configure a cluster of 3 from clean slate of trunc;
2. Start the first node;
3. Start the second node; 
4. As soon as the second node starts ans says Finished Hinted Handoff to node 1, start node 3;
5. Load the schema from YAML using JMX or use Thrift to define some schema so that we could use nodetool (CASSANDRA-1286)

I ended up with a strange ring configuration:

[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.132 ring
Address         Status State   Load            Token                                       
                                       162149197822681929203296559575110871861    
10.50.26.133    Up     Normal  41.61 KB        34543310227330005404531081788197792565      
10.50.26.132    Up     Normal  41.61 KB        77078606092447313337452907717168818997      
10.50.26.132    Up     Normal  41.61 KB        162149197822681929203296559575110871861     
[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.133 ring
Address         Status State   Load            Token                                       
                                       162149197822681929203296559575110871861    
10.50.26.133    Up     Normal  41.61 KB        34543310227330005404531081788197792565      
10.50.26.132    Up     Normal  41.61 KB        77078606092447313337452907717168818997      
10.50.26.132    Up     Normal  41.61 KB        162149197822681929203296559575110871861     
[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.134 ring
Address         Status State   Load            Token                                       
                                       162149197822681929203296559575110871861    
10.50.26.133    Up     Normal  41.61 KB        34543310227330005404531081788197792565      
10.50.26.132    Up     Normal  41.61 KB        77078606092447313337452907717168818997      
10.50.26.132    Up     Normal  41.61 KB        162149197822681929203296559575110871861     


In my case

10.50.26.132 = node 1
10.50.26.133 = node 2
10.50.26.134 = node 3 

As you see node2 is repeated Twice. Node 3 is running however does not show up in the ring!

Now, if in step 4 if I wait for bootstrap of node 2 to kick in and finish, I won't have this problem.","CentOS 5.2
Trunk",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20089,,,Thu Jul 29 23:38:09 UTC 2010,,,,,,,,,,"0|i0g4gn:",92153,,,,,Normal,,,,,,,,,,,,,,,,,"30/Jul/10 07:38;jbellis;same as CASSANDRA-1291;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 Thrift interface needs a  Ruby namespace,CASSANDRA-327,12431829,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,eweaver,eweaver,eweaver,31/Jul/09 01:38,16/Apr/19 17:33,22/Mar/23 14:57,10/Aug/09 10:49,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Aug/09 00:09;eweaver;CASSANDRA-327-2.diff;https://issues.apache.org/jira/secure/attachment/12415998/CASSANDRA-327-2.diff","31/Jul/09 01:40;eweaver;CASSANDRA-327.diff;https://issues.apache.org/jira/secure/attachment/12415042/CASSANDRA-327.diff",,,,,,,,,,,,,2.0,eweaver,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19637,,,Mon Aug 10 14:35:53 UTC 2009,,,,,,,,,,"0|i0fy9b:",91148,,,,,Normal,,,,,,,,,,,,,,,,,"06/Aug/09 06:59;beisenberg;works for me;;;","06/Aug/09 07:42;jbellis;Is the bug mentioned in this diff reported to the thrift project?

+1 after linking the thrift bug report.;;;","06/Aug/09 08:21;nzkoz;looks good to me, definitely annoying how it's structured at present;;;","07/Aug/09 05:01;schacon;works for me;;;","10/Aug/09 00:09;eweaver;Generate patch with git.;;;","10/Aug/09 00:14;eweaver;http://issues.apache.org/jira/browse/THRIFT-556;;;","10/Aug/09 10:49;jbellis;committed;;;","10/Aug/09 22:35;hudson;Integrated in Cassandra #163 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/163/])
    add Ruby namespace. patch by Evan Weaver; reviewed by Michael Koziarski and Scott Chacon for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConcurrentModificationException during WriteResponseHandler,CASSANDRA-865,12458568,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,btoddb,btoddb,10/Mar/10 02:22,16/Apr/19 17:33,22/Mar/23 14:57,10/Mar/10 02:55,,,,,0,,,,,,"using cassandra-0.6.0-beta2/

2010-03-09 03:57:39,970 ERROR [pool-1-thread-7] [Cassandra.java:1391] Internal error processing insert
java.util.ConcurrentModificationException
        at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
        at java.util.AbstractList$Itr.next(AbstractList.java:343)
        at org.apache.cassandra.service.WriteResponseHandler.get(WriteResponseHandler.java:78)
        at org.apache.cassandra.service.StorageProxy.mutateBlocking(StorageProxy.java:230)
        at org.apache.cassandra.thrift.CassandraServer.doInsert(CassandraServer.java:451)
        at org.apache.cassandra.thrift.CassandraServer.insert(CassandraServer.java:361)
        at org.apache.cassandra.thrift.Cassandra$Processor$insert.process(Cassandra.java:1383)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:1114)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19895,,,Tue Mar 09 18:55:31 UTC 2010,,,,,,,,,,"0|i0g1jz:",91682,,,,,Normal,,,,,,,,,,,,,,,,,"10/Mar/10 02:55;jbellis;will fix in patch for CASSANDRA-864;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ArrayIndexOutOfBoundsException in RequestResponseStage,CASSANDRA-2002,12495900,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,mikaels,mikaels,18/Jan/11 22:17,16/Apr/19 17:33,22/Mar/23 14:57,19/Jan/11 00:05,,,,,0,,,,,,"During the execution of Cassandra, i see error message in log
ERROR [RequestResponseStage:10] 2011-01-18 00:31:03,514 AbstractCassandraDaemon.java (line 91) Fatal exception in thread Thread[RequestResponseStage:10,5,main]
java.lang.ArrayIndexOutOfBoundsException: -1
	at java.util.ArrayList.fastRemove(ArrayList.java:441)
	at java.util.ArrayList.remove(ArrayList.java:424)
	at com.google.common.collect.AbstractMultimap.remove(AbstractMultimap.java:219)
	at com.google.common.collect.ArrayListMultimap.remove(ArrayListMultimap.java:60)
	at org.apache.cassandra.net.MessagingService.responseReceivedFrom(MessagingService.java:436)
	at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:40)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:63)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
","Linux, no jna, java 1.6.22",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20398,,,Tue Jan 18 16:04:47 UTC 2011,,,,,,,,,,"0|i0g8pr:",92842,,,,,Normal,,,,,,,,,,,,,,,,,"19/Jan/11 00:04;nickmbailey;This is the same as CASSANDRA-1959 which has been resolved;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
incorrect live endpoint checks in StorageProxy.getRangeSlice() and scan(),CASSANDRA-2132,12497919,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,amorton,amorton,08/Feb/11 06:55,16/Apr/19 17:33,22/Mar/23 14:57,08/Feb/11 07:34,,,,,0,,,,,,"If a get_range_slice() is started without any live endpoints the following appears in the logs and the request fails with an ApplicationError rather than an UnavailableError 

ERROR [pool-1-thread-61] 2011-02-04 16:11:30,725 Cassandra.java (line org.apache.cassandra.thrift.Cassandra$Processor) Internal error processing get_range_slices
java.lang.AssertionError
        at org.apache.cassandra.service.RangeSliceResponseResolver.<init>(RangeSliceResponseResolver.java:52)
        at org.apache.cassandra.service.StorageProxy.getRangeSlice(StorageProxy.java:459)
        at org.apache.cassandra.thrift.CassandraServer.get_range_slices(CassandraServer.java:473)
        at org.apache.cassandra.thrift.Cassandra$Processor$get_range_slices.process(Cassandra.java:2868)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2555)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)

I think we need to...

- Remove the assertion in RangeSliceResponseResolver ctor
- call handler.assureSufficientLiveNodes() after line 733 in  StorageProxy.getRangeSlice()

Also StorageProxy.scan does a manual check of the live node count at line 1016, this means does not use the special logic for DatacenterReadCallback.assureSufficientLiveNodes()

I've not checked this in the trunk. Will not have time to work on these during the day. 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20459,,,Mon Feb 07 23:34:48 UTC 2011,,,,,,,,,,"0|i0g9i7:",92970,,,,,Normal,,,,,,,,,,,,,,,,,"08/Feb/11 07:34;jbellis;fixed in the patch for CASSANDRA-2069;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Attempting to delete a non-existent file on upgrade to 0.7.2 from 0.7.1,CASSANDRA-2185,12498966,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Duplicate,,j.casares,j.casares,18/Feb/11 06:47,16/Apr/19 17:33,22/Mar/23 14:57,22/Feb/11 04:43,0.7.3,,,,0,,,,,,"Upgraded to cassandra 0.7.2 with data and logs directories from cassandra 0.7.1, but the cassandra installation itself was completely new. 14 of my 15 nodes started up fine, but one of them failed to start up due to the error below. 

Workaround: creating the file it was wanting to delete in the system keyspace. 

ERROR 18:15:35,158 Exception encountered during startup. 
java.lang.AssertionError: attempted to delete non-existing file Schema-tmp-f-763-Filter.db 
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:46) 
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:41) 
        at org.apache.cassandra.io.sstable.SSTable.delete(SSTable.java:138) 
        at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:468) 
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:126) 
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316) 
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79) 
Exception encountered during startup. 
java.lang.AssertionError: attempted to delete non-existing file Schema-tmp-f-763-Filter.db 
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:46) 
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:41) 
        at org.apache.cassandra.io.sstable.SSTable.delete(SSTable.java:138) 
        at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:468) 
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:126) 
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316) 
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79) ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20488,,,Mon Feb 21 20:43:35 UTC 2011,,,,,,,,,,"0|i0g9u7:",93024,,,,,Low,,,,,,,,,,,,,,,,,"22/Feb/11 04:43;jbellis;fixed in CASSANDRA-2206;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE during compaction,CASSANDRA-774,12455546,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Duplicate,,brandon.williams,brandon.williams,07/Feb/10 02:11,16/Apr/19 17:33,22/Mar/23 14:57,07/Feb/10 02:33,,,,,0,,,,,,"After an update on trunk, I restarted and began receiving this exception during compaction:

ERROR - Error in executor futuretask
java.util.concurrent.ExecutionException: java.io.IOException: Unable to create compaction marker
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:65)
        at org.apache.cassandra.db.CompactionManager$CompactionExecutor.afterExecute(CompactionManager.java:566)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.IOException: Unable to create compaction marker
        at org.apache.cassandra.io.SSTableReader.markCompacted(SSTableReader.java:458)
        at org.apache.cassandra.io.SSTableTracker.replace(SSTableTracker.java:57)
        at org.apache.cassandra.db.ColumnFamilyStore.replaceCompactedSSTables(ColumnFamilyStore.java:621)
        at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:307)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:101)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:82)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        ... 2 more

I restarted a few more times and now only receive this exception:

java.util.concurrent.ExecutionException: java.lang.NullPointerException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:65)
        at org.apache.cassandra.db.CompactionManager$CompactionExecutor.afterExecute(CompactionManager.java:566)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.io.SSTableReader.markCompacted(SSTableReader.java:460)
        at org.apache.cassandra.io.SSTableTracker.replace(SSTableTracker.java:57)
        at org.apache.cassandra.db.ColumnFamilyStore.replaceCompactedSSTables(ColumnFamilyStore.java:621)
        at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:307)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:101)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:82)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        ... 2 more
    
","debian lenny amd64 Java HotSpot(TM) 64-Bit Server VM (build 11.2-b01, mixed mode)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19858,,,Sat Feb 06 18:33:38 UTC 2010,,,,,,,,,,"0|i0g0zr:",91591,,,,,Low,,,,,,,,,,,,,,,,,"07/Feb/10 02:22;brandon.williams;Perhaps not the correct way to solve this, but I patch SSTableTracker to add a finalizing reference to the old sstables before calling markCompacted and that seems to work.;;;","07/Feb/10 02:22;jbellis;i think Stu's patch for CASSANDRA-772 (now in trunk) should fix this, can you verify?;;;","07/Feb/10 02:29;brandon.williams;Yes, CASSANDRA-772 solves it.;;;","07/Feb/10 02:33;jbellis;Great, thanks.  Sorry about the run around.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConcurrentModificationException after discarding obsolete commit,CASSANDRA-877,12458712,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,matteo,matteo,11/Mar/10 03:51,16/Apr/19 17:33,22/Mar/23 14:57,11/Mar/10 04:22,,,,,0,,,,,,"I executed batch_mutate about 40k times, each one with about 100 or so mutations.
In the log I see 3 exceptions, all following a 'discard obsolete commit log...'

INFO 19:42:18,739 Discarding obsolete commit log:CommitLogSegment(/Volumes/teo1000/cassandra/var/commitlog/CommitLog-1268249027547.log)
ERROR 19:42:18,740 Error in executor futuretask
java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.util.ConcurrentModificationException
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:86)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:637)
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.util.ConcurrentModificationException
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	... 2 more
Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.util.ConcurrentModificationException
	at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegments(CommitLog.java:357)
	at org.apache.cassandra.db.ColumnFamilyStore$2.runMayThrow(ColumnFamilyStore.java:392)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 6 more
Caused by: java.util.concurrent.ExecutionException: java.util.ConcurrentModificationException
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegments(CommitLog.java:349)
	... 8 more
Caused by: java.util.ConcurrentModificationException
	at java.util.ArrayDeque$DeqIterator.next(ArrayDeque.java:605)
	at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegmentsInternal(CommitLog.java:385)
	at org.apache.cassandra.db.commitlog.CommitLog.access$300(CommitLog.java:71)
	at org.apache.cassandra.db.commitlog.CommitLog$6.call(CommitLog.java:343)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at org.apache.cassandra.db.commitlog.CommitLogExecutorService.process(CommitLogExecutorService.java:113)
	at org.apache.cassandra.db.commitlog.CommitLogExecutorService.access$200(CommitLogExecutorService.java:35)
	at org.apache.cassandra.db.commitlog.CommitLogExecutorService$1.runMayThrow(CommitLogExecutorService.java:67)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 1 more
","odropico:python dikappa$ uname -a
Darwin odropico.local 10.2.0 Darwin Kernel Version 10.2.0: Tue Nov  3 10:37:10 PST 2009; root:xnu-1486.2.11~1/RELEASE_I386 i386

odropico:python dikappa$ java -version
java version ""1.6.0_17""
Java(TM) SE Runtime Environment (build 1.6.0_17-b04-248-10M3025)
Java HotSpot(TM) 64-Bit Server VM (build 14.3-b01-101, mixed mode)
cassandra beta-2",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19898,,,Wed Mar 10 20:22:03 UTC 2010,,,,,,,,,,"0|i0g1mn:",91694,,,,,Normal,,,,,,,,,,,,,,,,,"11/Mar/10 04:22;jbellis;fixed in CASSANDRA-853.  use 0.6 branch from svn until we get an RC1 release out.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLI still requires quotes for row key,CASSANDRA-1728,12479675,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Duplicate,,jbellis,jbellis,11/Nov/10 11:12,16/Apr/19 17:33,22/Mar/23 14:57,11/Nov/10 22:18,0.7.0 rc 1,,Legacy/Tools,,0,,,,,,"[default@test] set cf2[12345][asdf] = 1  
Syntax error at position 8: mismatched input '12345' expecting set null

(this should be valid, but cli complains unless I add quotes around '12345')",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20276,,,Thu Nov 11 14:18:02 UTC 2010,,,,,,,,,,"0|i0g70v:",92568,,,,,Low,,,,,,,,,,,,,,,,,"11/Nov/10 22:18;jbellis;fixed in CASSANDRA-1687;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL create keyspace throws exceptions,CASSANDRA-2535,12504929,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,lenn0x,lenn0x,22/Apr/11 06:45,16/Apr/19 17:33,22/Mar/23 14:57,22/Apr/11 06:49,,,,,0,CQL,,,,,"Was trying out CQL:

cqlsh> create keyspace foo with replication_factor=1 and strategy_class='org.apache.cassandra.locator.SimpleStrategy';
Bad Request: org.apache.cassandra.config.ConfigurationException: SimpleStrategy requires a replication_factor strategy option.

On Cassandra side:

 INFO 15:41:14,423 Applying migration 768b63c0-6c68-11e0-0000-242d50cf1fbf Add keyspace: foorep strategy:SimpleStrategy{}
 INFO 15:41:14,424 Enqueuing flush of Memtable-Migrations@17649447(6489/8111 serialized/live bytes, 1 ops)
 INFO 15:41:14,425 Enqueuing flush of Memtable-Schema@186829279(2599/3248 serialized/live bytes, 3 ops)
 INFO 15:41:14,425 Writing Memtable-Migrations@17649447(6489/8111 serialized/live bytes, 1 ops)
 INFO 15:41:14,435 Completed flushing /var/lib/cassandra/data/system/Migrations-f-1-Data.db (6553 bytes)
 INFO 15:41:14,436 Writing Memtable-Schema@186829279(2599/3248 serialized/live bytes, 3 ops)
 INFO 15:41:14,449 Completed flushing /var/lib/cassandra/data/system/Schema-f-1-Data.db (2749 bytes)
ERROR 15:41:14,452 Fatal exception in thread Thread[MigrationStage:1,5,main]
java.lang.RuntimeException: org.apache.cassandra.config.ConfigurationException: SimpleStrategy requires a replication_factor strategy option.
	at org.apache.cassandra.db.Table.<init>(Table.java:278)
	at org.apache.cassandra.db.Table.open(Table.java:110)
	at org.apache.cassandra.db.migration.AddKeyspace.applyModels(AddKeyspace.java:74)
	at org.apache.cassandra.db.migration.Migration.apply(Migration.java:154)
	at org.apache.cassandra.cql.QueryProcessor$1.call(QueryProcessor.java:339)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
Caused by: org.apache.cassandra.config.ConfigurationException: SimpleStrategy requires a replication_factor strategy option.
	at org.apache.cassandra.locator.SimpleStrategy.validateOptions(SimpleStrategy.java:79)
	at org.apache.cassandra.locator.AbstractReplicationStrategy.createReplicationStrategy(AbstractReplicationStrategy.java:262)
	at org.apache.cassandra.db.Table.createReplicationStrategy(Table.java:328)
	at org.apache.cassandra.db.Table.<init>(Table.java:274)
	... 9 more
 INFO 15:41:37,210 Applying migration 843c7c70-6c68-11e0-0000-242d50cf1fbf Drop keyspace: foo
 INFO 15:41:37,211 Enqueuing flush of Memtable-Migrations@1289702396(6372/7965 serialized/live bytes, 1 ops)
 INFO 15:41:37,211 Writing Memtable-Migrations@1289702396(6372/7965 serialized/live bytes, 1 ops)
 INFO 15:41:37,212 Enqueuing flush of Memtable-Schema@1475720401(2529/3161 serialized/live bytes, 2 ops)
 INFO 15:41:37,222 Completed flushing /var/lib/cassandra/data/system/Migrations-f-2-Data.db (6436 bytes)
 INFO 15:41:37,223 Writing Memtable-Schema@1475720401(2529/3161 serialized/live bytes, 2 ops)
 INFO 15:41:37,244 Completed flushing /var/lib/cassandra/data/system/Schema-f-2-Data.db (2679 bytes)

The keyspace gets created anyway and I can no longer drop it, I get this message:

cqlsh> drop keyspace foo;
Exception: TSocket read 0 bytes


ERROR 15:41:37,246 Fatal exception in thread Thread[MigrationStage:1,5,main]
java.lang.AssertionError
	at org.apache.cassandra.db.migration.DropKeyspace.applyModels(DropKeyspace.java:81)
	at org.apache.cassandra.db.migration.Migration.apply(Migration.java:154)
	at org.apache.cassandra.cql.QueryProcessor$1.call(QueryProcessor.java:339)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
ERROR 15:41:37,246 Thrift error occurred during processing of message.
org.apache.thrift.protocol.TProtocolException: Required field 'why' was not present! Struct: InvalidRequestException(why:null)
	at org.apache.cassandra.thrift.InvalidRequestException.validate(InvalidRequestException.java:334)
	at org.apache.cassandra.thrift.InvalidRequestException.write(InvalidRequestException.java:303)
	at org.apache.cassandra.thrift.Cassandra$execute_cql_query_result.write(Cassandra.java:32011)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.process(Cassandra.java:4091)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20681,,,Thu Apr 21 22:49:54 UTC 2011,,,,,,,,,,"0|i0gbxr:",93364,,,,,Normal,,,,,,,,,,,,,,,,,"22/Apr/11 06:49;jbellis;CASSANDRA-2525;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"sstable2json dies with ""Too many open files"", regardless of ulimit",CASSANDRA-2304,12500973,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,alienth,alienth,10/Mar/11 08:54,16/Apr/19 17:33,22/Mar/23 14:57,11/Mar/11 01:13,0.7.4,,Legacy/Tools,,0,,,,,,"Running sstable2json on the attached sstable eventually results in the following:

{code}
Exception in thread ""main"" java.io.IOError: java.io.FileNotFoundException: /var/lib/cassandra/data/reddit/CommentSortsCache-f-9764-Data.db (Too many open files)
        at org.apache.cassandra.io.util.BufferedSegmentedFile.getSegment(BufferedSegmentedFile.java:68)
        at org.apache.cassandra.io.sstable.SSTableReader.getFileDataInput(SSTableReader.java:567)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:49)
        at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:68)
        at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:80)
        at org.apache.cassandra.tools.SSTableExport.serializeRow(SSTableExport.java:187)
        at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:355)
        at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:377)
        at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:390)
        at org.apache.cassandra.tools.SSTableExport.main(SSTableExport.java:448)
Caused by: java.io.FileNotFoundException: /var/lib/cassandra/data/reddit/CommentSortsCache-f-9764-Data.db (Too many open files)
        at java.io.RandomAccessFile.open(Native Method)
        at java.io.RandomAccessFile.<init>(RandomAccessFile.java:233)
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:111)
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:106)
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:91)
        at org.apache.cassandra.io.util.BufferedSegmentedFile.getSegment(BufferedSegmentedFile.java:62)
{code}

Set my ulimit -n to 60000 and got the same result. Leaking file descriptors?",,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,"11/Mar/11 00:38;jbellis;2304.txt;https://issues.apache.org/jira/secure/attachment/12473282/2304.txt","11/Mar/11 00:56;xedin;CASSANDRA-2304-v2.patch;https://issues.apache.org/jira/secure/attachment/12473288/CASSANDRA-2304-v2.patch","10/Mar/11 09:06;alienth;sstable.tar.bz2;https://issues.apache.org/jira/secure/attachment/12473227/sstable.tar.bz2",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20549,,,Thu Mar 10 17:40:56 UTC 2011,,,,,,,,,,"0|i0gal3:",93145,,xedin,,xedin,Low,,,,,,,,,,,,,,,,,"10/Mar/11 09:06;alienth;Sstable which causes sstable2json to die. Grabbed from an 0.7.3 node.;;;","10/Mar/11 09:07;alienth;Output from lsof. Thousands of lines of the following:

{code}
java      1766       root 1080r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1081r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1082r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1083r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1084r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1085r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1086r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1087r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1088r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1089r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1090r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1091r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1092r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1093r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1094r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1095r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
{code};;;","11/Mar/11 00:38;jbellis;This is a bug with non-mmap'd I/O.;;;","11/Mar/11 00:38;jbellis;patch to close column iterator and only do one pass per row;;;","11/Mar/11 00:56;xedin;with number of the exported columns properly incremented. LGTM.;;;","11/Mar/11 01:13;jbellis;committed v2;;;","11/Mar/11 01:40;hudson;Integrated in Cassandra-0.7 #370 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/370/])
    fix fd leak in sstable2json with non-mmap'd i/o
patch by jbellis; reviewed by Pavel Yaskevich for CASSANDRA-2304
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get_range_slice() returns removed columns,CASSANDRA-647,12443844,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,vomjom,vomjom,20/Dec/09 02:22,16/Apr/19 17:33,22/Mar/23 14:57,25/Dec/09 11:03,0.5,,,,0,,,,,,"Here's an example (using my new python library at http://github.com/vomjom/pycassa ):

>>> import pycassa
>>> test = pycassa.ColumnFamily(pycassa.connect(), 'Test Keyspace', 'Test UTF8')
>>> list(test.get_range())
[]
>>> test.insert('key', {'column': 'value'})
1261512409
>>> list(test.get_range())
[('key', {'column': 'value'})]
>>> test.remove('key', 'column')
1261512421
>>> list(test.get_range())
[('key', {'column': 'K0\xd2\x85'})]
",Linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Dec/09 06:39;jbellis;647.patch;https://issues.apache.org/jira/secure/attachment/12428932/647.patch",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19799,,,Fri Dec 25 03:03:11 UTC 2009,,,,,,,,,,"0|i0g07z:",91466,,,,,Low,,,,,,,,,,,,,,,,,"25/Dec/09 06:39;jbellis;adds thrift test and fixes bug;;;","25/Dec/09 07:49;lenn0x;+1;;;","25/Dec/09 11:03;jbellis;committed to 0.5 and trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.io.StreamCorruptedException: invalid stream header: 0000000A,CASSANDRA-2226,12499481,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,osmadja,osmadja,23/Feb/11 20:46,16/Apr/19 17:33,22/Mar/23 14:57,23/Feb/11 22:15,,,Legacy/Documentation and Website,,0,,,,,,"Starting cassandra server: i get the following exceptions:


olivier-smadjas-macbook-pro:bin OlivierSmadja$  INFO 09:39:30,465 Logging initialized
 INFO 09:39:30,496 Heap size: 1052770304/1052770304
 INFO 09:39:30,500 JNA not found. Native methods will be disabled.
 INFO 09:39:30,564 Loading settings from file:/Users/olivier/Apps/apache-cassandra-0.7.2/conf/cassandra.yaml
 INFO 09:39:30,900 DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
 INFO 09:39:31,117 Creating new commitlog segment /Users/olivier/Data/Cassandra/commitlog/CommitLog-1298464771117.log
 INFO 09:39:31,363 Deleted /Users/olivier/Data/Cassandra/data/system/Schema-f-69
 INFO 09:39:31,364 Deleted /Users/olivier/Data/Cassandra/data/system/Schema-f-67
 INFO 09:39:31,366 Deleted /Users/olivier/Data/Cassandra/data/system/Schema-f-80
 INFO 09:39:31,367 Deleted /Users/olivier/Data/Cassandra/data/system/Schema-f-66
 INFO 09:39:31,397 Deleted /Users/olivier/Data/Cassandra/data/system/Schema-f-72
 INFO 09:39:31,398 Deleted /Users/olivier/Data/Cassandra/data/system/Schema-f-79
 INFO 09:39:31,400 Deleted /Users/olivier/Data/Cassandra/data/system/Schema-f-75
 INFO 09:39:31,401 Deleted /Users/olivier/Data/Cassandra/data/system/Schema-f-74
 INFO 09:39:31,402 Deleted /Users/olivier/Data/Cassandra/data/system/Schema-f-77
 INFO 09:39:31,403 Deleted /Users/olivier/Data/Cassandra/data/system/Schema-f-76
 INFO 09:39:31,410 Deleted /Users/olivier/Data/Cassandra/data/system/Schema-f-71
 INFO 09:39:31,411 Deleted /Users/olivier/Data/Cassandra/data/system/Schema-f-68
 INFO 09:39:31,412 Deleted /Users/olivier/Data/Cassandra/data/system/Schema-f-70
 INFO 09:39:31,413 Deleted /Users/olivier/Data/Cassandra/data/system/Schema-f-78
 INFO 09:39:31,424 Deleted /Users/olivier/Data/Cassandra/data/system/Migrations-f-68
 INFO 09:39:31,425 Deleted /Users/olivier/Data/Cassandra/data/system/Migrations-f-73
 INFO 09:39:31,426 Deleted /Users/olivier/Data/Cassandra/data/system/Migrations-f-67
 INFO 09:39:31,426 Deleted /Users/olivier/Data/Cassandra/data/system/Migrations-f-78
 INFO 09:39:31,427 Deleted /Users/olivier/Data/Cassandra/data/system/Migrations-f-79
 INFO 09:39:31,428 Deleted /Users/olivier/Data/Cassandra/data/system/Migrations-f-69
 INFO 09:39:31,429 Deleted /Users/olivier/Data/Cassandra/data/system/Migrations-f-66
 INFO 09:39:31,430 Deleted /Users/olivier/Data/Cassandra/data/system/Migrations-f-74
 INFO 09:39:31,431 Deleted /Users/olivier/Data/Cassandra/data/system/Migrations-f-72
 INFO 09:39:31,432 Deleted /Users/olivier/Data/Cassandra/data/system/Migrations-f-80
 INFO 09:39:31,433 Deleted /Users/olivier/Data/Cassandra/data/system/Migrations-f-77
 INFO 09:39:31,434 Deleted /Users/olivier/Data/Cassandra/data/system/Migrations-f-71
 INFO 09:39:31,437 Deleted /Users/olivier/Data/Cassandra/data/system/Migrations-f-76
 INFO 09:39:31,438 Deleted /Users/olivier/Data/Cassandra/data/system/Migrations-f-65
 INFO 09:39:31,451 Deleted /Users/olivier/Data/Cassandra/data/system/Migrations-f-70
 INFO 09:39:31,452 Deleted /Users/olivier/Data/Cassandra/data/system/Migrations-f-75
 INFO 09:39:31,598 reading saved cache /Users/olivier/Data/Cassandra/saved_caches/system-IndexInfo-KeyCache
 WARN 09:39:31,619 error reading saved cache /Users/olivier/Data/Cassandra/saved_caches/system-IndexInfo-KeyCache
java.io.StreamCorruptedException: invalid stream header: 0000000A
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:782)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:279)
	at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:255)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:198)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:451)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:432)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:207)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:129)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
 INFO 09:39:31,633 Opening /Users/olivier/Data/Cassandra/data/system/IndexInfo-f-23
 INFO 09:39:31,693 Opening /Users/olivier/Data/Cassandra/data/system/IndexInfo-f-22
 INFO 09:39:31,707 Opening /Users/olivier/Data/Cassandra/data/system/IndexInfo-f-21
 INFO 09:39:31,734 reading saved cache /Users/olivier/Data/Cassandra/saved_caches/system-Schema-KeyCache
 WARN 09:39:31,735 error reading saved cache /Users/olivier/Data/Cassandra/saved_caches/system-Schema-KeyCache
java.io.StreamCorruptedException: invalid stream header: 0000000E
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:782)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:279)
	at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:255)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:198)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:451)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:432)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:207)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:129)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
 INFO 09:39:31,737 Opening /Users/olivier/Data/Cassandra/data/system/Schema-f-82
 INFO 09:39:31,759 reading saved cache /Users/olivier/Data/Cassandra/saved_caches/system-Migrations-KeyCache
 WARN 09:39:31,760 error reading saved cache /Users/olivier/Data/Cassandra/saved_caches/system-Migrations-KeyCache
java.io.EOFException
	at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2280)
	at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2749)
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:779)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:279)
	at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:255)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:198)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:451)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:432)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:207)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:129)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
 INFO 09:39:31,762 Opening /Users/olivier/Data/Cassandra/data/system/Migrations-f-81
 INFO 09:39:31,780 Opening /Users/olivier/Data/Cassandra/data/system/Migrations-f-82
 INFO 09:39:31,814 reading saved cache /Users/olivier/Data/Cassandra/saved_caches/system-LocationInfo-KeyCache
 WARN 09:39:31,816 error reading saved cache /Users/olivier/Data/Cassandra/saved_caches/system-LocationInfo-KeyCache
java.io.StreamCorruptedException: invalid stream header: 00000001
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:782)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:279)
	at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:255)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:198)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:451)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:432)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:207)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:129)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
 INFO 09:39:31,822 Opening /Users/olivier/Data/Cassandra/data/system/LocationInfo-f-5
 INFO 09:39:31,836 Opening /Users/olivier/Data/Cassandra/data/system/LocationInfo-f-6
 INFO 09:39:31,874 Opening /Users/olivier/Data/Cassandra/data/system/LocationInfo-f-7
 INFO 09:39:31,880 reading saved cache /Users/olivier/Data/Cassandra/saved_caches/system-HintsColumnFamily-KeyCache
 WARN 09:39:31,881 error reading saved cache /Users/olivier/Data/Cassandra/saved_caches/system-HintsColumnFamily-KeyCache
java.io.EOFException
	at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2280)
	at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2749)
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:779)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:279)
	at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:255)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:198)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:451)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:432)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:207)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:129)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
 INFO 09:39:32,020 Loading schema version 00adafed-3e85-11e0-90a8-e700f669bcfc
 WARN 09:39:32,492 Schema definitions were defined both locally and in cassandra.yaml. Definitions in cassandra.yaml were ignored.
 INFO 09:39:32,953 Deleted /Users/olivier/Data/Cassandra/data/tapix_prod/StreamElement-f-9
 INFO 09:39:32,955 Deleted /Users/olivier/Data/Cassandra/data/tapix_prod/StreamElement-f-12
 INFO 09:39:32,971 Deleted /Users/olivier/Data/Cassandra/data/tapix_prod/StreamElement-f-11
 INFO 09:39:32,973 Deleted /Users/olivier/Data/Cassandra/data/tapix_prod/StreamElement-f-10
 INFO 09:39:32,996 Deleted /Users/olivier/Data/Cassandra/data/tapix_prod/StreamFollower-f-6
 INFO 09:39:32,998 Deleted /Users/olivier/Data/Cassandra/data/tapix_prod/StreamFollower-f-8
 INFO 09:39:32,999 Deleted /Users/olivier/Data/Cassandra/data/tapix_prod/StreamFollower-f-7
 INFO 09:39:33,001 Deleted /Users/olivier/Data/Cassandra/data/tapix_prod/StreamFollower-f-5
 INFO 09:39:33,011 Deleted /Users/olivier/Data/Cassandra/data/tapix_prod/Log-f-15
 INFO 09:39:33,013 Deleted /Users/olivier/Data/Cassandra/data/tapix_prod/Log-f-14
 INFO 09:39:33,014 Deleted /Users/olivier/Data/Cassandra/data/tapix_prod/Log-f-13
 INFO 09:39:33,016 Deleted /Users/olivier/Data/Cassandra/data/tapix_prod/Log-f-16
 INFO 09:39:33,035 Deleted /Users/olivier/Data/Cassandra/data/tapix_prod/Stream-f-5
 INFO 09:39:33,037 Deleted /Users/olivier/Data/Cassandra/data/tapix_prod/Stream-f-8
 INFO 09:39:33,038 Deleted /Users/olivier/Data/Cassandra/data/tapix_prod/Stream-f-6
 INFO 09:39:33,038 Deleted /Users/olivier/Data/Cassandra/data/tapix_prod/Stream-f-7
 INFO 09:39:33,041 Deleted /Users/olivier/Data/Cassandra/data/tapix_prod/Stream.index_stream_owner_user_oid-f-7
 INFO 09:39:33,042 Deleted /Users/olivier/Data/Cassandra/data/tapix_prod/Stream.index_stream_owner_user_oid-f-6
 INFO 09:39:33,043 Deleted /Users/olivier/Data/Cassandra/data/tapix_prod/Stream.index_stream_owner_user_oid-f-5
 INFO 09:39:33,057 Deleted /Users/olivier/Data/Cassandra/data/tapix_prod/Following-f-6
 INFO 09:39:33,058 Deleted /Users/olivier/Data/Cassandra/data/tapix_prod/Following-f-5
 INFO 09:39:33,059 Deleted /Users/olivier/Data/Cassandra/data/tapix_prod/Following-f-8
 INFO 09:39:33,060 Deleted /Users/olivier/Data/Cassandra/data/tapix_prod/Following-f-7
 INFO 09:39:33,184 reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_prod-LastQuote-KeyCache
 WARN 09:39:33,184 error reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_prod-LastQuote-KeyCache
java.io.EOFException
	at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2280)
	at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2749)
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:779)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:279)
	at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:255)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:198)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:451)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:432)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:162)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
 INFO 09:39:33,188 reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_prod-User-KeyCache
 WARN 09:39:33,200 error reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_prod-User-KeyCache
java.io.StreamCorruptedException: invalid stream header: 00000024
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:782)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:279)
	at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:255)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:198)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:451)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:432)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:162)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
 INFO 09:39:33,214 Opening /Users/olivier/Data/Cassandra/data/tapix_prod/User-f-2
 INFO 09:39:33,228 Opening /Users/olivier/Data/Cassandra/data/tapix_prod/User-f-1
 INFO 09:39:33,243 Opening /Users/olivier/Data/Cassandra/data/tapix_prod/User-f-3
 INFO 09:39:33,304 Opening /Users/olivier/Data/Cassandra/data/tapix_prod/User.index_user_email-f-2
 INFO 09:39:33,308 Opening /Users/olivier/Data/Cassandra/data/tapix_prod/User.index_user_email-f-3
 INFO 09:39:33,312 Opening /Users/olivier/Data/Cassandra/data/tapix_prod/User.index_user_email-f-1
 INFO 09:39:33,347 Opening /Users/olivier/Data/Cassandra/data/tapix_prod/User.index_user_external_key-f-2
 INFO 09:39:33,351 Opening /Users/olivier/Data/Cassandra/data/tapix_prod/User.index_user_external_key-f-1
 INFO 09:39:33,362 reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_prod-UserStreamInfo-KeyCache
 WARN 09:39:33,370 error reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_prod-UserStreamInfo-KeyCache
java.io.StreamCorruptedException: invalid stream header: 00000024
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:782)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:279)
	at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:255)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:198)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:451)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:432)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:162)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
 INFO 09:39:33,372 Opening /Users/olivier/Data/Cassandra/data/tapix_prod/UserStreamInfo-f-2
 INFO 09:39:33,384 Opening /Users/olivier/Data/Cassandra/data/tapix_prod/UserStreamInfo-f-1
 INFO 09:39:33,390 reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_prod-TapixConfig-KeyCache
 WARN 09:39:33,391 error reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_prod-TapixConfig-KeyCache
java.io.StreamCorruptedException: invalid stream header: 00000001
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:782)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:279)
	at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:255)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:198)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:451)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:432)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:162)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
 INFO 09:39:33,393 Opening /Users/olivier/Data/Cassandra/data/tapix_prod/TapixConfig-f-2
 INFO 09:39:33,411 Opening /Users/olivier/Data/Cassandra/data/tapix_prod/TapixConfig-f-1
 INFO 09:39:33,428 reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_prod-PendingInvitation-KeyCache
 WARN 09:39:33,450 error reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_prod-PendingInvitation-KeyCache
java.io.StreamCorruptedException: invalid stream header: 00000024
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:782)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:279)
	at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:255)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:198)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:451)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:432)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:162)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
 INFO 09:39:33,452 Opening /Users/olivier/Data/Cassandra/data/tapix_prod/PendingInvitation-f-5
 INFO 09:39:33,478 Opening /Users/olivier/Data/Cassandra/data/tapix_prod/PendingInvitation.index_invitation_user_creator_oid-f-5
 INFO 09:39:33,485 Opening /Users/olivier/Data/Cassandra/data/tapix_prod/PendingInvitation.index_invitation_invitation_email-f-5
 INFO 09:39:33,492 Opening /Users/olivier/Data/Cassandra/data/tapix_prod/PendingInvitation.index_invitation_external_key-f-5
 INFO 09:39:33,499 reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_prod-Invitation-KeyCache
 WARN 09:39:33,499 error reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_prod-Invitation-KeyCache
java.io.EOFException
	at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2280)
	at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2749)
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:779)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:279)
	at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:255)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:198)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:451)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:432)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:162)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
 INFO 09:39:33,518 Opening /Users/olivier/Data/Cassandra/data/tapix_prod/StreamElement-f-13
 INFO 09:39:33,525 Opening /Users/olivier/Data/Cassandra/data/tapix_prod/StreamElement-f-14
 INFO 09:39:33,534 Opening /Users/olivier/Data/Cassandra/data/tapix_prod/StreamElement-f-15
 INFO 09:39:33,549 reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_prod-CompanyArea-KeyCache
 WARN 09:39:33,549 error reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_prod-CompanyArea-KeyCache
java.io.EOFException
	at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2280)
	at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2749)
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:779)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:279)
	at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:255)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:198)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:451)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:432)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:162)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
 INFO 09:39:33,553 Opening /Users/olivier/Data/Cassandra/data/tapix_prod/StreamFollower-f-9
 INFO 09:39:33,588 reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_prod-Log-KeyCache
 WARN 09:39:33,589 error reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_prod-Log-KeyCache
java.io.EOFException
	at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2280)
	at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2749)
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:779)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:279)
	at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:255)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:198)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:451)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:432)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:162)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
 INFO 09:39:33,597 Opening /Users/olivier/Data/Cassandra/data/tapix_prod/Log-f-19
 INFO 09:39:33,625 Opening /Users/olivier/Data/Cassandra/data/tapix_prod/Log-f-18
 INFO 09:39:33,639 Opening /Users/olivier/Data/Cassandra/data/tapix_prod/Log-f-17
 INFO 09:39:33,661 reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_prod-CompanySymbol-KeyCache
 WARN 09:39:33,664 error reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_prod-CompanySymbol-KeyCache
java.io.EOFException
	at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2280)
	at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2749)
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:779)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:279)
	at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:255)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:198)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:451)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:432)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:162)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
 INFO 09:39:33,667 reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_prod-Stream-KeyCache
 WARN 09:39:33,676 error reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_prod-Stream-KeyCache
java.io.StreamCorruptedException: invalid stream header: 00000024
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:782)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:279)
	at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:255)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:198)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:451)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:432)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:162)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
 INFO 09:39:33,678 Opening /Users/olivier/Data/Cassandra/data/tapix_prod/Stream-f-9
 INFO 09:39:33,693 Opening /Users/olivier/Data/Cassandra/data/tapix_prod/Stream.index_stream_owner_user_oid-f-9
 INFO 09:39:33,699 reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_prod-UserInfo-KeyCache
 WARN 09:39:33,700 error reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_prod-UserInfo-KeyCache
java.io.StreamCorruptedException: invalid stream header: 00000024
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:782)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:279)
	at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:255)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:198)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:451)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:432)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:162)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
 INFO 09:39:33,702 Opening /Users/olivier/Data/Cassandra/data/tapix_prod/UserInfo-f-1
 INFO 09:39:33,715 Opening /Users/olivier/Data/Cassandra/data/tapix_prod/UserInfo-f-2
 INFO 09:39:33,732 reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_prod-Following-KeyCache
 WARN 09:39:33,733 error reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_prod-Following-KeyCache
java.io.EOFException
	at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2280)
	at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2749)
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:779)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:279)
	at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:255)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:198)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:451)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:432)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:162)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
 INFO 09:39:33,735 Opening /Users/olivier/Data/Cassandra/data/tapix_prod/Following-f-9
 INFO 09:39:33,741 reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_prod-Company-KeyCache
 WARN 09:39:33,742 error reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_prod-Company-KeyCache
java.io.EOFException
	at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2280)
	at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2749)
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:779)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:279)
	at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:255)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:198)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:451)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:432)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:162)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
 INFO 09:39:33,747 reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_prod-Marker-KeyCache
 WARN 09:39:33,747 error reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_prod-Marker-KeyCache
java.io.EOFException
	at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2280)
	at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2749)
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:779)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:279)
	at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:255)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:198)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:451)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:432)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:162)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
 INFO 09:39:33,766 reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_test-LastQuote-KeyCache
 WARN 09:39:33,767 error reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_test-LastQuote-KeyCache
java.io.EOFException
	at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2280)
	at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2749)
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:779)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:279)
	at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:255)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:198)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:451)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:432)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:162)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
 INFO 09:39:33,769 reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_test-User-KeyCache
 WARN 09:39:33,771 error reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_test-User-KeyCache
java.io.StreamCorruptedException: invalid stream header: 00000024
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:782)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:279)
	at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:255)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:198)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:451)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:432)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:162)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
 INFO 09:39:33,781 reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_test-UserStreamInfo-KeyCache
 WARN 09:39:33,809 error reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_test-UserStreamInfo-KeyCache
java.io.EOFException
	at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2280)
	at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2749)
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:779)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:279)
	at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:255)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:198)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:451)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:432)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:162)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
 INFO 09:39:33,817 reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_test-TapixConfig-KeyCache
 WARN 09:39:33,833 error reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_test-TapixConfig-KeyCache
java.io.StreamCorruptedException: invalid stream header: 00000024
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:782)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:279)
	at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:255)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:198)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:451)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:432)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:162)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
 INFO 09:39:33,836 reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_test-PendingInvitation-KeyCache
 WARN 09:39:33,837 error reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_test-PendingInvitation-KeyCache
java.io.EOFException
	at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2280)
	at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2749)
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:779)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:279)
	at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:255)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:198)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:451)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:432)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:162)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
 INFO 09:39:33,849 reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_test-Invitation-KeyCache
 WARN 09:39:33,849 error reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_test-Invitation-KeyCache
java.io.EOFException
	at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2280)
	at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2749)
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:779)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:279)
	at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:255)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:198)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:451)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:432)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:162)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
 INFO 09:39:33,857 reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_test-StreamElement-KeyCache
 WARN 09:39:33,857 error reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_test-StreamElement-KeyCache
java.io.EOFException
	at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2280)
	at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2749)
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:779)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:279)
	at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:255)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:198)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:451)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:432)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:162)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
 INFO 09:39:33,860 reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_test-CompanyArea-KeyCache
 WARN 09:39:33,863 error reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_test-CompanyArea-KeyCache
java.io.StreamCorruptedException: invalid stream header: 00000024
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:782)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:279)
	at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:255)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:198)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:451)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:432)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:162)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
 INFO 09:39:33,867 reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_test-StreamFollower-KeyCache
 WARN 09:39:33,868 error reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_test-StreamFollower-KeyCache
java.io.EOFException
	at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2280)
	at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2749)
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:779)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:279)
	at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:255)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:198)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:451)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:432)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:162)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
 INFO 09:39:33,870 reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_test-Log-KeyCache
 WARN 09:39:33,870 error reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_test-Log-KeyCache
java.io.EOFException
	at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2280)
	at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2749)
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:779)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:279)
	at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:255)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:198)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:451)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:432)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:162)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
 INFO 09:39:33,872 reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_test-CompanySymbol-KeyCache
 WARN 09:39:33,873 error reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_test-CompanySymbol-KeyCache
java.io.StreamCorruptedException: invalid stream header: 00000021
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:782)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:279)
	at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:255)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:198)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:451)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:432)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:162)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
 INFO 09:39:33,874 reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_test-Stream-KeyCache
 WARN 09:39:33,875 error reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_test-Stream-KeyCache
java.io.EOFException
	at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2280)
	at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2749)
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:779)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:279)
	at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:255)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:198)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:451)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:432)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:162)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
 INFO 09:39:33,878 reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_test-UserInfo-KeyCache
 WARN 09:39:33,879 error reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_test-UserInfo-KeyCache
java.io.EOFException
	at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2280)
	at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2749)
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:779)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:279)
	at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:255)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:198)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:451)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:432)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:162)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
 INFO 09:39:33,882 reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_test-Following-KeyCache
 WARN 09:39:33,883 error reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_test-Following-KeyCache
java.io.EOFException
	at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2280)
	at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2749)
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:779)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:279)
	at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:255)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:198)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:451)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:432)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:162)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
 INFO 09:39:33,884 reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_test-Company-KeyCache
 WARN 09:39:33,885 error reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_test-Company-KeyCache
java.io.StreamCorruptedException: invalid stream header: 00000012
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:782)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:279)
	at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:255)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:198)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:451)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:432)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:162)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
 INFO 09:39:33,888 reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_test-Marker-KeyCache
 WARN 09:39:33,888 error reading saved cache /Users/olivier/Data/Cassandra/saved_caches/tapix_test-Marker-KeyCache
java.io.EOFException
	at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2280)
	at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2749)
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:779)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:279)
	at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:255)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:198)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:451)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:432)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:162)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
 INFO 09:39:33,949 Replaying /Users/olivier/Data/Cassandra/commitlog/CommitLog-1298111107654.log
 INFO 09:39:34,354 Finished reading /Users/olivier/Data/Cassandra/commitlog/CommitLog-1298111107654.log
 INFO 09:39:34,355 Skipped 8 mutations from unknown (probably removed) CF with id 1019
 INFO 09:39:34,355 Skipped 3 mutations from unknown (probably removed) CF with id 1023
 INFO 09:39:34,355 Skipped 4 mutations from unknown (probably removed) CF with id 1024
 INFO 09:39:34,355 Skipped 4 mutations from unknown (probably removed) CF with id 1009
 INFO 09:39:34,356 Skipped 4 mutations from unknown (probably removed) CF with id 1025
 INFO 09:39:34,356 Skipped 4 mutations from unknown (probably removed) CF with id 1026
 INFO 09:39:34,356 Skipped 4 mutations from unknown (probably removed) CF with id 1056
 INFO 09:39:34,356 Skipped 3 mutations from unknown (probably removed) CF with id 1027
 INFO 09:39:34,356 Skipped 8 mutations from unknown (probably removed) CF with id 1011
 INFO 09:39:34,357 Skipped 86 mutations from unknown (probably removed) CF with id 1000
 INFO 09:39:34,357 Skipped 53 mutations from unknown (probably removed) CF with id 1005
 INFO 09:39:34,357 Skipped 4 mutations from unknown (probably removed) CF with id 1055
 INFO 09:39:34,357 Skipped 1239 mutations from unknown (probably removed) CF with id 1007
 INFO 09:39:34,357 Skipped 75 mutations from unknown (probably removed) CF with id 1109
 INFO 09:39:34,358 Skipped 74 mutations from unknown (probably removed) CF with id 1110
 INFO 09:39:34,361 switching in a fresh Memtable for StreamElement at CommitLogContext(file='/Users/olivier/Data/Cassandra/commitlog/CommitLog-1298464771117.log', position=0)
 INFO 09:39:34,378 Enqueuing flush of Memtable-StreamElement@931090363(40945 bytes, 164 operations)
 INFO 09:39:34,380 Writing Memtable-StreamElement@931090363(40945 bytes, 164 operations)
 INFO 09:39:34,381 switching in a fresh Memtable for TapixConfig at CommitLogContext(file='/Users/olivier/Data/Cassandra/commitlog/CommitLog-1298464771117.log', position=0)
 INFO 09:39:34,382 Enqueuing flush of Memtable-TapixConfig@1766524955(108 bytes, 3 operations)
 INFO 09:39:34,634 Completed flushing /Users/olivier/Data/Cassandra/data/test/StreamElement-f-1-Data.db (46706 bytes)
 INFO 09:39:34,642 Writing Memtable-TapixConfig@1766524955(108 bytes, 3 operations)
 INFO 09:39:34,771 Completed flushing /Users/olivier/Data/Cassandra/data/tapix_test/TapixConfig-f-1-Data.db (159 bytes)
 INFO 09:39:34,773 Log replay complete
 INFO 09:39:34,855 Cassandra version: 0.7.2
 INFO 09:39:34,856 Thrift API version: 19.4.0
 INFO 09:39:34,856 Loading persisted ring state
 INFO 09:39:34,857 Starting up server gossip
 INFO 09:39:34,872 switching in a fresh Memtable for LocationInfo at CommitLogContext(file='/Users/olivier/Data/Cassandra/commitlog/CommitLog-1298464771117.log', position=148)
 INFO 09:39:34,872 Enqueuing flush of Memtable-LocationInfo@240567247(29 bytes, 1 operations)
 INFO 09:39:34,873 Writing Memtable-LocationInfo@240567247(29 bytes, 1 operations)
 INFO 09:39:34,920 Completed flushing /Users/olivier/Data/Cassandra/data/system/LocationInfo-f-8-Data.db (80 bytes)
 INFO 09:39:34,922 Compacting [org.apache.cassandra.io.sstable.SSTableReader(path='/Users/olivier/Data/Cassandra/data/system/LocationInfo-f-5-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='/Users/olivier/Data/Cassandra/data/system/LocationInfo-f-6-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='/Users/olivier/Data/Cassandra/data/system/LocationInfo-f-7-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='/Users/olivier/Data/Cassandra/data/system/LocationInfo-f-8-Data.db')]
 INFO 09:39:35,217 Using saved token 135895498325263028505710835440675748080
 INFO 09:39:35,229 switching in a fresh Memtable for LocationInfo at CommitLogContext(file='/Users/olivier/Data/Cassandra/commitlog/CommitLog-1298464771117.log', position=444)
 INFO 09:39:35,231 Enqueuing flush of Memtable-LocationInfo@1092296064(53 bytes, 2 operations)
 INFO 09:39:35,231 Writing Memtable-LocationInfo@1092296064(53 bytes, 2 operations)
 INFO 09:39:35,253 Compacted to /Users/olivier/Data/Cassandra/data/system/LocationInfo-tmp-f-9-Data.db.  770 to 447 (~58% of original) bytes for 3 keys.  Time: 331ms.
 INFO 09:39:35,294 Completed flushing /Users/olivier/Data/Cassandra/data/system/LocationInfo-f-10-Data.db (163 bytes)
 INFO 09:39:35,299 Will not load MX4J, mx4j-tools.jar is not in the classpath
olivier-smadjas-macbook-pro:bin OlivierSmadja$ 
",mac os x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20515,,,Wed Feb 23 14:15:58 UTC 2011,,,,,,,,,,"0|i0ga3r:",93067,,,,,Normal,,,,,,,,,,,,,,,,,"23/Feb/11 22:15;jbellis;fixed in CASSANDRA-2174;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BufferedRandomAccessFile.read doesn't always do full reads,CASSANDRA-565,12441227,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,junrao,junrao,junrao,20/Nov/09 05:17,16/Apr/19 17:33,22/Mar/23 14:57,20/Nov/09 06:26,0.5,,,,0,,,,,,"BufferedRandomAccessFile.read may read fewer bytes than required, even when EOF is not reached. This breaks commit log recovery, which assumes that when a read returns less than required, the EOF is reached.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Nov/09 05:19;junrao;issue565.patchev1;https://issues.apache.org/jira/secure/attachment/12425527/issue565.patchev1","20/Nov/09 06:06;junrao;issue565.patchev2;https://issues.apache.org/jira/secure/attachment/12425532/issue565.patchev2",,,,,,,,,,,,,2.0,junrao,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19757,,,Fri Nov 20 12:35:25 UTC 2009,,,,,,,,,,"0|i0fzpr:",91384,,,,,Normal,,,,,,,,,,,,,,,,,"20/Nov/09 05:19;junrao;Attach a patch.;;;","20/Nov/09 05:27;jbellis;Can you add a test illustrating the bug?;;;","20/Nov/09 05:30;jbellis;since the contract for RandomAccessFile that BRAF is patterned on is that read() may return less bytes even if it is not EOF, IMO we should probably make the read-with-retry loop a separate method (readFully?);;;","20/Nov/09 05:49;jbellis;(now that I see that this is documented behavior I withdraw the request for a test);;;","20/Nov/09 06:06;junrao;Patch v2. Use the existing readFully method in CommitLog instead.;;;","20/Nov/09 06:12;jbellis;+1;;;","20/Nov/09 06:26;junrao;committed to trunk;;;","20/Nov/09 20:35;hudson;Integrated in Cassandra #264 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/264/])
    BufferedRandomAccessFile.read doesn't always do full reads; patched by junrao, reviewed by jbellis for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TimeUUID comparator identify different UUID as long as they have same timestamp,CASSANDRA-907,12459667,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,19/Mar/10 23:57,16/Apr/19 17:33,22/Mar/23 14:57,20/Mar/10 03:51,0.6,,,,0,,,,,,Everything's in the title. As such TimeUUID behave as simple timestamp which is weird at best.,,david.pan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Mar/10 23:58;slebresne;TimeUUID_compareSameTimestamp.diff;https://issues.apache.org/jira/secure/attachment/12439283/TimeUUID_compareSameTimestamp.diff","20/Mar/10 00:12;jalessi;test case.txt;https://issues.apache.org/jira/secure/attachment/12439287/test+case.txt","20/Mar/10 00:41;slebresne;testDifferentTimeUUIDSameTimestamp.diff;https://issues.apache.org/jira/secure/attachment/12439292/testDifferentTimeUUIDSameTimestamp.diff",,,,,,,,,,,,3.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19913,,,Fri Mar 19 19:51:11 UTC 2010,,,,,,,,,,"0|i0g1tb:",91724,,,,,Low,,,,,,,,,,,,,,,,,"20/Mar/10 00:03;jbellis;can you add a Test case that catches the original bug?;;;","20/Mar/10 00:12;jalessi;The attached file has a test case in it that reproduces the bug;;;","20/Mar/10 00:18;jbellis;to clarify: a junit test case for ""ant test"" :);;;","20/Mar/10 00:41;slebresne;Posting a test for the test_server.py script.
I can come up with a test for the junit test if you prefer but there 
doesn't seems to be junit tests for TimeUUID right now.

As a side note, the system tests (test_server) seems a bit 
broken. The 'test_bad_calls' fails when insert is called with 
null argument because the thrift exception sent must have 
changed somehow. But you have to regenerate the thrift 
java binding (and recompile) to see the failing tests.;;;","20/Mar/10 03:51;jbellis;added junit test and committed to 0.6 and trunk, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"when a supercolumn is marked deleted, need to check for newer data in subcolumns",CASSANDRA-583,12441641,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jbellis,jbellis,jbellis,25/Nov/09 13:50,16/Apr/19 17:33,22/Mar/23 14:57,04/Dec/09 05:04,0.5,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Dec/09 06:05;jbellis;583.patch;https://issues.apache.org/jira/secure/attachment/12426584/583.patch","25/Nov/09 13:52;jbellis;dumbfix.patch;https://issues.apache.org/jira/secure/attachment/12426072/dumbfix.patch",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19765,,,Sat Dec 05 12:34:26 UTC 2009,,,,,,,,,,"0|i0fztr:",91402,,,,,Critical,,,,,,,,,,,,,,,,,"25/Nov/09 13:52;jbellis;brute-force fix (removeDeleted makes sure we don't return redundant data, but this could be inefficient);;;","02/Dec/09 06:05;jbellis;Here is the ""right"" fix, with a test case modification that catches the bug.;;;","04/Dec/09 01:39;junrao;+1.;;;","04/Dec/09 05:04;jbellis;committed;;;","05/Dec/09 20:34;hudson;Integrated in Cassandra #278 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/278/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.lang.NullPointerException getColumnFamily,CASSANDRA-690,12445262,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,lenn0x,lenn0x,12/Jan/10 14:42,16/Apr/19 17:33,22/Mar/23 14:57,13/Jan/10 14:33,,,,,0,,,,,,"Noticed this in trunk too

ERROR [ROW-READ-STAGE:20] 2010-01-11 22:38:29,850 CassandraDaemon.java (line 77) Fatal exception in thread Thread[ROW-READ-STAGE:20,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:806)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:750)
        at org.apache.cassandra.db.Table.getRow(Table.java:398)
        at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:59)
        at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:80)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19820,,,Wed Jan 13 06:33:38 UTC 2010,,,,,,,,,,"0|i0g0hb:",91508,,,,,Normal,,,,,,,,,,,,,,,,,"13/Jan/10 14:33;jbellis;dup of CASSANDRA-689 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Insert, Delete and Insert into a column family doesnt work... ",CASSANDRA-703,12445593,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,vijay2win@yahoo.com,vijay2win@yahoo.com,15/Jan/10 12:35,16/Apr/19 17:33,22/Mar/23 14:57,30/Jan/10 07:47,0.5,,,,0,,,,,,"Here is the code to reproduce the issue...

        ColumnPath colpath = new ColumnPath().setColumn_family(""VERSIONS"").setSuper_column(""123"".getBytes()).setColumn(""1234"".getBytes());
        con.insert(""WBXCDOCUMENT"", ""vijay"", colpath, ""test"".getBytes(), System.currentTimeMillis(), 2);

        ColumnPath path = new ColumnPath().setColumn_family(""VERSIONS"").setSuper_column(""123"".getBytes());
        con.remove(""WBXCDOCUMENT"", ""vijay"", path, System.currentTimeMillis(), 2);

        con.insert(""WBXCDOCUMENT"", ""vijay"", colpath, ""test"".getBytes(), System.currentTimeMillis(), 2);

        ColumnOrSuperColumn col = con.get(""WBXCDOCUMENT"", ""vijay"", path, 2);
        assertEquals(col.getSuper_column().getColumns() != null, true);

Expected result, get the column family..... but it throws notfound exception which is wrong.","Linux, Cassandra .5",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jan/10 03:46;jbellis;703-05.txt;https://issues.apache.org/jira/secure/attachment/12431453/703-05.txt","27/Jan/10 03:31;jbellis;703-trunk.txt;https://issues.apache.org/jira/secure/attachment/12431451/703-trunk.txt","16/Jan/10 05:53;vijay2win@yahoo.com;bug-fix-703.txt;https://issues.apache.org/jira/secure/attachment/12430444/bug-fix-703.txt",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19826,,,Fri Jan 29 23:47:47 UTC 2010,,,,,,,,,,"0|i0g0k7:",91521,,,,,Low,,,,,,,,,,,,,,,,,"16/Jan/10 05:53;vijay2win@yahoo.com;Suggesting this changes....  This might need the users to flush the data to disk.... and then use it.... is this incompatable because the seralizer, deserializer is changed... we might need to add one more field timestamp.... not sure if it is the right way to do it.

1) Added the timestamp to the constructor for the supercolumn.
2) Added the timestamp to serializer deserializer
3) change the classes dependent on it to fix the initialization
4) When new col is added the recent chang timestamp is updated.

Thanks
Vijay;;;","26/Jan/10 05:12;jbellis;just to rule out the obvious, did you test using timestamps 0, 1, 2 instead of System.currentTimeMillis()?  if the remove takes less than 1ms, the following insert will have the same timestamp and the remove will take precedence (ties go to tombstone).;;;","26/Jan/10 05:28;vijay2win@yahoo.com;Hi Jonathan, yes actually i did the following...

first insert
System.currentTimeMillis() 

first delete
System.currentTimeMillis()  + 5

secound insert
System.currentTimeMillis()  + 10

        String supcol = ""12356"";
        Client con = ConnectionCacheUtils.getinstance().getCassandraConnection().getclient();
        ColumnPath colpath = new ColumnPath().setColumn_family(""VERSIONS"").setSuper_column(supcol.getBytes()).setColumn(""1234"".getBytes());
        con.insert(""WBXCDOCUMENT"", ""vijay"", colpath, ""test"".getBytes(), System.currentTimeMillis(), 2);
        
        ColumnPath path = new ColumnPath().setColumn_family(""VERSIONS"").setSuper_column(supcol.getBytes());
        ColumnOrSuperColumn col = con.get(""WBXCDOCUMENT"", ""vijay"", path, 2);
        assertEquals(col.getSuper_column().getColumns() != null, true);
        
        con.remove(""WBXCDOCUMENT"", ""vijay"", path, System.currentTimeMillis() + 5, 2);
        con.insert(""WBXCDOCUMENT"", ""vijay"", colpath, ""test"".getBytes(), System.currentTimeMillis() + 10, 2);
        
        ColumnOrSuperColumn col2 = con.get(""WBXCDOCUMENT"", ""vijay"", path, 2);
        assertEquals(col2.getSuper_column().getColumns() != null, true);

Regards
Vijay;;;","27/Jan/10 00:33;jbellis;I can reproduce with this system test:

    def test_vijay(self):
        key = 'vijay'
        client.insert('Keyspace1', key, ColumnPath('Super1', 'sc1', _i64(4)), 'value4', 0, ConsistencyLevel.ONE)

        client.remove('Keyspace1', key, ColumnPath('Super1', 'sc1'), 1, ConsistencyLevel.ONE)

        client.insert('Keyspace1', key, ColumnPath('Super1', 'sc1', _i64(4)), 'value4', 2, ConsistencyLevel.ONE)

        result = client.get('Keyspace1', key, ColumnPath('Super1', 'sc1'), ConsistencyLevel.ONE)
        assert result.super_column.columns is not None, result.super_column
;;;","27/Jan/10 03:31;jbellis;the internals are fine; the bug is in turning the data from the internal representation into Thrift objects.  patch attached.;;;","27/Jan/10 03:46;jbellis;0.5 version of patch;;;","27/Jan/10 04:30;vijay2win@yahoo.com;+1, tested and works... thanks Jonathan..;;;","30/Jan/10 07:47;jbellis;committed to 0.5 and trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
org.apache.cassandra.db.UnserializableColumnFamilyException,CASSANDRA-1868,12493437,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,mconrad,mconrad,16/Dec/10 22:15,16/Apr/19 17:33,22/Mar/23 14:57,17/Dec/10 03:48,,,,,0,,,,,,"I recently updated a test cluster with 7 nodes from cassandra 0.6.8 to cassandra 0.7-rc2. At first it worked, but after some time the client gets a TimeoutException:

me.prettyprint.hector.api.exceptions.HTimedOutException: TimedOutException()
        at me.prettyprint.cassandra.service.ExceptionsTranslatorImpl.translate(ExceptionsTranslatorImpl.java:31)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl$3.execute(KeyspaceServiceImpl.java:161)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl$3.execute(KeyspaceServiceImpl.java:143)
        at me.prettyprint.cassandra.service.Operation.executeAndSetResult(Operation.java:89)
        at me.prettyprint.cassandra.connection.HConnectionManager.operateWithFailover(HConnectionManager.java:142)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl.operateWithFailover(KeyspaceServiceImpl.java:129)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl.getRangeSlices(KeyspaceServiceImpl.java:165)
        at me.prettyprint.cassandra.model.thrift.ThriftRangeSlicesQuery$1.doInKeyspace(ThriftRangeSlicesQuery.java:67)
        at me.prettyprint.cassandra.model.thrift.ThriftRangeSlicesQuery$1.doInKeyspace(ThriftRangeSlicesQuery.java:63)
        at me.prettyprint.cassandra.model.KeyspaceOperationCallback.doInKeyspaceAndMeasure(KeyspaceOperationCallback.java:20)
        at me.prettyprint.cassandra.model.ExecutingKeyspace.doExecute(ExecutingKeyspace.java:65)
        at me.prettyprint.cassandra.model.thrift.ThriftRangeSlicesQuery.execute(ThriftRangeSlicesQuery.java:62)
Caused by: TimedOutException()
        at org.apache.cassandra.thrift.Cassandra$get_range_slices_result.read(Cassandra.java:12104)
        at org.apache.cassandra.thrift.Cassandra$Client.recv_get_range_slices(Cassandra.java:732)
        at org.apache.cassandra.thrift.Cassandra$Client.get_range_slices(Cassandra.java:704)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl$3.execute(KeyspaceServiceImpl.java:149)
        ... 25 more


on the server the following exception is thrown:

 INFO [FLUSH-WRITER-POOL:1] 2010-12-07 14:32:41,282 Memtable.java (line 149) Writing Memt
 INFO [GossipStage:1] 2010-12-16 14:08:44,280 Gossiper.java (line 583) Node /94.242.198.16 has restarted, now UP again
 INFO [HintedHandoff:1] 2010-12-16 14:08:44,280 HintedHandOffManager.java (line 191) Started hinted handoff for endpoint /94.242.198.16
 INFO [HintedHandoff:1] 2010-12-16 14:08:44,281 HintedHandOffManager.java (line 247) Finished hinted handoff of 0 rows to endpoint /94.242.198.16
 INFO [GossipStage:1] 2010-12-16 14:08:44,281 StorageService.java (line 660) Node /94.242.198.16 state jump to normal
ERROR [MutationStage:30] 2010-12-16 14:09:50,247 RowMutationVerbHandler.java (line 83) Error in row mutation
org.apache.cassandra.db.UnserializableColumnFamilyException: Couldn't find cfId=962945840
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:117)
        at org.apache.cassandra.db.RowMutationSerializer.defreezeTheMaps(RowMutation.java:383)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:393)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:351)
        at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:52)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:63)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
",Cassandra 0.7.0-RC2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20351,,,Fri Dec 17 08:08:50 UTC 2010,,,,,,,,,,"0|i0g7wf:",92710,,,,,Normal,,,,,,,,,,,,,,,,,"16/Dec/10 22:26;tjake;This is most likely an effect of CASSANDRA-1847

If you feel adventurous, apply the patch or test with 0.7 branch.;;;","17/Dec/10 16:08;mconrad;I tried build #84 and now it seems to work. Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SlicePredicate does not always round-trip correctly,CASSANDRA-1049,12463696,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,05/May/10 03:29,16/Apr/19 17:33,22/Mar/23 14:57,25/May/10 03:45,0.6.2,,,,0,,,,,,,,anty,scottfines,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/May/10 02:59;jbellis;1049-0.6.txt;https://issues.apache.org/jira/secure/attachment/12445367/1049-0.6.txt","05/May/10 03:48;jbellis;1049-test.txt;https://issues.apache.org/jira/secure/attachment/12443620/1049-test.txt","25/May/10 02:11;jbellis;1049.txt;https://issues.apache.org/jira/secure/attachment/12445359/1049.txt","13/May/10 21:50;scottfines;SliceRangeSerializationTest.java;https://issues.apache.org/jira/secure/attachment/12444399/SliceRangeSerializationTest.java",,,,,,,,,,,4.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19974,,,Mon May 24 19:45:54 UTC 2010,,,,,,,,,,"0|i0g2ov:",91866,,,,,Normal,,,,,,,,,,,,,,,,,"05/May/10 03:48;jbellis;converted Mark Schnitzius's example from the ML to a failing unit test;;;","05/May/10 05:37;jbellis;Jeremy reports that he can reproduce using Thrift 0.3 rc, too.;;;","13/May/10 21:48;scottfines;I can confirm that this is also an issue with SliceRange itself, not just with SlicePredicate. I am attaching my UnitTest to this report as well(Though perhaps it should be set as a linked Issue instead).

I'm a bit new to Cassandra, so please let me know if there is something amiss with this unittest.;;;","18/May/10 00:05;jbellis;The bug is, thrift json serialization is broken.

patch to switch to using binary serializer, converted to a hex string.  inefficient but this is not a performance-sensitive method.;;;","25/May/10 00:22;urandom;Can you rebase please?;;;","25/May/10 02:11;jbellis;rebased;;;","25/May/10 02:38;jbellis;rebased to 0.6;;;","25/May/10 02:59;jbellis;0.6 patch w/ unit test;;;","25/May/10 03:30;urandom;+1;;;","25/May/10 03:45;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLI command create keyspace can destroy the schema,CASSANDRA-1633,12477802,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,tonnerre,tonnerre,20/Oct/10 07:22,16/Apr/19 17:33,22/Mar/23 14:57,20/Oct/10 07:37,,,Legacy/CQL,,0,,,,,,"When trying to create a keyspace through the cli, wrong parameters can break the schema and make the database unusable.

[default@unknown] create keyspace foo with placement_strategy = 'org.apache.cassandra.locator.NetworkTopologyStrategy' and replication_factor = 2
java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
[default@unknown] create keyspace foo with placement_strategy = 'org.apache.cassandra.locator.NetworkTopologyStrategy' and replication_factor = 1
Keyspace already exists.
[default@unknown] drop keyspace foo
Exception null
[default@unknown]

From the cassandra log:

ERROR [GC inspection] 2010-10-20 01:00:44,403 AbstractCassandraDaemon.java (line 88) Fatal exception in thread Thread[GC inspection,5,main]
java.lang.reflect.UndeclaredThrowableException
        at $Proxy5.getActiveCount(Unknown Source)
        at org.apache.cassandra.service.GCInspector.logThreadPoolStats(GCInspector.java:156)
        at org.apache.cassandra.service.GCInspector.logIntervalGCStats(GCInspector.java:136)
        at org.apache.cassandra.service.GCInspector.access$000(GCInspector.java:39)
        at org.apache.cassandra.service.GCInspector$1.run(GCInspector.java:93)
        at java.util.TimerThread.mainLoop(Timer.java:555)
        at java.util.TimerThread.run(Timer.java:505)
Caused by: javax.management.AttributeNotFoundException: No such attribute: ActiveCount
        at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:81)
        at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:204)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:647)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:668)
        at javax.management.MBeanServerInvocationHandler.invoke(MBeanServerInvocationHandler.java:280)
        ... 7 more
ERROR [MIGRATION_STAGE:1] 2010-10-20 01:00:44,417 AbstractCassandraDaemon.java (line 88) Fatal exception in thread Thread[MIGRATION_STAGE:1,5,main]
java.lang.RuntimeException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
        at org.apache.cassandra.service.StorageService.createReplicationStrategy(StorageService.java:287)
        at org.apache.cassandra.service.StorageService.initReplicationStrategy(StorageService.java:262)
        at org.apache.cassandra.config.DatabaseDescriptor.setTableDefinition(DatabaseDescriptor.java:1006)
        at org.apache.cassandra.db.migration.AddKeyspace.applyModels(AddKeyspace.java:74)
        at org.apache.cassandra.db.migration.Migration.apply(Migration.java:157)
        at org.apache.cassandra.thrift.CassandraServer$1.call(CassandraServer.java:645)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:717)
Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
        at org.apache.cassandra.locator.AbstractReplicationStrategy.createReplicationStrategy(AbstractReplicationStrategy.java:241)
        at org.apache.cassandra.service.StorageService.createReplicationStrategy(StorageService.java:277)
        ... 10 more
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:531)
        at org.apache.cassandra.locator.AbstractReplicationStrategy.createReplicationStrategy(AbstractReplicationStrategy.java:237)
        ... 11 more
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.locator.NetworkTopologyStrategy.<init>(NetworkTopologyStrategy.java:62)
        ... 16 more

After that, the server won't start anymore:

Exception encountered during startup.
java.lang.NullPointerException
        at org.apache.cassandra.io.sstable.IndexSummary.complete(IndexSummary.java:60)
        at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:270)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:173)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:157)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:334)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:322)
        at org.apache.cassandra.db.Table.initCf(Table.java:301)
        at org.apache.cassandra.db.Table.<init>(Table.java:249)
        at org.apache.cassandra.db.Table.open(Table.java:102)
        at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:142)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:99)
        at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:54)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:201)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:133)
","NetBSD methusalix.pas-un-geek-en-tant-que-tel.ch 5.99.39 NetBSD 5.99.39 (GENERIC) #2: Sun Oct 10 00:40:52 CEST 2010  tonnerre@methusalix.pas-un-geek-en-tant-que-tel.ch:/usr/obj/sys/arch/amd64/compile/GENERIC amd64
OpenJDK Runtime Environment (build 1.7.0-internal-pkgsrc_2010_06_24_02_32-b00)
OpenJDK 64-Bit Server VM (build 18.0-b04, mixed mode)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20227,,,Tue Oct 19 23:37:44 UTC 2010,,,,,,,,,,"0|i0g6fj:",92472,,,,,Normal,,,,,,,,,,,,,,,,,"20/Oct/10 07:37;jbellis;fixed in CASSANDRA-1626;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wordcount contrib does not work in Hadoop distributed mode,CASSANDRA-817,12457030,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,johanoskarsson,johanoskarsson,johanoskarsson,23/Feb/10 00:41,16/Apr/19 17:33,22/Mar/23 14:57,23/Feb/10 01:23,0.6,,,,0,,,,,,The column name is set in a static variable in the job setup. That variable will be empty when the job has been distributed to the tasktrackers. The variable must be set via the setup method in the mapper.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Feb/10 00:46;johanoskarsson;CASSANDRA-817.patch;https://issues.apache.org/jira/secure/attachment/12436594/CASSANDRA-817.patch",,,,,,,,,,,,,,1.0,johanoskarsson,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19879,,,Wed Feb 24 13:07:31 UTC 2010,,,,,,,,,,"0|i0g19b:",91634,,,,,Low,,,,,,,,,,,,,,,,,"23/Feb/10 00:46;johanoskarsson;Sets the column name using the configuration and the setup method.;;;","23/Feb/10 00:51;jbellis;+1;;;","23/Feb/10 01:23;johanoskarsson;Committed to trunk and the 0.6 branch.;;;","24/Feb/10 21:07;hudson;Integrated in Cassandra #364 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/364/])
    Fix bug to allow distributed Hadoop jobs. Patch by johan, review by jbellis. 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
race condition with Gossiper with receiving messages when starting up?,CASSANDRA-1010,12462724,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Duplicate,gdusbabek,erickt,erickt,22/Apr/10 10:27,16/Apr/19 17:33,22/Mar/23 14:57,12/Jun/10 04:37,0.7 beta 1,,,,0,,,,,,"I occasionally get this exception when starting a node:

ERROR 17:53:52,941 Fatal exception in thread Thread[GMFD:4,5,main]
java.lang.AssertionError
	at org.apache.cassandra.net.Header.<init>(Header.java:55)
	at org.apache.cassandra.net.Header.<init>(Header.java:73)
	at org.apache.cassandra.net.Message.<init>(Message.java:58)
	at org.apache.cassandra.gms.Gossiper.makeGossipDigestAckMessage(Gossiper.java:295)
	at org.apache.cassandra.gms.Gossiper$GossipDigestSynVerbHandler.doVerb(Gossiper.java:888)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:41)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)

I believe I've tracked down this to there being a small window of time between starting the Gossiper.GossipSynVerbHandler thread and when Gossiper.start is called with it's endpoint. This is because the thread is started in the StorageService constructor, which seems to be getting initialized before thrift.CassandraDaemon.setup is being called.",,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-1160,,,,,,,,,,,,,,,,,,,,,,,,,0.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19954,,,Thu Apr 22 02:31:40 UTC 2010,,,,,,,,,,"0|i0g2g7:",91827,,,,,Low,,,,,,,,,,,,,,,,,"22/Apr/10 10:31;erickt;Forgot to mention that this has been happening when I have 10 node starting basically at the same time.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DatacenterReadResolver not triggering repair,CASSANDRA-2556,12505139,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,stuhood,stuhood,26/Apr/11 05:39,16/Apr/19 17:33,22/Mar/23 14:57,28/Apr/11 21:37,0.7.6,0.8.0 beta 2,,,0,,,,,,DatacenterReadResolver only calls maybeResolveForRepair for local reads.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Apr/11 04:46;jbellis;2556.txt;https://issues.apache.org/jira/secure/attachment/12477580/2556.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20695,,,Thu May 12 14:42:59 UTC 2011,,,,,,,,,,"0|i0gc2f:",93385,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"28/Apr/11 04:46;jbellis;patch that extracts the ""is this message one that should count towards blockfor"" logic into the waitingFor method; DatacenterReadCallback now only overrides those instead of all of the response methods, which fixes the bug and reduces the surface for introducing similar bugs in the future.  (applies on top of CASSANDRA-2552 v2.);;;","28/Apr/11 19:50;slebresne;+1
The patch applies on top of 0.8 (that is on top of CASSANDRA-2552 v2 for 0.8). Should be backported to 0.7 first I think.;;;","28/Apr/11 21:37;jbellis;committed;;;","28/Apr/11 21:59;hudson;Integrated in Cassandra-0.7 #460 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/460/])
    trigger read repair correctly forLOCAL_QUORUM reads
patch by jbellis; reviewed by slebresne for CASSANDRA-2556
;;;","12/May/11 22:42;jbellis;This also fixed a second bug: the old code checked for n == blockFor on LOCAL_QUORUM, so there was a race where if we exceeded blockFor with two responses coming in at nearly the same time, it would never test at the moment of equality and the coordinator would throw TOE even though enough responses were actually received.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NegativeArraySizeException in column bloom filter deserialization,CASSANDRA-2234,12499562,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,kunda,kunda,24/Feb/11 16:05,16/Apr/19 17:33,22/Mar/23 14:57,24/Feb/11 16:17,0.7.3,,,,0,Error,,,,,"This seems to be related to CASSANDRA-2165
the bitLength read from the InputStreams is negative:

ERROR [ReadStage:107] 2011-02-24 09:49:50,538 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[ReadStage:107,5,main]
java.lang.RuntimeException: java.lang.NegativeArraySizeException
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.NegativeArraySizeException
        at org.apache.cassandra.utils.BloomFilterSerializer.deserialize(BloomFilterSerializer.java:49)
        at org.apache.cassandra.utils.BloomFilterSerializer.deserialize(BloomFilterSerializer.java:30)
        at org.apache.cassandra.io.sstable.IndexHelper.defreezeBloomFilter(IndexHelper.java:108)
        at org.apache.cassandra.db.columniterator.SSTableNamesIterator.read(SSTableNamesIterator.java:106)
        at org.apache.cassandra.db.columniterator.SSTableNamesIterator.<init>(SSTableNamesIterator.java:71)
        at org.apache.cassandra.db.filter.NamesQueryFilter.getSSTableColumnIterator(NamesQueryFilter.java:59)
        at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:80)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1275)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1167)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1095)
        at org.apache.cassandra.db.Table.getRow(Table.java:384)
        at org.apache.cassandra.db.SliceByNamesReadCommand.getRow(SliceByNamesReadCommand.java:60)
        at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:473)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-2165,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20517,,,Thu Feb 24 08:19:12 UTC 2011,,,,,,,,,,"0|i0ga5j:",93075,,,,,Normal,,,,,,,,,,,,,,,,,"24/Feb/11 16:17;slebresne;Duplicate of CASSANDRA-2195;;;","24/Feb/11 16:19;kunda;Reported duplicate;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTableImport adds columns marked for delete incorrectly in methods addToStandardCF & addToSuperCF,CASSANDRA-1753,12480245,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,bryantower,pushpinder.heer,pushpinder.heer,18/Nov/10 03:50,16/Apr/19 17:33,22/Mar/23 14:57,18/Nov/10 18:14,0.7.0 rc 1,,Legacy/Tools,,0,,,,,,"The logic for adding column families in the methods addToStandardCF & addToSuperCF appears to be backwards

            if (col.isDeleted) {
                cfamily.addColumn(path, hexToBytes(col.value), new TimestampClock(col.timestamp));
            } else {
                cfamily.addTombstone(path, hexToBytes(col.value), new TimestampClock(col.timestamp));
            }

",,bryantower,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,"18/Nov/10 06:26;bryantower;cassandra-0.7-1753.txt;https://issues.apache.org/jira/secure/attachment/12459838/cassandra-0.7-1753.txt",,,,,,,,,,,,,,1.0,bryantower,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20292,,,Fri Nov 19 15:12:34 UTC 2010,,,,,,,,,,"0|i0g76f:",92593,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"18/Nov/10 06:26;bryantower;This patch adds a check to the SSTableImportTest to make sure that the retrieved Column is not deleted and fixes the bug in the SSTableImport.java by switching the logic on the isDeletedCheck.
The patch is for the cassandra-0.7 branch.;;;","18/Nov/10 08:21;bryantower;This bug prevents anyone importing to SSTables from JSON format.  All of the data that was exported shows up as tombstoned when trying to do an import on the 0.7 branch.  This patch is simple and is isolated to the SSTableImport;;;","18/Nov/10 16:23;slebresne;+1;;;","18/Nov/10 18:14;jbellis;committed, thanks!;;;","19/Nov/10 23:12;hudson;Integrated in Cassandra-0.7 #16 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/16/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GossipTimerTask throws ConcurrentModificationException after node decommission ,CASSANDRA-1239,12468227,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,xluke,xluke,30/Jun/10 15:12,16/Apr/19 17:33,22/Mar/23 14:57,30/Jun/10 22:35,,,,,0,,,,,,"Yesterday I made decommission for a cassandra node, and this morning the GossipTimerTask threads of  all nodes get the fatal exception and crash.
Gossiper.doStatusCheck() is visiting endPointStateMap_,keySet()  by for-loop  and get  a ConcurrentModificationException after
Gossiper.removeEndPoint() was called in the loop and endPointStateMap_ was modified.


INFO [Timer-0] 2010-06-30 07:53:51,290 Gossiper.java (line 401) FatClient /222.222.111.111 has been silent for 3600000ms, removing from gossip
ERROR [Timer-0] 2010-06-30 07:53:51,291 CassandraDaemon.java (line 78) Fatal exception in thread Thread[Timer-0,5,RMI Runtime]
java.lang.RuntimeException: java.util.ConcurrentModificationException
        at org.apache.cassandra.gms.Gossiper$GossipTimerTask.run(Gossiper.java:97)
        at java.util.TimerThread.mainLoop(Timer.java:512)
        at java.util.TimerThread.run(Timer.java:462)
Caused by: java.util.ConcurrentModificationException
        at java.util.Hashtable$Enumerator.next(Hashtable.java:1031)
        at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:382)
        at org.apache.cassandra.gms.Gossiper$GossipTimerTask.run(Gossiper.java:91)
        ... 2 more",,rschildmeijer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20045,,,Wed Jun 30 14:35:04 UTC 2010,,,,,,,,,,"0|i0g3uf:",92053,,,,,Normal,,,,,,,,,,,,,,,,,"30/Jun/10 22:35;jbellis;fixed for 0.7 in CASSANDRA-757;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wordcount contrib is not including all required jars,CASSANDRA-816,12457009,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,johanoskarsson,johanoskarsson,johanoskarsson,22/Feb/10 22:45,16/Apr/19 17:33,22/Mar/23 14:57,23/Feb/10 01:50,0.6,,,,0,,,,,,The wordcount contrib build process is not including the libraries downloaded using ivy in the final jar file.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Feb/10 00:16;johanoskarsson;CASSANDRA-816.patch;https://issues.apache.org/jira/secure/attachment/12436585/CASSANDRA-816.patch","22/Feb/10 22:46;johanoskarsson;CASSANDRA-816.patch;https://issues.apache.org/jira/secure/attachment/12436574/CASSANDRA-816.patch",,,,,,,,,,,,,2.0,johanoskarsson,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19878,,,Wed Feb 24 13:07:31 UTC 2010,,,,,,,,,,"0|i0g193:",91633,,,,,Low,,,,,,,,,,,,,,,,,"22/Feb/10 22:46;johanoskarsson;Adds the required files to the jar.;;;","23/Feb/10 00:16;johanoskarsson;Created the patch in the wrong dir, this is the corrected one;;;","23/Feb/10 00:28;jbellis;do we need this in 0.6 or does this only affect the post-0.6 ivy changes?;;;","23/Feb/10 01:03;johanoskarsson;According to CASSANDRA-802 the ivy changes were in 0.6, so this one should also go into both 0.6 and trunk.;;;","23/Feb/10 01:11;jbellis;+1

please commit first to 0.6 then merge to trunk with repo merge (that is, not old-style cherry picking).  thanks!;;;","23/Feb/10 01:50;johanoskarsson;Committed to trunk and the 0.6 branch.;;;","24/Feb/10 21:07;hudson;Integrated in Cassandra #364 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/364/])
    Fix bug where ivy downloaded jar files were not included. Patch by johan, review by jbellis. 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
saved row cache doesn't save the cache,CASSANDRA-2102,12497520,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,mdennis,mdennis,mdennis,03/Feb/11 06:42,16/Apr/19 17:33,22/Mar/23 14:57,11/Feb/11 01:09,0.7.1,,,,0,,,,,,"saving row caches works by periodically iterating of the keySet() on the caches and writing the keys for the cached contents to disk.  The cache keys are DecoratedKeys.  DecoratedKeys contain a Token token and a ByteBuffer key.  The underlying buffer on the key gets reused so the contents change.  This means that all the cache entries have distinct tokens but only a handful of distinct key values.  This means that when the cache is loaded you only end up loading a handful of keys instead of the ones actually in your cache.

",,cburroughs,scode,,,,,,,,,,,,,,,,,,,,240,240,,0%,240,240,,,,,,,,,,,,,,,,,,,,"04/Feb/11 02:41;mdennis;2102-cassandra-0.7-v2.txt;https://issues.apache.org/jira/secure/attachment/12470173/2102-cassandra-0.7-v2.txt","03/Feb/11 06:51;mdennis;2102-cassandra-0.7.txt;https://issues.apache.org/jira/secure/attachment/12470082/2102-cassandra-0.7.txt",,,,,,,,,,,,,2.0,mdennis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20444,,,Thu Feb 10 22:11:36 UTC 2011,,,,,,,,,,"0|i0g9br:",92941,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"03/Feb/11 06:51;mdennis;attached patch switches to saving tokens to disk instead of keys, removes DecoratedKey.key from the entries in caches (fixing the bug and reducing memory consumption) and versions (kind of) the saved cache files.;;;","03/Feb/11 07:09;jbellis;bq. The underlying buffer on the key gets reused

Where does this reuse happen?  RowMutation.deepCopy should be taking care of it on the local side, and sent-over-the-network buffers are not reused.;;;","04/Feb/11 00:45;jbellis;duplicate of CASSANDRA-2076;;;","04/Feb/11 02:39;mdennis;not really a duplicate.

This deals with writing the cache correctly.

CASSANDRA-2076 should better handle in general when a cache file is corrupt.;;;","04/Feb/11 02:41;mdennis;per IRC discussion v2 patch just copies the key when inserting into the cache.

Since CASSANDRA-1034 looks likely in the somewhat near future, it's pointless to make the changes in the original patch.;;;","04/Feb/11 04:16;jbellis;Let's clone in storageproxy since it's not necessary for keys read over MessagingService;;;","09/Feb/11 08:01;mdennis;I really think safety is the way to go here (e.g. clone right before we put it into the cache).  Not to mention cloning early would be generating memcopies for things we don't need to if they never actually end up in the cache (e.g. things already in the cache, rejected by bloom filters).;;;","11/Feb/11 00:56;jbellis;I think you're right, if your cache is doing its job (high hit rate) then copy-before-insert will be less copies than copy-for-local-operation.;;;","11/Feb/11 01:09;jbellis;committed.  (even without TFFT enabled, it's still a good idea for the same reason as CASSANDRA-1801;;;","11/Feb/11 06:11;hudson;Integrated in Cassandra-0.7 #276 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/276/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"get_string_list_property(""tables"") does not include CompareSubcolumnsWith",CASSANDRA-326,12431766,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,eweaver,eweaver,eweaver,30/Jul/09 13:11,16/Apr/19 17:33,22/Mar/23 14:57,30/Jul/09 23:17,0.4,,,,0,,,,,,,,eweaver,hammer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Jul/09 13:27;eweaver;CASSANDRA-326.diff;https://issues.apache.org/jira/secure/attachment/12414987/CASSANDRA-326.diff",,,,,,,,,,,,,,1.0,eweaver,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19636,,,Fri Jul 31 12:34:07 UTC 2009,,,,,,,,,,"0|i0fy93:",91147,,,,,Normal,,,,,,,,,,,,,,,,,"30/Jul/09 13:35;euphoria;Patch only applies to trunk with fuzz.  Not sure about the null assignment change -- objects are null by default in Java, no need to change that.

Fixes the bug though.  +1 minus the null assignment.;;;","30/Jul/09 13:37;eweaver; I'm not sure about the null either. It didn't want to build without it. Some variable scope issue, or me being dumb.;;;","30/Jul/09 13:45;euphoria;Go figure, it does complain about ""variable subcolumnComparator might not have been initialized"" if you don't set that null.

Well, +1s all around then.  Ship it!;;;","30/Jul/09 23:17;jbellis;committed;;;","31/Jul/09 20:34;hudson;Integrated in Cassandra #153 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/153/])
    add CompareSubcolumnsWith to describe_keyspace.  patch by Evan Weaver; reviewed by Michael Greene for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MIGRATION-STAGE: IllegalArgumentException: value already present,CASSANDRA-1400,12471768,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,gdusbabek,arya,arya,17/Aug/10 09:55,16/Apr/19 17:33,22/Mar/23 14:57,18/Aug/10 05:49,0.7 beta 2,,,,0,,,,,,"While inserting into a 3 node cluster, one of the nodes got this exception in its log:

ERROR [MIGRATION-STAGE:1] 2010-08-16 17:46:24,090 CassandraDaemon.java (line 82) Uncaught exception in thread Thread[MIGRATION-STAGE:1,5,main]
java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: value already present: 1017
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
        at java.util.concurrent.FutureTask.get(FutureTask.java:111)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:87)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1118)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.IllegalArgumentException: value already present: 1017
        at com.google.common.base.Preconditions.checkArgument(Preconditions.java:115)
        at com.google.common.collect.AbstractBiMap.putInBothMaps(AbstractBiMap.java:109)
        at com.google.common.collect.AbstractBiMap.put(AbstractBiMap.java:94)
        at com.google.common.collect.HashBiMap.put(HashBiMap.java:83)
        at org.apache.cassandra.config.CFMetaData.map(CFMetaData.java:170)
        at org.apache.cassandra.db.migration.AddColumnFamily.applyModels(AddColumnFamily.java:78)
        at org.apache.cassandra.db.migration.Migration.apply(Migration.java:157)
        at org.apache.cassandra.thrift.CassandraServer$2.call(CassandraServer.java:729)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        ... 2 more","CentOS 5.2
Trunc August 16th 2010 at 2pm pst",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Aug/10 09:56;arya;system.log;https://issues.apache.org/jira/secure/attachment/12452242/system.log",,,,,,,,,,,,,,1.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20118,,,Tue Aug 17 21:49:25 UTC 2010,,,,,,,,,,"0|i0g4tr:",92212,,,,,Normal,,,,,,,,,,,,,,,,,"17/Aug/10 09:56;arya;Attached, please find the log of the node which produces the exception. ;;;","18/Aug/10 05:49;gdusbabek;I forgot about this ticket when I created CASSANDRA-1403.  Either way... fixed!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get_column call broken with recent ReadCommand change,CASSANDRA-90,12423321,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,junrao,junrao,junrao,21/Apr/09 08:25,16/Apr/19 17:33,22/Mar/23 14:57,21/Apr/09 10:59,,,,,0,,,,,,get_column returns null on an existing column. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Apr/09 08:30;junrao;issue90.patch001;https://issues.apache.org/jira/secure/attachment/12405974/issue90.patch001","21/Apr/09 08:31;junrao;issue90.patch002;https://issues.apache.org/jira/secure/attachment/12405975/issue90.patch002",,,,,,,,,,,,,2.0,junrao,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19544,,,Tue Apr 21 02:59:28 UTC 2009,,,,,,,,,,"0|i0fwtb:",90914,,,,,Normal,,,,,,,,,,,,,,,,,"21/Apr/09 08:30;junrao;Attach a testcase that identifies the problem. The problem is caused because when colnames is not provided in ReadCommand, colnames_ doesn't have the same reference as EMPTY_COLUMNS. This broken the test in ReadCommand.doRow().;;;","21/Apr/09 08:31;junrao;Attach a fix.;;;","21/Apr/09 10:59;jbellis;+1

committed w/ minor modification of changing the test to !columnNames.isEmpty() instead of trying to preserve EMPTY_COLUMNS equality.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get_slice needs to allow returning all columns,CASSANDRA-262,12429005,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,lenn0x,lenn0x,27/Jun/09 06:53,16/Apr/19 17:33,22/Mar/23 14:57,04/Jul/09 13:45,0.4,,,,0,,,,,,"Right now get_slice requires you to enter a 'large' value, -1 used to indicate 'all columns'. We should allow this.",,eweaver,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19612,,,Sun Nov 08 00:57:10 UTC 2009,,,,,,,,,,"0|i0fxv3:",91084,,,,,Normal,,,,,,,,,,,,,,,,,"01/Jul/09 23:43;jbellis;How about if we add a configuration parameter for ""maximum columns to slice at once,"" that defaults to something high enough (1000?) that you really ought to page if you need more than that anyway, and low enough that it's in no danger of crashing your server from OOM?;;;","01/Jul/09 23:51;lenn0x;+1. I am okay with this.;;;","02/Jul/09 00:01;jbellis;Eric, since you're already in the slice code for CASSANDRA-263, do you think you could take a stab at this too?;;;","02/Jul/09 06:35;jbellis;Actually, as with many of my ""brilliant"" ideas, this one looks worse given a little more time. :)

I don't see what purpose this would serve over the client just sending 1000 for the count, other than obfuscation.;;;","04/Jul/09 09:10;eweaver;For what it's worth, I'm having the Ruby client currently default to limit = 100...I couldn't really care less what the server thinks the limit is. I expect other clients would be similar.
;;;","04/Jul/09 13:45;jbellis;yes, that's the right idea.

I set the default for all the count parameters to 100 in cassandra.thrift.  I don't know of any languages for which thrift actually generates working defaults but it's thrift's bug now. :);;;","04/Jul/09 20:34;hudson;Integrated in Cassandra #128 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/128/])
    default all count args to 100.  patch by jbellis for 
;;;","08/Nov/09 08:57;kristopolous;FYI, The last link is now dead.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
migrate to junit,CASSANDRA-162,12425156,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,12/May/09 07:55,16/Apr/19 17:33,22/Mar/23 14:57,12/May/09 23:28,,,,,0,,,,,,junit has a fork option which will allow our tests to be 100% independent of each other instead of sorta kinda independent.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/May/09 07:56;jbellis;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-162-migrate-to-JUnit-4.6.txt;https://issues.apache.org/jira/secure/attachment/12407831/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-162-migrate-to-JUnit-4.6.txt","12/May/09 07:56;jbellis;ASF.LICENSE.NOT.GRANTED--0002-extract-tests-that-could-conflict-with-each-other-to-a.txt;https://issues.apache.org/jira/secure/attachment/12407832/ASF.LICENSE.NOT.GRANTED--0002-extract-tests-that-could-conflict-with-each-other-to-a.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19577,,,Wed May 13 09:26:38 UTC 2009,,,,,,,,,,"0|i0fx93:",90985,,,,,Normal,,,,,,,,,,,,,,,,,"12/May/09 09:18;jbellis;you'll want to put the junit4 jar in lib/ to run the tests w/ this applied.  (I'm using 4.6.);;;","12/May/09 18:13;johanoskarsson;I won't have time to review the patch this week unfortunately, but +1 on moving to junit4;;;","12/May/09 23:04;sandeep_tata;I get failures for namesort, timesort, testcompaction. I'm getting these even on a clean checkout of trunk -- so this is unrelated to the move to junit.

RackUnawareStrategyTest fails with no runnable methods. (Everything in it seems to be commented out).

I'm +1 after fixing RackUnawareStrategyTest (taking a quick look at the code, the other failures seem unrelated to the junit move).


;;;","12/May/09 23:18;sandeep_tata;BTW, the tests take approx 2x the time on my machine. 
I think that's an acceptable price to pay for independent tests.
;;;","12/May/09 23:28;jbellis;yeah, agreed about the speed.

I created CASSANDRA-163 for RackUnawareStrategyTest which is also unrelated to the move -- it wasn't getting run by testng b/c it was in the wrong directory before.

committed the basic migration.;;;","13/May/09 17:26;hudson;Integrated in Cassandra #73 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/73/])
    extract tests that could conflict with each other to a separate test class (= gets own jvm)
patch by jbellis; reviewed by Sandeep Tata for 
migrate to JUnit 4.6.  patch by jbellis; reviewed by Sandeep Tata for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra does not start due to: attempted to delete non-existing file HintsColumnFamily-tmp-f-180-Data.db,CASSANDRA-2385,12502383,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,tbritz,tbritz,25/Mar/11 22:27,16/Apr/19 17:33,22/Mar/23 14:57,26/Mar/11 01:19,,,,,0,,,,,,"Had one node restarting again and again (after the kernel killing the process due to all memory being used).

 INFO [main] 2011-03-25 15:23:43,467 AbstractCassandraDaemon.java (line 77) Logging initialized
 INFO [main] 2011-03-25 15:23:43,482 AbstractCassandraDaemon.java (line 95) Heap size: 3328180224/3328180224
 INFO [main] 2011-03-25 15:23:43,484 CLibrary.java (line 61) JNA not found. Native methods will be disabled.
 INFO [main] 2011-03-25 15:23:43,494 DatabaseDescriptor.java (line 120) Loading settings from file:/software/cassandra/conf/cassandra.yaml
 INFO [main] 2011-03-25 15:23:43,910 DatabaseDescriptor.java (line 186) DiskAccessMode is standard, indexAccessMode is mmap
ERROR [main] 2011-03-25 15:23:43,953 AbstractCassandraDaemon.java (line 331) Exception encountered during startup.
java.lang.AssertionError: attempted to delete non-existing file HintsColumnFamily-tmp-f-180-Data.db
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:46)
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:41)
        at org.apache.cassandra.io.sstable.SSTable.delete(SSTable.java:133)
        at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:489)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:124)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:314)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)

		
I deleted the corresponding statics file and cassandra restarted.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20596,,,Fri Mar 25 17:19:31 UTC 2011,,,,,,,,,,"0|i0gb2v:",93225,,,,,Normal,,,,,,,,,,,,,,,,,"26/Mar/11 00:30;jbellis;Looks like CASSANDRA-2185. Are you sure the node in question is running 0.7.4?;;;","26/Mar/11 00:38;tbritz;Now, as you ask, maybe. I'm not 100% sure it's 0.7.4, as we upgraded this morning.
;;;","26/Mar/11 01:19;jbellis;I'll close as duplicate for now then. Please re-open if you see it again.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stress.java indexed range slicing is broken,CASSANDRA-2326,12501404,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,brandon.williams,brandon.williams,15/Mar/11 05:26,16/Apr/19 17:33,22/Mar/23 14:57,14/Apr/11 04:37,0.7.5,0.8 beta 1,,,0,,,,,,"I probably broke it when I fixed the build that CASSANDRA-2312 broke.  Now it compiles, but never works.",,cburroughs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Apr/11 02:23;xedin;CASSANDRA-2326-trunk.patch;https://issues.apache.org/jira/secure/attachment/12476147/CASSANDRA-2326-trunk.patch","13/Apr/11 02:23;xedin;CASSANDRA-2326.patch;https://issues.apache.org/jira/secure/attachment/12476146/CASSANDRA-2326.patch",,,,,,,,,,,,,2.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20563,,,Tue Apr 19 00:04:18 UTC 2011,,,,,,,,,,"0|i0gapz:",93167,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"15/Mar/11 06:11;jbellis;do you get back incomplete data or no data at all?;;;","15/Mar/11 06:15;brandon.williams;{noformat}
contrib/stress/bin/stress -n 300000 -d cassandra-1,cassandra-2,cassandra-3 -i 1 -t 300 -x KEYS -l2 -o INDEXED_RANGE_SLICE
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
Operation [77] retried 10 times - error on calling get_indexed_slices for offset 0 
{noformat}

Not much further with keep-going:

{noformat}
contrib/stress/bin/stress -n 300000 -d cassandra-1,cassandra-2,cassandra-3 -i 1 -t 300 -x KEYS -l2 -o INDEXED_RANGE_SLICE -k
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
Operation [99] retried 1 times - error on calling get_indexed_slices for offset 0 

Index: 0, Size: 0
Operation [178] retried 1 times - error on calling get_indexed_slices for offset 0 

Index: 0, Size: 0
Operation [173] retried 1 times - error on calling get_indexed_slices for offset 0 

Index: 0, Size: 0
Operation [54] retried 1 times - error on calling get_indexed_slices for offset 0 

Index: 0, Size: 0
Operation [190] retried 1 times - error on calling get_indexed_slices for offset 0 

Index: 0, Size: 0
Operation [35] retried 1 times - error on calling get_indexed_slices for offset 0 

Index: 0, Size: 0
Operation [9] retried 1 times - error on calling get_indexed_slices for offset 0 

Index: 0, Size: 0
Operation [104] retried 1 times - error on calling get_indexed_slices for offset 0 

Index: 0, Size: 0
Operation [139] retried 1 times - error on calling get_indexed_slices for offset 0 

Index: 0, Size: 0
Operation [195] retried 1 times - error on calling get_indexed_slices for offset 0 

Index: 0, Size: 0
{noformat};;;","15/Mar/11 06:21;jbellis;looks like stress.java could stand to print out InvalidRequestException.why;;;","28/Mar/11 05:29;xedin;It does print an exception message when it's not null. 

It seems to be imposible to currently implement IndexedRangeSlice operation because Operation.generateValues() now generates strings using randomizer (IndexedRangeSlice implies that we know an exact value we are looking keys for) so each of the runs can potentially give 0 results and we can loop infinitely...;;;","11/Apr/11 18:45;xedin;We can maybe offer users to provide list of the values for indexes range slices, don't have any other solution right now...;;;","12/Apr/11 23:49;jbellis;a --values [list of values] option sounds like a good solution to me.  doesn't have to be specific to index queries, but it's most useful there obviously.;;;","12/Apr/11 23:52;xedin;I agree, I was also thinking about flag which will use old values generator instead of random as I possible solution.;;;","13/Apr/11 00:07;brandon.williams;I like the idea of adding a flag for the old behavior, that Just Worked and didn't require more command line mess.  I'd even be onboard with making it the default and having random as an option, since random hasn't bought us much, if anything, yet.;;;","13/Apr/11 00:09;xedin;I agree with Brandon on this, what do you think Jonathan?;;;","13/Apr/11 00:12;jbellis;sgtm;;;","13/Apr/11 02:23;xedin;-V to generate randomized average size values, old behaviour by default;;;","14/Apr/11 04:37;brandon.williams;Committed;;;","19/Apr/11 08:04;jbellis;(rebased + committed trunk version);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clean up after failed compaction,CASSANDRA-2468,12504212,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,amorton,jbellis,jbellis,14/Apr/11 05:05,16/Apr/19 17:33,22/Mar/23 14:57,23/Jun/11 13:49,1.0.0,,,,1,,,,,,(Started in CASSANDRA-2088.),,skamio,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-2602,,CASSANDRA-2576,CASSANDRA-2629,,,,,,,,,,,,,,"22/Jun/11 10:03;stuhood;0001-CASSANDRA-2468-clean-up-temp-files-after-failed-compac.txt;https://issues.apache.org/jira/secure/attachment/12483390/0001-CASSANDRA-2468-clean-up-temp-files-after-failed-compac.txt","06/Jun/11 09:59;amorton;0001-clean-up-temp-files-after-failed-compaction-v08-2.patch;https://issues.apache.org/jira/secure/attachment/12481527/0001-clean-up-temp-files-after-failed-compaction-v08-2.patch","08/Jun/11 18:11;amorton;0001-clean-up-temp-files-after-failed-compaction-v08-3.patch;https://issues.apache.org/jira/secure/attachment/12481796/0001-clean-up-temp-files-after-failed-compaction-v08-3.patch","06/May/11 10:57;amorton;0001-clean-up-temp-files-after-failed-compaction-v08.patch;https://issues.apache.org/jira/secure/attachment/12478361/0001-clean-up-temp-files-after-failed-compaction-v08.patch","06/May/11 10:57;amorton;0001-cleanup-temp-files-after-failed-compaction-v07.patch;https://issues.apache.org/jira/secure/attachment/12478360/0001-cleanup-temp-files-after-failed-compaction-v07.patch",,,,,,,,,,5.0,amorton,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20642,,,Thu Jun 23 07:09:36 UTC 2011,,,,,,,,,,"0|i0gbjz:",93302,,stuhood,,stuhood,Low,,,,,,,,,,,,,,,,,"29/Apr/11 22:01;slebresne;@Aaron: are you still working on this ? It would be nice to fix this quickly. And we should probably target 0.7 for that one.;;;","30/Apr/11 05:51;amorton;should have time this weekend. ;;;","01/May/11 19:16;amorton;Attached patches for 0.7 and 0.8 rely on CASSANDRA-2588 to correctly detect sstables on disk. 

Added closeAndDeleteQuietly() to SSTableWriter and SSTableWriter.Builder to close open files and delete the SSTable files found on disk.

Checks for failures in CompactionManager doCompaction(), doScrub() and doCleanupCompaction() and submitSSTableBuild() 

Does not check in CompactionManager.submitIndexBuild() because the builder works against memtables and does not use an SSTableWriter. 

Checks for failures in Memtable.writeSortedContents();;;","03/May/11 17:59;slebresne;Comment:
    * I'm uncomfortable with having closeAndDeleteQuietly() delete the non tmp files (those really do not belong to the writer). Sure the calls are always conditioned so that we shouldn't messed up, but I'm not sure it's worth the risk of foot-shooting (it makes the function harder to use). I'd rather remove that part, rename closeAndDeleteQuietly() to something like cleanupIfNecessary() and call it every time in the finally block (which would remove the need to check if the reader was successfully created every damn time).
    * Nitpick: I'd prefer moving the code for SSTableWriter.Builder inside build() itself.
;;;","03/May/11 19:50;amorton;The check to delete the non temp files was to cover the unlikely event that rename() threw an IOError part way through renaming the files. I thought about potentially deleting the wrong files but nothing else see the non temp files until the SSTR is returned. 

Happy to make the change tomorrow if you still want, it makes things a little safer. ;;;","03/May/11 20:05;slebresne;bq. Happy to make the change tomorrow if you still want, it makes things a little safer.

Thanks, I prefer safer :) (I'm not too worried about rename failing);;;","05/May/11 03:42;amorton;Updated patches with suggested changes:
- only deletes temp files
- cleanupIfNecessary() used
- SSTW.Builder.build() cleans up self

NOTE: requires CASSANDRA-2602 to detect temp files correctly.;;;","05/May/11 21:59;slebresne;Looks good but this needs rebasing (at least for 0.7, don't bother too much with 0.8, I'll try to merge it unless this is a pain);;;","06/May/11 10:57;amorton;rebased both the v07 and v08;;;","03/Jun/11 03:18;stuhood;What's shakin' here?;;;","06/Jun/11 09:59;amorton;rebased the v08 version today, attached as 0001-clean-up-temp-files-after-failed-compaction-v08-2

Have not updated v07, let me know if you need it.  ;;;","07/Jun/11 02:52;stuhood;* SSTable.delete will throw an IOError on IOException, which might kill cleanupIfNecessary... should we consider having delete throw IOException? I'd prefer not to catch IOError.
* {{SSTable.tempComponentsFor}} could probably be merged with componentsFor and an enum {{LIVE}}, {{TEMP}} or {{LIVE | TEMP}}? Not a blocker, just sugar.

After the IOError is fixed, +1 from me. Thanks Aaron!;;;","07/Jun/11 03:00;jbellis;bq. SSTable.tempComponentsFor could probably be merged with componentsFor

I prefer the distinct, flag-less methods.;;;","07/Jun/11 03:43;stuhood;bq. I prefer the distinct, flag-less methods.
componentsFor already has a boolean flag... my suggestion was to make it a ternary flag. Again, no big deal.;;;","07/Jun/11 04:34;jbellis;Eh, you're right.  Then flags is probably better than flags AND multiple methods. :);;;","08/Jun/11 18:11;amorton;version 3 for v0.8 modified SSTable.delete() to raise an IOException so cleanupIfNecessary() can catch it. Also changes componentsFor to accept an enum. 

Do we want this in 0.7?;;;","11/Jun/11 02:20;stuhood;+1 on the patch for 0.8/trunk
Thanks Aaron!;;;","12/Jun/11 14:29;stuhood;Rebased for post-1610-trunk as v4.;;;","22/Jun/11 10:03;stuhood;Rebased for trunk (assuming r1138084 is reverted).;;;","23/Jun/11 13:49;jbellis;committed, thanks all!;;;","23/Jun/11 15:09;hudson;Integrated in Cassandra #936 (See [https://builds.apache.org/job/Cassandra/936/])
    clean up tmpfiles after failed compaction
patch by Aaron Morton; reviewed by slebresne and Stu Hood for CASSANDRA-2468

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1138740
Files : 
* /cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTable.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/CompactionTask.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/Memtable.java
* /cassandra/trunk/src/java/org/apache/cassandra/io/sstable/Descriptor.java
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/io/util/FileUtils.java
* /cassandra/trunk/test/unit/org/apache/cassandra/io/sstable/SSTableTest.java
* /cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableReader.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/CompactionManager.java
* /cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableDeletingReference.java
* /cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableWriter.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
batch_mutate operations with CL=LOCAL_QUORUM throw TimeOutException when there aren't sufficient live nodes,CASSANDRA-2514,12504738,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,nar3ndra,nar3ndra,nar3ndra,20/Apr/11 08:29,16/Apr/19 17:33,22/Mar/23 14:57,21/Apr/11 02:16,0.7.5,,,,0,,,,,,"We have a 2 DC setup with RF = 4. There are 2 nodes in each DC. Following is the keyspace definition:
<snip>
keyspaces:
    - name: KeyspaceMetadata
      replica_placement_strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
      strategy_options:
        DC1 : 2
        DC2 : 2
      replication_factor: 4
</snip>

I shutdown all except one node and waited for the live node to recognize that other nodes are dead. Following is the nodetool ring output on the live node:
Address         Status State   Load            Owns    Token                                       
                                                       169579575332184635438912517119426957796     
10.17.221.19    Down   Normal  ?               29.20%  49117425183422571410176530597442406739      
10.17.221.17    Up     Normal  81.64 KB        4.41%   56615248844645582918169246064691229930      
10.16.80.54     Down   Normal  ?               21.13%  92563519227261352488017033924602789201      
10.17.221.18    Down   Normal  ?               45.27%  169579575332184635438912517119426957796     

I expect UnavailableException when I send batch_mutate request to node that is up. However, it returned TimeOutException:
TimedOutException()
    at org.apache.cassandra.thrift.Cassandra$batch_mutate_result.read(Cassandra.java:16493)
    at org.apache.cassandra.thrift.Cassandra$Client.recv_batch_mutate(Cassandra.java:916)
    at org.apache.cassandra.thrift.Cassandra$Client.batch_mutate(Cassandra.java:890)

Following is the cassandra-topology.properties
# Cassandra Node IP=Data Center:Rack
10.17.221.17=DC1:RAC1
10.17.221.19=DC1:RAC2

10.17.221.18=DC2:RAC1
10.16.80.54=DC2:RAC2
","1. Cassandra 0.7.4 running on RHEL 5.5
2. 2 DC setup
3. RF = 4 (DC1 = 2, DC2 = 2)
4. CL = LOCAL_QUORUM",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Apr/11 00:32;jbellis;2514-v2.txt;https://issues.apache.org/jira/secure/attachment/12476909/2514-v2.txt","20/Apr/11 08:41;nar3ndra;CASSANDRA-2514.patch;https://issues.apache.org/jira/secure/attachment/12476809/CASSANDRA-2514.patch",,,,,,,,,,,,,2.0,nar3ndra,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20667,,,Wed Apr 20 19:46:08 UTC 2011,,,,,,,,,,"0|i0gbtb:",93344,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"20/Apr/11 08:37;nar3ndra;I think the issue is because DatacenterWriteResponseHandler.assureSufficientLiveNodes is not checking for live nodes.

DatacenterWriteResponseHandler.assureSufficientLiveNodes works on writeEndpoints. writeEndpoints contains list of the all the endpoints (may be more if there are nodes bootstrapping).

I think either writeEndpoints should ignore dead/unreachable nodes or DatacenterWriteResponseHandler.assureSufficientLiveNodes should use hintedEndpoints.keySet() as that contains the live endpoints.
I compared the implementation with WriteResponseHandler.assureSufficientLiveNodes and found that it uses hintedEndpoints.


I am attaching the patch that works for me.;;;","20/Apr/11 08:41;nar3ndra;Use hintedEndpoints instead of writeEndpoints to work on live endpoints only.;;;","20/Apr/11 08:52;nar3ndra;The code to reproduce this issue is a simple batch mutate operation. The operation I performed involved adding 2 columns to a SuperColumn. Let me know if it is not reproducible. I will provide the sample code.;;;","21/Apr/11 00:32;jbellis;Good catch, that is a bug.

v2 adds a couple improvements:

- only count the hinted endpoint towards live count if it's a normal write destination (hints can be sent elsewhere if all the write destinations are dead)
- similar fix for DSWRH (EACH_QUORUM)
- unrelated fix in WRH for CL.ANY not to continue through to the CL.Q/ALL code;;;","21/Apr/11 00:32;jbellis;how does that look to you?;;;","21/Apr/11 01:22;nar3ndra;Looks good to me. 

Just one comment/question:
hintedEndpoints is subset of writeEndpoints. So is the additional check writeEndpoints.contains(destination), while we are iterating over hintedEndpoints, needed? I think assert would be better here.

;;;","21/Apr/11 01:32;jbellis;That's the point, hintedEndpoints is *usually* but not always a subset of writeEndpoints. Here is the code from getHintedEndpoints:

{code}
        // assign dead endpoints to be hinted to the closest live one, or to the local node
        // (since it is trivially the closest) if none are alive.  This way, the cost of doing
        // a hint is only adding the hint header, rather than doing a full extra write, if any
        // destination nodes are alive.
        //
        // we do a 2nd pass on targets instead of using temporary storage,
        // to optimize for the common case (everything was alive).
        InetAddress localAddress = FBUtilities.getLocalAddress();
        for (InetAddress ep : targets)
        {
            if (map.containsKey(ep))
                continue;
            if (!StorageProxy.shouldHint(ep))
            {
                if (logger.isDebugEnabled())
                    logger.debug(""not hinting "" + ep + "" which has been down "" + Gossiper.instance.getEndpointDowntime(ep) + ""ms"");
                continue;
            }

            InetAddress destination = map.isEmpty()
                                    ? localAddress
                                    : snitch.getSortedListByProximity(localAddress, map.keySet()).get(0);
            map.put(destination, ep);
        }
{code};;;","21/Apr/11 01:33;jbellis;that is: our last-resort local hint storage may not be part of writeEndpoints (probably won't be, on a large cluster).;;;","21/Apr/11 01:53;nar3ndra;Got it. In my setup I had HH disabled. So I overlooked the rest of the getHintedEndpoints.

The change looks good to me now. Thanks!

;;;","21/Apr/11 02:16;jbellis;committed, thanks!;;;","21/Apr/11 03:46;hudson;Integrated in Cassandra-0.7 #451 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/451/])
    fixes for verifying destinationavailability under hinted conditions
patch by Narendra Sharma and jbellis for CASSANDRA-2514
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cluster restart re-adds removed tokens,CASSANDRA-1609,12477154,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,nickmbailey,nickmbailey,13/Oct/10 00:49,16/Apr/19 17:33,22/Mar/23 14:57,13/Oct/10 23:57,0.7 beta 3,,,,0,,,,,,"After a cluster restart one of our nodes began reporting tokens that had been removed a good while ago (week or more) in it's nodetool ring output.  This probably has something to do with our change to persist the ring in CASSANDRA-1518 and removetoken changes in CASSANDRA-1216. The node didn't actually gossip the removed tokens so they showed up in TMD but not gossip.

Additionally all nodes began reporting a node that had been removed maybe an hour ago.  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Oct/10 05:54;jbellis;1609.txt;https://issues.apache.org/jira/secure/attachment/12457019/1609.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20215,,,Thu Oct 14 12:49:47 UTC 2010,,,,,,,,,,"0|i0g6a7:",92448,,nickmbailey,,nickmbailey,Normal,,,,,,,,,,,,,,,,,"13/Oct/10 01:24;nickmbailey;So the first 4 tokens were removed using decomission and the last one using removetoken. It doesn't look like either of those processes remove nodes from the saved state. The nodes then show up in nodetool ring but not gossip since they don't actually have an application state to begin with.;;;","13/Oct/10 03:09;nickmbailey;So it looks like we don't remove tokens from the system table when we decomission which leads to the following scenario:

# Node A has token 1
# Node A loadbalance to token 2
# Node A dies
# Node A removed
# Cluster restart
# Node A reappears with token 1 since that was never removed/overwritten in the system table.

At least i think thats how its happening. Not sure about one of our nodes seeing the four decomissioned tokens and the others not.;;;","13/Oct/10 05:54;jbellis;ring state management is a mess.  token removal happens in 3 places:

 1) node receives decommission notice (STATE_LEFT)
 2) node receives removetoken notice, piggy-backed on STATE_NORMAL
 3) node coordinates removetoken (gossiper will not trigger notifications for state changes that initiated locally, so this needs to be handled separately from 2)

2) was updating the SystemTable w/ the removal but the others were not.

patch attached to move this logic into excise() method and call from all 3 places.;;;","13/Oct/10 11:38;nickmbailey; * handleStateLeft wasn't actually removing the endpoint from gossip before. Was that an oversight or intended?
 * you removed a check from handleStateLeft to make sure the token was a member before removing it.  Intentional?
;;;","13/Oct/10 23:14;jbellis;bq. handleStateLeft wasn't actually removing the endpoint from gossip before

right, that was a bug.

bq. you removed a check from handleStateLeft to make sure the token was a member

yes, we skip the check everywhere else because either way the result is the same.  there's no meaningful action we can take if the check fails so it's redundant.;;;","13/Oct/10 23:53;nickmbailey;+1;;;","13/Oct/10 23:57;jbellis;committed;;;","14/Oct/10 20:49;hudson;Integrated in Cassandra #565 (See [https://hudson.apache.org/hudson/job/Cassandra/565/])
    fix removing tokens from SystemTable on decommission and removetoken.
patch by jbellis; reviewed by Nick Bailey for CASSANDRA-1609
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Load balancing does not account for the load of the moving node,CASSANDRA-762,12455243,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,stuhood,stuhood,stuhood,04/Feb/10 07:20,16/Apr/19 17:33,22/Mar/23 14:57,10/Feb/10 10:15,0.5,,,,1,,,,,,"Given a node A (with load 10 gb) and a node B (with load 20 gb), running the loadbalance command against node A will:
1. Remove node A from the ring
  * Recalculates pending ranges so that node B is responsible for the entire ring
2. Pick the most loaded node
  * node B is still reporting 20 gb load, because that is all it has locally
3. Choose a token that divides the range of the most loaded node in half

Since the token calculation doesn't take into account the load that node B is 'inheriting' from node A, the token will divide node B's load in half and swap the loads. Instead, the token calculation needs to pretend that B has already inherited the 10 gb from node A, for a total of 30 gb. The token that should be chosen falls at 15 gb of the total load, or 5 gb into node B's load.",,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Feb/10 08:09;stuhood;0001-Wait-BROADCAST_INTERVAL-for-load-information-and-cal.patch;https://issues.apache.org/jira/secure/attachment/12435142/0001-Wait-BROADCAST_INTERVAL-for-load-information-and-cal.patch","08/Feb/10 08:58;stuhood;for-0.5-0001-Wait-BROADCAST_INTERVAL-for-load-information-and-cal.patch;https://issues.apache.org/jira/secure/attachment/12435144/for-0.5-0001-Wait-BROADCAST_INTERVAL-for-load-information-and-cal.patch",,,,,,,,,,,,,2.0,stuhood,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,18550,,,Wed Feb 10 02:15:24 UTC 2010,,,,,,,,,,"0|i0g0x3:",91579,,,,,Low,,,,,,,,,,,,,,,,,"05/Feb/10 03:26;jbellis;As I said in IRC, a loadbalancing node doesn't compute its destination token until after all its data has been transferred off.  (We *should* add a sleep until load is rebroadcast, but that will only affect which node gets picked to pull half the range from.)  So if there is a bug here it is in the code, not the algorithm.

Can you reproduce this?;;;","08/Feb/10 08:09;stuhood;This patch waits for load information after leaving the ring, and lowers the BROADCAST_INTERVAL to .9 of RING_DELAY.;;;","08/Feb/10 08:58;stuhood;Here is a rebased copy of the patch for 0.5. Because 0.5 is still stat'ing files, it doesn't lower the BROADCAST_INTERVAL, although we may want to.;;;","09/Feb/10 00:12;jbellis;did you test on 0.5?;;;","09/Feb/10 00:49;stuhood;Yes, I tested on trunk and 0.5;;;","09/Feb/10 02:02;jbellis;looking at 0.5 patch:

patch does not build.  (looks like the Gossiper diff made it in by mistake.)

the load wait should probably be for BROADCAST_INTERVAL + RING_DELAY.
;;;","09/Feb/10 14:04;stuhood;Hmm... I just verified that 'for-0.5-0001...' applies cleanly to the cassandra-0.5 branch at git revision 4331781362a16e8ea444fdf478c8c63882af7bb3. There isn't anything related to gossip in that patch.

Also, the patch for trunk applies (with some offsets), and also doesn't contain anything related to gossip.;;;","10/Feb/10 07:37;jbellis;committed to 0.5 w/ my suggested change to sleep duration.

working on committing to trunk but asf svn seems to be dead in the water atm.;;;","10/Feb/10 10:15;jbellis;got the merge to trunk done.  chickened out a bit and set the broadcast period to 60s to be conservative.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compaction regression,CASSANDRA-80,12422729,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jbellis,jbellis,jbellis,14/Apr/09 04:58,16/Apr/19 17:33,22/Mar/23 14:57,14/Apr/09 13:13,,,,,0,,,,,,The partitioner refactor caused a regression in compaction with RandomPartitioner.  This adds a test to expose the bug and a patch to fix it.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Apr/09 06:00;jbellis;0001-add-test-showing-compaction-regression.patch;https://issues.apache.org/jira/secure/attachment/12405358/0001-add-test-showing-compaction-regression.patch","14/Apr/09 06:00;jbellis;0002-fix-regression.patch;https://issues.apache.org/jira/secure/attachment/12405359/0002-fix-regression.patch",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19537,,,Tue Apr 14 05:13:14 UTC 2009,,,,,,,,,,"0|i0fwr3:",90904,,,,,Critical,,,,,,,,,,,,,,,,,"14/Apr/09 08:42;tlipcon;One comment:

Do you actually need random values in testCompaction? I prefer that tests be deterministic wherever possible (even if that means using a constant random seed);;;","14/Apr/09 10:35;junrao;The patch looks fine to me. The testcase only tests that keys are inserted to SSTables in ascending order. The values associated with the keys are not used. ;;;","14/Apr/09 13:13;jbellis;committed w/ unnecessary random data removed (use byte[0] instead).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Column MetaData: Index_name should not be allowed if index_type is not set.,CASSANDRA-1759,12480444,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jhermes,jhermes,jhermes,20/Nov/10 04:24,16/Apr/19 17:33,22/Mar/23 14:57,24/Nov/10 00:57,0.7.0 rc 2,,,,0,,,,,,"Giving an indexName starts the automatic index creation process.
If indexType is not also set, then that process barfs.
If a name is present, a type must be also (but the reverse is not necessarily true).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Nov/10 06:39;jhermes;1759-2.txt;https://issues.apache.org/jira/secure/attachment/12460067/1759-2.txt","20/Nov/10 05:37;jhermes;1759.txt;https://issues.apache.org/jira/secure/attachment/12460056/1759.txt",,,,,,,,,,,,,2.0,jhermes,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20296,,,Sat Dec 11 07:35:14 UTC 2010,,,,,,,,,,"0|i0g77r:",92599,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"20/Nov/10 05:37;jhermes;Similar to the patch for CASSANDRA-1527, this validates the creation of ColumnDefs from YAML and then from thrift/avro.

For simplicity, both name and index need to be set or unset at the same time. I don't think anyone is complaining that it's too hard to introduce non-obvious bugs, so I'm being explicit here.;;;","20/Nov/10 05:57;jbellis;Looks like you got a little too clever with the xors -- index_type set but index_name not, is valid.;;;","20/Nov/10 06:39;jhermes;Whoops, I read CFMD:279 wrong. I thought it was a bug that we were auto-generating the name incorrectly. Back to previous logic.;;;","23/Nov/10 00:08;jhermes;CASSANDRA-1761 is the bug that I originally caught and is separate from this one.;;;","24/Nov/10 00:57;jbellis;committed;;;","11/Dec/10 15:35;hudson;Integrated in Cassandra-0.7 #70 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/70/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
antrl generated files should not be included in the source release,CASSANDRA-316,12431459,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,urandom,urandom,urandom,26/Jul/09 23:33,16/Apr/19 17:33,22/Mar/23 14:57,10/Nov/09 05:19,0.4,0.5,Legacy/Tools,,0,,,,,,Files located under src/gen-java should not be included in the source distribution since they are removed on clean and generated on build.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Nov/09 05:01;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-316-don-t-ship-antrl-generated-files-in-sour.txt;https://issues.apache.org/jira/secure/attachment/12424396/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-316-don-t-ship-antrl-generated-files-in-sour.txt",,,,,,,,,,,,,,1.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19632,,,Mon Nov 09 21:19:12 UTC 2009,,,,,,,,,,"0|i0fy6v:",91137,,,,,Normal,,,,,,,,,,,,,,,,,"10/Nov/09 02:53;jbellis;is this still a problem?;;;","10/Nov/09 03:49;urandom;Yes. :(;;;","10/Nov/09 03:53;jbellis;would it be easier to just use svn export to get a clean tree for the source release?;;;","10/Nov/09 04:53;urandom;If I understand what you're asking, then probably not in the long run (because you'd be adding an extra (manual )step), as opposed to having it all rolled up into ""ant release"".;;;","10/Nov/09 05:03;urandom;For some reason I remembered it being more complicated than this.;;;","10/Nov/09 05:03;jbellis;http://subclipse.tigris.org/svnant.html maybe? :);;;","10/Nov/09 05:12;jbellis;+1 for patch;;;","10/Nov/09 05:19;urandom;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
incorrect package name for property_snitch class files,CASSANDRA-1259,12468770,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,urandom,urandom,urandom,08/Jul/10 06:44,16/Apr/19 17:33,22/Mar/23 14:57,09/Jul/10 12:43,0.6.4,,,,0,,,,,,"The classes in contrib/property_snitch both have ""src.java"" prepended to them (probably a bug introduced by an IDE).

The attached trivial patch fixes this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jul/10 06:45;urandom;0001-remove-erroneous-src.java-from-package-name.patch;https://issues.apache.org/jira/secure/attachment/12448931/0001-remove-erroneous-src.java-from-package-name.patch",,,,,,,,,,,,,,1.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20052,,,Fri Jul 09 04:43:09 UTC 2010,,,,,,,,,,"0|i0g3yv:",92073,,,,,Low,,,,,,,,,,,,,,,,,"08/Jul/10 06:59;jbellis;+1;;;","09/Jul/10 12:43;urandom;committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-cli 'use Keyspace user pass' breaks with SimpleAuth,CASSANDRA-2111,12497714,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,thobbs,thobbs,05/Feb/11 04:41,16/Apr/19 17:33,22/Mar/23 14:57,10/Feb/11 10:43,0.7.1,,Legacy/Tools,,0,,,,,,"If SimpleAuth is used and the -Daccess.properties... JVM options are passed in, the CLI's ""use Keyspace user 'password'"" command breaks.  However, if the --username and --password options are used, you can still authenticate.",,,,,,,,,,,,,,,,,,,,,,";06/Feb/11 00:03;xedin;7200",7200,0,7200,100%,7200,0,7200,,,,,,,,,,,,,,,,,,,"10/Feb/11 08:34;xedin;CASSANDRA-2111-v2.patch;https://issues.apache.org/jira/secure/attachment/12470744/CASSANDRA-2111-v2.patch","06/Feb/11 00:03;xedin;CASSANDRA-2111.patch;https://issues.apache.org/jira/secure/attachment/12470370/CASSANDRA-2111.patch",,,,,,,,,,,,,2.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20448,,,Thu Feb 10 04:34:53 UTC 2011,,,,,,,,,,"0|i0g9dr:",92950,,thobbs,,thobbs,Low,,,,,,,,,,,,,,,,,"05/Feb/11 06:32;xedin;Looks like that right now without a valid user/password given by --username/--password you won't be able to connect the cassandra instance, that means that if you will try to use `use Keyspace <username> '<password>';` it will tell you that `Keyspace is not found` which is a wrong error message in this case. 

I will make following changes: 

a). add support for setting username/password at `connect host/port;` command; 
b). change error to show that you are not connected to node instead of '<keyspace> not found' for `use <keyspace>` (other commands show the right error);;;;","06/Feb/11 00:03;xedin;`connect` command enhancement:
`connect host/port <username> '<password>';`

and changes from previous comment.

;;;","10/Feb/11 08:22;thobbs;The normal 'help' output does not show that you can specify a user/pass combination with connect.  The 'help connect' output does, though.

Other than that, it looks good.;;;","10/Feb/11 08:34;xedin;v2 fixes missing note about user/pass for connect command when just `help` is used without `connect` attribute. This patch could be applied to both trunk and cassandra-0.7 branches.;;;","10/Feb/11 08:41;thobbs;+1;;;","10/Feb/11 10:43;jbellis;committed;;;","10/Feb/11 12:34;hudson;Integrated in Cassandra-0.7 #273 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/273/])
    add cli support for setting username/password at 'connect' command
patch by Pavel Yaskevich; reviewed by thobbs for CASSANDRA-2111
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ivy.jar is included in the binary distribution,CASSANDRA-2046,12496610,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,urandom,stephenc,stephenc,25/Jan/11 06:15,16/Apr/19 17:33,22/Mar/23 14:57,25/Jan/11 06:21,0.7.1,,,,0,,,,,,"The build currently copys ivy.xml into the bin.tar.gz

according to Eric, this is a bug


-rw-r--r-- 0/0          910990 2011-01-06 16:46 apache-cassandra-0.7.0/lib/ivy-2.1.0.jar
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20412,,,Mon Jan 24 22:34:10 UTC 2011,,,,,,,,,,"0|i0g8zr:",92887,,,,,Normal,,,,,,,,,,,,,,,,,"25/Jan/11 06:21;urandom;committed.;;;","25/Jan/11 06:34;hudson;Integrated in Cassandra-0.7 #202 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/202/])
    do not install ivy jar to artifacts

Patch by eevans for CASSANDRA-2046
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTables per read metric is incorrect,CASSANDRA-2422,12503468,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,stuhood,stuhood,stuhood,06/Apr/11 06:25,16/Apr/19 17:33,22/Mar/23 14:57,06/Apr/11 06:47,0.8 beta 1,,,,0,,,,,,"The sstables per read metric is currently recording the count of live sstables, rather than the number that are being read.",,cburroughs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Apr/11 06:43;stuhood;0001-Ugh.txt;https://issues.apache.org/jira/secure/attachment/12475538/0001-Ugh.txt",,,,,,,,,,,,,,1.0,stuhood,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20616,,,Wed Apr 06 00:03:16 UTC 2011,,,,,,,,,,"0|i0gba7:",93258,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"06/Apr/11 06:33;jbellis;Unless this is a trunk-only regression, let's fix for 0.7.5;;;","06/Apr/11 06:43;stuhood;Patch for trunk: not a problem in the 0.7 branch.;;;","06/Apr/11 06:47;jbellis;committed, thanks!;;;","06/Apr/11 08:03;hudson;Integrated in Cassandra #830 (See [https://hudson.apache.org/hudson/job/Cassandra/830/])
    fix sstable read count regression
patch by Stu Hood; reviewed by jbellis for CASSANDRA-2422
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
removetoken is broken,CASSANDRA-1650,12478109,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,nickmbailey,brandon.williams,brandon.williams,23/Oct/10 03:44,16/Apr/19 17:33,22/Mar/23 14:57,23/Oct/10 05:14,0.7 beta 3,,,,0,,,,,,"When running removetoken on a dead node, it hangs forever.  The debug log shows:

DEBUG 19:45:53,794 Node /10.179.111.137 ranges [(115049868157599339472315320703867977321,62676456546693435176060154681903071729]]
DEBUG 19:45:53,795 Range (115049868157599339472315320703867977321,62676456546693435176060154681903071729] will be responsibility of cassandra-1/10.179.65.102
DEBUG 19:45:53,798 Pending ranges:
cassandra-1/10.179.65.102:(115049868157599339472315320703867977321,62676456546693435176060154681903071729]

DEBUG 19:45:53,798 Node /10.179.111.137 ranges [(115049868157599339472315320703867977321,62676456546693435176060154681903071729]]
DEBUG 19:45:53,799 Range (115049868157599339472315320703867977321,62676456546693435176060154681903071729] will be responsibility of cassandra-1/10.179.65.102
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Oct/10 05:05;nickmbailey;0001-Don-t-wait-for-confirmation-when-replication-factor-.patch;https://issues.apache.org/jira/secure/attachment/12457872/0001-Don-t-wait-for-confirmation-when-replication-factor-.patch",,,,,,,,,,,,,,1.0,nickmbailey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20240,,,Sat Oct 23 12:49:04 UTC 2010,,,,,,,,,,"0|i0g6jb:",92489,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"23/Oct/10 04:03;nickmbailey;So removetoken blocks for all new replicas to confirm they have the data for the node being removed.  You can actually run ""removetoken status"" to see which nodes are being waited on.  Since a dropped confirm message or stream request could cause it to block forever you can run ""removetoken force"" to finish a removal already in progress.  

There could be a bug here as well or possibly a dropped message?

There should probably be new documentation about this in the wiki.;;;","23/Oct/10 04:06;brandon.williams;My RF is 1, so there are no replicas to block for.  Here's the status output:

RemovalStatus: Removing token (62676456546693435176060154681903071729). Waiting for replication confirmation from [cassandra-1/10.179.65.102].

It seems to be waiting for confirmation from itself.;;;","23/Oct/10 04:07;brandon.williams;Also, force does not work:

Exception in thread ""main"" java.lang.UnsupportedOperationException: Node /10.179.111.137 is already being removed.
;;;","23/Oct/10 05:05;nickmbailey;When replication factor is 1 it is impossible to recover data so we shouldn't wait for confirmation.;;;","23/Oct/10 05:14;brandon.williams;+1, committed.;;;","23/Oct/10 20:49;hudson;Integrated in Cassandra #574 (See [https://hudson.apache.org/hudson/job/Cassandra/574/])
    Don't wait for confirmation when removing token and RF=1.  Patch by Nick Bailey, reviewed by brandonwilliams for CASSANDRA-1650
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra should chdir / when daemonizing,CASSANDRA-1718,12479400,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,urandom,thepaul,thepaul,09/Nov/10 03:40,16/Apr/19 17:33,22/Mar/23 14:57,21/Jan/11 00:11,0.7.1,,Packaging,,0,,,,,,"Common practice when daemonizing is to cd / to avoid pinning a filesystem.  For example, if the oper happens to start Cassandra (by itself, or with a manual jsvc invocation, or with the initscript) in /mnt/usb-storage, and there is something mounted there, then the oper will not be able to unmount the usb device that was mounted at that location, since the cassandra process has it open as its cwd.

evidence that this isn't being done already:

{noformat}
~% sudo lsof -p 9775 | awk '$4==""cwd""'
jsvc    9775 cassandra  cwd    DIR                8,1     4096 147675 /home/paul/packages/cassandra/trunk
{noformat}

(That instance was invoked using the Debian initscript.)

Obviously chdir(""/"") isn't necessary when not daemonizing, although it shouldn't hurt either.

If there are concerns about Cassandra having an ongoing ability to open filenames relative to its original working directory, then it should be sufficient just to do a ""cd /"" in the initscript before starting Cassandra.  That case, at least, is particularly important.","Debian squeeze, Cassandra 0.7.0-beta3 and trunk (r1032649)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Jan/11 00:32;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-1718-switch-to-home-directory-on-startup.txt;https://issues.apache.org/jira/secure/attachment/12468136/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-1718-switch-to-home-directory-on-startup.txt","15/Jan/11 04:21;urandom;ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-1718-chdir-on-startup.txt;https://issues.apache.org/jira/secure/attachment/12468399/ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-1718-chdir-on-startup.txt",,,,,,,,,,,,,2.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20270,,,Thu Jan 20 16:44:06 UTC 2011,,,,,,,,,,"0|i0g6yn:",92558,,,,,Low,,,,,,,,,,,,,,,,,"09/Nov/10 03:47;jbellis;tagging fix-for 0.7.1 because I don't want to risk any more breakage for 0.7.0;;;","08/Dec/10 01:49;urandom;Huh. I thought jsvc was doing this (and I want to say that it _was_ at some point). It really seems like it ought to be.

bq. If there are concerns about Cassandra having an ongoing ability to open filenames relative to its original working directory, then it should be sufficient just to do a ""cd /"" in the initscript before starting Cassandra. That case, at least, is particularly important.

We should consider it a bug if anything here relies on relative paths (and I don't think it does).;;;","12/Jan/11 04:00;jbellis;Note: this means hprof and err files will be created in /, since they are dumped in CWD.;;;","12/Jan/11 23:58;thepaul;Maybe best would be to chdir() to cassandra's data directory, then. It should be ok to pin that. But definitely we want it to be predictable, not ""whatever directory the admin was in the last time she started it up"".;;;","13/Jan/11 00:27;jbellis;bq. Maybe best would be to chdir() to cassandra's data directory

I like that idea.;;;","13/Jan/11 00:34;urandom;Is there some way to tell the JVM to put its  detritus elsewhere?  Barring that, /var/lib/cassandra is an improvement over, ""wherever"".;;;","13/Jan/11 00:42;thepaul;+1 this patch;;;","13/Jan/11 00:43;jbellis;bq. Is there some way to tell the JVM to put its detritus elsewhere?

http://www.oracle.com/technetwork/java/javase/tech/vmoptions-jsp-140102.html lists -XX:HeapDumpPath but nothing for a JVM crash log that I see.;;;","14/Jan/11 10:52;thepaul;-XX:ErrorFile ?;;;","14/Jan/11 11:10;jbellis;Bingo.;;;","21/Jan/11 00:11;urandom;Did.;;;","21/Jan/11 00:44;hudson;Integrated in Cassandra-0.7 #182 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/182/])
    chdir / on startup

Patch by eevans for CASSANDRA-1718
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EOFException in LazilyCompactedRow,CASSANDRA-1299,12469630,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jbellis,stuhood,stuhood,20/Jul/10 00:18,16/Apr/19 17:33,22/Mar/23 14:57,20/Jul/10 01:34,0.7 beta 1,,,,0,,,,,,"Post CASSANDRA-270, 'ant clean long-test' fails with an EOFException in LazilyCompactedRow.

{code}java.io.IOError: java.io.EOFException
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.next(SSTableIdentityIterator.java:103)
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.next(SSTableIdentityIterator.java:32)
	at org.apache.commons.collections.iterators.CollatingIterator.set(CollatingIterator.java:284)
	at org.apache.commons.collections.iterators.CollatingIterator.least(CollatingIterator.java:326)
	at org.apache.commons.collections.iterators.CollatingIterator.next(CollatingIterator.java:230)
	at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:68)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
	at com.google.common.collect.Iterators$7.computeNext(Iterators.java:604)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
	at org.apache.cassandra.db.ColumnIndexer.serializeInternal(ColumnIndexer.java:76)
	at org.apache.cassandra.db.ColumnIndexer.serialize(ColumnIndexer.java:50)
	at org.apache.cassandra.io.LazilyCompactedRow.<init>(LazilyCompactedRow.java:62)
	at org.apache.cassandra.io.CompactionIterator.getCompactedRow(CompactionIterator.java:135)
	at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:107)
	at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:46)
	at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:73)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
	at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
	at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
	at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:334)
	at org.apache.cassandra.db.LongCompactionSpeedTest.testCompaction(LongCompactionSpeedTest.java:101)
	at org.apache.cassandra.db.LongCompactionSpeedTest.testCompactionWide(LongCompactionSpeedTest.java:49)
Caused by: java.io.EOFException
	at java.io.RandomAccessFile.readInt(RandomAccessFile.java:725)
	at java.io.RandomAccessFile.readLong(RandomAccessFile.java:758)
	at org.apache.cassandra.db.TimestampClockSerializer.deserialize(TimestampClock.java:128)
	at org.apache.cassandra.db.TimestampClockSerializer.deserialize(TimestampClock.java:119)
	at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:90)
	at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:31)
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.next(SSTableIdentityIterator.java:99)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jul/10 01:25;jbellis;1299.txt;https://issues.apache.org/jira/secure/attachment/12449859/1299.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20068,,,Wed Jul 21 12:50:26 UTC 2010,,,,,,,,,,"0|i0g47r:",92113,,,,,Critical,,,,,,,,,,,,,,,,,"20/Jul/10 01:25;jbellis;CASSANDRA-270 exposed a long-standing bug in SSTableUtils where it was writing garbage data while bypassing memtable to assemble its own SSTables by hand.  It needs to limit the data passed to SSTableWriter to the actual valid data in the DataOutputBuffer.  Attached is a patch that takes the simplest approach to this by copying out only the valid data into a separate byte[].  Ideally I'd prefer to remove the append(key, byte[]) method entirely but that will have to wait until we have a BMT replacement.;;;","20/Jul/10 01:28;stuhood;Ha, yea... that would do it. Thanks!

+1;;;","20/Jul/10 01:34;jbellis;committed;;;","21/Jul/10 20:50;hudson;Integrated in Cassandra #496 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/496/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTable gets removed if index file is missing,CASSANDRA-343,12432217,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,lenn0x,lenn0x,lenn0x,05/Aug/09 07:31,16/Apr/19 17:33,22/Mar/23 14:57,02/Sep/09 07:38,0.4,,,,0,,,,,,"If an SSTable is found but is missing an index, the data file is marked corrupt and deleted. We should fix this, and have index re-generation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Sep/09 06:38;lenn0x;0001-CASSANDRA-343.-Do-not-delete-an-SSTable-if-it-s-corr.patch;https://issues.apache.org/jira/secure/attachment/12418310/0001-CASSANDRA-343.-Do-not-delete-an-SSTable-if-it-s-corr.patch",,,,,,,,,,,,,,1.0,lenn0x,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19644,,,Tue Sep 01 23:38:58 UTC 2009,,,,,,,,,,"0|i0fycv:",91164,,,,,Normal,,,,,,,,,,,,,,,,,"05/Aug/09 07:45;jbellis;How are you getting in that state?  The index and bloom filter files are renamed first.  If you're manually deleting index files it's not Cassandra's job to guess what you want it to do.  Otherwise we have a bug.

(In general, if we are in a ""can't happen"" state then we should leave things the way they are and let a human intervene.  In this case I would fail the startup and let the operator decide whether he wants to delete the sstable or build an index.);;;","05/Aug/09 07:49;lenn0x;I deleted the index file. But in any case, if the index is missing, I think we should try to at least regenerate if possible. There could be a chance of corruption. Also it does delete the data file, which I am thinking is a _bad_ idea no matter what state its in.;;;","05/Aug/09 07:51;lenn0x;I am for letting the operator re-generate the index and fail startup. So that means we need to bail and not delete. And how would we provide the option to re-generate?;;;","05/Aug/09 08:10;jbellis;> how would we provide the option to re-generate? 

I suppose we could inaugurate a contrib/ section if you want to write such a tool.  Definitely shouldn't be part of the core tho.;;;","05/Aug/09 08:14;jbellis;Actually we should probably just post it here, and if anyone else ever needs it, _then_ put it in contrib/ :);;;","02/Sep/09 06:38;lenn0x;Do not delete table if corrupt.;;;","02/Sep/09 07:38;jbellis;committed w/ minor tweaking;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CliTest crashing intermittently,CASSANDRA-1648,12478071,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,jbellis,jbellis,22/Oct/10 22:37,16/Apr/19 17:33,22/Mar/23 14:57,27/Oct/10 07:20,0.7 beta 3,,Legacy/Tools,,0,,,,,,"About 50% of the time I get

{code}
$ ant test -Dtest.name=CliTest
...
    [junit] Test org.apache.cassandra.cli.CliTest FAILED (crashed)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Oct/10 23:49;xedin;CASSANDRA-1648.patch;https://issues.apache.org/jira/secure/attachment/12458077/CASSANDRA-1648.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20238,,,Wed Oct 27 08:44:46 UTC 2010,,,,,,,,,,"0|i0g6iv:",92487,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"22/Oct/10 23:06;xedin;I got such a thing few times too - this happens because EmbeddedCassandraService fails to start sometimes for no reason... Thats why I though it would be better to ask user to start server by hand first...;;;","27/Oct/10 07:05;jbellis;Does just extending CleanupHelper fix it?;;;","27/Oct/10 07:20;jbellis;Yes, CleanupHelper ftw.

Thanks for figuring out the problem!;;;","27/Oct/10 16:44;xedin;Yes, this is what was needed and I overlooked that we have CleanupHelper, sorry...;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"SSTableExport can not accept -f , -k options",CASSANDRA-766,12455377,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,santal,santal,05/Feb/10 09:43,16/Apr/19 17:33,22/Mar/23 14:57,11/Feb/10 03:50,0.6,,Legacy/Tools,,0,,,,,,"the SSTableExport command can not accept -f , -k options correct, always said as bellow:

[root@hfdevcasda01 bin]# ./sstable2json -f out.json /opt/cassandra-wbx/data/CONTENT_HF/ChangeHistory-2-Data.db
You must supply exactly one sstable
Usage: org.apache.cassandra.tools.SSTableExport [-f outfile] <sstable> [-k key [-k key [...]]]",Linux,,,,,,,,,,,,,,,,,,,,,,60,60,,0%,60,60,,,,,,,,,,,,,,,,,,,,"11/Feb/10 03:09;jbellis;766-v2.txt;https://issues.apache.org/jira/secure/attachment/12435475/766-v2.txt","11/Feb/10 01:45;jbellis;766.txt;https://issues.apache.org/jira/secure/attachment/12435471/766.txt","05/Feb/10 09:51;santal;issue766.patch;https://issues.apache.org/jira/secure/attachment/12434916/issue766.patch",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19855,,,Wed Feb 17 17:54:46 UTC 2010,,,,,,,,,,"0|i0g0xz:",91583,,,,,Low,,,,,,,,,,,,,,,,,"05/Feb/10 09:44;santal;just remove one extra line will resolve this problem.;;;","05/Feb/10 09:46;santal;patch submit;;;","05/Feb/10 09:51;santal;my bad, create a wrong patch file;;;","11/Feb/10 01:45;jbellis;-f was supposed to be removed but I did it poorly.  this patch finishes the job and uses log4j of WARN,stderr for bin/ tools so that doesn't get in the way of redirecting stdout.;;;","11/Feb/10 03:09;jbellis;v2 now with log4j-tools goodness;;;","11/Feb/10 03:31;urandom;+1;;;","11/Feb/10 03:50;jbellis;committed;;;","18/Feb/10 01:54;hudson;Integrated in Cassandra #357 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/357/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
file descriptor leak in getKeyRange,CASSANDRA-552,12440657,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,14/Nov/09 03:57,16/Apr/19 17:33,22/Mar/23 14:57,14/Nov/09 05:28,0.5,,,,0,,,,,,"paste from mailing list:

Cassandra reported the following:

WARN [GMFD:1] 2009-11-12 16:07:24,961 MessagingService.java (line 393)
Exception was generated at : 11/12/2009 16:07:24 on thread GMFD:1
Too many open files
java.net.SocketException: Too many open files
       at sun.nio.ch.Net.socket0(Native Method)
       at sun.nio.ch.Net.socket(Unknown Source)
       at sun.nio.ch.DatagramChannelImpl.<init>(Unknown Source)
       at sun.nio.ch.SelectorProviderImpl.openDatagramChannel(Unknown
Source)
       at java.nio.channels.DatagramChannel.open(Unknown Source)
       at
org.apache.cassandra.net.UdpConnection.init(UdpConnection.java:49)
       at
org.apache.cassandra.net.MessagingService.sendUdpOneWay(MessagingService.java:388)
       at
org.apache.cassandra.gms.GossipDigestSynVerbHandler.doVerb(Gossiper.java:889)
       at
org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)
       at
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown
Source)
       at java.lang.Thread.run(Unknown Source)

Lsof reports that the java process has 65486 files open (I have ulimit
-n 65535 set in cassandra.in.sh).  Many of the lsof entries include a
trailing '(deleted)' comment after the file path.

This appears to be similar to CASSANDRA-283.  Anyone have a work around
for this?  Would a forced GC take care of the ones marked deleted?

Here is my sample code to count the number of keys:

public class CClient
{
   public static void main(String[] args)
   throws TException, InvalidRequestException, UnavailableException,
UnsupportedEncodingException, NotFoundException
   {
       TTransport tr = new TSocket(""localhost"", 9160);
       TProtocol proto = new TBinaryProtocol(tr);
       Cassandra.Client client = new Cassandra.Client(proto);
       tr.open();

       int     count = 0;
       int     block = 1000;
       String  key   = "" "";

       while (true)
       {
           List<String> list = client.get_key_range(""Keyspace1"",
               ""Standard1"", key, ""~"", block, ConsistencyLevel.ONE);
           int size = list.size();
           if (size == 0)
               break;
           count += size;
           key = list.get(size - 1) + '~';
           System.out.println(""Count: "" + Integer.toString(count));
      }
       tr.close();
   }
}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Nov/09 04:00;jbellis;552.patch;https://issues.apache.org/jira/secure/attachment/12424887/552.patch",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19752,,,Sat Nov 14 12:33:58 UTC 2009,,,,,,,,,,"0|i0fzmv:",91371,,,,,Normal,,,,,,,,,,,,,,,,,"14/Nov/09 05:17;elsif;Wow, that was fast!  The patch fixes the problem.

Thanks,
elsif;;;","14/Nov/09 05:27;jbellis;committed to trunk.

0.4 code is different enough that this patch will not apply.  (It shouldn't be necessary, either, from my reading of the code, but if 0.4 does indeed leak FDs I don't think fixing it is worth the effort with 0.5 coming soon.);;;","14/Nov/09 20:33;hudson;Integrated in Cassandra #258 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/258/])
    make iterator closeable, again
patch by jbellis; tested by ""elsif"" for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
exception when a node is joining,CASSANDRA-114,12424096,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,nk11,nk11,29/Apr/09 05:58,16/Apr/19 17:33,22/Mar/23 14:57,01/May/09 00:05,0.3,,,,0,,,,,,"When a new node is joining, StorageService is notified and the log shows this:

java.lang.NumberFormatException: For input string: ""T""
        at java.lang.NumberFormatException.forInputString(Unknown Source)
        at java.lang.Integer.parseInt(Unknown Source)
        at java.math.BigInteger.<init>(Unknown Source)
        at java.math.BigInteger.<init>(Unknown Source)
        at org.apache.cassandra.dht.RandomPartitioner$3.fromString(RandomPartitioner.java:97)
        at org.apache.cassandra.service.StorageService.onChange(StorageService.java:646)
        at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:770)
        at org.apache.cassandra.gms.Gossiper.handleNewJoin(Gossiper.java:644)

In my case, the EndPointState had a nodeIdState who's state_ was ""Token(165385238122940489124770581854348071118)"" and produced the exception.",,hammer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Apr/09 03:39;jbellis;114-v2.patch;https://issues.apache.org/jira/secure/attachment/12406821/114-v2.patch","29/Apr/09 10:45;jbellis;115.patch;https://issues.apache.org/jira/secure/attachment/12406735/115.patch",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19554,,,Fri May 01 13:09:51 UTC 2009,,,,,,,,,,"0|i0fwyn:",90938,,,,,Normal,,,,,,,,,,,,,,,,,"29/Apr/09 06:27;nk11;Perhaps in the StorageService.updateToken() the added ApplicationState should not be created using Token's toString();;;","29/Apr/09 10:45;jbellis;the attached patch should fix the problem.  can you test?;;;","30/Apr/09 03:30;nk11;It doesn't work yet.
In the StorageService.setup() there still is a Token.toString:
new ApplicationState(storageMetadata_.getStorageId().toString())
;;;","30/Apr/09 03:39;jbellis;you're right.  updated patch attached.;;;","01/May/09 00:04;nk11;the last one did it ;;;","01/May/09 00:10;jbellis;committed patch.;;;","01/May/09 21:09;hudson;Integrated in Cassandra #55 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/55/])
    add string de/serialize code to TokenFactory for ApplicationState's benefit.
patch by jbellis; tested by nk11 and MarkR42 for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Repair using abnormally large amounts of disk space,CASSANDRA-1674,12478581,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,stuhood,jbellis,jbellis,28/Oct/10 22:38,16/Apr/19 17:33,22/Mar/23 14:57,18/Nov/10 09:06,0.6.9,0.7.0 rc 1,,,1,,,,,,"I'm watching a repair on a 7 node cluster.  Repair was sent to one node; the node had 18G of data.  No other node has more than 28G.  The node where the repair initiated is now up to 261G with 53/60 AES tasks outstanding.

I have seen repair take more space than expected on 0.6 but nothing this extreme.

Other nodes in the cluster are occasionally logging
WARN [ScheduledTasks:1] 2010-10-28 08:31:14,305 MessagingService.java (line 515) Dropped 7 messages in the last 1000ms

The cluster is quiesced except for the repair.  Not sure if the dropped messages are contributing the the disk space (b/c of retries?).",,chipdude,johanoskarsson,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Nov/10 03:30;stuhood;0001-Only-repair-the-intersecting-portion-of-a-differing-ra.txt;https://issues.apache.org/jira/secure/attachment/12459816/0001-Only-repair-the-intersecting-portion-of-a-differing-ra.txt","18/Nov/10 03:30;stuhood;for-0.6-0001-Only-repair-the-intersecting-portion-of-a-differing-ra.txt;https://issues.apache.org/jira/secure/attachment/12459817/for-0.6-0001-Only-repair-the-intersecting-portion-of-a-differing-ra.txt",,,,,,,,,,,,,2.0,stuhood,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20253,,,Thu Nov 18 01:46:01 UTC 2010,,,,,,,,,,"0|i0g6ov:",92514,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"28/Oct/10 22:38;jbellis;(this is rc1 snapshot, btw);;;","28/Oct/10 23:40;jbellis;bq. WARN [ScheduledTasks:1] 2010-10-28 08:31:14,305 MessagingService.java (line 515) Dropped 7 messages in the last 1000ms

Created CASSANDRA-1676 for this.;;;","28/Oct/10 23:47;stuhood;I haven't tested the theory, but various messages are executed in the single threaded MISC stage.;;;","29/Oct/10 00:05;jbellis;it doesn't have to be single threaded to drop messages, it just has to not run the verbhandler until RPC_TIMEOUT after the message was received.

In practice though I don't see any place where the verbhandler would be blocking or otherwise slow, including the ones running on MISC.;;;","01/Nov/10 12:08;jbellis;so we have CASSANDRA-1685 and CASSANDRA-1676 committed now, but I don't think those are the root cause here since for them to cause lots of disk space usage repair would have to retry dropped requests which I don't see it doing.  (is this correct?);;;","12/Nov/10 11:10;jbellis;Here is one way to reproduce something similar, at least.

I start with 1 node, and put 1M rows on it.  Then I add a 2nd node, then a third, then run cleanup.  So I have 25% 50% 25% of the 1M rows on those 3 machines:

{code}
$ nodetool -h localhost -p 8080 ring
Address         Status State   Load            Token                                       
                                       164074424718159380631425626216484638578    
127.0.0.2       Up     Normal  95.39 MB        36509681143663337709904175976843745575      
127.0.0.1       Up     Normal  190.72 MB       121466565577718822332569289829746132598     
127.0.0.3       Up     Normal  95.4 MB         164074424718159380631425626216484638578     
{code}

I increase the RF to 2, and run repair against .2:

{code}
Address         Status State   Load            Token                                       
                                       164074424718159380631425626216484638578    
127.0.0.2       Up     Normal  381.41 MB       36509681143663337709904175976843745575      
127.0.0.1       Up     Normal  381.41 MB       121466565577718822332569289829746132598     
127.0.0.3       Up     Normal  190.75 MB       164074424718159380631425626216484638578     
{code}

Let's call the ranges of data that .2, .1, and .3 originally had R, G, and B.  Post repair, .2 should have RB but it has RGB.  Node 1 should have GB but it also has RGB.

Cleanup puts things in the expected state:
{code}
Address         Status State   Load            Token                                       
                                       164074424718159380631425626216484638578    
127.0.0.2       Up     Normal  190.74 MB       36509681143663337709904175976843745575      
127.0.0.1       Up     Normal  285.61 MB       121466565577718822332569289829746132598     
127.0.0.3       Up     Normal  190.75 MB       164074424718159380631425626216484638578     
{code}

This is reproducible against 0.6 as well as 0.7.;;;","17/Nov/10 10:03;stuhood;After finding the differences for all data held on both nodes, repair was not limiting the repaired area to the ranges that the nodes had in common. jbellis: In your example above, because the nodes had no data in common to start with, the MerkleTree was determining that they were different for (0,0], but rather than intersecting the different range with the range of responsibility they had in common, it was repairing all of (0,0].

Attaching a patch for 0.7 to only repair the intersecting ranges.;;;","17/Nov/10 10:04;stuhood;I started rebasing this for 0.6, but noticed some odd behaviour in the 0.6 branch: in what cases would SS.getLocalToken not equal the RHS of SS.getLocalPrimaryRange?;;;","17/Nov/10 10:35;chipdude;Surely something that can overflow disks, even when below 50% usage, is worthy of fix before 0.7.0 is released...;;;","17/Nov/10 11:10;stuhood;Chip: agreed. While it was broken before 0.7, it's a small enough fix that we should try to get it in 0.7.0;;;","17/Nov/10 16:56;jbellis;bq. in what cases would SS.getLocalToken not equal the RHS of SS.getLocalPrimaryRange? 

at a guess, some test is mocking up a TokenMetadata that isn't consistent w/ localtoken;;;","18/Nov/10 03:30;stuhood;Patches for 0.6 and 0.7/trunk: ready for review.;;;","18/Nov/10 04:08;jbellis;committed, but I think there is a bug.  With a similar setup to the above (200K keys instead of 1M), the pre-RF change setup is

{code}
Address         Status State   Load            Token                                       
                                       106239986353888428655683112465158427815    
127.0.0.2       Up     Normal  37.97 MB        21212647344528771789748883276744400257      
127.0.0.3       Up     Normal  18.98 MB        63523312719601176253752035031089272162      
127.0.0.1       Up     Normal  19.05 MB        106239986353888428655683112465158427815     
{code}

post-repair is
{code}
Address         Status State   Load            Token                                       
                                       106239986353888428655683112465158427815    
127.0.0.2       Up     Normal  57.01 MB        21212647344528771789748883276744400257      
127.0.0.3       Up     Normal  56.94 MB        63523312719601176253752035031089272162      
127.0.0.1       Up     Normal  19.07 MB        106239986353888428655683112465158427815     
{code}

So eyeballing it looks reasonable.  But when I kill node 2 and run

{code}$ python contrib/py_stress/stress.py -n 200000 -o read{code}

I get a ton of key-not-found exceptions, indicating that not all the data on 2 got replicated to 3.;;;","18/Nov/10 04:37;hudson;Integrated in Cassandra-0.6 #7 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/7/])
    limit repaired ranges to what the nodes have in common
patch by Stu Hood; reviewed by jbellis for CASSANDRA-1674
;;;","18/Nov/10 09:06;jbellis;the not-found was because i only repaired .2, but the RF increase meant requests to .1 thought they could find the .3 data locally.  not a bug.  Thanks to Stu for pointing this out on IRC.;;;","18/Nov/10 09:46;stuhood;This still needs to be committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Message Serializer slows down/stops responding,CASSANDRA-487,12438043,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,sammy.yu,sammy.yu,sammy.yu,14/Oct/09 12:54,16/Apr/19 17:33,22/Mar/23 14:57,17/Oct/09 04:36,0.4,,,,0,,,,,,"We ran into an issue with where the MESSAGE-SERIALIZER-POOL piles up with tasks.
$ /usr/sbin/nodeprobe -host localhost tpstats
FILEUTILS-DELETE-POOL, pending tasks=0
MESSAGING-SERVICE-POOL, pending tasks=0
MESSAGE-SERIALIZER-POOL, pending tasks=10785714
RESPONSE-STAGE, pending tasks=0
BOOT-STRAPPER, pending tasks=0
ROW-READ-STAGE, pending tasks=0
MESSAGE-DESERIALIZER-POOL, pending tasks=0
GMFD, pending tasks=0
LB-TARGET, pending tasks=0
CONSISTENCY-MANAGER, pending tasks=0
ROW-MUTATION-STAGE, pending tasks=0
MESSAGE-STREAMING-POOL, pending tasks=0
LOAD-BALANCER-STAGE, pending tasks=0
MEMTABLE-FLUSHER-POOL, pending tasks=0

In the log, this seems to have happened when we stopped 2 of the other nodes in our cluster.  This node will  time out on any thrift requests.  Looking through the logs we found the following two exceptions:
java.util.ConcurrentModificationException
        at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
        at java.util.AbstractList$Itr.next(AbstractList.java:349)
        at java.util.Collections.sort(Collections.java:120)
        at org.apache.cassandra.net.TcpConnectionManager.getLeastLoaded(TcpConnectionManager.java:108)
        at org.apache.cassandra.net.TcpConnectionManager.getConnection(TcpConnectionManager.java:71)
        at org.apache.cassandra.net.MessagingService.getConnection(MessagingService.java:306)
        at org.apache.cassandra.net.MessageSerializationTask.run(MessageSerializationTask.java:66)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)

java.util.NoSuchElementException
        at java.util.AbstractList$Itr.next(AbstractList.java:350)
        at java.util.Collections.sort(Collections.java:120)
        at org.apache.cassandra.net.TcpConnectionManager.getLeastLoaded(TcpConnectionManager.java:108)
        at org.apache.cassandra.net.TcpConnectionManager.getConnection(TcpConnectionManager.java:71)
        at org.apache.cassandra.net.MessagingService.getConnection(MessagingService.java:306)
        at org.apache.cassandra.net.MessageSerializationTask.run(MessageSerializationTask.java:66)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)

This appears to have happened on all 4 MESSAGE-SERIALIZER-POOL threads 
I will attach the complete log.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Oct/09 21:21;sammy.yu;0001-Added-locks-around-remove-operation-so-that-Concurre.patch;https://issues.apache.org/jira/secure/attachment/12422101/0001-Added-locks-around-remove-operation-so-that-Concurre.patch","15/Oct/09 00:51;jbellis;487-lock-all-connection-ops.patch;https://issues.apache.org/jira/secure/attachment/12422115/487-lock-all-connection-ops.patch","14/Oct/09 12:56;sammy.yu;system-487.log.gz;https://issues.apache.org/jira/secure/attachment/12422062/system-487.log.gz",,,,,,,,,,,,3.0,sammy.yu,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19716,,,Fri Oct 16 20:36:46 UTC 2009,,,,,,,,,,"0|i0fz8f:",91306,,,,,Normal,,,,,,,,,,,,,,,,,"14/Oct/09 13:21;sammy.yu;I suspect this is because the TcpConnectionManager.removeConnection does not have a lock wrapped around it.
;;;","14/Oct/09 21:21;sammy.yu;This looks like it exists in 0.5 as well.  Attached patch applies for 0.5.  I've wrapped the removeConnections method by using locks.  We may forgo this by using CopyOnWriteArrayList, but it may be too expensive.
;;;","15/Oct/09 00:30;junrao;The patch looks good to me. Have your tried it in your deployment?;;;","15/Oct/09 00:38;sammy.yu;I've done some light testing, but we'll stress it out some more today.;;;","15/Oct/09 00:51;jbellis;the problem is that the original code tries to ""cheat"" and rely on Vector's built-in synchronization for one-line ops.  but as this exception shows, even that has problems since operations like Collections.sort aren't synchronized (even though superfically it looks like a one-liner).

here is a more general patch that turns the Vector into an ArrayList and always does explicit locking of the collection, to remove the temptation to cheat like that.;;;","15/Oct/09 01:04;jbellis;One other race in addToPool (not in my patch):

            if (contains(connection))
            {
                return;
            }

needs to be inside the lock_.;;;","15/Oct/09 01:45;jbellis;Created CASSANDRA-488 for more deep fixes to the TcpConnManager area.;;;","15/Oct/09 08:27;sammy.yu;Tested jbellis' patch in production-like environment with normal operating state and restarted multiple nodes.
;;;","17/Oct/09 04:36;jbellis;committed to 0.4 and trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RejectedExecutionException in getKeyRange,CASSANDRA-165,12425259,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,13/May/09 03:19,16/Apr/19 17:33,22/Mar/23 14:57,14/May/09 06:37,0.3,,,,0,,,,,,"Haven't been able to repro in unit tests but fails in system tests about 50% of the time.

java.lang.RuntimeException: java.util.concurrent.RejectedExecutionException
	at org.apache.cassandra.service.RangeVerbHandler.doVerb(RangeVerbHandler.java:27)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:46)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.util.concurrent.RejectedExecutionException
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:1760)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:767)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:658)
	at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:92)
	at org.apache.cassandra.db.Memtable.sortedKeyIterator(Memtable.java:412)
	at org.apache.cassandra.db.Table.getKeyRangeUnsafe(Table.java:912)
	at org.apache.cassandra.db.Table.getKeyRange(Table.java:883)

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/May/09 03:58;jbellis;165-2-v2.patch;https://issues.apache.org/jira/secure/attachment/12408036/165-2-v2.patch","14/May/09 01:38;jbellis;165-2.patch;https://issues.apache.org/jira/secure/attachment/12408017/165-2.patch","13/May/09 04:43;jbellis;CASSANDRA-165.patch;https://issues.apache.org/jira/secure/attachment/12407918/CASSANDRA-165.patch",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19580,,,Wed May 13 22:37:19 UTC 2009,,,,,,,,,,"0|i0fx9r:",90988,,,,,Normal,,,,,,,,,,,,,,,,,"13/May/09 05:34;jbellis;[commented on 163 initially by mistake]

here is the problem

    void put(String key, ColumnFamily columnFamily, CommitLog.CommitLogContext cLogCtx) throws IOException
    {
        isDirty_ = true;
        executor_.submit(new Putter(key, columnFamily));
        if (isThresholdViolated())
        {
            enqueueFlush(cLogCtx);
        }
    }

(enqueueFlush is the one that swaps out this memtable for a new one in CFS and calls shutdown)

the problem is that we submit first and ask questions later, so we can clearly submit to this [old] memtable on one thread after another thread starts the shutdown.

the only option I see is going to back to the old double-checked-ish logic of checking threshold first, then recursing to resubmit if we switch memtables. (overriding TPE.execute/addIfUnderMaximumPoolSize is not an option since liberal use of private variables is made.);;;","13/May/09 06:35;urandom;+1;;;","13/May/09 06:48;urandom;Spoke too soon. After more local test runs, I got a RejectedExecutionException in the same place.;;;","13/May/09 17:26;hudson;Integrated in Cassandra #73 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/73/])
    to avoid adding work to terminating executor (which will raise an exception) we need to check for threshold violated _first_, and move puts to the new memtable once flush has started.  patch by jbellis; reviewed by Eric Evans for 
;;;","14/May/09 01:38;jbellis;    replace executor with locking.  the interaction between the executor service terminating and the CFS
    was inherently unsafe -- you would have to lock anyway to make it safe, the atomic reference wasn't
    enough, so at that point you might as well get rid of the executor.
;;;","14/May/09 03:58;jbellis;fixes regression mentioned in IRC.  (need to use a writelock for the put call too, not just the memtable switch.  duh.);;;","14/May/09 06:01;urandom;+1;;;","14/May/09 06:37;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The describe_host API method is misleading in that it returns the interface associated with gossip traffic,CASSANDRA-1777,12480888,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,zznate,zznate,25/Nov/10 13:06,16/Apr/19 17:33,22/Mar/23 14:57,04/Aug/11 05:46,0.8.4,,,,3,,,,,,"If the hardware is configured to use separate interfaces for thrift and gossip, the gossip interface will be returned, given the results come out of the ReplicationStrategy eventually.

I understand the approach, but given this is on the API, it effective worthless in situations of host auto discovery via describe_ring from a client. I actually see this as the primary use case of this method - why else would I care about the gossip iface from the client perspective? It's current form should be relegated to JMX only. 

At the same time, we should add port information as well. 

describe_splits probably has similar issues.

I see the potential cart-before-horse issues here and that this will probably be non-trivial to fix, but I think ""give me a set of all the hosts to which I can talk"" is pretty important from a client perspective.",,clavoie,sebastienc,,,,,,,,,,,,,,,,,,,,57600,57600,,0%,57600,57600,,,,,,,,,,,,,,CASSANDRA-2882,CASSANDRA-3173,,,,,"04/Aug/11 04:12;brandon.williams;1777-v2.txt;https://issues.apache.org/jira/secure/attachment/12489239/1777-v2.txt","20/Jan/11 07:06;brandon.williams;1777.txt;https://issues.apache.org/jira/secure/attachment/12468792/1777.txt",,,,,,,,,,,,,2.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,659,,,Wed Aug 03 22:19:59 UTC 2011,,,,,,,,,,"0|i0g7bz:",92618,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"20/Jan/11 07:06;brandon.williams;This is actually simple enough to apply to 0.7 if we leave out the rpc port (since that would require a thrift api change.)  describe_splits doesn't appear to be affected, just describe_ring.;;;","21/Jan/11 02:57;jbellis;gossip is for internal cluster state, we definitely shouldn't be using it for rpc_address and rpc_port.

90% convinced the answer is ""don't use describe_ring for node discovery, that's what RRDNS is for."";;;","21/Jan/11 03:20;brandon.williams;You can't build a routing-aware client from RRDNS though because you'll have no way to map the IP to the token.  describe_ring really is useless if the gossip iface isn't the same as the rpc address right now (describe_ring is already using gossip for the info it returns, it's just the wrong info.);;;","21/Jan/11 07:31;kingryan;Unless you have a dns server that can understand cassandra membership, RRDNS is actually a rough way to do this. I'd prefer to supply something for clients that works correctly.;;;","21/Jan/11 12:26;urandom;I don't see how you could build a non-Java routing-aware client without replicating the partitioner and replica placement client-side. Ick.;;;","22/Jan/11 01:59;kingryan;I don't care about making it routing-aware. I just want to do discovery.;;;","10/May/11 05:24;nickmbailey;Besides just auto_discovery this breaks the pig storage func in contrib if you use different interfaces. That class uses describe ring to generate the input splits for map tasks to work on, which obviously doesn't work if it returns the gossip interface instead of the thrift interface.

+1 on fixing this and backporting to 0.7;;;","10/May/11 05:29;brandon.williams;Since the problem is rooted in CFIF, this actually runs deeper than just Pig.;;;","04/Aug/11 02:22;jbellis;Brandon, what is the status here?;;;","04/Aug/11 02:27;brandon.williams;describe_ring, as is, is completely useless if you don't have thrift bound to the same interface as the storage proto, so I stand by the change to advertise the thrift address instead.;;;","04/Aug/11 03:07;jbellis;is that what the attached patch does?;;;","04/Aug/11 03:12;brandon.williams;That's what it did 8 months ago :)  It gets the rpc address/port information for each machine via gossip and returns that in describe_ring.;;;","04/Aug/11 03:22;jbellis;oh yeah.  the whole ""gossip is for internal cluster state, we definitely shouldn't be using it for rpc_address and rpc_port"" thing I objected to originally.

however, I don't have a better solution (asking nodes directly would mean we couldn't include nodes that are currently unreachable), so +1 I guess on the general approach.

if we're not going to expose individual rpc_port can we just require that it be the same cluster-wide and not bother gossiping it?  it's kind of a misfeature anyway to allow different ones.

the old getRangeToEndpointMap is unused and can be removed now right?

Committing to 0.8 is probably fine but let's leave 0.7 alone.;;;","04/Aug/11 03:27;brandon.williams;bq. if we're not going to expose individual rpc_port can we just require that it be the same cluster-wide and not bother gossiping it? it's kind of a misfeature anyway to allow different ones.

I agree, I'll remove the port.

bq. the old getRangeToEndpointMap is unused and can be removed now right?

I think I originally left it in because it's exposed over JMX and I didn't want to break it if anyone was using it for something.;;;","04/Aug/11 04:12;brandon.williams;v2 only communicate the rpc address over gossip, and fixes a problem in v1 when the rpc address is left blank.;;;","04/Aug/11 05:08;jbellis;+1, but let's remove getRangeToEndpointMap when you merge to trunk.;;;","04/Aug/11 05:46;brandon.williams;Committed.;;;","04/Aug/11 06:19;hudson;Integrated in Cassandra-0.8 #254 (See [https://builds.apache.org/job/Cassandra-0.8/254/])
    describe_ring returns the interface thrift is bound to instead of the
one the storage proto is bound to.
Patch by brandonwilliams, reviewed by jbellis for CASSANDRA-1777

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1153683
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/VersionedValue.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/CassandraServer.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageService.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/ApplicationState.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stress performance is artificially limited,CASSANDRA-2578,12505731,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,brandon.williams,brandon.williams,29/Apr/11 06:09,16/Apr/19 17:33,22/Mar/23 14:57,29/Apr/11 09:41,0.7.6,,Legacy/Tools,,0,,,,,,"With stress I only get about 7k inserts/s against a single server, and the load and cpu usage from stress is higher than the server.  Pystress gets 15-20k inserts/s against the same machine.  Stress isn't cpu-limited however, so there must be something else holding it back.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Apr/11 08:08;xedin;CASSANDRA-2578-0.7.patch;https://issues.apache.org/jira/secure/attachment/12477717/CASSANDRA-2578-0.7.patch","29/Apr/11 08:01;xedin;CASSANDRA-2578.patch;https://issues.apache.org/jira/secure/attachment/12477715/CASSANDRA-2578.patch",,,,,,,,,,,,,2.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20709,,,Fri Apr 29 01:55:45 UTC 2011,,,,,,,,,,"0|i0gc7b:",93407,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"29/Apr/11 08:01;xedin;caching generateValues in Inserter/IndexedRangeSlicer. Patch against trunk but can be applied to cassandra-0.8 without any problem.;;;","29/Apr/11 09:41;brandon.williams;Great! Committed.;;;","29/Apr/11 09:55;hudson;Integrated in Cassandra-0.7 #461 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/461/])
    cache generateValues in Inserter/IndexedRangeSlicer.
Patch by Pavel Yaskevich, reviewed by brandonwilliams for CASSANDRA-2578
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing or incorrect SVN properties in http://svn.apache.org/repos/asf/incubator/cassandra/tags/cassandra-0.4.0-beta1/,CASSANDRA-378,12433147,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,sebb,sebb,sebb,15/Aug/09 17:27,16/Apr/19 17:33,22/Mar/23 14:57,21/Aug/09 00:52,0.4,,Legacy/Tools,,0,,,,,,Missing SVN properties - see attached file to follow,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/09 19:57;sebb;cassandra-0.4.0-beta1.sh;https://issues.apache.org/jira/secure/attachment/12416659/cassandra-0.4.0-beta1.sh",,,,,,,,,,,,,,1.0,sebb,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19660,,,Fri Aug 21 12:34:39 UTC 2009,,,,,,,,,,"0|i0fyif:",91189,,,,,Normal,,,,,,,,,,,,,,,,,"15/Aug/09 19:27;euphoria;This patch targets a different project.;;;","15/Aug/09 20:00;sebb;Sorry, attached the wrong file by mistake.

The issue is for CASSANDRA, not WINK, so please revert it to the proper project;;;","15/Aug/09 20:25;jbellis;Hmm.  I clicked on CASSANDRA-368 and got taken to WINK-139?  I think someone fixed the wrong problem. :);;;","15/Aug/09 20:28;jbellis;personally I am -1 on propsetting every file to eol-style native. I am unlikely to remember to propset every new file that gets created so this will get out of date much faster than our current ""just use unix line endings and use an editor that can handle that"" plan.;;;","15/Aug/09 20:44;sebb;You don't need to remember to add the propset; just ensure that your SVN configuration is correct:

http://www.apache.org/dev/version-control.html#https-svn-config;;;","19/Aug/09 23:37;euphoria;I proposed the same in CASSANDRA-111 and am still +1 on it.;;;","21/Aug/09 00:52;jbellis;committed;;;","21/Aug/09 20:34;hudson;Integrated in Cassandra #174 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/174/])
    set miscellaneous svn properties (mime-type, eol-style LF on .sh).  patch by Sebb; reviewed by Michael Greene for 
svn ps svn:eol-style native *. patch by Sebb; reviewed by Michael Greene for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hadoop streaming -> Cassandra ColumnFamilyOutputFormat not respecting partitioner,CASSANDRA-1455,12473166,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,stuhood,mrflip,mrflip,03/Sep/10 01:42,16/Apr/19 17:33,22/Mar/23 14:57,05/Sep/10 02:20,,,,,0,,,,,,"The Hadoop streaming shim (hadoop streaming client => avro => ColumnFamilyOutputFormat => cassandra ring) is only connecting to one or a couple clients on the ring.  With 24 hadoop clients launched,  ` sudo netstat -antp | grep 9160 | wc -l ` gave 24 on one machine, and only 1-3 on any other node.

I'll attach the script and runner I used.","Ubuntu lucid ; Hadoop 0.20.1 CDH3b1 ; Cassandra 0.7beta1 + #1368, 1358, 1322, 1315 + patch to allow flat StreamingMutation avro schema",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Sep/10 01:45;mrflip;avromapper.rb;https://issues.apache.org/jira/secure/attachment/12453703/avromapper.rb","03/Sep/10 01:45;mrflip;streamer.sh;https://issues.apache.org/jira/secure/attachment/12453704/streamer.sh",,,,,,,,,,,,,2.0,stuhood,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20148,,,Sat Sep 04 18:20:06 UTC 2010,,,,,,,,,,"0|i0g55z:",92267,,,,,Critical,,,,,,,,,,,,,,,,,"03/Sep/10 01:45;mrflip;Note that in its current state requires a patch to use flat avro schema.;;;","03/Sep/10 14:20;stuhood;Flip: could you try out the fix from CASSANDRA-1434?;;;","03/Sep/10 20:20;mrflip;I tried it with the first rev of the patches, no success; I see there was a second rev and I'll give that a shot in a bit.;;;","03/Sep/10 22:38;mrflip;I had a look at the code. I'm not really set up to debug Java atm, but did a finger trace thru.

In storeDescribedRing in RingCache, line 37 reads
 host = range.endpoints.get(0)
AFAICT, it's outside the loop over endpoints. Is it possible that host needs to be set inside the loop from the endpoints map?

flip
----
http://infochimps.org
Find any dataset in the world




;;;","04/Sep/10 01:26;stuhood;> host = range.endpoints.get(0)
Aha... good eye Flip. Looks like I broke that in 1322: so it is only collecting the primary endpoints. That makes the ""alternate through replicas of a range"" fix on 1434 moot, but you should still be seeing an even number of connections to all the nodes in your cluster.

I'll roll another 1434, but I don't have any other ideas here at the moment: in tests against a RF=1 12 node cluster (with OPP even), I saw an equal number of connections to all nodes, even with the above bug.;;;","05/Sep/10 02:20;mrflip;The combination of patches now shows inbound connections on all hosts in the ring. Thanks Stu!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get_columns_since broken when called remotely,CASSANDRA-93,12423411,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,junrao,junrao,junrao,22/Apr/09 05:04,16/Apr/19 17:33,22/Mar/23 14:57,01/May/09 05:53,,,,,0,,,,,,get_columns_since returns wrong result when called from a node that doesn't own the data.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Apr/09 05:11;junrao;issue93.patch001;https://issues.apache.org/jira/secure/attachment/12406057/issue93.patch001",,,,,,,,,,,,,,1.0,junrao,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19547,,,Thu Apr 30 21:53:12 UTC 2009,,,,,,,,,,"0|i0fwtz:",90917,,,,,Normal,,,,,,,,,,,,,,,,,"22/Apr/09 05:11;junrao;Attach a fix. The problem is that the remote path (in ReadVerbHandler) is not consistent with the local path. In particular, the path for get_columns_since is missing.;;;","01/May/09 05:53;jbellis;committed a while ago;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTableDeletingReference only deletes data files,CASSANDRA-2059,12496836,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,26/Jan/11 23:28,16/Apr/19 17:33,22/Mar/23 14:57,28/Jan/11 08:09,0.7.1,,,,0,,,,,,"Ching-Cheng Chen reports on the mailing list:
	

In SSTableDeletingReference, it try this operation

components.remove(Component.DATA);

before

STable.delete(desc, components);

However, the components was reference to the components object which was created inside SSTable by

this.components = Collections.unmodifiableSet(dataComponents);

As you can see, you can't try the remove operation on that components object.",,cburroughs,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,"26/Jan/11 23:48;jbellis;2059.txt;https://issues.apache.org/jira/secure/attachment/12469428/2059.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20420,,,Fri Jan 28 00:46:26 UTC 2011,,,,,,,,,,"0|i0g92n:",92900,,stuhood,,stuhood,Normal,,,,,,,,,,,,,,,,,"26/Jan/11 23:48;jbellis;Patch to use Sets.difference instead of mutating components;;;","27/Jan/11 12:29;stuhood;+1

Do we have a separate issue to fix the lack-of-logged-exception problem? Opened CASSANDRA-2061... might be a dupe.;;;","28/Jan/11 08:09;jbellis;committed;;;","28/Jan/11 08:46;hudson;Integrated in Cassandra-0.7 #224 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/224/])
    fix deletionof sstable non-data components
patch by jbellis; reviewed by stuhood for CASSANDRA-2059
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Saved row cache continues to be read past max cache size,CASSANDRA-2082,12497246,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,cburroughs,cburroughs,cburroughs,01/Feb/11 03:46,16/Apr/19 17:33,22/Mar/23 14:57,27/Jul/11 02:58,1.0.0,,,,0,,,,,,"Scenario:
 - Node has a saved row cache of size n
 - node OOMs
 - Make row cache size = .5n to prevent OOM while we debug, restart node.
 - n items are still read from the row cache, making startup take twice as long as needed.


(This is intended as a straightforward bug, not as a hackish CASSANDRA-1966.)",,cburroughs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jul/11 03:00;cburroughs;0001-v1-Only-load-up-to-capacity-items-into-the-cache.patch;https://issues.apache.org/jira/secure/attachment/12487465/0001-v1-Only-load-up-to-capacity-items-into-the-cache.patch","23/Jul/11 03:00;cburroughs;0002-v1-unit-test-for-CASSANDRA-2082.patch;https://issues.apache.org/jira/secure/attachment/12487466/0002-v1-unit-test-for-CASSANDRA-2082.patch",,,,,,,,,,,,,2.0,cburroughs,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20432,,,Tue Jul 26 19:19:38 UTC 2011,,,,,,,,,,"0|i0g97b:",92921,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"23/Jul/11 03:00;cburroughs;Note that the second patch applies on top of CASSANDRA-1966.;;;","27/Jul/11 02:58;jbellis;Committed since the code involved is small, but I note that the applicability is limited because you can't change the cache setting via schema or JMX until the node has started successfully.  So it's only useful if you notice memory pressure and change the setting before the node OOMs.
;;;","27/Jul/11 03:19;hudson;Integrated in Cassandra #975 (See [https://builds.apache.org/job/Cassandra/975/])
    stop reading cache after max size-to-save is reached
patch by Chris Burroughs; reviewed by jbellis for CASSANDRA-2082

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1151211
Files : 
* /cassandra/trunk/test/unit/org/apache/cassandra/db/RowCacheTest.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DebuggableScheduledThreadPoolExecutor only schedules a task once,CASSANDRA-455,12436431,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,junrao,junrao,24/Sep/09 00:58,16/Apr/19 17:33,22/Mar/23 14:57,29/Sep/09 02:44,0.4,0.5,,,0,,,,,,"DebuggableScheduledThreadPoolExecutor only schedules a task exactly once, instead of periodically. This affects scheduled flushers and periodic hints delivery.
",,johanoskarsson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Sep/09 23:12;jbellis;455.patch;https://issues.apache.org/jira/secure/attachment/12420702/455.patch",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19696,,,Mon Sep 28 18:44:34 UTC 2009,,,,,,,,,,"0|i0fz1j:",91275,,,,,Normal,,,,,,,,,,,,,,,,,"24/Sep/09 01:00;junrao;The problem seems to be in DebuggableScheduledThreadPoolExecutor.afterExecute(). If I remove the line
 DebuggableThreadPoolExecutor.logFutureExceptions(r);
then all tasks are scheduled periodically as expected. Not sure if this is the right fix though.
;;;","24/Sep/09 22:54;junrao;It seems that one needs to call DebuggableThreadPoolExecutor.logFutureExceptions(r) to catch any exception while running the scheduled task. However, once get() is called on the FutureTask, the task is no longer scheduled any more. Anyone knows how do address this?;;;","24/Sep/09 23:10;jbellis;scheduleAtFixedRate ""Creates and executes a periodic action that becomes enabled first after the given initial delay, and subsequently with the given period; that is executions will commence after initialDelay then initialDelay+period, then initialDelay + 2 * period, and so on. If any execution of the task encounters an exception, subsequent executions are suppressed...""

so get() shouldn't be causing a cancel, but if an exception is found then we need to re-schedule it manually.  (If you cast the Runnable in afterExecute to ScheduledFutureTask you can get access to the scheduling info.)

if get is causing the cancel even w/o any exceptions being involved then I guess you'll need to source dive in ScheduledThreadPoolExecutor to see what is going on.;;;","24/Sep/09 23:20;junrao;""if get is causing the cancel even w/o any exceptions being involved then I guess you'll need to source dive in ScheduledThreadPoolExecutor to see what is going on.""

That seems to be the case. Here is what happens. 

Without exception:
When DebuggableThreadPoolExecutor.logFutureExceptions(r) is called in afterExecute(), tasks are no longer executed afterwards. When DebuggableThreadPoolExecutor.logFutureExceptions(r) is removed, tasks are scheduled as expected.

With exception:
If DebuggableThreadPoolExecutor.logFutureExceptions(r) is removed, exception is not logged.;;;","24/Sep/09 23:28;jbellis;then i guess we should always reschedule after checking for exceptions.

but i'd be more comfortable if i saw the code that is un-scheduling it, so i could be sure there are no conditions under which we could end up w/ 2 copies of the same task scheduled.;;;","26/Sep/09 04:25;jbellis;Okay, here's what's going on.

ScheduledFutureTask overrides FT.run as follows:

        public void run() {
            if (isPeriodic())
                runPeriodic();
            else
                ScheduledFutureTask.super.run();
        }

we are scheduling periodic tasks, so the normal run() method, which just wrapps sync.innerRun(), is not called.  Instead we call runPeriodic, of which the important part here is

            boolean ok = ScheduledFutureTask.super.runAndReset();

not run()!  runAndReset()!  which says

    /**
     * Executes the computation without setting its result, and then
     * resets this Future to initial state, failing to do so if the
     * computation encounters an exception or is cancelled.  This is
     * designed for use with tasks that intrinsically execute more
     * than once.
     * @return true if successfully run and reset
     */
    protected boolean runAndReset() {
        return sync.innerRunAndReset();
    }

key point is ""... without setting its result.""  sure enough, sync.innerRAR sets the state back to 0 when it is done -- but get() will block until state is RAN or CANCELLED, i.e. until there is a result.  So the reason get() makes the task stop executing is it deadlocks the executor thread that it's running on.

as far as i can see there is no public way to get an exception out of a FutureTask.sync w/o calling get.  I guess we can get it out using reflection tho.

For 0.4 I will just revert the commit that added logFutureExceptions to DebuggableScheduledThreadPoolExecutor.;;;","26/Sep/09 04:37;jbellis;ScheduledFutureTask is private.  @Q#%$;;;","26/Sep/09 04:52;jbellis;performed the revert on the 0.4 branch.;;;","26/Sep/09 05:25;jbellis;I took a stab at using STPE.decorateTask to make a new ScheduledFuture that doesn't eat exceptions but it's going to be messy and fragile.

Ripping that crap out and using old-school Timers looks like a better option.;;;","28/Sep/09 23:12;jbellis;    Replace DebuggableScheduledThreadPoolExecutor with non-Scheduled Executors and Timers.  This allows logging
    exceptions from repeated tasks, which is basically impossible with STPE.
;;;","29/Sep/09 01:48;junrao;Looks good to me. Perhaps we should add some logging for the starting of each scheduled task.;;;","29/Sep/09 02:42;jbellis;both flushing and HHO log as soon as they start so I think adding logging at the timer level clutters things up more than clarifies.  (maybe if we decide we have too many Timer threads around and consolidate into a global service, though.);;;","29/Sep/09 02:44;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
multiget returns empty ColumnOrSuperColumn instead of null,CASSANDRA-739,12446636,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,26/Jan/10 13:03,16/Apr/19 17:33,22/Mar/23 14:57,28/Apr/10 06:13,0.7 beta 1,,,,0,,,,,,"the later is more intuitive, and the former violates the rule that COSC should have exactly one of {column, super_column} set.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Apr/10 05:26;jbellis;739-rebased.txt;https://issues.apache.org/jira/secure/attachment/12443004/739-rebased.txt","26/Apr/10 23:42;jbellis;739.txt;https://issues.apache.org/jira/secure/attachment/12442861/739.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19847,,,Tue Apr 27 22:13:01 UTC 2010,,,,,,,,,,"0|i0g0rz:",91556,,,,,Low,,,,,,,,,,,,,,,,,"26/Apr/10 23:42;jbellis;simplest solution: remove the deprecated multiget method.  (as opposed to multiget_slices, which doesn't have this problem.)

bumped thrift major version to 5.0.0.;;;","28/Apr/10 05:49;brandon.williams;+1;;;","28/Apr/10 06:13;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Too many splits for ColumnFamily with only a few rows,CASSANDRA-1050,12463697,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,johanoskarsson,joosto,joosto,05/May/10 03:31,16/Apr/19 17:33,22/Mar/23 14:57,25/May/10 15:41,0.7 beta 1,,,,0,hadoop,keyspace,split,,,"ColumnFamilyInputFormat creates splits for the entire Keyspace.  If one ColumnFamily has 100 Million rows and another has only 100 rows, the number of splits will be the 1,526 (assuming 64k rows per split) for either one, since it is based on the total number of unique keys across the whole keyspace, and not on the number of rows in the ColumnFamily.",,anty,bendiken,johanoskarsson,joosto,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/May/10 04:08;johanoskarsson;CASSANDRA-0.6-1050.patch;https://issues.apache.org/jira/secure/attachment/12445374/CASSANDRA-0.6-1050.patch","21/May/10 22:15;johanoskarsson;CASSANDRA-1050.patch;https://issues.apache.org/jira/secure/attachment/12445169/CASSANDRA-1050.patch",,,,,,,,,,,,,2.0,johanoskarsson,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19975,,,Tue May 25 13:22:28 UTC 2010,,,,,,,,,,"0|i0g2p3:",91867,,,,,Normal,,,,,,,,,,,,,,,,,"21/May/10 22:15;johanoskarsson;This patch for trunk should help with the problem. Run ant gen-thrift-java as the generated code is not in the patch.
I ran the word count test and that worked, have not had time to try it in the scenario described in the ticket though.;;;","21/May/10 22:32;jeromatron;I wonder if this would address the problem in CASSANDRA-1042.  I'll give it a try and see.;;;","22/May/10 00:59;jeromatron;Johan - I applied this patch on my local trunk and ran the word count on it - I get perfect results on all but the /tmp/wordcount3 - that gets 1006 instead of 1000.  It looks like it resolves many of the issues that were happening with CASSANDRA-1042 though.;;;","24/May/10 23:59;jbellis;+1;;;","25/May/10 04:08;johanoskarsson;Slightly modified version for 0.6;;;","25/May/10 06:45;jbellis;Belatedly realized that if we're changing the [thrift] method signature we should leave it alone in the stable 0.6 series.  let's just commit to trunk.  Sorry about the extra work.;;;","25/May/10 15:41;johanoskarsson;Committed to trunk.;;;","25/May/10 20:32;jbellis;btw, if you build the thrift code with ""ant gen-thrift-java"" it will re-run rat for you to avoid blowing away the apache license headers in the generated code;;;","25/May/10 20:56;hudson;Integrated in Cassandra #445 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/445/])
    Hadoop input format now uses the specified column family to figure out the number of splits instead of the whole keyspace. Patch by johan, review by jbellis. CASSANDRA-1050
;;;","25/May/10 21:22;johanoskarsson;I usually do, but my default thrift installation is 0.2.0 (used in all my other projects) and this required a newer version.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fat clients are never removed,CASSANDRA-1730,12479741,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,12/Nov/10 03:17,16/Apr/19 17:33,22/Mar/23 14:57,04/Dec/10 05:05,0.6.9,,,,0,,,,,,"After a failed bootstrap, these lines repeat infinitely:

 INFO [Timer-0] 2010-11-11 01:58:32,708 Gossiper.java (line 406) FatClient /10.104.73.164 has been silent for 3600000ms, removing from gossip
 INFO [GMFD:1] 2010-11-11 01:59:03,685 Gossiper.java (line 591) Node /10.104.73.164 is now part of the cluster

Changing the IP on the node but using the same token causes a conflict, requiring either a full cluster restart or changing the token.  This is especially easy to run into in practice in a virtual environment such as ec2.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Dec/10 04:18;brandon.williams;1730.txt;https://issues.apache.org/jira/secure/attachment/12465270/1730.txt",,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20278,,,Sat Dec 11 07:35:12 UTC 2010,,,,,,,,,,"0|i0g71b:",92570,,,,,Normal,,,,,,,,,,,,,,,,,"24/Nov/10 03:50;jbellis;is this related to CASSANDRA-1518?;;;","24/Nov/10 03:53;brandon.williams;I don't think so since this was encountered on 0.6;;;","04/Dec/10 04:18;brandon.williams;The problem here is that node A removes the fat client, but node B's timeout hasn't been exceeded yet, so it still has it.  RING_DELAY elapses, and B propagates the fat client back to A, and when B does remove it A will do the inverse.  Part of the problem here appears to be that the FD is adaptive and RING_DELAY is static, and since both occurrences of this I know of have been on ec2, I suspect somehow the difference between all nodes marking the client as dead has exceeded RING_DELAY.  Since by way of CASSANDRA-644 the 1h timeout for fatclients appears to have been chosen arbitrarily, I think we should reduce this to RING_DELAY / 2, giving justRemovedEndpoints plenty of leeway to prevent re-gossiping the fatclient to other nodes that have already removed it and evicted it from jRE.  Trivial patch attached. ;;;","04/Dec/10 04:22;jbellis;+1;;;","04/Dec/10 05:05;brandon.williams;Committed.;;;","04/Dec/10 06:01;hudson;Integrated in Cassandra-0.6 #16 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/16/])
    Reduce FatClient timeout to RING_DELAY / 2.  Patch by brandonwilliams, reviewed by jbellis for CASSANDRA-1730.
;;;","08/Dec/10 04:30;hudson;Integrated in Cassandra #615 (See [https://hudson.apache.org/hudson/job/Cassandra/615/])
    ;;;","11/Dec/10 15:35;hudson;Integrated in Cassandra-0.7 #70 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/70/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RemoveToken waits for dead nodes,CASSANDRA-1605,12477046,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,nickmbailey,nickmbailey,nickmbailey,12/Oct/10 01:59,16/Apr/19 17:33,22/Mar/23 14:57,12/Oct/10 23:10,0.7 beta 3,,Legacy/Tools,,0,,,,,,RemoveToken will wait for replication confirmation from nodes that are down.   It should only wait for live nodes.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Oct/10 03:46;nickmbailey;0001-Use-failure-detector-when-detecting-new-nodes.patch;https://issues.apache.org/jira/secure/attachment/12456885/0001-Use-failure-detector-when-detecting-new-nodes.patch",,,,,,,,,,,,,,1.0,nickmbailey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20213,,,Thu Oct 14 12:49:49 UTC 2010,,,,,,,,,,"0|i0g69b:",92444,,,,,Normal,,,,,,,,,,,,,,,,,"12/Oct/10 03:46;nickmbailey;Updated to use the failure detector when determining who to wait for.;;;","12/Oct/10 23:10;jbellis;committed w/ inline of getNewEndpoints since neither the name nor the javadoc was an accurate description of what it did anymore;;;","14/Oct/10 20:49;hudson;Integrated in Cassandra #565 (See [https://hudson.apache.org/hudson/job/Cassandra/565/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bootstrapping might skip needed ranges.,CASSANDRA-902,12459419,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,gdusbabek,gdusbabek,gdusbabek,18/Mar/10 03:40,16/Apr/19 17:33,22/Mar/23 14:57,18/Mar/10 06:23,0.6,,,,0,,,,,,"Bootstrapper.getRangeWithSources should return a multimap with as many keys as myRangeAddresses.  But with the way the two loops are structured, they are not guaranteed to ever examine all of myRanges.  To see why, consider a scenario where the inner-loop breaks on the first element in myRanges.  myRangeAddresses will only ever have one key in it.

Solution is to swap the order of the loops.",,david.pan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Mar/10 03:43;gdusbabek;bootstrap-range-addr-calculation.txt;https://issues.apache.org/jira/secure/attachment/12439060/bootstrap-range-addr-calculation.txt",,,,,,,,,,,,,,1.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19911,,,Wed Mar 17 20:29:54 UTC 2010,,,,,,,,,,"0|i0g1s7:",91719,,,,,Normal,,,,,,,,,,,,,,,,,"18/Mar/10 03:43;gdusbabek;patched against trunk.  The same code exists in 5, so is likely a problem there too.;;;","18/Mar/10 04:17;gdusbabek;This appears to only be a problem when the number of pending ranges for a node is greater than the number of ranges currently in place (which can't happen in 0.6).  If so, this only needs to be fixed in trunk and I can roll it in to CASSANDRA-826.;;;","18/Mar/10 04:29;jbellis;+1 0.6 and trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
testNameSort fails,CASSANDRA-59,12422158,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,johanoskarsson,johanoskarsson,07/Apr/09 05:00,16/Apr/19 17:33,22/Mar/23 14:57,17/Apr/09 04:42,0.3,,,,0,,,,,," [testng] FAILED: testNameSort
   [testng] java.lang.AssertionError
   [testng]     at org.apache.cassandra.db.ColumnFamilyStoreTest.validateNameSort(ColumnFamilyStoreTest.java:188)
   [testng]     at org.apache.cassandra.db.ColumnFamilyStoreTest.testNameSort(ColumnFamilyStoreTest.java:72)
   [testng] ... Removed 22 stack frames
","java version ""1.6.0_07"", Ubuntu",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-9,,CASSANDRA-85,,,,,,,,,,,,,,,"16/Apr/09 05:29;jbellis;59.patch;https://issues.apache.org/jira/secure/attachment/12405582/59.patch",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19530,,,Thu Apr 16 20:42:18 UTC 2009,,,,,,,,,,"0|i0fwmf:",90883,,,,,Normal,,,,,,,,,,,,,,,,,"07/Apr/09 06:38;jbellis;The problem is that waitForFlush doesn't always do what it says.

I'm working on a more elegant solution but in the meantime adding Thread.sleep(1000) at the end of waitForFlush will work around this.;;;","07/Apr/09 06:44;jbellis;committed the sleep workaround temporarily.;;;","16/Apr/09 05:30;jbellis;patch to make forceFlush block until the flush action is queued on MemtableManager.  That way calling forceFlush; waitForFlush will be guaranteed that the action waitFF puts on MtM will run after the flush completes, i.e., the wait will actually do what it's supposed to.
;;;","16/Apr/09 05:33;jbellis;Johan, can you review?;;;","17/Apr/09 04:30;urandom;Looks good to me. +1;;;","17/Apr/09 04:42;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incompletely written commitlog keeps Cassandra from starting,CASSANDRA-2055,12496735,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,ketralnis,ketralnis,26/Jan/11 06:43,16/Apr/19 17:33,22/Mar/23 14:57,09/Feb/11 23:07,,,Legacy/CQL,,0,,,,,,"I had to pull the plug in a failing Cassandra node. On boot, it dies with this exception. The process dies and is unable to start

{{
Jan 25 15:35:58 pmc01 cas:  INFO 15:35:58,870 Finished reading /cassandra/commitlog/CommitLog-1295991089449.log
Jan 25 15:35:58 pmc01 cas: ERROR 15:35:58,872 Exception encountered during startup.
Jan 25 15:35:58 pmc01 cas: java.io.EOFException
Jan 25 15:35:58 pmc01 cas: ^Iat java.io.DataInputStream.readUnsignedShort(DataInputStream.java:340)
Jan 25 15:35:58 pmc01 cas: ^Iat java.io.DataInputStream.readUTF(DataInputStream.java:589)
Jan 25 15:35:58 pmc01 cas: ^Iat java.io.DataInputStream.readUTF(DataInputStream.java:564)
Jan 25 15:35:58 pmc01 cas: ^Iat org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:363)
Jan 25 15:35:58 pmc01 cas: ^Iat org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:318)
Jan 25 15:35:58 pmc01 cas: ^Iat org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:240)
Jan 25 15:35:58 pmc01 cas: ^Iat org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:172)
Jan 25 15:35:58 pmc01 cas: ^Iat org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:115)
Jan 25 15:35:58 pmc01 cas: ^Iat org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
Jan 25 15:35:58 pmc01 cas: Exception encountered during startup.
Jan 25 15:35:58 pmc01 cas: java.io.EOFException
Jan 25 15:35:58 pmc01 cas: ^Iat java.io.DataInputStream.readUnsignedShort(DataInputStream.java:340)
Jan 25 15:35:58 pmc01 cas: ^Iat java.io.DataInputStream.readUTF(DataInputStream.java:589)
Jan 25 15:35:58 pmc01 cas: ^Iat java.io.DataInputStream.readUTF(DataInputStream.java:564)
Jan 25 15:35:58 pmc01 cas: ^Iat org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:363)
Jan 25 15:35:58 pmc01 cas: ^Iat org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:318)
Jan 25 15:35:58 pmc01 cas: ^Iat org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:240)
Jan 25 15:35:58 pmc01 cas: ^Iat org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:172)
Jan 25 15:35:58 pmc01 cas: ^Iat org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:115)
Jan 25 15:35:58 pmc01 cas: ^Iat org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20417,,,Wed Feb 09 15:07:55 UTC 2011,,,,,,,,,,"0|i0g91r:",92896,,,,,Normal,,,,,,,,,,,,,,,,,"26/Jan/11 06:49;ketralnis;It would be especially helpful to also indicate *which* commitlog was malformed in the exception message;;;","26/Jan/11 06:51;jbellis;Turning on debug logging will give you which commitlog, as well as the location in the file of the RowMutation in question.  In particular, we're interested in whether that mutation is at the end of the file.  (You'll have to get file size from ls -l.);;;","26/Jan/11 06:55;ketralnis;Blast :( Sadly I've already deleted the commitlog to get the node back up quickly (I had two neighbours down);;;","26/Jan/11 07:26;jbellis;what, two-minute response time wasn't fast enough for you? :);;;","09/Feb/11 23:07;jbellis;following up on CASSANDRA-2128;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Commit log replay issues,CASSANDRA-370,12433347,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jbellis,jamesgolick,jamesgolick,19/Aug/09 00:41,16/Apr/19 17:33,22/Mar/23 14:57,25/Aug/09 06:03,0.4,,,,0,,,,,,"I've been having a bunch of trouble replaying commit logs. I've seen various exceptions, including:

java.lang.NegativeArraySizeException
        at org.apache.cassandra.db.CommitLog.recover(CommitLog.java:273)
        at org.apache.cassandra.db.RecoveryManager.doRecovery(RecoveryManager.java:63)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:95)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:156)
Exception encountered during startup.
java.lang.NegativeArraySizeException
        at org.apache.cassandra.db.CommitLog.recover(CommitLog.java:273)
        at org.apache.cassandra.db.RecoveryManager.doRecovery(RecoveryManager.java:63)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:95)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:156)

I also got this:

java.lang.RuntimeException: Unable to load comparator class 'org.apache.cassandra.db.marshal.UTF8Typ'.  probably this means you have obsolete sstables lying around
        at org.apache.cassandra.db.ColumnFamily$ColumnFamilySerializer.readComparator(ColumnFamily.java:525)
        at org.apache.cassandra.db.ColumnFamily$ColumnFamilySerializer.deserializeEmpty(ColumnFamily.java:535)
        at org.apache.cassandra.db.ColumnFamily$ColumnFamilySerializer.deserialize(ColumnFamily.java:500)
        at org.apache.cassandra.db.RowSerializer.deserialize(Row.java:225)
        at org.apache.cassandra.db.CommitLog.recover(CommitLog.java:284)
        at org.apache.cassandra.db.RecoveryManager.doRecovery(RecoveryManager.java:63)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:95)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:156)
Caused by: java.lang.ClassNotFoundException: org/apache/cassandra/db/marshal/UTF8Typ
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:169)
        at org.apache.cassandra.db.ColumnFamily$ColumnFamilySerializer.readComparator(ColumnFamily.java:521)
        ... 7 more

and this:

java.io.EOFException
        at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:323)
        at java.io.DataInputStream.readUTF(DataInputStream.java:572)
        at java.io.DataInputStream.readUTF(DataInputStream.java:547)
        at org.apache.cassandra.db.RowSerializer.deserialize(Row.java:218)
        at org.apache.cassandra.db.CommitLog.recover(CommitLog.java:284)
        at org.apache.cassandra.db.RecoveryManager.doRecovery(RecoveryManager.java:63)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:95)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:156)
Exception encountered during startup.

Not sure if any of them are related.",OS X 10.5.7,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Aug/09 05:30;jbellis;01-fixed.diff;https://issues.apache.org/jira/secure/attachment/12417530/01-fixed.diff","19/Aug/09 01:41;jbellis;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-370-avoid-opening-multiple-writers-for-the-c.txt;https://issues.apache.org/jira/secure/attachment/12416896/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-370-avoid-opening-multiple-writers-for-the-c.txt","19/Aug/09 01:41;jbellis;ASF.LICENSE.NOT.GRANTED--0002-read-bytes-doesn-t-automatically-throw-EOFException-i.txt;https://issues.apache.org/jira/secure/attachment/12416897/ASF.LICENSE.NOT.GRANTED--0002-read-bytes-doesn-t-automatically-throw-EOFException-i.txt",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19655,,,Tue Aug 25 14:21:27 UTC 2009,,,,,,,,,,"0|i0fyiv:",91191,,,,,Critical,,,,,,,,,,,,,,,,,"19/Aug/09 01:43;jbellis;02
    read(bytes) doesn't automatically throw EOFException if it reads less than asked for, so we need to check for that maniually

01
    avoid opening multiple writers for the current file; the buffered nature of the global
    logWriter_ could cause problems

james, in all likelihood the problem fixed by 02 is what you were running into, and that patch should fix replaying your old commitlog as-is (magic! :);;;","25/Aug/09 04:12;sammy.yu;+1 on both 0001 and 0002 patches
;;;","25/Aug/09 04:33;jbellis;committed;;;","25/Aug/09 05:14;jbellis;patch 01 causes issues.  reverted.  (02 is still applied.);;;","25/Aug/09 05:30;jbellis;fixed -- wasn't seeking back to current-end-of-CL in the header update code, so it would start clobbering the hell out of itself as soon as that code path ran.;;;","25/Aug/09 05:57;sammy.yu;01-fixed.diff +1   Tested it for real this time.  Unit test passes and our production system no longer exhibits this bad behavior.


;;;","25/Aug/09 06:03;jbellis;committed;;;","25/Aug/09 22:21;hudson;Integrated in Cassandra #177 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/177/])
    [fixed version] avoid opening multiple writers for the current file; the buffered nature of the global logWriter_ could cause problems
patch by jbellis; reviewed for  by Sammy Yu
Revert "" avoid opening multiple writers for the current file; the buffered nature of the global logWriter_ could cause problems""
read(bytes) doesn't automatically throw EOFException if it reads less than asked for, so we need to check for that.
patch by jbellis; reviewed by Sammy Yu for 
 avoid opening multiple writers for the current file; the buffered nature of the global logWriter_ could cause problems
patch by jbellis; reviewed by Sammy Yu for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
compilation fails in eclipse,CASSANDRA-86,12423025,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,junrao,junrao,junrao,17/Apr/09 00:45,16/Apr/19 17:33,22/Mar/23 14:57,17/Apr/09 02:24,,,,,0,,,,,,"Although ant build works, CassandraServer,java fails to compile in eclipse. Complaining about missing com.facebook.thrift.TException.",Eclipse on Windows,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/09 01:32;jbellis;cassandra86-v2.patch;https://issues.apache.org/jira/secure/attachment/12405673/cassandra86-v2.patch","17/Apr/09 00:49;junrao;cassandra86.patch;https://issues.apache.org/jira/secure/attachment/12405666/cassandra86.patch",,,,,,,,,,,,,2.0,junrao,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19542,,,Thu Apr 16 17:46:10 UTC 2009,,,,,,,,,,"0|i0fwsf:",90910,,,,,Normal,,,,,,,,,,,,,,,,,"17/Apr/09 00:49;junrao;Attach a patch. It seems the problem is when referring to FacebookBase, which is probably compiled with the old thrift package. 

Going forward, we should either include the source code in libfb303.jar or remove all dependencies on it.
;;;","17/Apr/09 01:32;jbellis;Like I said in IRC, I prefer to get rid of it entirely.  Does the attached patch look OK to you?;;;","17/Apr/09 01:46;junrao;V2 patch looks good to me. We should also remove lib/libfb303.jar.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
memtable_flush_after_mins setting not working,CASSANDRA-2183,12498922,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,chenyy,chenyy,18/Feb/11 00:51,16/Apr/19 17:33,22/Mar/23 14:57,23/Feb/11 12:57,0.7.3,,,,0,,,,,,"We have observed the behavior that memtable_flush_after_mins setting not working occasionally.   After some testing and code digging, we finally figured out what going on.
The memtable_flush_after_mins won't work on certain condition with current implementation in Cassandra.

In org.apache.cassandra.db.Table,  the scheduled flush task is setup by the following code during construction.

------------------------------------------------------------------------------------------------------------------
int minCheckMs = Integer.MAX_VALUE;
       
for (ColumnFamilyStore cfs : columnFamilyStores.values())  
{
    minCheckMs = Math.min(minCheckMs, cfs.getMemtableFlushAfterMins() * 60 * 1000);
}

Runnable runnable = new Runnable()
{
   public void run()
   {
       for (ColumnFamilyStore cfs : columnFamilyStores.values())
       {
           cfs.forceFlushIfExpired();
       }
   }
};
flushTask = StorageService.scheduledTasks.scheduleWithFixedDelay(runnable, minCheckMs, minCheckMs, TimeUnit.MILLISECONDS);
------------------------------------------------------------------------------------------------------------------------------

Now for our application, we will create a keyspacewithout without any columnfamily first.  And only add needed columnfamily later depends on request.

However, when keyspacegot created (without any columnfamily ), the above code will actually schedule a fixed delay flush check task with Integer.MAX_VALUE ms
since there is no columnfamily yet.

Later when you add columnfamily to this empty keyspace, the initCf() method in Table.java doesn't check whether the scheduled flush check task interval need
to be updated or not.   To fix this, we'd need to restart the Cassandra after columnfamily added into the keyspace. 

I would suggest that add additional logic in initCf() method to recreate a scheduled flush check task if needed.
",,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,"18/Feb/11 00:57;jbellis;2183.txt;https://issues.apache.org/jira/secure/attachment/12471294/2183.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20486,,,Wed Feb 23 15:18:09 UTC 2011,,,,,,,,,,"0|i0g9tr:",93022,,scode,,scode,Low,,,,,,,,,,,,,,,,,"18/Feb/11 00:57;jbellis;The original approach of checking barely more often than we think is necessary is overengineering the problem; even with 1000s of CFs, checking every 10s would have no affect whatsoever on performance.  Patch attached.;;;","22/Feb/11 06:25;scode;(was asked to review) I agree on the overengineering. Patch looks good to me. The obvious caveat of being limited to 10 second resolution should be utterly irrelevant for the purposes for which the time based flushing is intended.
;;;","23/Feb/11 12:57;jbellis;committed;;;","23/Feb/11 23:18;hudson;Integrated in Cassandra-0.7 #309 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/309/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expring columns can expire between the two phase of LazilyCompactedRow.,CASSANDRA-2349,12501688,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,slebresne,slebresne,slebresne,17/Mar/11 22:37,16/Apr/19 17:33,22/Mar/23 14:57,18/Mar/11 12:31,0.7.5,,,,0,,,,,,"LazilyCompactedRow reads the columns to compact twice. First to create the index, bloom filter and calculate the data size, and then another phase to actually write the columns. But a column can expire between those two phase, which will result in a bad data size in the sstable (and a possibly corrupted row index).",,,,,,,,,,,,,,,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,,,,,"18/Mar/11 01:25;slebresne;0001-Introduce-expireBefore-and-keep-it-constant-all-thro.patch;https://issues.apache.org/jira/secure/attachment/12473924/0001-Introduce-expireBefore-and-keep-it-constant-all-thro.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20572,,,Mon Mar 21 22:15:38 UTC 2011,,,,,,,,,,"0|i0gav3:",93190,,jbellis,,jbellis,Critical,,,,,,,,,,,,,,,,,"17/Mar/11 22:40;slebresne;This could be fixed by making LazilyCompactedRow do only one phase. We could write a place holder header, write the column, and seek back at the end to write the header. The tricky part is that this won't fit well with AbstractCompactedRow, since it is assume that the column count is known before write is called.;;;","17/Mar/11 23:07;slebresne;Actually, this won't work because we cannot anticipate the index size.;;;","17/Mar/11 23:56;slebresne;Attaching simpler solution consisting in not transforming expired columns into tombstones, so that the second phase of LazilyCompactedRow sees the same columns than the first phase.

I would be happy to come up with a unit test for this, but this is a subtle race condition and I'm really not sure how to write a test that capture it.;;;","18/Mar/11 00:11;jbellis;don't we need forceKeepExpired for both passes or we have the opposite problem?  If so, would prefer fKE a constructor arg for the iterator instead of setter.;;;","18/Mar/11 00:14;jbellis;Or: should we have an expireBefore parameter like gcBefore so it can stay constant during the compaction?;;;","18/Mar/11 00:27;slebresne;bq. don't we need forceKeepExpired for both passes or we have the opposite problem? If so, would prefer fKE a constructor arg for the iterator instead of setter.

There is no opposite problem, since you can't have a column expired for the first phase but not for the second one.

Note that the point of transforming expired columns to tombstone is to re-gain quickly the disk space used by the column value (without having to wait for gcGrace). So using fKE in phase 1 would prevent that for LazilyCompactedRow, which would be a pity.;;;","18/Mar/11 00:47;jbellis;bq. There is no opposite problem, since you can't have a column expired for the first phase but not for the second one.

But you can have an expired column counted as a tombstone for the first but not in the second since you have enabled forceKE.  No?

bq. the point of transforming expired columns to tombstone is to re-gain quickly the disk space used by the column value

Right, which is why using a constant expireBefore value that we pass to the serializer instead of computing it locally each time seems like a better solution. ;;;","18/Mar/11 00:58;slebresne;bq. But you can have an expired column counted as a tombstone for the first but not in the second since you have enabled forceKE. No?

You are right. Oups.

You are right, having an expireBefore is a better solution. I'll work out the patch.;;;","18/Mar/11 01:25;slebresne;Attached patch using the expireBefore idea.;;;","18/Mar/11 12:31;jbellis;committed;;;","22/Mar/11 06:15;hudson;Integrated in Cassandra-0.7 #397 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/397/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NameSortTest.testNameSort100 fails,CASSANDRA-1044,12463547,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,gdusbabek,johanoskarsson,johanoskarsson,03/May/10 17:09,16/Apr/19 17:33,22/Mar/23 14:57,23/May/10 01:18,,,,,0,,,,,,"The hudson build is failing due to this test failing: NameSortTest.testNameSort100
http://hudson.zones.apache.org/hudson/job/Cassandra/422/testReport/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19971,,,Mon May 03 13:35:42 UTC 2010,,,,,,,,,,"0|i0g2nr:",91861,,,,,Normal,,,,,,,,,,,,,,,,,"03/May/10 21:35;gdusbabek;I went ahead and committed a change to increase the timeout to 60s to see if that helps.  That test takes nowhere near 30s on my development machine though, so I don't know what hudson's problem is.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rebuffer called excessively during seeks,CASSANDRA-2581,12505751,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,lenn0x,lenn0x,lenn0x,29/Apr/11 12:34,16/Apr/19 17:33,22/Mar/23 14:57,03/May/11 23:50,0.7.6,0.8.0,,,0,,,,,,"When doing an strace tonight, I noticed during memtable flushes that we were only writing 1KB per every write() system call...After diving more into it, it's because of a bug in the seek() code. 

if (newPosition >= bufferOffset + validBufferBytes || newPosition < bufferOffset)

vs.

if (newPosition > (bufferOffset + validBufferBytes) || newPosition < bufferOffset)

Two things I noticed, we shouldn't need to rebuffer if newPosition is equal to bufferOffset + validBufferBytes, second the evaluation was doing (newPosition >= bufferOffset) + validBufferBytes which always seemed to be true.
",,alanliang,cburroughs,doubleday,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Apr/11 12:37;lenn0x;0001-Rebuffer-called-excessively-during-seeks.patch;https://issues.apache.org/jira/secure/attachment/12477725/0001-Rebuffer-called-excessively-during-seeks.patch","29/Apr/11 23:53;jbellis;2581.txt;https://issues.apache.org/jira/secure/attachment/12477807/2581.txt",,,,,,,,,,,,,2.0,lenn0x,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20711,,,Tue May 03 16:04:04 UTC 2011,,,,,,,,,,"0|i0gc7z:",93410,,xedin,,xedin,Low,,,,,,,,,,,,,,,,,"29/Apr/11 23:53;jbellis;bq. we shouldn't need to rebuffer if newPosition is equal to bufferOffset + validBufferBytes

I think that is correct, patch attached. I will ask Pavel to also review since this is extremely important not to break.

bq. second the evaluation was doing (newPosition >= bufferOffset) + validBufferBytes

No, addition is higher precedence than comparison (or the logical operations). In fact if you force the grouping you suggest, javac will reject it since you cannot add a boolean and an int.;;;","03/May/11 18:49;xedin;+1, we can just change ""current"" if we point on the last byte of the the buffer.;;;","03/May/11 21:57;jbellis;bq. we can just change ""current"" if we point on the last byte of the the buffer

Meaning you suggest an additional code change?;;;","03/May/11 22:01;xedin;No, I mean that the patch is correct, sorry :);;;","03/May/11 23:50;jbellis;committed goffinet's patch with the additional parentheses for clarity;;;","04/May/11 00:04;hudson;Integrated in Cassandra-0.7 #466 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/466/])
    fix excessively pessimistic rebuffering in BRAF writes
patch by goffinet; reviewed by jbellis and Pavel Yaskevich for CASSANDRA-2581
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ColumnFamilyOutputFormat only writes one column (per key),CASSANDRA-1774,12480793,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,mck,mck,mck,24/Nov/10 20:17,16/Apr/19 17:33,22/Mar/23 14:57,26/Nov/10 01:24,0.7.0 rc 2,,,,0,,,,,,"From mailing list http://thread.gmane.org/gmane.comp.db.cassandra.user/10385

ColumnFamilyOutputFormat will only write out one column
per key.

Alex Burkoff also reported this nearly two months ago, but nobody ever
replied...
 http://article.gmane.org/gmane.comp.db.cassandra.user/9325

has anyone any ideas? 
should it be possible to write multiple columns out?

This is very easy to reproduce. Use the contrib/wordcount example, with
OUTPUT_REDUCER=cassandra and in WordCount.java add at line 132

>              results.add(getMutation(key, sum));
> +            results.add(getMutation(new Text(""doubled""), sum*2));

Only the last mutation for any key seems to be written.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Nov/10 15:34;mck;CASSANDRA-1774.patch;https://issues.apache.org/jira/secure/attachment/12460435/CASSANDRA-1774.patch",,,,,,,,,,,,,,1.0,mck,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20306,,,Sat Dec 11 07:35:15 UTC 2010,,,,,,,,,,"0|i0g7bb:",92615,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"25/Nov/10 15:34;mck;The problem was the list of mutations inside the Map<ByteBuffer, Map<String, List<Mutation>>> that is sent to batch_mutate(..) isn't appended to, instead it was overridden.

This patch allows subsequent mutations (under the same columnFamily and key) to be appended to the existing list.;;;","26/Nov/10 01:24;jbellis;Thanks for the fix!  Committed w/ minor change to emphasize that subBatch is only created once.;;;","26/Nov/10 07:19;karthick;+1 on the patch.;;;","11/Dec/10 15:35;hudson;Integrated in Cassandra-0.7 #70 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/70/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Read operation with ConsistencyLevel.ALL throws exception,CASSANDRA-1152,12466039,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,gdusbabek,yukim,yukim,03/Jun/10 10:18,16/Apr/19 17:33,22/Mar/23 14:57,03/Jun/10 22:20,0.6.3,0.7 beta 1,,,0,,,,,,"Read operations which use thrift.CassandraServer#readColumnFamily should allow consistency_level == ALL.
Current implementation just throws InvalidRequestException when consistency level is ALL.
Same thing applies to avro implementation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jun/10 11:36;dylanegan;read_column_family_on_all;https://issues.apache.org/jira/secure/attachment/12446220/read_column_family_on_all","03/Jun/10 11:36;dylanegan;read_column_family_on_all_inline_with_thrift;https://issues.apache.org/jira/secure/attachment/12446219/read_column_family_on_all_inline_with_thrift",,,,,,,,,,,,,2.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20011,,,Thu Jun 03 17:27:20 UTC 2010,,,,,,,,,,"0|i0g3bb:",91967,,,,,Normal,,,,,,,,,,,,,,,,,"03/Jun/10 11:36;dylanegan;Created two patches to alleviate the ALL problem, but one of the patches brings the Avro interface inline with thrift in not supporting CL.ANY on read operations too.

Im not really following the Avro updates, but I assume the interface should be the same.;;;","03/Jun/10 22:20;gdusbabek;Committed.  Thanks for the patch.;;;","04/Jun/10 01:13;dylanegan;Hi Gary,

Just to help me understand, why would the Avro interface be different to the Thrift one in terms of what is supported at the read level.

Cheers,

Dylan.;;;","04/Jun/10 01:21;gdusbabek;My bad.  I intended to remove the CL.ANY block from avro and keep the CL.ALL block out.  CL.ANY isn't in our avro generation file yet.;;;","04/Jun/10 01:27;gdusbabek;Recommitted.  Thanks Dylan!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect parameter names in Thrift interface,CASSANDRA-214,12427125,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,tve,tve,04/Jun/09 23:33,16/Apr/19 17:33,22/Mar/23 14:57,06/Jun/09 03:44,0.4,,,,0,,,,,,"The Thrift interface is incorrect as follows. This does not affect correctnes, it just makes it hard to understand the operations.

The Thrift interface for get_superColumn is incorrect. It seems to me that ""3:string columnFamily"" should really be ""3:string columnFamily_superColumnName"" (I know this doesn't have any functional impact, just makes it hard to understand what the operation does)

The Thrift interface for get_slice_super is incorrect. It seems to me that ""3:string columnFamily_superColumnName"" should really be ""3:string columnFamily""
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Jun/09 22:25;jbellis;214-v2.patch;https://issues.apache.org/jira/secure/attachment/12409988/214-v2.patch","05/Jun/09 00:13;jbellis;214.patch;https://issues.apache.org/jira/secure/attachment/12409887/214.patch",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19597,,,Fri Jun 05 19:44:09 UTC 2009,,,,,,,,,,"0|i0fxkf:",91036,,,,,Low,,,,,,,,,,,,,,,,,"05/Jun/09 01:38;junrao;The patch looks fine.

Now that we are on Thrift api. 
3:string columnFamily_column in get_slice is also confusing. In a regular CF, the parameter should be just CF. In a super CF, the parameter could also be CF_superColumn. Neither matches the current parameter name. Since most users are likely on regular CF, may be we should rename 3 to just columnFamily? 

Similarly, I propose changing 3:string columnFamily_column in get_column_count to just columnFamily.
 ;;;","05/Jun/09 02:00;jbellis;i don't know that being wrong X% of the time is that much better than 100 - X% :)

what if we name it something more generic?  `columnFamily_params`, or `column_path`?;;;","05/Jun/09 02:16;junrao;How about columnFamily__OR__columnFamily_superColumn ?
;;;","05/Jun/09 02:25;jbellis;better, but a little unwieldy :)

columnParent?  columnOwner?  columnContainer?;;;","05/Jun/09 04:58;tve;Maybe instead of focusing on the name it would be good to first focus on the actual spec? I.e. in which operations can which forms of column / super column spec be used? IMHO it would be more important to add this in the form of comments in the interface spec than trying to come up with names that capture all this in_one_word...;;;","05/Jun/09 05:16;sandeep_tata;There are a bunch of places in the code that have the same problem -- it is not clear if a variable should contain ""CF:col"" or ""CF"" ... we should slowly clean that up using whatever naming convention we pick here.  (In addition to writing better comments.);;;","05/Jun/09 05:19;jbellis;we ought to be able to do better than that in the code w/ overloaded methods that each have a real signature where a string is a string not two connected with a colon :);;;","05/Jun/09 22:25;jbellis;Okay, second stab here at the interface file.  I'll update CassandraServer if these are reasonable.

for convenience, here is the terminology used in the patch:

# CF = ColumnFamily name
# SC = SuperColumn name
# C = Column name
# columnParent: the parent of the columns you are specifying.  ""CF"" or ""CF:SC"".
# columnPath: full path to a column.  ""CF:C"" or ""CF:SC:C"".
# superColumnPath: full path to a supercolumn.  ""CF:SC"" only.
# columnPathOrParent: remove will wipe out any layer.  ""CF"" or ""CF:C"" or ""CF:SC"" or ""CF:SC:C"".
;;;","06/Jun/09 00:46;junrao;Looks fine to me in general. For get_columns_since, 3:string columnPath should be columnFamily.;;;","06/Jun/09 01:03;jbellis;can't you do a _since on a supercolumn?  the subcolumns are sorted by time.;;;","06/Jun/09 01:43;junrao;It seems get_slice_since can be used on a SCF. So the parameter should be columnParent instead.;;;","06/Jun/09 03:44;jbellis;You're right, should be columnParent.

Committed w/ that change.

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove loadbalance command,CASSANDRA-2448,12503953,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,nickmbailey,nickmbailey,nickmbailey,12/Apr/11 01:00,16/Apr/19 17:33,22/Mar/23 14:57,12/Apr/11 01:35,0.8 beta 1,,Legacy/Tools,,0,,,,,,"With the update to how the move command works, the loadbalance command is even less useful that it was previously.  The loadbalance command now calculates the token it is going to move to before it leaves which means it isn't considering the load it is giving away. Given that, I think we should just remove the loadbalance command entirely. Anyone who wants to do an old style loadbalance can just do decommission then bootstrap.

This is a minor change, and honestly I think it might count as a 'bug' so I think we should squeeze it into 0.8, post-freeze. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Apr/11 01:00;nickmbailey;0001-Remove-loadbalance-command-from-nodetool.patch;https://issues.apache.org/jira/secure/attachment/12476019/0001-Remove-loadbalance-command-from-nodetool.patch",,,,,,,,,,,,,,1.0,nickmbailey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20630,,,Wed Apr 20 23:46:43 UTC 2011,,,,,,,,,,"0|i0gbfj:",93282,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"12/Apr/11 01:35;jbellis;lgtm, committed;;;","12/Apr/11 03:11;nickmbailey;this should also be applied to trunk;;;","12/Apr/11 03:36;jbellis;merging to trunk is asynchronous but will happen;;;","21/Apr/11 07:43;cdaw;Not sure how documentation gets updated, but we still mention using the loadbalance command on the wiki:

http://wiki.apache.org/cassandra/Operations#Moving_or_Removing_nodes;;;","21/Apr/11 07:46;nickmbailey;Updated the wiki to indicate that the loadbalance command is only available in versions 0.7.* and lower.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When you omit keyspace in the ""show keyspace"" command in the CLI your connection gets terminated",CASSANDRA-551,12440652,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,hsbis,hsbis,hsbis,14/Nov/09 03:18,16/Apr/19 17:33,22/Mar/23 14:57,14/Nov/09 05:16,0.5,,,,0,,,,,,"$ bin/cassandra-cli --host localhost --port 9160

cassandra> describe keyspace          
line 0:-1 mismatched input '<EOF>' expecting Identifier
Exception Required field 'keyspace' was not present! Struct: describe_keyspace_args(keyspace:null)
org.apache.thrift.protocol.TProtocolException: Required field 'keyspace' was not present! Struct: describe_keyspace_args(keyspace:null)
	at org.apache.cassandra.service.Cassandra$describe_keyspace_args.validate(Cassandra.java:10723)
	at org.apache.cassandra.service.Cassandra$describe_keyspace_args.write(Cassandra.java:10692)
	at org.apache.cassandra.service.Cassandra$Client.send_describe_keyspace(Cassandra.java:558)
	at org.apache.cassandra.service.Cassandra$Client.describe_keyspace(Cassandra.java:549)
	at org.apache.cassandra.cli.CliClient.executeDescribeTable(CliClient.java:259)
	at org.apache.cassandra.cli.CliClient.executeCLIStmt(CliClient.java:75)
	at org.apache.cassandra.cli.CliMain.processCLIStmt(CliMain.java:108)
	at org.apache.cassandra.cli.CliMain.main(CliMain.java:148)
cassandra> describe keyspace Keyspace1
Exception Cannot read. Remote side has closed. Tried to read 4 bytes, but only got 0 bytes.
org.apache.thrift.transport.TTransportException: Cannot read. Remote side has closed. Tried to read 4 bytes, but only got 0 bytes.
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:314)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:262)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:192)
	at org.apache.cassandra.service.Cassandra$Client.recv_describe_keyspace(Cassandra.java:565)
	at org.apache.cassandra.service.Cassandra$Client.describe_keyspace(Cassandra.java:550)
	at org.apache.cassandra.cli.CliClient.executeDescribeTable(CliClient.java:259)
	at org.apache.cassandra.cli.CliClient.executeCLIStmt(CliClient.java:75)
	at org.apache.cassandra.cli.CliMain.processCLIStmt(CliMain.java:108)
	at org.apache.cassandra.cli.CliMain.main(CliMain.java:148)
","Ubuntu, java 6",,,,,,,,,,,,,,,,,,,,,,600,600,,0%,600,600,,,,,,,,,,,,,,,,,,,,"14/Nov/09 04:14;hsbis;CASSANDRA-551.patch;https://issues.apache.org/jira/secure/attachment/12424889/CASSANDRA-551.patch",,,,,,,,,,,,,,1.0,hsbis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19751,,,Sat Nov 14 12:33:58 UTC 2009,,,,,,,,,,"0|i0fzmn:",91370,,,,,Low,,,,,,,,,,,,,,,,,"14/Nov/09 04:14;hsbis;Now the users gets an error message and the connection isn't terminated.;;;","14/Nov/09 05:16;urandom;committed; thanks;;;","14/Nov/09 20:33;hudson;Integrated in Cassandra #258 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/258/])
    gracefully handle missing keyspace argument (cli)

Patch by Hafsteinn Baldvinsson; reviewed by eevans for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL does not preserve column order in select statement,CASSANDRA-2493,12504467,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,17/Apr/11 11:36,16/Apr/19 17:33,22/Mar/23 14:57,17/Apr/11 13:10,0.8 beta 1,,Legacy/CQL,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/11 12:44;jbellis;2493.txt;https://issues.apache.org/jira/secure/attachment/12476548/2493.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20653,,,Sun Apr 17 06:24:46 UTC 2011,,,,,,,,,,"0|i0gbov:",93324,,urandom,,urandom,Low,,,,,,,,,,,,,,,,,"17/Apr/11 11:48;jbellis;patch to preserve column order.  columns that do not exist in a given row come back as null; this requires changing value and ts fields in thrift Column struct to optional. manual checking for set-ness is added to ThriftValidation to make up for this.;;;","17/Apr/11 11:53;jbellis;updated patch fixes uses of Column constructor that no longer exists;;;","17/Apr/11 12:44;jbellis;new patch also fixes bugs in CassandraResultSet & updates jdbc tests for new behavior;;;","17/Apr/11 12:55;urandom;lgtm. +1;;;","17/Apr/11 13:10;jbellis;committed;;;","17/Apr/11 14:24;hudson;Integrated in Cassandra-0.8 #12 (See [https://hudson.apache.org/hudson/job/Cassandra-0.8/12/])
    preserve column order in CQL result sets
patch by jbellis; reviewed by eevans for CASSANDRA-2493
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bootstrapping is not threadsafe,CASSANDRA-779,12455685,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,gdusbabek,gdusbabek,gdusbabek,09/Feb/10 05:21,16/Apr/19 17:33,22/Mar/23 14:57,09/Feb/10 05:55,0.6,,,,0,,,,,,"The bootstrapper thread (called from the main thread which has acquired the lock for SS via SS.init) currently makes a few calls into SS that require its lock.

Those methods need to be thread-safe, but do not need the same lock required by SS.init.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Feb/10 05:25;gdusbabek;0001-fix-bootstrapping-deadlock.patch;https://issues.apache.org/jira/secure/attachment/12435205/0001-fix-bootstrapping-deadlock.patch",,,,,,,,,,,,,,1.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19860,,,Wed Feb 17 17:54:43 UTC 2010,,,,,,,,,,"0|i0g10v:",91596,,,,,Normal,,,,,,,,,,,,,,,,,"09/Feb/10 05:27;gdusbabek;This fixes the problem, but I don't see the point of having Bootstrapper launch a separate thread to make the file requests while SS.init waits for bootstrapping to finish.  Its work could mostly be replaced with a method in SS that calls into the static methods currently in Bootstrapper.;;;","09/Feb/10 05:34;jbellis;+1 on the patch.  I'm not sure what the point of having the separate BS thread is, either.  Maybe it made more sense a dozen refactors ago. :);;;","09/Feb/10 05:55;gdusbabek;r907816;;;","18/Feb/10 01:54;hudson;Integrated in Cassandra #357 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/357/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make iterator-based read code the One True Path,CASSANDRA-287,12429949,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,09/Jul/09 23:14,16/Apr/19 17:33,22/Mar/23 14:57,15/Jul/09 05:17,0.4,,,,0,,,,,,"Since CASSANDRA-172 we've had two read paths; the old, ad-hoc path based on the faulty assumption that we could skip checking older sstables if we got a hit earlier in the path (fixed in CASSANDRA-223 but still bearing the marks of its origin) and the new iterator-based path.

This makes all read operations go through the iterator path, which cleans things up enormously and sets the stage for CASSANDRA-139.

I introduce a new QueryFilter interface, which has 3 main methods:

    /**
     * returns an iterator that returns columns from the given memtable
     * matching the Filter criteria in sorted order.
     */
    public abstract ColumnIterator getMemColumnIterator(Memtable memtable);

    /**
     * returns an iterator that returns columns from the given SSTable
     * matching the Filter criteria in sorted order.
     */
    public abstract ColumnIterator getSSTableColumnIterator(SSTableReader sstable) throws IOException;

    /**
     * subcolumns of a supercolumn are unindexed, so to pick out parts of those we operate in-memory.
     * @param superColumn
     */
    public abstract void filterSuperColumn(SuperColumn superColumn);

The first two are for pulling out indexed top-level columns, from a memtable or an sstable, respectively.

If the query is on subcolumns of a supercolumn, which are unindexed, CFS.getColumnFamily does an indexed Name filter on the supercolumn, then asks filterSuperColumn on the primary QueryFilter to pick out the parts the user is requesting.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Jul/09 23:25;jbellis;0007-fixes.patch;https://issues.apache.org/jira/secure/attachment/12413127/0007-fixes.patch","10/Jul/09 23:26;jbellis;0008-fix-empty-CF-handling-should-always-be-null.patch;https://issues.apache.org/jira/secure/attachment/12413128/0008-fix-empty-CF-handling-should-always-be-null.patch","09/Jul/09 23:16;jbellis;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-287-r-m-unused-and-dangerous-RowReadComman.txt;https://issues.apache.org/jira/secure/attachment/12413023/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-287-r-m-unused-and-dangerous-RowReadComman.txt","09/Jul/09 23:16;jbellis;ASF.LICENSE.NOT.GRANTED--0002-rename-lock_-sstableLock_.txt;https://issues.apache.org/jira/secure/attachment/12413024/ASF.LICENSE.NOT.GRANTED--0002-rename-lock_-sstableLock_.txt","09/Jul/09 23:16;jbellis;ASF.LICENSE.NOT.GRANTED--0003-refactor-out-QueryFilter-SliceQueryFilter.txt;https://issues.apache.org/jira/secure/attachment/12413025/ASF.LICENSE.NOT.GRANTED--0003-refactor-out-QueryFilter-SliceQueryFilter.txt","09/Jul/09 23:16;jbellis;ASF.LICENSE.NOT.GRANTED--0004-replace-namesfilter-with-NamesQueryFilter.-mv-filter.txt;https://issues.apache.org/jira/secure/attachment/12413026/ASF.LICENSE.NOT.GRANTED--0004-replace-namesfilter-with-NamesQueryFilter.-mv-filter.txt","09/Jul/09 23:16;jbellis;ASF.LICENSE.NOT.GRANTED--0005-nuke-timefilter.txt;https://issues.apache.org/jira/secure/attachment/12413027/ASF.LICENSE.NOT.GRANTED--0005-nuke-timefilter.txt","09/Jul/09 23:16;jbellis;ASF.LICENSE.NOT.GRANTED--0006-add-IdentityQueryFilter-finish-removing-IFilter.txt;https://issues.apache.org/jira/secure/attachment/12413028/ASF.LICENSE.NOT.GRANTED--0006-add-IdentityQueryFilter-finish-removing-IFilter.txt",,,,,,,8.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19622,,,Fri Jul 10 17:49:00 UTC 2009,,,,,,,,,,"0|i0fy0f:",91108,,,,,Normal,,,,,,,,,,,,,,,,,"09/Jul/09 23:17;jbellis;(01 and 02 are automated refactorings.)

06
    add IdentityQueryFilter; finish removing IFilter

05
    add TimeQueryFilter and nuke Timefilter

04
    replace namesfilter with NamesQueryFilter.  mv filter code into separate package.

03
    refactor out QueryFilter, SliceQueryFilter

02
    rename lock_ -> sstableLock_

01
    CASSANDRA-287 r/m unused (and dangerous) RowReadCommand
;;;","10/Jul/09 07:04;junrao;Review comments:
1. 0006 patch breaks compilation in unit test (merge conflicts included in the patch)
2. We should remove unused CMD_TYPE_GET_ROW,GET_SLICE AND SLICE_BY_RANGE in ReadCommand and renumber the rest.
3. SystemTable.StorageMetadata, 2nd line, change IdentityQueryFilter  to NamesQueryFilter.
4. What happens to those updateReadStatistics calls in various getRow in Table? Are they just got removed?
5. Need to add @override to all overridden methods in various filters class.
;;;","10/Jul/09 08:19;jbellis;thanks for having a look, Jun.

1. oops. one too many rebases before submitting.  fixed.
2. fixed; if I'm feeling extra energetic I'll make it an enum
3. hmm, I'll need to take a closer look at this one -- making that change breaks SystemTableTest
4. yes, so I'm cheating a bit here to make the merge easier -- this is partially done in https://issues.apache.org/jira/browse/CASSANDRA-272 and will be completed in another patch
5. IMO it's bad form to @override an abstract method; are there others that I omitted?;;;","10/Jul/09 23:25;jbellis;07 fixes points 1 and 2;;;","10/Jul/09 23:26;jbellis;08 fixes point 3.  the problem was inconsistent handling of empty CFs -- to be consistent with the rest of cassandra, following the original FB conventions, an empty CF should be represented with a null reference.

(IMO this is the wrong convention to pick but changing that is outside my scope for this ticket.);;;","11/Jul/09 01:12;junrao;The new patch looks good.;;;","11/Jul/09 01:49;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Load spikes,CASSANDRA-2170,12498681,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,jbellis,jbellis,16/Feb/11 03:46,16/Apr/19 17:33,22/Mar/23 14:57,05/Oct/11 01:28,,,,,0,,,,,,"as reported on CASSANDRA-2058, some users are still seeing load spikes on 0.6.11, even with fairly low-volume read workloads.",,bcoverston,kzadorozhny,scode,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-2357,,,,,,,,,,,,,,,,,,,,,,0.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,18533,,,Wed Oct 12 17:46:46 UTC 2011,,,,,,,,,,"0|i0g9qv:",93009,,,,,Normal,,,,,,,,,,,,,,,,,"16/Feb/11 03:48;jbellis;AFAIK nobody has seen this on 0.7.1.;;;","10/Mar/11 01:16;jbellis;I wonder if this could have been CASSANDRA-2175.;;;","10/Mar/11 01:16;jbellis;(I.e. memory pressure caused by key cache preheating being too aggressive.);;;","14/Mar/11 21:45;jbellis;No new reports of similar behavior on 0.6.11 or 0.6.12;;;","19/May/11 06:40;brandon.williams;I never saw anyone reliably report this on any platform except ec2, so I strongly the suspect the cause was what is covered in this link:

https://silverline.librato.com/blog/main/EC2_Users_Should_be_Cautious_When_Booting_Ubuntu_10_04_AMIs;;;","10/Aug/11 13:08;alienth;Re-opening per request of driftx.

So, still seeing this problem ever since our upgrade from 0.6.7.

It is 100% consistent on 0.7.1, 0.7.2, 0.7.3, 0.7.4, 0.8.0, 0.8.1. I've tried Sun JRE and OpenJDK. Tried with JNA and without. Tried Ubuntu 08.04/10.04/10.10/11.04, as well as RHEL5.1. It *only* happens on coordinator nodes.

For the 0.8 ring, I created a brand new ring and added data from our app one CF at a time. As soon as I added a busy CF, the problem popped up again. The load on the boxes in the new ring is under 1 all the time, except for when the load spike occurs.;;;","16/Aug/11 06:23;brandon.williams;From the comment [here|https://issues.apache.org/jira/browse/CASSANDRA-2845?focusedCommentId=13083455&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13083455] I would suggest try running without jsvc (if you are now and have JNA enabled);;;","16/Sep/11 03:34;alienth;Tried without jsvc, same result.

This problem is also consistent on 0.8.5 on Debian Squeeze. I replicated the issue on a coordinator which only owned 1 token.

The node I'm replicating it on now is at a load of 0 almost all of the time, except during the spikes.

The symptoms continue to appear identical to #2054;;;","17/Sep/11 05:27;alienth;I did a couple more thread dumps on spiking nodes. One interesting thing I'm seeing is that there are a high number of NBHM threads in the runnable state during the load spikes. One the dumps I analyzed, there were often 200-300 of these threads in RUNNABLE.

Here is an example of the threads: https://gist.github.com/ef215227b85bdff5f033;;;","18/Sep/11 04:13;scode;Wow, interesting. Are you sure it's 0.8.5 though? The stack trace is not matching what I see in the 0.8.5 tag (mismatched line number for MessagingService.addCallback()).

We've been seeing load spikes on 0.7, but havent reported it because it's such an old version. However we were never able to grab stacks because no JMX query would ever succeed during this condition.

The stack trace indicates it's stuck doing resize operations on the NBHM where each thread is trying to help the resizing operation along by performing potentially duplicate (for forward progress producing) work.

Do you have a list of all stacks? Do you find any thread (should be 0 or 1) that are executing in ExpiringMap.CacheMonitor.run() at the time of the load spikes?

I guess we're seeing some kind of fallen-and-cant-get-up senario having to do with the resize. Maybe dogpiling the resize is making it overall slow enough that it never gets unstuck without a temporary stop in incoming requests. Or some such. That's gut feely speculation without having actually looked at it carefully, so take it with a grain of salt :)
;;;","28/Sep/11 07:29;alienth;The stack trace in that example was from a 0.8.1 node. The same problem has occurred on my 0.8.5 nodes.

I checked the stack trace that I had, and I couldn't find any threads executing in any ExpiringMap stuff.

We switched to HSHA about a week ago and it appears to have resolved the loads pikes. Is the stack trace I gave interesting enough to pursue further investigation into the issue, or should I just leave the answer as 'HSHA' ?;;;","05/Oct/11 01:28;brandon.williams;bq. Is the stack trace I gave interesting enough to pursue further investigation into the issue, or should I just leave the answer as 'HSHA' ?

I'm content with calling this 'ec2 sucks when there are tons of threads.' (Jason told me he's opening 2000+ connections);;;","13/Oct/11 01:46;brandon.williams;FWIW, I was able to finally reproduce this by throwing a ton of connections at a cold cassandra instance.  Increasing the rpc_min_threads initially seems to help avoid locking the machine up briefly.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add then drop Keyspace without putting anything in it causes exception,CASSANDRA-1378,12471309,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,gdusbabek,jjordan,jjordan,11/Aug/10 23:33,16/Apr/19 17:33,22/Mar/23 14:57,14/Aug/10 02:01,0.7 beta 2,,,,0,,,,,,"The following from python causes an exception on apache-cassandra-2010-08-10_13-08-19-bin.tar.gz and a bunch of earlier builds in the 0.7 line:
        socket = TSocket.TSocket(host, 9160)
        transport = TTransport.TFramedTransport(socket)
        protocol = TBinaryProtocol.TBinaryProtocolAccelerated(transport)
        client = Cassandra.Client(protocol)
        transport.open()
        try:
            client.describe_keyspace(dbName)
        except NotFoundException, e:
            keyspaceDef = KsDef(name=dbName,
 
strategy_class='org.apache.cassandra.locator.RackUnawareStrategy',
                                replication_factor=replicationFactor,
                                cf_defs=[])
            client.set_keyspace('system')
            client.system_add_keyspace(keyspaceDef)

        try:
            client.describe_keyspace(dbName)
            client.set_keyspace('system')
            client.system_drop_keyspace(dbName)
        except NotFoundException, e:
            pass

The system_drop_keyspace throws:
InvalidRequestException(why='java.util.concurrent.ExecutionException:
java.lang.NullPointerException')

If I put a system_add_column_family in the middle it doesn't crash.
I think this broke sometime after apache-cassandra-2010-07-06_13-27-21",Single node.  Both Linux and Windows.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Aug/10 22:33;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-handle-graveyard-cleanups-gracefully-when-there-is-not.txt;https://issues.apache.org/jira/secure/attachment/12451903/ASF.LICENSE.NOT.GRANTED--v1-0001-handle-graveyard-cleanups-gracefully-when-there-is-not.txt",,,,,,,,,,,,,,1.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20111,,,Sat Aug 14 12:48:36 UTC 2010,,,,,,,,,,"0|i0g4ov:",92190,,,,,Low,,,,,,,,,,,,,,,,,"12/Aug/10 22:11;gdusbabek;full error:


ERROR [CompactionExecutor:1] 2010-08-12 08:59:31,088 CassandraDaemon.java (line 82) Uncaught exception in thread Thread[CompactionExecutor:1,5,main]
java.util.concurrent.ExecutionException: java.lang.NullPointerException
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:87)
	at org.apache.cassandra.db.CompactionManager$CompactionExecutor.afterExecute(CompactionManager.java:650)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:637)
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:90)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	... 2 more
ERROR [MIGRATION-STAGE:1] 2010-08-12 08:59:31,089 CassandraDaemon.java (line 82) Uncaught exception in thread Thread[MIGRATION-STAGE:1,5,main]
java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NullPointerException
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:87)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:637)
Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NullPointerException
	at org.apache.cassandra.db.migration.Migration.cleanupDeadFiles(Migration.java:246)
	at org.apache.cassandra.db.migration.DropKeyspace.applyModels(DropKeyspace.java:86)
	at org.apache.cassandra.db.migration.Migration.apply(Migration.java:156)
	at org.apache.cassandra.thrift.CassandraServer$2.call(CassandraServer.java:722)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	... 2 more
Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.db.migration.Migration.cleanupDeadFiles(Migration.java:238)
	... 8 more
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:90)
	... 5 more
;;;","14/Aug/10 00:44;jbellis;+1;;;","14/Aug/10 20:48;hudson;Integrated in Cassandra #514 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/514/])
    handle graveyard cleanups gracefully when there is nothing to delete. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1378
trap ConfigExceptions so they don't become RTEs. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1378
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
testGetCompactionBuckets sometimes fail,CASSANDRA-57,12422138,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,johanoskarsson,johanoskarsson,07/Apr/09 01:05,16/Apr/19 17:33,22/Mar/23 14:57,11/Apr/09 03:09,0.3,,,,0,,,,,,"testGetCompactionBuckets fails randomly on the exact same build of Cassandra. Not sure how to reproduce it.

The only log output I have:
   [testng] FAILED: testGetCompactionBuckets
   [testng] java.lang.AssertionError
   [testng]     at org.apache.cassandra.db.ColumnFamilyStoreTest.testGetCompactionBuckets(Unknown Source)
   [testng] ... Removed 22 stack frames
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Apr/09 02:24;jbellis;57.patch;https://issues.apache.org/jira/secure/attachment/12405178/57.patch",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19529,,,Fri Apr 10 19:09:15 UTC 2009,,,,,,,,,,"0|i0fwlz:",90881,,,,,Normal,,,,,,,,,,,,,,,,,"11/Apr/09 02:22;jbellis;something is broken in NonBlockingHashMap re removing and re-adding the same collection mid-iteration; it ends up with multiple references to that collection.  (I tried using iter.remove too; same problem.)  switch back to ConcurrentHashMap here; bloated memory overhead is a non-issue for this use case.;;;","11/Apr/09 03:06;urandom;Patch looks good to me. +1;;;","11/Apr/09 03:09;jbellis;applied;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JDBC ResultSet does not honor column value typing for the CF and uses default validator for all column value types.,CASSANDRA-2410,12503071,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,gdusbabek,ardot,ardot,01/Apr/11 04:55,16/Apr/19 17:33,22/Mar/23 14:57,17/Apr/11 07:10,0.8 beta 1,,Legacy/CQL,,0,cql,,,,,"Assume a CF declared in CQL as :
{code}
CREATE COLUMNFAMILY TestCF(KEY utf8 PRIMARY KEY,description utf8, anumber int)
  WITH comparator = ascii AND default_validation = long;
{code}

If the {{ResultSet}} is fetched thusly:


{code}
Statement stmt = con.createStatement();
ResultSet rs = stmt.executeQuery(query);

String description;
Integer anumber;

    while (rs.next())
    {
      description = rs.getString(1);
      System.out.print(""description : ""+ description);
      anumber = rs.getInt(2);
      System.out.print(""anumber     : ""+ anumber);
    }
{code}

It will immediately fail with a message of: 

{code}
org.apache.cassandra.db.marshal.MarshalException: A long is exactly 8 bytes: 16
	at org.apache.cassandra.db.marshal.LongType.getString(LongType.java:66)
	at org.apache.cassandra.cql.jdbc.TypedColumn.<init>(TypedColumn.java:45)
	at org.apache.cassandra.cql.jdbc.ColumnDecoder.makeCol(ColumnDecoder.java:158)
	at org.apache.cassandra.cql.jdbc.CassandraResultSet.next(CassandraResultSet.java:1073)
	at da.access.testing.TestJDBC.selectAll(TestJDBC.java:83)
         ...
{code}


It appears that the {{makeCol}} method of {{ColumnDecoder.java}} chooses NOT to use the {{CfDef}} to look up the possible occurrence of a column? That's not right. Right? 

{code}
    /** constructs a typed column */
    public TypedColumn makeCol(String keyspace, String columnFamily, byte[] name, byte[] value)
    {
        CfDef cfDef = cfDefs.get(String.format(""%s.%s"", keyspace, columnFamily));
        AbstractType comparator = getComparator(keyspace, columnFamily, Specifier.Comparator, cfDef);
        AbstractType validator = getComparator(keyspace, columnFamily, Specifier.Validator, null);
        return new TypedColumn(comparator, name, validator, value);
    }
{code}



",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/11 05:08;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-honor-specific-column-validators-in-JDBC-driver.txt;https://issues.apache.org/jira/secure/attachment/12476536/ASF.LICENSE.NOT.GRANTED--v1-0001-honor-specific-column-validators-in-JDBC-driver.txt",,,,,,,,,,,,,,1.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20607,,,Sun Apr 17 01:11:50 UTC 2011,,,,,,,,,,"0|i0gb7j:",93246,,,,,Low,,,,,,,,,,,,,,,,,"17/Apr/11 05:09;gdusbabek;I noticed a failure of org.apache.cassandra.db.RecoveryManagerTest after applying the patch.  Chances are it was failing before--I can't see how a jdbc change would affect that.;;;","17/Apr/11 05:57;jbellis;+1;;;","17/Apr/11 07:10;gdusbabek;committed.;;;","17/Apr/11 09:11;hudson;Integrated in Cassandra-0.8 #10 (See [https://hudson.apache.org/hudson/job/Cassandra-0.8/10/])
    honor specific column validators in JDBC driver. patch by gdusbabek, reviewed by jbellis. CASSANDRA-2410
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
intellibootstrap,CASSANDRA-385,12433531,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,20/Aug/09 11:59,16/Apr/19 17:33,22/Mar/23 14:57,08/Oct/09 00:24,0.5,,,,0,,,,,,"ideally bootstrap mode should determine its new position on the ring by examining the Load of the existing nodes in the cluster.  (currently Load is just disk space used but making this pluggable is another ticket.)  having found the highest-load-node-that-is-not-participating-in-bootstrap, it should ask that node for a Token which would move half the keys over to the new node.

This is easily computed since we have a periodic sampling of keys in memory of all the keys on disk, and even SSTable.getIndexedKeys that merges all such keys.  So pick the midpoint, and turn it into a token (these are decorated keys so that is always possible).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Oct/09 23:52;jbellis;0007-switch-to-messagingservice-to-get-bootstrap-token.patch;https://issues.apache.org/jira/secure/attachment/12421532/0007-switch-to-messagingservice-to-get-bootstrap-token.patch","07/Oct/09 23:52;jbellis;0008-rename-getMessagingInstance-instance-r-m-unused.patch;https://issues.apache.org/jira/secure/attachment/12421534/0008-rename-getMessagingInstance-instance-r-m-unused.patch","06/Oct/09 02:26;jbellis;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-385-clean-up-loadinfo-and-SLB.txt;https://issues.apache.org/jira/secure/attachment/12421310/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-385-clean-up-loadinfo-and-SLB.txt","06/Oct/09 02:26;jbellis;ASF.LICENSE.NOT.GRANTED--0002-make-LoadInfo-a-double-instead-of-going-from-long-s.txt;https://issues.apache.org/jira/secure/attachment/12421311/ASF.LICENSE.NOT.GRANTED--0002-make-LoadInfo-a-double-instead-of-going-from-long-s.txt","06/Oct/09 02:26;jbellis;ASF.LICENSE.NOT.GRANTED--0003-move-javadoc-into-mbean.-inline-methods-where-wrappin.txt;https://issues.apache.org/jira/secure/attachment/12421312/ASF.LICENSE.NOT.GRANTED--0003-move-javadoc-into-mbean.-inline-methods-where-wrappin.txt","06/Oct/09 02:26;jbellis;ASF.LICENSE.NOT.GRANTED--0004-use-Strings-instead-of-Endpoints-in-jmx-methods.-merge.txt;https://issues.apache.org/jira/secure/attachment/12421313/ASF.LICENSE.NOT.GRANTED--0004-use-Strings-instead-of-Endpoints-in-jmx-methods.-merge.txt","06/Oct/09 02:26;jbellis;ASF.LICENSE.NOT.GRANTED--0005-add-getLoadMap-jmx-method-add-load-info-to-nodeprobe.txt;https://issues.apache.org/jira/secure/attachment/12421314/ASF.LICENSE.NOT.GRANTED--0005-add-getLoadMap-jmx-method-add-load-info-to-nodeprobe.txt","06/Oct/09 02:26;jbellis;ASF.LICENSE.NOT.GRANTED--0006-get-token-on-bootstrap-that-gives-us-half-of-the-keys.txt;https://issues.apache.org/jira/secure/attachment/12421315/ASF.LICENSE.NOT.GRANTED--0006-get-token-on-bootstrap-that-gives-us-half-of-the-keys.txt",,,,,,,8.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19664,,,Sun Oct 11 12:35:11 UTC 2009,,,,,,,,,,"0|i0fylz:",91205,,,,,Normal,,,,,,,,,,,,,,,,,"20/Aug/09 12:38;eweaver;this would be super...would obviate the need for fancy LB for quite a while, at least for us.;;;","25/Sep/09 06:19;jbellis;06
    get token on bootstrap that gives us half of the keys from the most heavily-loaded node.
    the ""splits"" list should also be useful for #342.

05
    add getLoadMap jmx method; add load info to nodeprobe ring

04
    use Strings instead of Endpoints in jmx methods. merge cluster info into ring.

03
    move javadoc into mbean.  inline methods where wrapping is not necessary to satisfy Demeter

02
    make LoadInfo a double instead of going from long -> string -> LoadInfo -> int

01
    clean up loadinfo and SLB
;;;","05/Oct/09 23:36;jbellis;rebased;;;","06/Oct/09 01:14;sandeep_tata;I get a build error. Perhaps a missing patch file in the set?

build-project:
     [echo] apache-cassandra-incubating: /home/stata/cassandra/build.xml
    [javac] Compiling 246 source files to /home/stata/cassandra/build/classes
    [javac] /home/stata/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1177: cannot find symbol
    [javac] symbol  : method undecorateKey(java.lang.String)
    [javac] location: interface org.apache.cassandra.dht.IPartitioner
    [javac]             String key = partitioner_.undecorateKey(decoratedKeys.get(index));
    [javac]                                      ^
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
    [javac] 1 error
;;;","06/Oct/09 02:26;jbellis;oops.  rebased better. :);;;","07/Oct/09 07:01;urandom;This looks good to me, but I do have one question/concern.

Introduced in 0006-get-token-on-bootstrap-that-gives-us-half-of-the-keys.txt, when a node bootstraps and hasn't been assigned a token, it selects one via StorageService.getBootstrapTokenFrom which uses Thrift to retrieve it as a property. 

My concern is that this is the first time we've used Thrift for anything that wasn't client oriented. Do we want to start now, or does it make sense to preserve that separation?

If nothing else, sticking with this approach will mean that work is needed to improve how  the Thrift service is bound to interfaces, (currently configured via ThriftAddress), because there is only one option that is guaranteed safe with this patch applied (0.0.0.0).;;;","07/Oct/09 07:38;jbellis;there's a couple motivations there:

 - i believe this same method call will be a good fit for Hadoop asking for input splits, so it will still be client-facing as well.  the convention is safe, or will be :)
 - RPC is a better fit for this than messagingservice's callback-oriented approach, since we do want to block the bootstrap until getting an answer

how bad is the thriftAddress pain going to be?;;;","07/Oct/09 23:53;jbellis;08
    rename getMessagingInstance -> instance; r/m unused methods from FBUtiltities

07
    switch to messagingservice to get bootstrap token

(will squash this into 06 for commit, but it's probably easy to review as a separate patch)
;;;","08/Oct/09 00:12;urandom;Looks good to me. +1;;;","08/Oct/09 00:24;jbellis;committed;;;","08/Oct/09 20:35;hudson;Integrated in Cassandra #221 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/221/])
    rename getMessagingInstance -> instance; r/m unused methods from FBUtiltities
patch by jbellis; reviewed by Eric Evans for 
get token on bootstrap that gives us half of the keys from the most heavily-loaded node. (the ""splits"" approach should also be useful for #342; adding it to Thrift is trivial)
patch by jbellis; reviewed by Eric Evans for 
add getLoadMap jmx method; add load info to nodeprobe ring
patch by jbellis; reviewed by Eric Evans for 
use Strings instead of Endpoints in jmx methods. merge cluster info into ring.
patch by jbellis; reviewed by Eric Evans for 
move javadoc into mbean.  inline methods where wrapping is not necessary to satisfy Demeter
patch by jbellis; reviewed by Eric Evans for 
make LoadInfo a double instead of going from long -> string -> LoadInfo -> int
patch by jbellis; reviewed by Eric Evans for 
clean up loadinfo and SLB
patch by jbellis; reviewed by Eric Evans for 
;;;","11/Oct/09 20:35;hudson;Integrated in Cassandra #224 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/224/])
    add AutoBootstrap config option
patch by jbellis.  reviewed by goffinet for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bootstrap broken in 0.4.1,CASSANDRA-501,12438644,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,sandeep_tata,sandeep_tata,sandeep_tata,21/Oct/09 07:59,16/Apr/19 17:33,22/Mar/23 14:57,04/Nov/09 04:04,0.4,,,,0,,,,,,Bootstrap fails with NPE in 0.4.1 when you start a node with the -b option,all,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Oct/09 08:03;sandeep_tata;bootstrapfix-v1.patch;https://issues.apache.org/jira/secure/attachment/12422749/bootstrapfix-v1.patch",,,,,,,,,,,,,,1.0,sandeep_tata,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19725,,,Tue Nov 03 20:04:32 UTC 2009,,,,,,,,,,"0|i0fzbj:",91320,,,,,Normal,,,,,,,,,,,,,,,,,"21/Oct/09 08:03;sandeep_tata;1. Fixes null token getting passed to Bootstapper
2. Correctly includes new nodes in src-target calculation (changes in tokenMetada for echoing writes probably broke this)
3. Fixed unit test;;;","21/Oct/09 08:04;sandeep_tata;Applicable for 0.4.1, not useful for trunk.

trunk runs:

for (int i=0; i< targets_.length; i++)
{
                tokenMetadata_.update(tokens_[i], targets_[i], false);
}

before calling getRangesWithSourceTarget(), so it won't have this problem. We should probably fix the unit test so we catch this in the future.

;;;","21/Oct/09 23:59;jbellis;I don't follow the reasoning behind cloneTokenEndPointMapIncludingBootstrapNodes.  If we include bootstrap-in-progress nodes in the source ranges, we could try to bootstrap from nodes that don't have all the data yet.  Better to bootstrap only from the original nodes; if there is overlap w/ existing bootstrapers, then we have some inefficiency, but that is better than not getting all the data.;;;","03/Nov/09 11:21;jbellis;Sandeep, how is testing looking on this?;;;","04/Nov/09 02:36;sandeep_tata;My 1->2 node tests were fine, I'm waiting to hear from the other team trying this. 
If you want to go ahead and commit this for 0.4.2, that's fine. I'll be able to run a bunch of comprehensive tests after Friday, and submit other fixes if needed.;;;","04/Nov/09 04:04;jbellis;ok, committed this to 0.4 branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Warn operator when there is not enough disk space for compaction,CASSANDRA-804,12456535,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,17/Feb/10 21:51,16/Apr/19 17:33,22/Mar/23 14:57,18/Feb/10 00:59,0.6,,,,0,,,,,,,,johanoskarsson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Feb/10 00:25;jbellis;804-v3.txt;https://issues.apache.org/jira/secure/attachment/12436112/804-v3.txt","17/Feb/10 22:48;jbellis;804.txt;https://issues.apache.org/jira/secure/attachment/12436109/804.txt","17/Feb/10 22:51;gdusbabek;reset-compactionFileLocation.txt;https://issues.apache.org/jira/secure/attachment/12436110/reset-compactionFileLocation.txt",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19871,,,Wed Feb 17 17:54:44 UTC 2010,,,,,,,,,,"0|i0g16f:",91621,,,,,Low,,,,,,,,,,,,,,,,,"17/Feb/10 22:35;gdusbabek;I don't understand the purpose of the while loop.  If there isn't enough space (compactionFileLocation==null), shouldn't the error be reported and the function return 0?;;;","17/Feb/10 22:41;jbellis;the purpose is, if we can't compact all N files, see if we can compact N - 1, N - 2, etc.  Merging rows will actually free space in the average case, so it's possible that by merging some files rather than giving up completely that we will actually succeed at all of them next time.;;;","17/Feb/10 22:48;jbellis;better patch.;;;","17/Feb/10 22:51;gdusbabek;That makes sense, but if I'm reading the code correctly, the loop will always empty out smallerSSTables since compactionFileLocation is invariant.  Perhaps it should be reset in the while loop.  (Apply reset-compactionFileLocation.txt on top of your patch to see.);;;","17/Feb/10 23:30;jbellis;definitely, +1 your fix.;;;","18/Feb/10 00:25;jbellis;combined patch attached;;;","18/Feb/10 00:49;gdusbabek;+1;;;","18/Feb/10 00:59;jbellis;committed;;;","18/Feb/10 01:54;hudson;Integrated in Cassandra #357 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/357/])
    use while loop instead of recursion when trimming sstables compaction list to avoid blowing stack in pathological cases.
patch by jbellis; reviewed by gdusbabe for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix consistencylevel during bootstrap,CASSANDRA-833,12457318,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,25/Feb/10 01:44,16/Apr/19 17:33,22/Mar/23 14:57,21/Sep/12 23:13,1.2.0 beta 2,,,,0,,,,,,"As originally designed, bootstrap nodes should *always* get *all* writes under any consistencylevel, so when bootstrap finishes the operator can run cleanup on the old nodes w/o fear that he might lose data.

but if a bootstrap operation fails or is aborted, that means all writes will fail until the ex-bootstrapping node is decommissioned.  so starting in CASSANDRA-722, we just ignore dead nodes in consistencylevel calculations.

but this breaks the original design.  CASSANDRA-822 adds a partial fix for this (just adding bootstrap targets into the RF targets and hinting normally), but this is still broken under certain conditions.  The real fix is to consider consistencylevel for two sets of nodes:

  1. the RF targets as currently existing (no pending ranges)
  2.  the RF targets as they will exist after all movement ops are done

If we satisfy CL for both sets then we will always be in good shape.

I'm not sure if we can easily calculate 2. from the current TokenMetadata, though.",,arya,cagatayk,scode,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/May/11 01:23;slebresne;0001-Increase-CL-with-boostrapping-leaving-node.patch;https://issues.apache.org/jira/secure/attachment/12478450/0001-Increase-CL-with-boostrapping-leaving-node.patch","18/May/11 05:21;jbellis;833-v2.txt;https://issues.apache.org/jira/secure/attachment/12479519/833-v2.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19884,,,Fri Sep 21 15:13:10 UTC 2012,,,,,,,,,,"0|i0g1cv:",91650,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"03/Mar/10 11:09;jbellis;To clarify: the #722 fix breaks the design because a bootstrapping node that goes ""down"" temporarily but completes bootstrap will not actually have all the writes that happened during bootstrap on it.;;;","05/Mar/10 22:10;jaakko;This issue is not only related to bootstrapping, since nodes leaving the ring will also cause pending ranges. If a node does not complete leaving operation properly, obsolete pending ranges will be left in metadata.

(2) above is actually almost exactly how pending ranges is calculated. All current move operations are finished and pending ranges is calculated according to what are the new natural endpoints for the ranges in question.

This is not directly related to bootstrapping IMHO, but to the fact that node movement increases quorum and due to node movement being uncertain, there is bigger possibility that something goes wrong and quorum nodes cannot be reached.
;;;","03/May/11 21:56;jbellis;Consider the case of CL=1, RF=3 to replicas A, B, C. We begin bootstrapping node D, and write a row K to the range being moved from C to D.

If the cluster is heavily loaded, it's possible that we write one copy to C, all the other writes get dropped, and once bootstrap completes we lose the row. Or if we write one copy to D, and cancel bootstrap, we again lose the row.

As said above, we want to satisfy CL for both the pre- and post-bootstrap nodes (in case bootstrap aborts).  This requires treating the old/new range owner as a unit: both D *and* C need to accept the write for it to count towards CL. So rather than considering {A, B, C, D} we should consider {A, B, (C, D)}.

This is a lot of complexity to introduce. A simplification that preserves correctness is to continue treating nodes independently but require *one more node* than normal CL. So CL=1 would actually require 2 nodes; CL=Q would require 3 (for RF=3), and so forth.  (Note that Q(3) + 1 is the same as Q(4), which is what the existing code computes; that is one reason I chose a CL=1 example to start with, since those are *not* the same even for the simple case of RF=3.)

This would mean we may fail a few writes unnecessarily (a write to A or B is actually sufficient to satisfy CL=1, but this scheme would time that out) but never allow a write to succeed that would leave CL unsatisfied post-bootstrap (or if bootstrap is cancelled).;;;","07/May/11 01:23;slebresne;Attaching patch that implements the ""simplification"" idea. The case of {LOCAL|EACH}_QUORUM requires some care but I think that by considering DC separately we are fine.;;;","18/May/11 05:21;jbellis;v2 tweaks getWriteEndpoints to avoid new Collection creation where possible, instead using Iterables.concat.

otherwise lgtm.;;;","08/Jun/11 23:37;slebresne;+1;;;","08/Jun/11 23:46;jbellis;committed;;;","09/Jun/11 10:50;hudson;Integrated in Cassandra-0.8 #158 (See [https://builds.apache.org/job/Cassandra-0.8/158/])
    fix inconsistency window duringbootstrap
patch by slebresne; reviewed by jbellis for CASSANDRA-833

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1133443
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/DatacenterSyncWriteResponseHandler.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/locator/SimpleStrategyTest.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageProxy.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/service/LeaveAndBootstrapTest.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/service/MoveTest.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/locator/AbstractReplicationStrategy.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/AbstractWriteResponseHandler.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/service/ConsistencyLevelTest.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/WriteResponseHandler.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/DatacenterWriteResponseHandler.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/locator/TokenMetadata.java
;;;","21/Sep/12 01:40;jbellis;Well, WTF.

{noformat}
commit 063c8f6cf7b12e976b0d7067037c52c548c6c0db
Author: Jonathan Ellis <jbellis@apache.org>
Date:   Thu Jun 9 00:16:27 2011 +0000

    revert 1133443
    
    git-svn-id: https://svn.apache.org/repos/asf/cassandra/branches/cassandra-0.8@1133610 13f79535-47bb-0310-9956-ffa450edef68

commit 31f0ee95e927c09183dca77be7739305ba2eeab0
Author: Jonathan Ellis <jbellis@apache.org>
Date:   Wed Jun 8 15:45:54 2011 +0000

    fix inconsistency window duringbootstrap
    patch by slebresne; reviewed by jbellis for CASSANDRA-833
    
    git-svn-id: https://svn.apache.org/repos/asf/cassandra/branches/cassandra-0.8@1133443 13f79535-47bb-0310-9956-ffa450edef68
{noformat}

I have no memory of this. :)

Maybe it caused a regression?;;;","21/Sep/12 09:11;jbellis;Pushed rebase to https://github.com/jbellis/cassandra/branches/833-3;;;","21/Sep/12 09:21;jbellis;(Not exactly a pure rebase since I split out pendingRangesFor instead of cramming everything into getWriteEndpoints.);;;","21/Sep/12 16:39;slebresne;In counterWriteTask, sendToHintedEndpoints should be called with remotes, not targets.

But other than that it lgtm. I don't remember either why it was reverted and I don't remember any specific problem with that. But in any case, you reverted it almost right away, so if that wasn't accidental the regression was likely easy to spot, so we'll see soon enough :).;;;","21/Sep/12 23:13;jbellis;committed w/ that fix;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update token metadata for NORMAL state,CASSANDRA-1934,12494667,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,nickmbailey,nickmbailey,05/Jan/11 10:06,16/Apr/19 17:33,22/Mar/23 14:57,21/Jan/11 05:37,0.7.1,,,,0,,,,,,"The handleStateNormal() method in StorageService.java doesn't update the tokenmetadata. This means if you try to decommission a node but for some reason it fails, and then you bring the node back up, all other nodes will see it in a 'Leaving' state. When the state jumps back to normal they should update the token metadata to reflect that.

This also means you won't be able to call 'removetoken' on that node, unless you restart another node in the cluster in order to put it back in a 'normal' state.",,,,,,,,,,,,,,,,,,,,,,,14400,14400,,0%,14400,14400,,,,,,,,,,,,,,,,,,,,"11/Jan/11 02:10;brandon.williams;1934.txt;https://issues.apache.org/jira/secure/attachment/12467903/1934.txt",,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20376,,,Thu Jan 20 21:53:26 UTC 2011,,,,,,,,,,"0|i0g8b3:",92776,,nickmbailey,,nickmbailey,Low,,,,,,,,,,,,,,,,,"11/Jan/11 02:10;brandon.williams;Patch to allow recovery to normal if we knew about the endpoint previously and its IP has not changed.;;;","21/Jan/11 04:12;jbellis;How does updating when endpoint.equals(currentNode) solve the problem?

It looks to me like the relevant path is

{code}
            logger_.info(String.format(""Nodes %s and %s have the same token %s.  %s is the new owner"",
                                       endpoint, currentNode, token, endpoint));
            tokenMetadata_.updateNormalToken(token, endpoint);
{code}

which is already doing The Right Thing.;;;","21/Jan/11 04:16;brandon.williams;If you are comparing endpoint to currentNode and they are the same, the generation difference can never be > 0;;;","21/Jan/11 04:22;jbellis;my point is that currentNode should always have the right view of itself anyway, it's the other nodes we are worried about (according to the issue description anyway). ;;;","21/Jan/11 04:35;brandon.williams;currentNode (which failed decom and then restarted) does have the correct view of itself (normal) but
{code}
        else if (endpoint.equals(currentNode))
        {
            // nothing to do
        }
{code}

Prevents the other nodes from updating the state from 'Leaving' back to 'normal'.

We almost update the state here:
{code}
        else if (Gossiper.instance.compareEndpointStartup(endpoint, currentNode) > 0)
        {
            logger_.info(String.format(""Nodes %s and %s have the same token %s.  %s is the new owner"",
                                       endpoint, currentNode, token, endpoint));
            tokenMetadata_.updateNormalToken(token, endpoint);
            if (!isClientMode)
                SystemTable.updateToken(endpoint, token);
        }
{code}
But don't because the generations will always be equal (in other words, this code only handles a *different* node updating the state, not the same node returning)
;;;","21/Jan/11 05:24;jbellis;I still don't get it -- endpoint.equals(currentNode) will never be true on the other nodes.;;;","21/Jan/11 05:29;jbellis;bq. I still don't get it - endpoint.equals(currentNode) will never be true on the other nodes. 

I was thinking that currentNode == localAddress but that is not the case.  It makes more sense now! :);;;","21/Jan/11 05:30;jbellis;+1;;;","21/Jan/11 05:30;brandon.williams;Yes it will.  endpoint is the node entering the ring, currentNode is the node that currently has the token passed in (which in this case will be endpoint's token also), but does not mean 'my address' to the node making the evaluation.  Thus, if we see the same endpoint and token, we do nothing in the current code, causing the state to never get updated.  ;;;","21/Jan/11 05:32;jbellis;(renamed currentNode to currentOwner so I can remember the difference in the future.);;;","21/Jan/11 05:37;brandon.williams;committed.;;;","21/Jan/11 05:53;hudson;Integrated in Cassandra-0.7 #186 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/186/])
    Update token metadata for NORMAL state when endpoint has not changed.
Patch by brandonwilliams, reviewed by jbellis for CASSANDRA-1934
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"when streaming sstables to other nodes, make sure they don't get compacted before they are streamed",CASSANDRA-538,12440374,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,11/Nov/09 10:02,16/Apr/19 17:33,22/Mar/23 14:57,13/Nov/09 07:37,0.5,,,,0,,,,,,"at one point (pre-ASF codebase?) this was handled with a bootstrap data dir, but anticompaction no longer puts files there.

at this point it's probably easier to just make sure streaming keeps a reference to the sstable objects, which will keep them from being deleted.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Nov/09 02:07;jbellis;538.patch;https://issues.apache.org/jira/secure/attachment/12424627/538.patch",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19745,,,Fri Nov 13 12:35:04 UTC 2009,,,,,,,,,,"0|i0fzjr:",91357,,,,,Normal,,,,,,,,,,,,,,,,,"12/Nov/09 02:08;jbellis;done as described.

also adds ""not null iterator"" to anticompaction similar to the one used in regular compaction.;;;","12/Nov/09 10:15;junrao;In CFS.doFileAntiCompaction, why do you need to wrap a FilterIterator with a notNullPredicate over the CompactionIterator? The CompactionIterator never returns null, right?;;;","12/Nov/09 10:21;jbellis;it returns null if either the row had a tombstone that no longer needs to be kept, or if there was a recoverable error (recoverable meaning we can skip to the next row);;;","13/Nov/09 00:15;junrao;The patch looks good then.;;;","13/Nov/09 07:37;jbellis;committed;;;","13/Nov/09 20:35;hudson;Integrated in Cassandra #257 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/257/])
    r/m unused bootstrap directory and ensure streaming files live to be streamed
patch by jbellis; reviewed by Jun Rao for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flush before repair,CASSANDRA-1748,12479991,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,thobbs,stuhood,stuhood,16/Nov/10 04:04,16/Apr/19 17:33,22/Mar/23 14:57,19/Dec/10 11:33,0.7.0 rc 3,,,,0,,,,,,"We don't currently flush before beginning a validation compaction, meaning that depending on the state of the memtables, we might end up with content on disk that is as different as a single memtable can make it (potentially, very different).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Dec/10 06:23;thobbs;1748-trunk.txt;https://issues.apache.org/jira/secure/attachment/12466172/1748-trunk.txt","14/Dec/10 04:02;thobbs;1748.txt;https://issues.apache.org/jira/secure/attachment/12466166/1748.txt",,,,,,,,,,,,,2.0,thobbs,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20289,,,Mon Dec 20 04:46:49 UTC 2010,,,,,,,,,,"0|i0g75b:",92588,,stuhood,,stuhood,Normal,,,,,,,,,,,,,,,,,"14/Dec/10 04:02;thobbs;Attached patch flushes inside of CompactionManager's doValidationCompaction() so that both the repairing node and its neighbors will flush before building Merkle trees.;;;","14/Dec/10 05:39;stuhood;This fails to build against trunk, but looks reasonable otherwise.;;;","14/Dec/10 06:23;thobbs;1748-trunk.txt should apply against trunk.;;;","14/Dec/10 06:46;jbellis;would switching validation to use range scans be a better solution?  that way you would automatically get everything in the memtables, without having to flush-and-re-scan.;;;","15/Dec/10 08:44;thobbs;Do you mean something like using CFS.getRangeSlice()?  The main problem I see is you can't iterate over the rows one-by-one with that in its current state as it returns a List; I'm not familiar with how that works, so I don't know how easy it would be to make an Iterator version.  Or is there something else you were referring to?;;;","15/Dec/10 08:51;jbellis;CFS.gRS is a pretty thin layer over RowIterator.;;;","15/Dec/10 09:01;thobbs;Ah, right, I see that now.  I don't see why it couldn't be done, then.  Do you want me to go ahead with that (and open a new ticket)?;;;","15/Dec/10 09:47;stuhood;> would switching validation to use range scans be a better solution?
We don't have a way to send a memtable to another node, so if we included it in what we validate, it might not actually end up being sent to the other node unless it was flushed at some point. So I don't think doing this would buy us much, and it is a significantly larger change. The goal is really just to repair at a ""point in time"", and flushing before repair gives us that, imo.;;;","19/Dec/10 11:33;jbellis;committed;;;","20/Dec/10 12:46;hudson;Integrated in Cassandra-0.7 #97 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/97/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Exception when run ""get Keyspace1.Standard1"" command in the CLI",CASSANDRA-1059,12463896,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,urandom,jamel.essoussi,jamel.essoussi,07/May/10 00:26,16/Apr/19 17:33,22/Mar/23 14:57,25/May/10 00:05,0.6.2,,Legacy/Tools,,0,bug,cassandra,cassandra-cli,cli,get,"Connected to: ""Test Cluster"" on 10.0.3.44/9160
cassandra> get Keyspace1.Standard1
line 0:-1 mismatched input '<EOF>' expecting '['
Exception in thread ""main"" java.lang.AssertionError
        at org.apache.cassandra.cli.CliClient.executeGet(CliClient.java:331)
        at org.apache.cassandra.cli.CliClient.executeCLIStmt(CliClient.java:74)
        at org.apache.cassandra.cli.CliMain.processCLIStmt(CliMain.java:213)
        at org.apache.cassandra.cli.CliMain.main(CliMain.java:270)",Linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/May/10 05:02;urandom;0001-input-errors-causes-cli-to-exit-w-AssertionErrors.txt;https://issues.apache.org/jira/secure/attachment/12445199/0001-input-errors-causes-cli-to-exit-w-AssertionErrors.txt",,,,,,,,,,,,,,1.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19979,,,Mon May 24 16:05:08 UTC 2010,,,,,,,,,,"0|i0g2qv:",91875,,,,,Low,,,,,,,,,,,,,,,,,"07/May/10 00:32;jbellis;this means you gave it an invalid command (you can't ""get"" an entire CF at once) and it didn't know how to give you a human-readable error from that.  ;;;","22/May/10 05:02;urandom;The attached patch replaces the assert for {{get}}, {{set}}, {{del}}, and {{count}} with a return so that fat fingering a command isn't fatal.;;;","24/May/10 23:57;jbellis;+1;;;","25/May/10 00:05;urandom;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"when I start up the cassandra-cli, a ClassNotFoundException occured:java.lang.ClassNotFoundException: org.apache.cassandra.cli.CliMain",CASSANDRA-1236,12468055,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,gdusbabek,ialand,ialand,28/Jun/10 21:22,16/Apr/19 17:33,22/Mar/23 14:57,22/Jul/10 00:18,0.6.4,,Legacy/Tools,,0,,,,,,"After start up the cassandra server, I went to the bin/ directory and run the cassandra-cli, but there's an Exception throwed out, I have set the CASSANDRA_HOME system variable,  I don't know why
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/cassandra/cli/CliMain
Caused by: java.lang.ClassNotFoundException: org.apache.cassandra.cli.CliMain
        at java.net.URLClassLoader$1.run(URLClassLoader.java:200)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:188)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:276)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:251)
        at java.lang.ClassLoader.loadClassInternal(ClassLoader.java:319)",windows XP,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Aug/10 12:59;l0s;cassandra-cli;https://issues.apache.org/jira/secure/attachment/12451744/cassandra-cli",,,,,,,,,,,,,,1.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20042,,,Thu Sep 08 13:02:21 UTC 2011,,,,,,,,,,"0|i0g3tr:",92050,,,,,Low,,,,,,,,,,,,,,,,,"29/Jun/10 12:04;jbellis;Please do not change the issue metadata.;;;","08/Jul/10 22:34;jbellis;can you reproduce in 0.6.3 gary?;;;","22/Jul/10 00:18;gdusbabek;fixed in trunk and 0.6.4.;;;","11/Aug/10 11:33;l0s;I am able to reproduce this in the 0.6.4 binary release.  I get the error when running cassandra-cli from the Cassandra directory as well as from the bin directory.;;;","11/Aug/10 11:56;gdusbabek;Thanks Carlos.  Do you have the know-how to fix the batch file?  If so, would you be willing to do this and attach it to this ticket?  I can take care of the rest.;;;","11/Aug/10 12:59;l0s;Attached is a fix for the Bourne shell script.  It was tested on Cygwin/XP.;;;","11/Aug/10 13:04;l0s;Hi Gary, I actually did not realise that there was a batch file to launch the CLI but I took a look and figured out what needed to be done to the sh script.  I don't know enough about Windows batch programming to fix the bat file.  Anyway, thanks for pointing me in the right direction.  I was just trying to get through the GettingStarted guide on the Wiki.;;;","08/Sep/11 16:25;smartree;Need the CASSANDRA_HOME system variable,
or Run the bat in CMD from Directory of ""CASSANDRA_HOME"">bin\cassandra-cli;;;","08/Sep/11 21:02;jbellis;Smartree, 

1) please submit changes as a patch generated by svn diff

2) adding lib/*.jar is done by cassandra.in.sh (the CASSANDRA_INCLUDE searched for at the beginning of -cli).  There's no need to do it a second time in the cli script.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"debian initscript sometimes mistakenly thinks it failed, gives extraneous output",CASSANDRA-1772,12480761,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thepaul,thepaul,thepaul,24/Nov/10 09:15,16/Apr/19 17:33,22/Mar/23 14:57,25/Nov/10 07:11,0.7.0 rc 2,,Packaging,,0,,,,,,"On my test systems, which are all relatively slow VMs, the Cassandra debian initscript usually thinks it fails to start, even though the startup was successful.  It appears that jsvc forks the daemon process and exits, and the initscript check for the running Cassandra service occurs before the new daemon is able to initialize itself and create its pidfile.

On top of that, most invocations end up spitting out a small amount of garbage from /bin/ps, in addition to the typical ""Stopping Cassandra: cassandra."" log messages one sees if verbose=yes in /etc/default/rcS.  This is not very flattering.

Finally, the initscript should provide the ""status"" command to meet current LSB spec. The functionality is mostly complete already anyway, and it can be quite useful.",Debian Squeeze with cassandra 0.7.0~rc1 on a slicehost VM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Nov/10 09:31;thepaul;cass-add-status.patch.txt;https://issues.apache.org/jira/secure/attachment/12460327/cass-add-status.patch.txt","24/Nov/10 09:34;thepaul;cass-wait-for-start.patch.txt;https://issues.apache.org/jira/secure/attachment/12460328/cass-wait-for-start.patch.txt",,,,,,,,,,,,,2.0,thepaul,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20305,,,Sat Dec 11 07:35:11 UTC 2010,,,,,,,,,,"0|i0g7av:",92613,,urandom,,urandom,Low,,,,,,,,,,,,,,,,,"24/Nov/10 09:31;thepaul;cass-add-status.patch adds a slightly more robust is_running check (ensures the command line of the process $(cat $PIDFILE) matches what we expect, instead of just checking that there is a process with that number).

It also gets rid of the extraneous ""ps"" output to the terminal, and adds the ""status"" command to the initscript.;;;","24/Nov/10 09:34;thepaul;cass-wait-for-startup.patch.txt allows the ""start"" and ""restart"" actions to wait for up to 10 seconds (configurable) for the Cassandra service to start up fully.

This should eliminate the false failure messages.;;;","25/Nov/10 07:11;urandom;committed, w/ discussed (minor )changes.;;;","11/Dec/10 15:35;hudson;Integrated in Cassandra-0.7 #70 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/70/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTableImportTest is looking for test resources that are not available via the pom,CASSANDRA-636,12443602,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,zznate,zznate,zznate,17/Dec/09 09:58,16/Apr/19 17:33,22/Mar/23 14:57,19/Dec/09 03:56,0.5,,,,0,,,,,,"Trying to run 'mvn test' and a fresh checkout of the 5.0 branch results in a failure of org.apache.cassandra.tools.SSTableImportTest

The test resources are currently located in [basedir]/test/resources yet there is no <testResources> directive to include such in the pom. The following patch includes a fix to the test case for looking in the top-level of the classpath as well as a fix to the pom for including these resources.",,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Dec/09 09:59;zznate;patch;https://issues.apache.org/jira/secure/attachment/12428250/patch","18/Dec/09 02:12;zznate;patch.txt;https://issues.apache.org/jira/secure/attachment/12428318/patch.txt",,,,,,,,,,,,,2.0,zznate,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19792,,,Fri Dec 18 19:56:27 UTC 2009,,,,,,,,,,"0|i0g05j:",91455,,,,,Normal,,,,,,,,,,,,,,,,,"17/Dec/09 09:59;zznate;patch for fixing test case and test resource inclusion in pom;;;","17/Dec/09 10:03;zznate;Sorry if that patch file is a little confusing - I just realized I yanked an unused import from src/java/org/apache/cassandra/cli/CliClient.java as well. Should I create a new issue for this and attach separately? Please advise, I'm a little new at this. 
edit: just created CASSANDRA-637 for the CliClient import.;;;","17/Dec/09 23:45;urandom;Unfortunately this patch breaks the ant run tests, can it be reworked to make both happy?

;;;","18/Dec/09 02:12;zznate;Indeed I did not check against the ant build - my apologies. This patch includes changes to build.xml which work similarly to those made against the pom.;;;","19/Dec/09 03:56;urandom;committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnsupportedOperationException in system_update_column_family,CASSANDRA-1768,12480629,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,gdusbabek,tjake,tjake,23/Nov/10 10:14,16/Apr/19 17:33,22/Mar/23 14:57,24/Nov/10 01:09,0.7.0 rc 2,,,,0,,,,,,"During testing I hit this section of code:

CFMetaData.java:662
{code}
 // remove the ones leaving.
       for (ByteBuffer indexName : toRemove)
           column_metadata.remove(indexName);
{code}

but column_metadata is defined as:

{code}
       this.column_metadata = Collections.unmodifiableMap(column_metadata);
{code}

So remove() will throw an exception.

{code}
java.lang.UnsupportedOperationException
        at java.util.Collections$UnmodifiableMap.remove(Collections.java:1288)
        at org.apache.cassandra.config.CFMetaData.apply(CFMetaData.java:662)
        at org.apache.cassandra.db.migration.UpdateColumnFamily.<init>(UpdateColumnFamily.java:56)
        at org.apache.cassandra.thrift.CassandraServer.system_update_column_family(CassandraServer.java:863)
        at org.apache.cassandra.thrift.Cassandra$Processor$system_update_column_family.process(Cassandra.java:3592)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2555)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:680)
{code}

This was introduced by CASSANDRA-1715",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Nov/10 23:44;gdusbabek;ASF.LICENSE.NOT.GRANTED--v0-0001-fix-add-remove-index-bugs-in-CFMetadata.txt;https://issues.apache.org/jira/secure/attachment/12460274/ASF.LICENSE.NOT.GRANTED--v0-0001-fix-add-remove-index-bugs-in-CFMetadata.txt",,,,,,,,,,,,,,1.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20302,,,Sat Dec 11 07:35:20 UTC 2010,,,,,,,,,,"0|i0g79r:",92608,,,,,Normal,,,,,,,,,,,,,,,,,"24/Nov/10 00:23;tjake;+1, there is no chance it could be called concurrently correct?;;;","24/Nov/10 00:25;gdusbabek;correct. the mutation state is single-threaded.;;;","25/Nov/10 02:19;hudson;Integrated in Cassandra #606 (See [https://hudson.apache.org/hudson/job/Cassandra/606/])
    fix add/remove index bugs in CFMetadata. patch by gdusbabek, reviewed by tjake. CASSANDRA-1768
;;;","11/Dec/10 15:35;hudson;Integrated in Cassandra-0.7 #70 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/70/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
validation of time uuid is incorrect,CASSANDRA-1910,12494185,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,gdusbabek,dave111,dave111,29/Dec/10 02:18,16/Apr/19 17:33,22/Mar/23 14:57,30/Dec/10 03:45,0.6.9,0.7.0,,,0,,,,,,"It appears _TimeUUIDType_ (as of 12/9) is checking the wrong bits when validating a time UUID as version 1.

Per the comment and rfc4122, ""_version is bits 4-7 of byte 6_"", however validate() is actually checking the least significant bits:

> _if ((slice.get() & 0x0f) != 1)_

Sample java/hector code:

{code}
// displays ""version 1"" but validation fails
java.util.UUID uuid1 = java.util.UUID.fromString(""00000000-0000-1000-0000-000000000000"");
System.out.println(uuid1 + "" "" + uuid1.version());
TimeUUIDType.instance.validate(UUIDSerializer.get().toByteBuffer(uuid1));

// displays ""version 2"" but validation succeeds
java.util.UUID uuid2 = java.util.UUID.fromString(""00000000-0000-2100-0000-000000000000"");
System.out.println(uuid2 + "" "" + uuid2.version());
TimeUUIDType.instance.validate(UUIDSerializer.get().toByteBuffer(uuid2));
{code}

The issue can be seen with any UUID where the timestamp doesn't start with 1:

b54adc00-67f9-10d9-9669-0800200c9a66, (timestamp year 1776) version 1 fails
b54adc00-67f9-12d9-9669-0800200c9a66, (timestamp year 2233) version 1 fails
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Dec/10 02:58;gdusbabek;1910-0.6.txt;https://issues.apache.org/jira/secure/attachment/12467129/1910-0.6.txt","30/Dec/10 02:40;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-examine-the-right-nibble-when-validating-TimeUUIDs.txt;https://issues.apache.org/jira/secure/attachment/12467127/ASF.LICENSE.NOT.GRANTED--v1-0001-examine-the-right-nibble-when-validating-TimeUUIDs.txt",,,,,,,,,,,,,2.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20367,,,Wed Dec 29 19:45:09 UTC 2010,,,,,,,,,,"0|i0g85z:",92753,,,,,Low,,,,,,,,,,,,,,,,,"30/Dec/10 01:30;jbellis;probably related: I have seen this error in the test suite --

{noformat}
    [junit] Testcase: testInvalidTimeUUID(org.apache.cassandra.db.marshal.TypeValidationTest):	FAILED
    [junit] Expected exception: org.apache.cassandra.db.marshal.MarshalException
    [junit] junit.framework.AssertionFailedError: Expected exception: org.apache.cassandra.db.marshal.MarshalException
{noformat};;;","30/Dec/10 02:44;gdusbabek;patch is for 0.7+trunk.  0.6 will be different.;;;","30/Dec/10 02:58;gdusbabek;Implements proper TimeUUID validation in 0.6.;;;","30/Dec/10 03:04;jbellis;+1

let's commit to 0.6, 0.7.0, 0.7, and trunk;;;","30/Dec/10 03:32;hudson;Integrated in Cassandra-0.6 #41 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/41/])
    examine the right nibble when validating TimeUUIDs. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1910
;;;","30/Dec/10 03:45;gdusbabek;committed everywhere.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MarshalException is thrown when cassandra-cli creates the example Keyspace specified by conf/schema-sample.txt,CASSANDRA-2390,12502469,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,yaojingguo,yaojingguo,27/Mar/11 00:31,16/Apr/19 17:33,22/Mar/23 14:57,27/Mar/11 13:28,0.7.5,,,,0,,,,,,"Use the following steps to recreate the bug:

1. Checkout the source code from trunk. For my case, revision is 1085753.
2. Run ""ant"" to build cassandra.
3. Run ""bin/cassandra -f"" to start cassandra.
4. Run ""bin/cassandra-cli -host localhost --file conf/schema-sample.txt"".

Then there is the following message:
{quote}
... schemas agree across the cluster
Line 9 => org.apache.cassandra.db.marshal.MarshalException: cannot parse 'birthdate' as hex bytes
{quote}
The root cause is BytesType's fromString method. FBUtilities's hexToBytes method is invoked with ""birthdate"". NumberFormatException is thrown since ""birthdate"" is not a hex string.

{code:title=BytesType.java|borderStyle=solid}

    public ByteBuffer fromString(String source)
    {
        try
        {
            return ByteBuffer.wrap(FBUtilities.hexToBytes(source));
        }
        catch (NumberFormatException e)
        {
            throw new MarshalException(String.format(""cannot parse '%s' as hex bytes"", source), e);
        }
    }
{code}",,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20598,,,Sun Mar 27 05:44:34 UTC 2011,,,,,,,,,,"0|i0gb3j:",93228,,,,,Low,,,,,,,,,,,,,,,,,"27/Mar/11 00:42;yaojingguo;[CASSANDRA-2262|https://issues.apache.org/jira/browse/CASSANDRA-2262] makes BytesType's fromString method only accept hex strings.;;;","27/Mar/11 13:28;jbellis;fixed in r1085877 by adding comparator=UTF8Type to the CF definition.  Thanks for catching that!;;;","27/Mar/11 13:44;hudson;Integrated in Cassandra-0.7 #409 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/409/])
    specify UTF8Type comparator to fix regression found by Jingguo Yao
patch by jbellis for CASSANDRA-2390
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Repair transfers more data than necessary,CASSANDRA-2324,12501390,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,brandon.williams,brandon.williams,15/Mar/11 03:43,16/Apr/19 17:33,22/Mar/23 14:57,11/Apr/11 02:16,0.8 beta 1,,,,2,,,,,,"To repro: 3 node cluster, stress.java 1M rows with -x KEYS and -l 2.  The index is enough to make some mutations drop (about 20-30k total in my tests).  Repair afterwards will repair a large amount of ranges the first time.  However, each subsequent run will repair the same set of small ranges every time.  INDEXED_RANGE_SLICE in stress never fully works.  Counting rows with sstablekeys shows there are 2M rows total as expected, however when trying to count the indexed keys, I get exceptions like:
{noformat}
Exception in thread ""main"" java.io.IOException: Key out of order! DecoratedKey(101571366040797913119296586470838356016, 0707ab782c5b5029d28a5e6d508ef72f0222528b5e28da3b7787492679dc51b96f868e0746073e54bc173be927049d0f51e25a6a95b3268213b8969abf40cea7d7) > DecoratedKey(12639574763031545147067490818595764132, 0bc414be3093348a2ad389ed28f18f0cc9a044b2e98587848a0d289dae13ed0ad479c74654900eeffc6236)
        at org.apache.cassandra.tools.SSTableExport.enumeratekeys(SSTableExport.java:206)
        at org.apache.cassandra.tools.SSTableExport.main(SSTableExport.java:388)
{noformat}",,dkuebric,jborgstrom,jeromatron,skamio,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-2316,,,,,,,,"08/Apr/11 09:29;slebresne;0001-Make-repair-operate-over-a-node-token-range-v2.patch;https://issues.apache.org/jira/secure/attachment/12475769/0001-Make-repair-operate-over-a-node-token-range-v2.patch","01/Apr/11 23:00;slebresne;0001-Make-repair-operate-over-a-node-token-range.patch;https://issues.apache.org/jira/secure/attachment/12475228/0001-Make-repair-operate-over-a-node-token-range.patch",,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20562,,,Sun Apr 10 18:37:30 UTC 2011,,,,,,,,,,"0|i0gapj:",93165,,stuhood,,stuhood,Normal,,,,,,,,,,,,,,,,,"15/Mar/11 03:44;jbellis;Key out of order is because sstableexport doesn't know that index sstables use LocalPartitioner instead of the cluster partitioner RP or BOPP.;;;","15/Mar/11 05:24;brandon.williams;It looks like INDEXED_RANGE_SLICE is broken in stress.java, so the only problem here is repair doing superfluous work.;;;","28/Mar/11 20:57;slebresne;The problem is, the ranges repair hashes are not actual node ranges.

Let's consider the following ring (RF=2), where I consider token being in [0..12] to simplify, and where everything is consistent:
{noformat}
                  _.-""""""""-._
 C (token: 11)  .'          `.
 [11,3][3,7]   /              \
              |                |
              |                | A(token: 3)
              |                | [3,7],[7,11]
               \              /
                `._        _.'
       B (token: 7)`-....-'
       [7,11],[11,3]
{noformat}
Now say I run a repair on node A. The problem is that the Merkle tree ranges are built by dividing the full range by 2 recursively. This means that in this example, the ranges in the tree will for instance be [0,2], [2, 4], [4, 6], [6, 8], [8,10] and [10,12].

If you compare the hashes for A and B on those ranges, changes are you'll find mismatches for [6,8] and [10,12] (because A don't have anyone on [11, 12] while B have, and B don't have anyone on [6, 7] while A have). As a consequence, the range [7,8] and [10,11] will be repaired, even though there is no inconsistencies.

What that means in practice is that it will be very rare for anti-antropy to actually consider the nodes in sync, it will almost surely ""repair"" something, even if the nodes are perfectly consistent. It's Very easy to check btw: with a cluster right the one above (3 nodes, RF=2), with as few as 5 keys for the whole cluster I'm able to have a repair do repairs over and over again.

Now the good question is: how bad is it ? I'm not sure, I depends a bit.

On a 3 nodes cluster (RF=2), I tried inserting 1M keys with stress (stress -l 2) and triggered repair afterwards. The amount of (unnecessarily) repaired keys was around 150 keys for a given node (it varies slightly for run to run because there is some randomness in the creation of the Merkle tree), corresponding to ~44KB streamed (that is the amount transfered to the node where repair has been ran, so for the total operation its twice this, since we stream in both ways). That's ~0.02% of keys (a given node have ~666 666 keys).  It's bad to do useless work, but not a really big deal.

However, the less keys we'll have, the worst it gets (and the bigger our rows are, the more useless transfer we do). With the same experiment inserting only 10K keys, there is 190 keys uselessly repaired. That's now close to 3% of the load. It also gets worst with increasing replication factor.


To fix this, we would need for the range in the Merkle tree to ""share"" the node range boundaries. An interesting way to do this would be to have the coordinating node give a list a range for which to calculate Merkle trees, and the node would compute one tree by range (for the coordinating node, that would be #RF's tree). A nice think with this is that it would leave room to optimizing repair since a node would need to do a validation compaction only on the range asked for, which means that only the coordinator node would validate all its data. The neighbors would do less work.
;;;","28/Mar/11 21:44;jbellis;bq. To fix this, we would need for the range in the Merkle tree to ""share"" the node range boundaries

couldn't we just take the interesection of the computed ranges w/ the range actually being repaired? ;;;","28/Mar/11 21:58;slebresne;bq. couldn't we just take the interesection of the computed ranges w/ the range actually being repaired?

We do that. But the problem is: you're node A and you receive a merkle tree from B that in particular says that for the range [0..10] the hash is x. And on [0..10] your has is x'. The problem is when [0..10] is partly one of your range, partly not. For instance it can be that you're a replica for [8..10] but not at all for [0..8].
This is due to the fact that the ranges for which the hashes are computed are computed without concern for actual node ranges. So now you know there is some inconsistency on [0..10] but it may just be that B is responsible for [0..8] and have data for it (and we don't since we are not in charge of that).
In that case, the code do take the intersection of [0..10] with the local range and will stream only [8..10]. But it's still useless.;;;","28/Mar/11 22:14;jbellis;I thought repair is per-token-range, i.e., if I say ""nodetool repair A"" then range (11, 3] and (3, 7] will be repaired independently.;;;","28/Mar/11 22:34;slebresne;No, not if I read this code correctly (but I think it should, that's roughly what I'm proposing to do).

Actually thinking about it, there is probably no need to construct multiple merkle trees, it will be enough for neighbors to only add to the tree the keys that are in the range of the node asking for the tree.;;;","28/Mar/11 22:47;jbellis;So what about this:

- change the atom of repair (in nodetool + StorageService) to be a single token range, so it's unambiguous what we're repairing.  This has the side benefit of making it enormously easier to repair an entire cluster w/o doing redundant work.
- provide backwards compatibility w/ existing repair command by splitting it into RF repair ranges and waiting on each of those futures in StorageService mbean
;;;","28/Mar/11 23:00;slebresne;Sounds good, will do.;;;","01/Apr/11 23:00;slebresne;Attached patch modify repair to operate on one token range at a time. Nodetool repair schedule as many repair session than the node have ranges to perform a full node repair. Note that this is more efficient than previously, since the neighbors of the node will only do a validation compaction on the range they have in common with the node coordinating the repair (instead of validating everything).

This moreover makes it trivial to add an option to nodetool so that the node only repair it's primary range. That way, you can repair a full cluster by calling this operation on every node and there is no duplication of work. The patch doesn't add this option yet though.

The patch is against trunk. Because the way we construct the merkleTree is fundamentally different, the trees created by 0.7 cannot be compared to the ones created with this patch. The strategy this patch adopts with respect to talking to 0.7 nodes is this:
  * If a 0.7 node asks for a merkleTree, since we are still able to do a full compaction validation, we do it and answer with that.
  * Since a 0.7 node cannot do a merkleTree that would be ok for us, we simply exclude 0.7 nodes from the endpoints we ask merkleTree from.

I don't feel this is a trivially enough patch to go to the 0.7 branch.;;;","02/Apr/11 06:24;stuhood;This change definitely makes sense: thanks for tackling it. The original implementation was intended to take advantage of naturally occurring compactions: I would still like to get in a position where that is possible, but living with the existing implementation until then isn't worth it.

From a quick skim: forceTableRepair incorrectly reports that the session has failed if the client thread dies: the repair will continue in the background (or used to).;;;","02/Apr/11 07:03;stuhood;I'll give this a more complete review over the weekend.;;;","07/Apr/11 16:22;stuhood;* SSTableBoundScanner might be much simpler if it iterates within a list of file offsets, as returned by SSTableReader.getPositionsForRanges
* SSTableReader.getKeySamples could perform two binary searches for min and max rather than doing sequential comparisons to the keys

Thanks again Sylvain: this is great!;;;","08/Apr/11 09:29;slebresne;bq. SSTableBoundScanner might be much simpler if it iterates within a list of file offsets, as returned by SSTableReader.getPositionsForRanges

Good call, that's much simpler. Thanks.

bq. SSTableReader.getKeySamples could perform two binary searches for min and max rather than doing sequential comparisons to the keys

Yeah, realized getKeySamples was buggy anyway since it wasn't handling wrapping ranges correctly.

Attaching patch that simplify the bounded scanner and fixes getKeySamples.
;;;","08/Apr/11 12:45;stuhood;+1
Thanks!;;;","11/Apr/11 02:16;slebresne;Committed as r1090840. Thanks.;;;","11/Apr/11 02:37;hudson;Integrated in Cassandra #844 (See [https://hudson.apache.org/hudson/job/Cassandra/844/])
    Make repair work on a token range instead of the full ring
patch by slebresne; reviewed by stuhood for CASSANDRA-2324
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
handle empty unbootstrap ranges,CASSANDRA-573,12441434,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jaakko,jaakko,jaakko,23/Nov/09 17:27,16/Apr/19 17:33,22/Mar/23 14:57,25/Nov/09 04:27,0.5,,,,0,,,,,,"If there are no ranges needing transfer during unbootstrap, unbootstrap fails to complete.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Nov/09 17:30;jaakko;573-handle-empty-unbootstrap-ranges.patch;https://issues.apache.org/jira/secure/attachment/12425818/573-handle-empty-unbootstrap-ranges.patch",,,,,,,,,,,,,,1.0,jaakko,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19760,,,Wed Nov 25 12:34:11 UTC 2009,,,,,,,,,,"0|i0fzrj:",91392,,,,,Normal,,,,,,,,,,,,,,,,,"23/Nov/09 17:30;jaakko;Moved leaving ring code to a separate helper function and call it right away if rangesMM is empty.
;;;","24/Nov/09 00:37;jbellis;How could you not have any ranges needing transfer during unbootstrap?

If it isn't part of the ring at all (i.e. never completed bootstrap in the first place), we should throw an error if unbootstrap is invoked.;;;","24/Nov/09 09:34;jaakko;Ranges needing transfer lists only ranges that the new endpoints do not already have. If ReplicationFactor == number_of_nodes, then rangesMM will be empty, as all nodes already have all ranges.
;;;","25/Nov/09 04:27;jbellis;committed;;;","25/Nov/09 20:34;hudson;Integrated in Cassandra #268 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/268/])
    Moved leaving ring code to a separate helper function and call it right away if rangesMM is empty.  patch by Jaakko Laine; reviewed by jbellis for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
read_repair_chance is missing from CfDef,CASSANDRA-1180,12466698,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,gdusbabek,arya,arya,11/Jun/10 07:02,16/Apr/19 17:33,22/Mar/23 14:57,15/Jun/10 22:51,0.7 beta 1,,,,0,,,,,,CfDef is missing read_repair_chance if that is going to remain a configuration option in 0.7.,,arya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jun/10 22:22;gdusbabek;0001-add-read_repair_chance-to-CfDef.patch;https://issues.apache.org/jira/secure/attachment/12447133/0001-add-read_repair_chance-to-CfDef.patch","15/Jun/10 22:22;gdusbabek;0002-avoid-constructor-proliferation-in-CFM.patch;https://issues.apache.org/jira/secure/attachment/12447134/0002-avoid-constructor-proliferation-in-CFM.patch",,,,,,,,,,,,,2.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20024,,,Tue Jun 15 14:51:59 UTC 2010,,,,,,,,,,"0|i0g3hj:",91995,,,,,Low,,,,,,,,,,,,,,,,,"12/Jun/10 03:39;gdusbabek;Looks like this was taken care of on 2 April as part of CASSANDRA-930;;;","15/Jun/10 04:56;arya;Hi Gary,

The patch in the bug you've mentioned above contains only changes to Java StorageProxy, CFMetaData, and DatabaseDescriptor. However it does not include changes to Interface/cassandra.thrift as of today's trunc. It reads:

/* describes a column family. */
struct CfDef {
    1: required string table,
    2: required string name,
    3: optional string column_type=""Standard"",
    4: optional string clock_type=""Timestamp"",
    5: optional string comparator_type=""BytesType"",
    6: optional string subcomparator_type="""",
    7: optional string reconciler="""",
    8: optional string comment="""",
    9: optional double row_cache_size=0,
    10: optional bool preload_row_cache=0,
    11: optional double key_cache_size=200000
}

Missing read_repair_chance which makes dynamic creation of CFs using the Thrift API to miss that configuration value. My apologies if my original description was not so clear.;;;","15/Jun/10 21:27;jbellis;don't we need some logic to connect the CfDef field with CFMetaData?;;;","15/Jun/10 22:24;gdusbabek;jbellis: oops.  fixed.  The second patch cleans up the constructors in CFMetadata.;;;","15/Jun/10 22:46;jbellis;+1 latest;;;","15/Jun/10 22:51;gdusbabek;Fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra fails to start on Windows: rename failed of \var\lib\cassandra\data\system\LocationInfo-e-1-Data.db,CASSANDRA-1802,12491828,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Duplicate,,faseidl,faseidl,02/Dec/10 06:42,16/Apr/19 17:33,22/Mar/23 14:57,02/Dec/10 06:49,,,,,0,,,,,,"Cassandra fails to start on Windows:

C:\Apache\apache-cassandra-0.7.0-rc1\bin>cassandra
Starting Cassandra Server
 INFO 16:47:38,941 Heap size: 1070399488/1070399488
 INFO 16:47:38,941 JNA not found. Native methods will be disabled.
 INFO 16:47:38,941 Loading settings from file:/C:/Apache/apache-cassandra-0.7.0-rc1/conf/cassandra.yaml
 INFO 16:47:39,019 DiskAccessMode 'auto' determined to be standard, indexAccessMode is standard
 INFO 16:47:39,066 Creating new commitlog segment /var/lib/cassandra/commitlog\CommitLog-1291240059066.log
 INFO 16:47:39,097 read 0 from saved key cache
 INFO 16:47:39,097 read 0 from saved key cache
 INFO 16:47:39,097 read 0 from saved key cache
 INFO 16:47:39,097 read 0 from saved key cache
 INFO 16:47:39,097 read 0 from saved key cache
 INFO 16:47:39,113 loading row cache for LocationInfo of system
 INFO 16:47:39,113 completed loading (0 ms; 0 keys)  row cache for LocationInfo of system
 INFO 16:47:39,113 loading row cache for HintsColumnFamily of system
 INFO 16:47:39,113 completed loading (0 ms; 0 keys)  row cache for HintsColumnFamily of system
 INFO 16:47:39,113 loading row cache for Migrations of system
 INFO 16:47:39,113 completed loading (0 ms; 0 keys)  row cache for Migrations of system
 INFO 16:47:39,113 loading row cache for Schema of system
 INFO 16:47:39,113 completed loading (0 ms; 0 keys)  row cache for Schema of system
 INFO 16:47:39,113 loading row cache for IndexInfo of system
 INFO 16:47:39,113 completed loading (0 ms; 0 keys)  row cache for IndexInfo of system
 INFO 16:47:39,128 Couldn't detect any schema definitions in local storage.
 INFO 16:47:39,128 Found table data in data directories. Consider using JMX to call org.apache.cassandra.service.StorageService.loadSchemaFromYaml().
 INFO 16:47:39,128 No commitlog files found; skipping replay
 INFO 16:47:39,144 Upgrading to 0.7. Purging hints if there are any. Old hints will be snapshotted.
 INFO 16:47:39,144 Cassandra version: 0.7.0-rc1
 INFO 16:47:39,144 Thrift API version: 19.4.0
 INFO 16:47:39,159 Loading persisted ring state
 INFO 16:47:39,159 Starting up server gossip
 INFO 16:47:39,159 switching in a fresh Memtable for LocationInfo at CommitLogContext(file='/var/lib/cassandra/commitlog\CommitLog-1291240059066.log', position=700)
 INFO 16:47:39,159 Enqueuing flush of Memtable-LocationInfo@9690924(227 bytes, 4 operations)
 INFO 16:47:39,159 Writing Memtable-LocationInfo@9690924(227 bytes, 4 operations)
ERROR 16:47:39,238 Fatal exception in thread Thread[FlushWriter:1,5,main]
java.io.IOError: java.io.IOException: rename failed of \var\lib\cassandra\data\system\LocationInfo-e-1-Data.db
        at org.apache.cassandra.io.sstable.SSTableWriter.rename(SSTableWriter.java:214)
        at org.apache.cassandra.io.sstable.SSTableWriter.closeAndOpenReader(SSTableWriter.java:184)
        at org.apache.cassandra.io.sstable.SSTableWriter.closeAndOpenReader(SSTableWriter.java:167)
        at org.apache.cassandra.db.Memtable.writeSortedContents(Memtable.java:161)
        at org.apache.cassandra.db.Memtable.access$000(Memtable.java:49)
        at org.apache.cassandra.db.Memtable$1.runMayThrow(Memtable.java:174)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.IOException: rename failed of \var\lib\cassandra\data\system\LocationInfo-e-1-Data.db
        at org.apache.cassandra.utils.FBUtilities.renameWithConfirm(FBUtilities.java:359)
        at org.apache.cassandra.io.sstable.SSTableWriter.rename(SSTableWriter.java:210)
        ... 12 more",Windows XP SP3,faseidl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20319,,,Wed Dec 01 22:49:06 UTC 2010,,,,,,,,,,"0|i0g7hz:",92645,,,,,Critical,,,,,,,,,,,,,,,,,"02/Dec/10 06:49;jbellis;fixed in CASSANDRA-1790;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
make forceUserDefinedCompaction actually do what it says,CASSANDRA-2575,12505353,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,28/Apr/11 05:07,16/Apr/19 17:33,22/Mar/23 14:57,29/Apr/11 00:16,0.8.0 beta 2,,,,0,,,,,,See http://www.mail-archive.com/user@cassandra.apache.org/msg12621.html for motivation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Apr/11 23:39;jbellis;2575-v2.txt;https://issues.apache.org/jira/secure/attachment/12477663/2575-v2.txt","28/Apr/11 11:43;jbellis;2575.txt;https://issues.apache.org/jira/secure/attachment/12477607/2575.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20708,,,Mon May 02 19:54:50 UTC 2011,,,,,,,,,,"0|i0gc6n:",93404,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"28/Apr/11 05:17;jbellis;patch splits out doCompactionWithoutSizeEstimates and calls that from submitUserDefined.;;;","28/Apr/11 10:31;stuhood;The patch for 2552 is attached.;;;","28/Apr/11 11:43;jbellis;corrected patch attached.;;;","28/Apr/11 18:32;slebresne;I suppose we could backport to 0.7 too.
Also wondering if it couldn't make sense to not compact if we end up with only 1 sstable in doCompaction, now that you still can do a userDefinedCompaction on only 1 sstable. Not being able to compact the 2 smallest sstables seems pathological enough and it would be nice to avoid the 'same sstable compacted forever' problem seen in the mail thread above.

Anyway, +1 on the patch. ;;;","28/Apr/11 23:39;jbellis;bq. Also wondering if it couldn't make sense to not compact if we end up with only 1 sstable in doCompaction, now that you still can do a userDefinedCompaction on only 1 sstable

Good idea. v2 attached with a rewrite of the retry loop to do this.;;;","28/Apr/11 23:47;slebresne;+1 on v2;;;","29/Apr/11 00:16;jbellis;committed;;;","03/May/11 03:54;hudson;Integrated in Cassandra-0.8 #58 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/58/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TCP writes get stuck,CASSANDRA-220,12427319,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,junrao,junrao,junrao,08/Jun/09 12:17,16/Apr/19 17:33,22/Mar/23 14:57,24/Jun/09 01:57,0.4,,,,0,,,,,,"In our test cluster, I observed that on some nodes, TCP writes get accumulated in TcpConnection.pendingWrites. However, the selector never gets a ready to write event. As a result, all writes get stuck. This is because write(message) and doPendingWrites() are not fully synchronized.  This following situation can happen:

1. write(message) adds stuff to pendingWrites.
2. a ready to write event happens; in doPendingWrites() buffered requests are written to socket
3. another write request happens, in write(message), the test for pendingWrites.isEmpty() is false
4. doPendingWrites() finishes writing all buffered request to socket
5. in write(message), the new request is added to pendingWrites

Now, ready to write events will never happen again and all write requests get stuck in pendingWrites.",,hammer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jun/09 23:02;jbellis;220-2.patch;https://issues.apache.org/jira/secure/attachment/12410136/220-2.patch","13/Jun/09 07:16;junrao;220-3.patch;https://issues.apache.org/jira/secure/attachment/12410523/220-3.patch","24/Jun/09 00:29;junrao;220-4.patch;https://issues.apache.org/jira/secure/attachment/12411547/220-4.patch","24/Jun/09 01:48;junrao;220-5.patch;https://issues.apache.org/jira/secure/attachment/12411554/220-5.patch","08/Jun/09 12:22;junrao;issue220.patchv1;https://issues.apache.org/jira/secure/attachment/12410103/issue220.patchv1",,,,,,,,,,5.0,junrao,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19599,,,Tue Jun 23 17:57:15 UTC 2009,,,,,,,,,,"0|i0fxlr:",91042,,,,,Normal,,,,,,,,,,,,,,,,,"08/Jun/09 12:22;junrao;Attache a patch to trunk by expanding the synchronization scope in TcpConnection.doPendingWrites().

We need to patch to 0.3 branch too.

Also, we need to revisit the code in TcpConnection. Under heavy writes, is it better to keep accumulating writes until we run out of memory, or to put a cap on the write buffer and simply block and possibly timeout the writer.;;;","08/Jun/09 23:02;jbellis;Committed to 0.3.  Will merge to trunk.

Here is another patch to clean up TcpConnection a little.  Look okay?;;;","08/Jun/09 23:22;junrao;The new patch looks fine to me.;;;","08/Jun/09 23:36;jbellis;committed;;;","13/Jun/09 07:16;junrao;Haven't beaten this one to death yet. There is another potential synchronization problem with respect to TcpConnection.connect. The following could happen:

1. Thread 1: TcpConnection.write(message) checks the channel and finds it not connected yet.
2. Thread 2: Sockect is connected and triggers TcpConnection.connect; pendingWrites is tested to be empty and ""interested in write"" bit is not set
3. Thread 1: adds message to pendingWrites

Now, pendingWrites is not empty and the ""read for write"" event is never going to happen since we didn't register an interest for it. All subsequent writes get stuck afterward.

Attach a patch in 220-3. It synchronizes in TcpConnection.connect too. Should patch 0.3 branch too.
;;;","13/Jun/09 08:32;jbellis;I know nio is sexier, but maybe it would be better to rip it out and just go with thread-per-socket.

http://java.dzone.com/articles/avoid-nio-get-better-throughpu (read the linked slides);;;","18/Jun/09 02:33;jbellis;on -3, my only suggestion is, maybe we should just synchronize the whole method for simplicity since it is only going to be called once.

what do you think?;;;","24/Jun/09 00:29;junrao;It took me a while to dig this out. Under heavy load and large column values, we still saw lockups in tcp connection. Here is the problem. The following code that sets the interest ops seems innocent, but it's the source of the problem. The reason is that this operation is not atomic. Another thread could sneak in between the reading of the ops and the setting of it. As a result, some wrong bits could be set.
   key_.interestOps(key_.interestOps() | SelectionKey.OP_READ)

This is a sequence that demonstrates how we can lose the OP_READ bit forever and thus jam the read channel:
1. Thread 1: we want to write a message and in write(Message) we are about to turn on OP_WRITE because the message can't be written in one shot.
2. Thread 2: a read comes in and in read(SelectionKey), we turn off OP_READ and submit the read request to ReadWorkItem in Thread 3.
3. Thread 1: read interestOps and see OP_READ as off.
4. Thread 3: finished processing the read request and turn OP_READ on
5. Thread 1: resumes and turn on OP_WRITE. However, by doing that, we also turned off OP_READ. The read channel is thus blocked forever after this.

Patch-4 makes changing interestOps atomic by synchronizing on the selection key.

Patch-4 also includes the earlier fix for the synchronization problem in connect. I left the fix as it is instead of making connect() a synchronized method. This way, it makes clear which part of the code in connect really requires synchronization.
;;;","24/Jun/09 00:42;jbellis;looks good.

can you patch the other uses of interestOps in UDPConnection and HttpConnection too?;;;","24/Jun/09 01:48;junrao;Attach patch-5 that fixes similar problems in HttpConnection and UdpConnection.;;;","24/Jun/09 01:57;jbellis;committed to 0.3 branch and merged to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
describe_schema_versions does not list downed hosts,CASSANDRA-1678,12478609,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,gdusbabek,appodictic,appodictic,29/Oct/10 02:15,16/Apr/19 17:33,22/Mar/23 14:57,02/Nov/10 22:05,0.7.0 rc 1,,Legacy/CQL,,0,,,,,,"According to the description unreachable hosts should be listed. It does not seem like they are.
{noformat}
 map<string, list<string>> describe_schema_versions()

  [java] key:c3f38ebc-e1c5-11df-95a0-e700f669bcfc
     [java] 	127.0.0.2
     [java] 	127.0.0.3
     [java] 	127.0.0.4
     [java] 	127.0.0.1

Address         Status State   Load            Token                                       
                                       105444142448428656124184491892431731479    
127.0.0.3       Up     Normal  56.53 KB        43021486531749787992103274496183765897      
127.0.0.1       Up     Normal  56.24 KB        49910048177093876350019363877113991186      
127.0.0.5       Down   Normal  52.49 KB        64377498999076014343862177049497951437      
127.0.0.2       Up     Normal  65.27 KB        84713069031498515281943177906254878023      
127.0.0.4       Up     Normal  55.95 KB        105444142448428656124184491892431731479
{noformat}

The code looks like this:
{noformat}
 Cassandra.Client client = fcw.getClient();
    Map<String,List<String>> sv =client.describe_schema_versions();
    for (Map.Entry<String,List<String>> mapEntry: sv.entrySet()){
      System.out.println(""key:""+mapEntry.getKey());
      for (String listForKey : mapEntry.getValue()){
        System.out.println(""\t""+listForKey);
      }
    }
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Nov/10 21:15;gdusbabek;ASF.LICENSE.NOT.GRANTED--v4-0001-include-dead-hosts-in-unreachable.txt;https://issues.apache.org/jira/secure/attachment/12458631/ASF.LICENSE.NOT.GRANTED--v4-0001-include-dead-hosts-in-unreachable.txt",,,,,,,,,,,,,,1.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,699,,,Wed Nov 03 12:49:55 UTC 2010,,,,,,,,,,"0|i0g6pr:",92518,,,,,Low,,,,,,,,,,,,,,,,,"29/Oct/10 02:30;jbellis;Odd, I see the code trying to add the missing hosts in describeSchemaVersions:

{code}
            results.put(DatabaseDescriptor.INITIAL_VERSION.toString(), missingHostNames);
{code}

But IMO the right fix is to avoid that entirely and do the simple thing instead:

{code}
  /**
   * for each schema version present in the cluster, returns a list of nodes at that version.
   * hosts that do not respond will not be included.
   * the cluster is all on the same version if the size of the map is 1 and the
   * length of the list	in that map value is the number of nodes in the cluster.
   */
{code};;;","29/Oct/10 12:53;appodictic;Yes changing the description would ""Fix"" the problem. At minimum the description should match what the method actually does. I imagine this is used by applications that want to ensure the schema is in place before doing writes. Ignoring downed hosts could be misleading, as users would have to ensure the list size matches the size it should be. You could view that as a separate problem but I think users would want both in one method.;;;","30/Oct/10 03:16;gdusbabek;the 'missing hosts' Jonathan referred to were the live hosts that did not respond to the version query.  There could be other missing hosts that getLiveMembers() didn't return because they were down.

Would it suit everybody if we included the unreachable members at version=DD.INITIAL_VERSION?;;;","30/Oct/10 03:28;jbellis;wfm, although not having a distinction between ""responded that he was at INITIAL_VERSION"" and ""didn't respond"" feels a little unsavory.;;;","30/Oct/10 03:33;gdusbabek;Since they're just strings I could just give them the value ""UNREACHABLE""?;;;","30/Oct/10 03:41;jbellis;+1;;;","30/Oct/10 04:01;appodictic;+1;;;","30/Oct/10 05:31;jbellis;seems to me it would be simpler to just loop through all hosts after versions are received; if host is not in ackedhosts then add it to the unreachable set.  this avoids problems caused by FD moving a node from live to dead or vice versa between requests being sent and processed.;;;","02/Nov/10 06:57;jbellis;minor point: allHosts can be replaced with Iterables.concat(live, unreachable)

is the ""check for version disagreement"" loop still useful now?;;;","02/Nov/10 21:17;gdusbabek;The check is still useful for logging the disagreeing hosts.  When the feature was first created I seem to recall someone wanting it to be logged (didn't make sense to me).

v4 uses the Iterables call.;;;","02/Nov/10 22:00;jbellis;+1;;;","03/Nov/10 20:49;hudson;Integrated in Cassandra #585 (See [https://hudson.apache.org/hudson/job/Cassandra/585/])
    include dead hosts in unreachable. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1678
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Read repair IndexOutOfBoundsException,CASSANDRA-1727,12479656,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,11/Nov/10 07:22,16/Apr/19 17:33,22/Mar/23 14:57,11/Nov/10 08:35,0.6.8,0.7.0 rc 1,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Nov/10 08:09;jbellis;1727-v2.txt;https://issues.apache.org/jira/secure/attachment/12459302/1727-v2.txt","11/Nov/10 07:34;jbellis;1727.txt;https://issues.apache.org/jira/secure/attachment/12459296/1727.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20275,,,Thu Nov 11 00:35:18 UTC 2010,,,,,,,,,,"0|i0g70n:",92567,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"11/Nov/10 07:22;jbellis;Originally reported on CASSANDRA-1719,

{code}
ERROR 20:14:11,591 Uncaught exception in thread Thread[CACHETABLE-TIMER-3,5,main]
java.lang.RuntimeException: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0
at org.apache.cassandra.service.ConsistencyChecker$DataRepairHandler.callMe(ConsistencyChecker.java:186)
at org.apache.cassandra.service.ConsistencyChecker$DataRepairHandler.callMe(ConsistencyChecker.java:141)
at org.apache.cassandra.utils.ExpiringMap$CacheMonitor.run(ExpiringMap.java:105)
at java.util.TimerThread.mainLoop(Timer.java:534)
at java.util.TimerThread.run(Timer.java:484)
Caused by: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0
at java.util.ArrayList.rangeCheck(ArrayList.java:571)
at java.util.ArrayList.get(ArrayList.java:349)
at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:131)
at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:45)
at org.apache.cassandra.service.ConsistencyChecker$DataRepairHandler.callMe(ConsistencyChecker.java:182)
... 4 more
{code}

{code}
ERROR 20:17:08,688 Uncaught exception in thread Thread[CACHETABLE-TIMER-8,5,main]
java.lang.RuntimeException: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0
at org.apache.cassandra.service.ConsistencyChecker$DataRepairHandler.callMe(ConsistencyChecker.java:186)
at org.apache.cassandra.service.ConsistencyChecker$DataRepairHandler.callMe(ConsistencyChecker.java:141)
at org.apache.cassandra.utils.ExpiringMap$CacheMonitor.run(ExpiringMap.java:105)
at java.util.TimerThread.mainLoop(Timer.java:512)
at java.util.TimerThread.run(Timer.java:462)
Caused by: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0
at java.util.ArrayList.RangeCheck(ArrayList.java:547)
at java.util.ArrayList.get(ArrayList.java:322)
at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:131)
at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:45)
at org.apache.cassandra.service.ConsistencyChecker$DataRepairHandler.callMe(ConsistencyChecker.java:182)
... 4 more
{code};;;","11/Nov/10 07:34;jbellis;Fix for regression from CASSANDRA-1622.;;;","11/Nov/10 08:09;jbellis;v2 also fixes the local response;;;","11/Nov/10 08:18;brandon.williams;+1;;;","11/Nov/10 08:35;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make nosetest retrieve the stdout and stderr message when Cassandra fails to start,CASSANDRA-306,12430718,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,sammy.yu,sammy.yu,sammy.yu,17/Jul/09 06:28,16/Apr/19 17:33,22/Mar/23 14:57,17/Jul/09 11:33,0.4,,Legacy/Tools,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jul/09 06:51;sammy.yu;0001-CASSANDRA-306.patch;https://issues.apache.org/jira/secure/attachment/12413753/0001-CASSANDRA-306.patch",,,,,,,,,,,,,,1.0,sammy.yu,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19626,,,Fri Jul 17 13:01:51 UTC 2009,,,,,,,,,,"0|i0fy4n:",91127,,,,,Normal,,,,,,,,,,,,,,,,,"17/Jul/09 06:51;sammy.yu;Made nosetest more robust in handling when Cassandra daemon doesn't start properly;;;","17/Jul/09 11:33;urandom;Committed, thanks Sammy!;;;","17/Jul/09 21:01;hudson;Integrated in Cassandra #140 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/140/])
    improved error handling for system test server startup

Capture stderr and stdout when process is dead; only kill if it isn't
already dead.

Patch by Sammy Yu; reviewed by eevans for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
move daemon to framed transport (thrift),CASSANDRA-241,12428401,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,urandom,urandom,urandom,20/Jun/09 03:43,16/Apr/19 17:33,22/Mar/23 14:57,11/Aug/09 02:57,0.4,,,,0,,,,,,"The framed transports in thrift wrap the underlying transport to prepend the message size as a 4 byte value.  There are purported benefits to buffering, but the main purpose of these wrappers is to allow non-blocking servers to perform reads without deserialization. Of course, if the server transport is framed, the client's must be as well, and vice versa, (framed and non-framed transports are incompatible). 

CassandraDaemon is currently a threaded server with the default transport, I believe we should change it to being framed, for compatibility with non-blocking clients (this actually came up during an attempt to use a Twisted client).

This will break all existing client apps, (even if fixing them is trivial).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jun/09 03:44;urandom;241.txt;https://issues.apache.org/jira/secure/attachment/12411254/241.txt","10/Aug/09 06:30;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-241-optional-support-for-framed-transport.txt;https://issues.apache.org/jira/secure/attachment/12416009/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-241-optional-support-for-framed-transport.txt","11/Aug/09 01:08;urandom;ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-241-optional-support-for-framed-transport.txt;https://issues.apache.org/jira/secure/attachment/12416089/ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-241-optional-support-for-framed-transport.txt",,,,,,,,,,,,3.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19607,,,Fri Nov 13 18:19:33 UTC 2009,,,,,,,,,,"0|i0fxqf:",91063,,,,,Normal,,,,,,,,,,,,,,,,,"20/Jun/09 03:58;jbellis;Does Thrift give a meaningful error if you try connecting w/ non-framed client to framed Cassandra server or does it just silently do nonsense?

If the latter, please post to -dev and -users that this change is going to happen Monday in trunk and then I will commit it.
;;;","20/Jun/09 04:00;urandom;It silently does nonsense. I will post to the lists.;;;","20/Jun/09 04:24;urandom;Also, once this is committed, http://wiki.apache.org/cassandra/ClientExamples will need to be updated accordingly.;;;","20/Jun/09 05:57;junrao;could you explain a bit more what exactly are non-blocking clients?;;;","22/Jun/09 22:17;urandom;A non-blocking client is one that uses a non-blocking socket and select/poll to determine when it is ready to read/write from/to the server.;;;","28/Jun/09 06:46;jbellis;I think the consensus on the list was that we should make the transport to use an option given that the following Thrift languages lack the Framed option.

C#
Cocoa
Haskell
Ocaml
Smalltalk;;;","22/Jul/09 07:16;urandom;Just to provide some update:

There are a couple of reasons why I've been avoiding a configuration option to enable/disable framed transport.

1. Whether or not the transport is framed or not is an implementation detail of Thrift, it sucks that we would be forced to expose this to our users.

2. Until THRIFT-210 is complete (better yet, THRIFT-538), there won't be any one setting that will let you run a server compatible with all clients. For example, with a framed transport enabled C# clients currently aren't possible, conversely without a framed transport, Twisted clients are left out in the cold.
 
It has been my hope that the Thrift team would close at least THRIFT-210 (there is a patch attached), so that we could accommodate all of our users with a framed transport (and without requiring it to be configured).

However, considering how long THRIFT-210 has been open (and considering the complete lack of response to THRIFT-538), it may in fact be necessary to add an option so that our users can at least choose what (not  )to support.

;;;","08/Aug/09 11:13;jbellis;Should we move this to 0.5?;;;","08/Aug/09 13:28;euphoria;It shouldn't be too difficult to make framed an option.  I think we should add the option to 0.4 (keeping the existing non-framed as default) until further evaluation for 0.5.;;;","09/Aug/09 10:50;jbellis;I guess it's up to Eric or other people who need Framed.  I'm okay either way.;;;","09/Aug/09 10:58;urandom;I'll follow up with a patch shortly.;;;","10/Aug/09 23:15;jbellis;can you add boolean sanity checking like this?

            String syncRaw = xmlUtils.getNodeValue(""/Storage/CommitLogSync"");
            if (!""false"".equals(syncRaw) && !""true"".equals(syncRaw))
            {
                // Bool.valueOf will silently assume false for values it doesn't recognize
                throw new ConfigurationException(""Unrecognized value for CommitLogSync.  Use 'true' or 'false'."");
            }
            commitLogSync_ = Boolean.valueOf(syncRaw);
;;;","10/Aug/09 23:50;urandom;I was trying to make this as optional as possible, and the pattern cited above doesn't support that.

For the vast majority of users this doesn't make any sense whatsoever; tweaking it is more likely to break, rather than enable. I really didn't even want to draw the sort of attention to it that an example entry in the sample config provides (and I'm still sort of wishing it was less ""documented"").

However, since I already feel ""beaten"" by this issue I'll update the patch if you feel it should be mandatory. :);;;","11/Aug/09 02:50;jbellis;+1;;;","11/Aug/09 02:54;euphoria;+1, looks good.  Thanks for accommodating, Eric.;;;","11/Aug/09 02:57;urandom;committed.;;;","11/Aug/09 20:57;hudson;Integrated in Cassandra #164 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/164/])
     optional support for framed transport
;;;","14/Nov/09 02:19;esteve;THRIFT-210 is fixed as of rev 835006. As for THRIFT-538, I think nobody is working on it, the Cocoa, Haskell, Ocaml and Smalltalk ports are not actively maintained, AFAIK.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replayed log data is not flushed before logs are wiped,CASSANDRA-204,12426618,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jbellis,jbellis,jbellis,29/May/09 03:36,16/Apr/19 17:33,22/Mar/23 14:57,29/May/09 05:10,0.3,,,,0,,,,,,"The memtable created by replaying commit logs on startup is supposed to be flushed as a SSTable before the commitlog is removed, but this is not happening.  So you can lose data by doing the following:

1. insert data
2. restart cassandra (using kill, to force replay)
3. restart cassandra again
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/May/09 03:46;jbellis;204.patch;https://issues.apache.org/jira/secure/attachment/12409297/204.patch",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19593,,,Thu May 28 21:10:27 UTC 2009,,,,,,,,,,"0|i0fxi7:",91026,,,,,Critical,,,,,,,,,,,,,,,,,"29/May/09 03:46;jbellis;Sorry, cleanup got mixed in with the bugfix here.

The fix is, call MT.put instead of putOnRecover.  pOR() didn't set dirty, so it was basically a broken put().  Removed it.  (Client code goes through apply(), which schedules put() on the executor.  So recover calling put directly is exactly what we want.)

The rest is removing comments that weren't actually accurate, and making startup messages more signal than noise (which is what made the problem clear).;;;","29/May/09 04:35;jbellis;s/schedules on the executor/locks and checks for flush/;;;","29/May/09 05:01;junrao;Patch looks fine to me.
;;;","29/May/09 05:10;jbellis;committed to 0.3 and merged to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SuperColumn.getSubColumn fails assertion when subcolumn is not present,CASSANDRA-91,12423322,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,sandeep_tata,sandeep_tata,sandeep_tata,21/Apr/09 09:15,16/Apr/19 17:33,22/Mar/23 14:57,21/Apr/09 11:07,0.3,,,,0,,,,,,"When a subColumn is not present, SuperColumn.getSubColumn fails an assertion instead of simply returning null. Returning null will let us write much cleaner unit tests for supercolumns. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Apr/09 09:16;sandeep_tata;CASSANDRA-91.patch;https://issues.apache.org/jira/secure/attachment/12405978/CASSANDRA-91.patch",,,,,,,,,,,,,,1.0,sandeep_tata,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19545,,,Tue Apr 21 03:07:58 UTC 2009,,,,,,,,,,"0|i0fwtj:",90915,,,,,Normal,,,,,,,,,,,,,,,,,"21/Apr/09 09:16;sandeep_tata;This has to be the easiest patch to review :-) Who wants it?;;;","21/Apr/09 11:07;jbellis;SuperColumn patch did not apply (!?) but not a big deal.  Changed assert to 

        assert column == null || column instanceof Column;

also removed the assert on value(String key) that was kind of a no-op (value() would error out if the isinstance were false, and there was a separate throw for null checking).;;;","21/Apr/09 11:07;jbellis;(applied);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Commitlog.recover can delete the commitlog segment instance() just created,CASSANDRA-1644,12478025,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jbellis,jbellis,jbellis,22/Oct/10 03:51,16/Apr/19 17:33,22/Mar/23 14:57,22/Oct/10 07:14,0.7 beta 3,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Oct/10 03:53;jbellis;1644.txt;https://issues.apache.org/jira/secure/attachment/12457778/1644.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20235,,,Fri Oct 22 12:52:28 UTC 2010,,,,,,,,,,"0|i0g6hz:",92483,,gdusbabek,,gdusbabek,Critical,,,,,,,,,,,,,,,,,"22/Oct/10 04:09;jbellis;As the comment says:

+                // we used to try to avoid instantiating commitlog (thus creating an empty segment ready for writes)
+                // until after recover was finished.  this turns out to be fragile; it is less error-prone to go
+                // ahead and allow writes before recover(), and just skip active segments when we do.
;;;","22/Oct/10 04:29;gdusbabek;+1;;;","22/Oct/10 07:14;jbellis;committed;;;","22/Oct/10 20:52;hudson;Integrated in Cassandra #573 (See [https://hudson.apache.org/hudson/job/Cassandra/573/])
    fix commitlog recovery deleting the newly-created segment as well as the old ones
patch by jbellis; reviewed by gdusbabek for CASSANDRA-1644
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"SSTables limited to (2^31)/15 keys",CASSANDRA-790,12456197,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,stuhood,stuhood,stuhood,13/Feb/10 06:27,16/Apr/19 17:33,22/Mar/23 14:57,16/Feb/10 12:01,0.5,,,,0,,,,,,"The current BloomFilter implementation requires a BitSet of (bucket_count * num_keys) in size, and that calculation is currently performed in an integer, which causes overflow for around 140 million keys in one SSTable.

Short term fix: perform the calculation in a long, and cap the value to the maximum size of a BitSet.
Long term fix: begin partitioning BitSets, perhaps using Linear Bloom Filters.",,bendiken,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-1555,,,,,,,,"16/Feb/10 06:01;stuhood;0001-Change-parameters-to-BloomCalculations-in-order-to-c.patch;https://issues.apache.org/jira/secure/attachment/12435910/0001-Change-parameters-to-BloomCalculations-in-order-to-c.patch","14/Feb/10 12:19;stuhood;0001-Change-parameters-to-BloomCalculations-in-order-to-c.patch;https://issues.apache.org/jira/secure/attachment/12435802/0001-Change-parameters-to-BloomCalculations-in-order-to-c.patch","16/Feb/10 06:01;stuhood;0002-Add-timeouts-to-forceBlockingFlush-during-tests.patch;https://issues.apache.org/jira/secure/attachment/12435911/0002-Add-timeouts-to-forceBlockingFlush-during-tests.patch","14/Feb/10 12:19;stuhood;0002-Add-timeouts-to-forceBlockingFlush-during-tests.patch;https://issues.apache.org/jira/secure/attachment/12435803/0002-Add-timeouts-to-forceBlockingFlush-during-tests.patch",,,,,,,,,,,4.0,stuhood,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19863,,,Tue Feb 16 04:01:01 UTC 2010,,,,,,,,,,"0|i0g13b:",91607,,,,,Critical,,,,,,,,,,,,,,,,,"13/Feb/10 06:35;stuhood;Here is the short term solution.;;;","13/Feb/10 06:41;jbellis;rather than just cap the filter size, you need to reduce bucketsPerElement too or you will get suboptimal number-of-hashes calculation, since you haven't told it it can't have as big a filter as it wanted.;;;","14/Feb/10 12:19;stuhood;Better implementation of the short term solution. Rather than capping the total number of buckets, this patch caps the number of buckets per element.

The second patch adds timeouts to blocking flushes, to minimize hanging tests.;;;","15/Feb/10 06:02;kingryan;Thanks for the work on this. I'll try and test it soon in the situation that brought up the problem.;;;","16/Feb/10 03:12;jbellis;patch 01 comments:

refactoring:
 - please split the refactoring into a separate patch; it's hard to tell what is part of the actual fix here
 - BF constructors that do not chain is a design smell; one of them only being called from tests is also a smell
 - instead of using min/max to force values into acceptable ranges, assert that they are sane
 - I feel part of the BF problems here is that BF is trying to be too high-level.  Wouldn't we be better served by having a low-level BF constructor taking hash & bucket counts, and then factories to do the high level things?

fix:
 - this feels like we're trading an obvious problem (BF constructor throws) for a more subtle one (BF is a no-op when we exceed the spec, as noted by TODO).  wouldn't it be better to log a warning, create the largest BF possible, and degrade gracefully?  This would be easier if the BF constructor were sane as mentioned above.;;;","16/Feb/10 03:21;stuhood;> please split the refactoring into a separate patch
Which refactoring? I don't think I did anything that wasn't necessary in order to cap the number of available buckets.

> BF constructors that do not chain is a design smell; one of them only being called from tests is also a smell
The 'maxFalsePosProb' constructor has never been called anywhere but tests, but it was very elegant, and someone spent a lot of time on it, so I wasn't sure whether to remove it.

> low-level BF constructor taking hash & bucket counts, and then factories to do the high level things
Agreed... that would be much better. I'll add factories with warnings.;;;","16/Feb/10 03:25;jbellis;> Which refactoring

the method renaming and constructor rearrangement;;;","16/Feb/10 06:01;stuhood;Updated to use factory functions rather than constructors. Also, for the factory function that takes a bucket/elem target, if the target can't be achieved, a warning will be logged, and the filter will degrade as gracefully as possible by using a minimum of 1 bucket/elem, and the maximum size filter.;;;","16/Feb/10 12:01;jbellis;committed 01 to 0.5 and trunk, with some changes (primarily using real exceptions for critical checks instead of asserts, and asserts instead of min() calls for others)

I'm not seeing any problems with flush that 02 is supposed to mitigate -- if you are, we should fix the code (or the test) to not be so sucky instead of not actually blocking for the requested op.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
forceFlush skips flush when there are pending operations,CASSANDRA-141,12424716,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,06/May/09 23:33,16/Apr/19 17:33,22/Mar/23 14:57,19/May/09 11:16,0.4,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/May/09 06:39;jbellis;141-2.patch;https://issues.apache.org/jira/secure/attachment/12407409/141-2.patch","06/May/09 23:34;jbellis;141.patch;https://issues.apache.org/jira/secure/attachment/12407364/141.patch",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19563,,,Thu May 07 14:55:33 UTC 2009,,,,,,,,,,"0|i08c2n:",46555,,,,,Normal,,,,,,,,,,,,,,,,,"07/May/09 00:48;urandom;+1;;;","07/May/09 01:04;jbellis;committed;;;","07/May/09 06:26;jbellis;still happens (less frequently);;;","07/May/09 06:39;jbellis;kill race condition DEAD;;;","07/May/09 21:35;hudson;Integrated in Cassandra #63 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/63/])
    force flush when there are pending ops on the memtable executor even when none have finished yet.  this fixes test case intermittent failures.  patch by jbellis; reviewed by Eric Evans for 
;;;","07/May/09 22:55;urandom;+1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-cli still relies on cassandra.in.sh instead of cassandra-env.sh,CASSANDRA-1446,12472890,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,urandom,brandon.williams,brandon.williams,31/Aug/10 06:45,16/Apr/19 17:33,22/Mar/23 14:57,31/Aug/10 11:15,0.6.6,0.7 beta 2,,,0,,,,,,"When we switched to cassandra-env.sh, we neglected to change the cli as well.  This leads to people unable to launch to the client due to heap size, and not having any idea how to change the heap for the cli itself.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20143,,,Sun Sep 12 19:39:05 UTC 2010,,,,,,,,,,"0|i0g53z:",92258,,,,,Low,,,,,,,,,,,,,,,,,"31/Aug/10 07:26;urandom;cassandra-cli never used the set of arguments from cassandra.in.sh which included (among other things) the heap size.  This was on purpose, the memory requirements of the CLI should be negligible, and there is definitely no correlation to the memory needed by the server. 

Do you know of a specific problem here? Can you provide any specifics?;;;","31/Aug/10 08:06;brandon.williams;The specific problem I'm seeing here is that the user can launch cassandra itself, but doesn't have enough memory to launch cassandra-cli afterwards, and there isn't an intuitive way to change cassandra-cli's heap.  I see cassandra-cli's script specifically trying to include cassandra.in.sh, though, on second look, it does not appear to be doing anything with it.  Maybe we should just remove the cassandra.in.sh cruft.;;;","31/Aug/10 10:57;urandom;{quote}
The specific problem I'm seeing here is that the user can launch cassandra itself, but doesn't have enough memory to launch cassandra-cli afterwards, and there isn't an intuitive way to change cassandra-cli's heap.
{quote}

Interesting.  I would have thought the default max heap size would be something... reasonable, but on my machine it seems to be 987MB (the hell?!).

Anyway, I just checked in a change that sets this to 256MB.  I don't know if that's the optimal value but it's considerably less than the almost-a-gig default.  The only reason it would ever need to be bigger than say 16MB is to buffer larger responses, and 256MB seems like a gracious plenty for any practical use of our CLI.

{quote}
I see cassandra-cli's script specifically trying to include cassandra.in.sh, though, on second look, it does not appear to be doing anything with it. Maybe we should just remove the cassandra.in.sh cruft.
{quote}

It is using it though, it's using it to setup the classpath; cassandra.in.sh is basically the common code for setting the classpath in all of those scripts.;;;","31/Aug/10 11:02;brandon.williams;256M works for me, but let's put this in 0.6 too.;;;","31/Aug/10 11:15;urandom;committed new max heap to 0.6 and trunk;;;","13/Sep/10 03:39;hudson;Integrated in Cassandra #533 (See [https://hudson.apache.org/hudson/job/Cassandra/533/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException when calling get_range_slice(),CASSANDRA-643,12443739,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,vomjom,vomjom,18/Dec/09 17:53,16/Apr/19 17:33,22/Mar/23 14:57,22/Dec/09 01:29,0.5,,,,0,,,,,,"This bug affects 0.5.0beta2.

In the SlicePredicate struct, column_names is defined as ""optional"", and so can be null.

get_range_slice() passes the ""predicate.getColumn_names()"" to ""ThriftValidation.validateColumns()"" without first checking if it's null, resulting in the following exception:

java.lang.NullPointerException
	at org.apache.cassandra.service.ThriftValidation.validateColumns(ThriftValidation.java:162)
	at org.apache.cassandra.service.ThriftValidation.validateColumns(ThriftValidation.java:181)
	at org.apache.cassandra.service.CassandraServer.get_range_slice(CassandraServer.java:562)
	at org.apache.cassandra.service.Cassandra$Processor$get_range_slice.process(Cassandra.java:1024)
	at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:817)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)

Strangely enough, calling it from python doesn't generate this exception, but calling it from C++ does, with the following code:
shared_ptr<TTransport> transport(new TSocket(host, port));
shared_ptr<TProtocol> proto(new TBinaryProtocol(transport));
CassandraClient client(proto);
transport->open();

ColumnParent column_parent;
column_parent.column_family = ""Test"";
SlicePredicate slice_predicate;

std::vector<KeySlice> keys;
client.get_range_slice(keys, keyspace, column_parent, slice_predicate, """", """", 100, ONE);


The fix is trivial, just check that predicate.getColumn_names() is not null in line 562 of CassandraServer.java",Linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Dec/09 05:00;jbellis;643.patch;https://issues.apache.org/jira/secure/attachment/12428481/643.patch",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19796,,,Mon Dec 21 17:29:39 UTC 2009,,,,,,,,,,"0|i0g073:",91462,,,,,Normal,,,,,,,,,,,,,,,,,"19/Dec/09 05:00;jbellis;range and column_names are both optional, but exactly one must be present.  (thrift IDL does not provide a way to specify that more rigorously.)

attached patch will return an InvalidRequestException if a SlicePredicate does not conform to this.;;;","22/Dec/09 00:56;vomjom;+1

Thanks.;;;","22/Dec/09 01:29;jbellis;committed to 0.5 and trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
disallow to querying a counter CF with non-counter operation,CASSANDRA-2321,12501289,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,mubarak.seyed,mubarak.seyed,13/Mar/11 13:27,16/Apr/19 17:33,22/Mar/23 14:57,30/Mar/11 04:39,0.8 beta 1,,,,0,,,,,,"CounterColumnType.getString() returns hexString.

{code}
public String getString(ByteBuffer bytes)
{ 
       return ByteBufferUtil.bytesToHex(bytes);
}
{code}
and python stress.py reader returns


[ColumnOrSuperColumn(column=None, super_column=SuperColumn(name='19', columns=[Column(timestamp=1299984960277, name='56', value='\x7f\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00\x00\x00\x00\x00\x00,', ttl=None), Column(timestamp=1299985019923, name='57', value='\x7f\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00;\x00\x00\x00\x00\x00\x00\x08\xfd', ttl=None))]",Linux,cburroughs,johanoskarsson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Mar/11 20:58;slebresne;0001-Don-t-allow-normal-query-on-counter-CF.patch;https://issues.apache.org/jira/secure/attachment/12474612/0001-Don-t-allow-normal-query-on-counter-CF.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20560,,,Tue Mar 29 23:53:54 UTC 2011,,,,,,,,,,"0|i0gaov:",93162,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"16/Mar/11 00:08;slebresne;Actually, having getString() return hexString is the right thing to do, because internally counters are bytes and getString is used by sstable2json in particular.

The problem however is that it is not disallowed to query a counter CF with non-counter operation. Attaching a patch to correct this. This is a bigger patch that one would hope because ThriftValidation doesn't help. So the patch does a bunch of refactoring to allow what it must do. As a side node, the refactoring makes it more efficient (We don't revalidate the column family for each mutation of a batch_mutate).

Btw, stress.py has no support for counters, but stress.java has it. ;;;","25/Mar/11 20:58;slebresne;Rebased patch attached;;;","26/Mar/11 01:37;jbellis;I'm not sure this refactor is much of an improvement.  Seems like the code moves around a lot but volume and complexity are not really reduced.

Can't we just add a ""validateCounterCF"" to counter calls and ""validateNonCounterCF"" otherwise?

(I'd prefer to say ""counter/noncounter"" vs ""noncommutative/commutative"" but if you really prefer the other that's okay too.);;;","26/Mar/11 03:00;slebresne;bq. I'm not sure this refactor is much of an improvement. Seems like the code moves around a lot but volume and complexity are not really reduced.

I do believe the refactor is an improvement (granted, maybe not a huge one). During validation we do a bunch of queries to DatabaseDescriptor to check the cf exists, then to get its type (super or standard), then to get its value validator, etc... Granted those are just hashMaps gets, but they are just unnecessary. I do think that the method used by the refactor, that is getting the metadata once and giving it to all other method is cleaner. There also the fact that we revalidate the column family for each Mutation of a batch_mutate, even if all of them are on the same cf, which this refactor fixes too.
I don't care so much about those and won't fight over it, but I think it would be a pity to leave ThriftValidation in that state (and the refactoring is really trivial). Note that I'll be perfectly ok with moving the refactor in another ticket if that makes it better.

bq. Can't we just add a ""validateCounterCF"" to counter calls and ""validateNonCounterCF"" otherwise?

Yes we could, up to the fact that the refactor would still make sense I think for the reason above :)

bq. (I'd prefer to say ""counter/noncounter"" vs ""noncommutative/commutative"" but if you really prefer the other that's okay too.)

When I have a few free cycles I plan to remove the 'commutative' wording completely as I think this was premature generalization and make the code less readable right now. So I'm on your side, I just went for consistency with the rest of the code for now.;;;","30/Mar/11 04:39;jbellis;committed;;;","30/Mar/11 07:53;hudson;Integrated in Cassandra #817 (See [https://hudson.apache.org/hudson/job/Cassandra/817/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make BinaryMemtable work,CASSANDRA-337,12432082,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,lenn0x,jbellis,jbellis,04/Aug/09 03:42,16/Apr/19 17:33,22/Mar/23 14:57,29/Aug/09 00:28,0.4,,,,0,,,,,,,,hammer,johanoskarsson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Aug/09 09:18;lenn0x;0001-Refactored-and-merged-BMT-to-MT-codepath-v3.patch;https://issues.apache.org/jira/secure/attachment/12417945/0001-Refactored-and-merged-BMT-to-MT-codepath-v3.patch","25/Aug/09 11:37;lenn0x;0001-Working-version-of-BMT.patch;https://issues.apache.org/jira/secure/attachment/12417565/0001-Working-version-of-BMT.patch","26/Aug/09 23:47;lenn0x;0002-Added-flushAndshutdown-v2.patch;https://issues.apache.org/jira/secure/attachment/12417746/0002-Added-flushAndshutdown-v2.patch","28/Aug/09 01:29;lenn0x;0003-Removed-decorateKey-in-writer.append-in-BMT.patch;https://issues.apache.org/jira/secure/attachment/12417901/0003-Removed-decorateKey-in-writer.append-in-BMT.patch","11/Aug/09 22:19;jbellis;337-rebased.patch;https://issues.apache.org/jira/secure/attachment/12416204/337-rebased.patch","28/Aug/09 23:45;jbellis;337-v3.patch;https://issues.apache.org/jira/secure/attachment/12417996/337-v3.patch","04/Aug/09 04:03;jbellis;337.patch;https://issues.apache.org/jira/secure/attachment/12415399/337.patch",,,,,,,,7.0,lenn0x,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19640,,,Sat Aug 29 12:35:41 UTC 2009,,,,,,,,,,"0|i0fybj:",91158,,,,,Normal,,,,,,,,,,,,,,,,,"04/Aug/09 04:03;jbellis;my humble contribution to the cleanup: use CFS executor for BMt flushes;;;","11/Aug/09 22:19;jbellis;rebased;;;","12/Aug/09 00:35;urandom;+1;;;","12/Aug/09 00:55;euphoria;I don't understand the scope of this issue and the submitted patches.  Can anyone elaborate?;;;","12/Aug/09 01:16;jbellis;BinaryMemtable is intended to allow writing pre-serialized data, but FB never really cleaned it up to make it useful for anyone but them.  Chris has done that and this ticket is to remind him to post diffs.

My patch just removes an unnecessary extra executor and class and has BMt use the CFS executor for flushes like normal Mt.;;;","12/Aug/09 01:17;jbellis;committed the CFS patch.;;;","12/Aug/09 01:41;lenn0x;-1

This should be reverted. As my discussion with Jonathan,  we want to make the Binary Memtable Flusher thread configurable, and we wouldn't want to have it under the same path as cfStore, since that should be a different tunable. I'll have my patches ready later this week for Binary Memtable work we have done at Digg.;;;","12/Aug/09 21:08;hudson;Integrated in Cassandra #165 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/165/])
    use CFS executor for BMt flushes.
patch by jbellis; reviewed by Eric Evans for 
;;;","25/Aug/09 11:37;lenn0x;Working version of BMT that we are running at Digg. Hopefully I didn't miss anything. Wanted to post this before I forget about it :);;;","25/Aug/09 11:43;lenn0x;Example of using BMT:

http://github.com/lenn0x/Cassandra-Hadoop-BMT/tree/master
;;;","26/Aug/09 05:07;jbellis;is there overlap b/t this and CASSANDRA-342?;;;","26/Aug/09 07:47;lenn0x;Looking at his code, no. I made some minor interface changes such as flushBinary() public but everything I have done affects the BMT work already there. He seems to be adding functionality, I have no Hadoop code in my BMT.;;;","26/Aug/09 13:23;jmhodges;Yeah, this looks to be clear of any interaction with CASSANDRA-342.;;;","26/Aug/09 14:39;lenn0x;Forgot to add the flushAndShutdown method;;;","26/Aug/09 21:57;johanoskarsson;I assume the two last patches are to be applied together? If I try to build trunk + those patches I get the following error:
    [javac] /home/cassandra/cassandra-trunk/src/java/org/apache/cassandra/net/MessagingService.java:505: cannot find symbol
    [javac] symbol  : method getConnections()
    [javac] location: class org.apache.cassandra.net.TcpConnectionManager
    [javac]             for(TcpConnection connection: entry.getValue().getConnections())
;;;","26/Aug/09 23:47;lenn0x;Fixed, forgot the getConnections;;;","28/Aug/09 01:29;lenn0x;Removed decorateKey() in writer.append;;;","28/Aug/09 04:17;jbellis;I still don't see any reason to use separate executors for BMT flush and normal MT.  In fact I was thinking normal MT should probably have threads = #disks.  (Why do you use 2x?)  I'd favor keeping a single executor, with configurable thread count.;;;","28/Aug/09 05:23;lenn0x;I can live with merging the executors for BMT flush and normal MT. Though I want the ability to specify a min and max for the threads in storage-conf.xml. During an import, we would want the ability to have a certain quality of service (I want a min of 4 threads during normal usage, but when we start binary import, I want a higher thread count). 

Merging both executors makes it a little harder, but the benefit is less code/cruft to manage -- that's a win for sure. When we were benchmarking our 7 node cluster with 600-800Mb/s per node we saw lower memory usage/better throughput on the flushing by having 2 threads per disk.

If this is acceptable (above) I can submit a patch.;;;","28/Aug/09 05:29;jbellis;> I want the ability to specify a min and max for the threads in storage-conf.xml

works for me.;;;","28/Aug/09 09:18;lenn0x;Refactored, and merged BMT to MT codepath. Added FlushMinThreads and FlushMaxThreads.;;;","28/Aug/09 23:48;jbellis;looking good.  here is my version with light cleanup done.

but I don't see a way to check if a flushandshutdown is complete, and it's safe to kill the node?

should we make           logger_.debug(""Shutdown invocation complete."");

run at info level instead of debug?  or something more sophisticated?;;;","29/Aug/09 00:08;lenn0x;I think going to logger_.info would be fine. Me and Sammy plan on opening another ticket for better shutdown support in Cassandra. We could get more sophisticated there.;;;","29/Aug/09 00:28;jbellis;committed v3 w/ the log info change.

do you want to submit the demo from github for contrib/ in another ticket?;;;","29/Aug/09 00:43;lenn0x;Sure thing. See CASSANDRA-398;;;","29/Aug/09 20:35;hudson;Integrated in Cassandra #181 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/181/])
    Fixes to make BinaryMemtable useful.  Highlights are configurable threads for [binary]memtable flushing and flushAndShutdown JMX/nodeprobe directive.
patch by Chris Goffinet; reviewed by jbellis for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra does not expire data after setting timeToLive argument for each value,CASSANDRA-1109,12464905,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jigneshdhruv,jigneshdhruv,jigneshdhruv,19/May/10 23:57,16/Apr/19 17:33,22/Mar/23 14:57,20/May/10 07:38,0.7 beta 1,,,,1,,,,,,"Hello,

I downloaded latest cassandra source code from svn trunk. I wanted to test expire data functionality. Using Thrift API, I set timeToLive parameter for each fieldValue, however cassandra ignored it and did not expire any data.

I debugged cassandra's source code and found a bug in src/java/org/apache/cassandra/db/RowMutation.java.

In RowMutation.addColumnOrSuperColumnToRowMutation() method, QueryPath was not setting timeToLive argument. I updated RowMutation.java locally and tested it and then my data expired after 'n' number of seconds.

I wanted to have this fix in the trunk also.

Index: src/java/org/apache/cassandra/db/RowMutation.java
===================================================================
--- src/java/org/apache/cassandra/db/RowMutation.java   (revision 946222)
+++ src/java/org/apache/cassandra/db/RowMutation.java   (working copy)
@@ -295,12 +295,12 @@
         {
             for (org.apache.cassandra.thrift.Column column : cosc.super_column.columns)
             {
-                rm.add(new QueryPath(cfName, cosc.super_column.name, column.name), column.value, column.timestamp);
+                rm.add(new QueryPath(cfName, cosc.super_column.name, column.name), column.value, column.timestamp, column.ttl);
             }
         }
         else
         {
-            rm.add(new QueryPath(cfName, null, cosc.column.name), cosc.column.value, cosc.column.timestamp);
+            rm.add(new QueryPath(cfName, null, cosc.column.name), cosc.column.value, cosc.column.timestamp, cosc.column.ttl);
         }
     }


Thanks,
Jignesh",,billa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/May/10 01:23;slebresne;0001-System-test-for-1109.patch;https://issues.apache.org/jira/secure/attachment/12444959/0001-System-test-for-1109.patch","20/May/10 00:02;jigneshdhruv;CASSANDRA-1109.patch;https://issues.apache.org/jira/secure/attachment/12444953/CASSANDRA-1109.patch",,,,,,,,,,,,,2.0,jigneshdhruv,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19996,,,Thu May 20 12:42:38 UTC 2010,,,,,,,,,,"0|i0g31z:",91925,,,,,Normal,,,,,,,,,,,,,,,,,"20/May/10 00:02;jigneshdhruv;Patch Submitted.;;;","20/May/10 00:58;slebresne;Indeed, good catch. Thanks

+1 on the patch;;;","20/May/10 01:23;slebresne;And because everything's look better with a system test, adding one;;;","20/May/10 07:38;jbellis;committed.  thanks!;;;","20/May/10 20:42;hudson;Integrated in Cassandra #441 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/441/])
    test + fix for expiring columns.  patch by Jignesh Dhruv and Sylvain Lebresne for CASSANDRA-1109
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Interrupted recovery requires manual intervention to fix,CASSANDRA-78,12422699,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jbellis,jbellis,jbellis,13/Apr/09 23:02,16/Apr/19 17:33,22/Mar/23 14:57,06/May/09 03:59,0.3,,,,0,,,,,,"Originally reported by Alexander Staubo: ""If you kill the server while it is going through its initial ""row recovery"" phase, you risk ending up with a database that's corrupt and will fail with ""negative seek"" exceptions and similar.""

Prashant replied:

""The commit logs are only deleted after a successful recovery. You should still have teh commit log if u killed the server while recovering ? When u restart the server it should generate a new file , for compactions we name intermediate files with a .tmp and only on successful dump do we place them as usable files , this same logic is required at recovery and there is a fix coming up which will do it .

""So with the state that u have today there will  be no data loss ass commit logs still exist but its a round about process to recover it since now u haave to delete the intermediate file and then do teh recovery again.""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/May/09 03:18;jbellis;0001-clean-up-anticompaction-code-a-little.patch;https://issues.apache.org/jira/secure/attachment/12407032/0001-clean-up-anticompaction-code-a-little.patch","02/May/09 03:18;jbellis;0002-use-getTempFileName-closeRename-to-avoid-problems.patch;https://issues.apache.org/jira/secure/attachment/12407033/0002-use-getTempFileName-closeRename-to-avoid-problems.patch",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19536,,,Tue May 05 19:59:09 UTC 2009,,,,,,,,,,"0|i0fwqn:",90902,,,,,Critical,,,,,,,,,,,,,,,,,"02/May/09 02:47;jbellis;Prashant's memory seems to be wrong here -- the only place getTempFileName is in the anticompaction (bootstrap) code.  Everything else uses getNextFileName directly.;;;","02/May/09 03:50;jbellis;while writing these patches I checked and closeRename does fsync (via FileChannel.force).  Have not audited commitlog similarly.

Additionally, the bootstrap code uses the potentially unsafe CFS.getFileName instead of getTempFileName.  Not sure if there's actually a problem there.  (Just a note to self to come back to this after 0.3);;;","04/May/09 23:52;urandom;+1;;;","05/May/09 21:43;hudson;Integrated in Cassandra #59 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/59/])
    use getTempFileName / closeRename to avoid problems w/ half-written sstables.
patch by jbellis; reviewed by Eric Evans for 
clean up anticompaction code a little.
patch by jbellis; reveiewed by Eric Evans for 
;;;","06/May/09 03:59;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the read race condition in CFStore for counters ,CASSANDRA-2105,12497550,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,03/Feb/11 18:51,16/Apr/19 17:33,22/Mar/23 14:57,28/Mar/11 22:45,0.8 beta 1,,,,1,counters,,,,,"There is a (known) race condition during counter read. Indeed, for standard
column family there is a small time during which a memtable is both active and
pending flush and similarly a small time during which a 'memtable' is both
pending flush and an active sstable. For counters that would imply sometime
reconciling twice during a read the same counterColumn and thus over-counting.

Current code changes this slightly by trading the possibility to count twice a
given counterColumn by the possibility to miss a counterColumn. Thus it trades
over-counts for under-counts.

But this is no fix and there is no hope to offer clients any kind of guarantee
on reads unless we fix this.
",,cburroughs,stuhood,,,,,,,,,,,,,,,,,,,,14400,14400,,0%,14400,14400,,,,,,,,,,,,,,,,,,,,"05/Feb/11 00:56;slebresne;2115_option1_withLock.patch;https://issues.apache.org/jira/secure/attachment/12470250/2115_option1_withLock.patch","05/Feb/11 00:56;slebresne;2115_option2_nolock.patch;https://issues.apache.org/jira/secure/attachment/12470251/2115_option2_nolock.patch",,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20446,,,Mon Mar 28 14:45:59 UTC 2011,,,,,,,,,,"0|i0g9cf:",92944,,,,,Normal,,,,,,,,,,,,,,,,,"03/Feb/11 18:51;slebresne;I plan to reuse the fix I had made for #1546 that uses a ReadWriteLock to fix
this (unless I find a better idea in the meantime). Unless proven otherwise I
don't think this will have a huge impact on counter read performance, but if
someone finds a better idea, I'm listening.;;;","05/Feb/11 00:56;slebresne;Attached not 1 but 2 options for this patch. I'm not sure with which version to go so I'm asking for opinions.

Version 1 is the one extracted from #1546. It uses a ReadWriteLock to protect from the race condition.

Version 2 don't use a lock. So less chances of lock contention which is always good. Only problem is, it still suffers in theory of a race condition. But I think this race condition is borderline impossible.
Basically, given a memtable m being flushed, let's call s(m) the sstable initially produced by its flushing and let's denote by s'(m) any sstable resulting of the compaction of s(m). The race is if a read thread sees m when grabbing the references to the memtable being flushed and sees s'(m) (not s(m), that is the initial race condition and this is not impossible at all) when grabing the reference to the sstables.
If it's unclear, the code has a comment explaining this that may be more clear.

So not sure which version to go with. I may slightly lean towards Version 1 because I usually side with correction before anything else, but since this is in a critical path it feels slightly wasteful to use a lock for this given how remote the race condition of version 2 seems.
;;;","08/Mar/11 22:52;slebresne;I've opened CASSANDRA-2284 that provides what I think is a better solution than the one I have attached previously to this problem (I've opened it separately because it's a more generic solution, not just a counter related fix).

;;;","28/Mar/11 22:36;hudson;Integrated in Cassandra #810 (See [https://hudson.apache.org/hudson/job/Cassandra/810/])
    Atomically switch cfstore memtables and sstables
patch by slebresne; reviewed by jbellis for CASSANDRA-2284 (and CASSANDRA-2105)
;;;","28/Mar/11 22:45;slebresne;Fixed by CASSANDRA-2284;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
update contrib/bmt_example to work post-CASSANDRA-44,CASSANDRA-1062,12463992,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,gdusbabek,jbellis,jbellis,07/May/10 22:39,16/Apr/19 17:33,22/Mar/23 14:57,19/May/10 20:46,0.7 beta 1,,,,0,,,,,,the main problem is we need to pass an id to new ColumnFamily(...).  Not sure what the right fix is here.,,johanoskarsson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/May/10 21:32;gdusbabek;0001-make-CassandraBulkLoader-compile.patch;https://issues.apache.org/jira/secure/attachment/12444790/0001-make-CassandraBulkLoader-compile.patch",,,,,,,,,,,,,,1.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19980,,,Thu May 20 12:42:37 UTC 2010,,,,,,,,,,"0|i0g2rj:",91878,,,,,Low,,,,,,,,,,,,,,,,,"18/May/10 02:13;gdusbabek;I don't see anything that needs to be done here, except maybe some documentation along the lines of ""make sure your schema is set up.""

CBL only accesses the cluster via fat client, which collects schema info via gossip.;;;","18/May/10 03:03;jbellis;the call to the CF constructor no longer compiles;;;","19/May/10 07:58;jbellis;+1;;;","20/May/10 20:42;hudson;Integrated in Cassandra #441 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/441/])
    make CassandraBulkLoader compile. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1062
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LongType should be network-endian,CASSANDRA-384,12433527,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,eweaver,eweaver,20/Aug/09 11:00,16/Apr/19 17:33,22/Mar/23 14:57,21/Aug/09 04:38,0.4,,,,0,,,,,,that's all,,eweaver,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Aug/09 11:00;eweaver;CASSANDRA-384.diff;https://issues.apache.org/jira/secure/attachment/12417091/CASSANDRA-384.diff",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19663,,,Fri Aug 21 12:34:39 UTC 2009,,,,,,,,,,"0|i0fylr:",91204,,,,,Normal,,,,,,,,,,,,,,,,,"20/Aug/09 11:13;euphoria;Looks like that's the JVM's natural endianness anyway.

+1;;;","20/Aug/09 11:35;jbellis;does this pass system tests?

if it does we need a better system test, b/c test_server.py does a bunch of int64s in little endian :);;;","20/Aug/09 11:37;eweaver;dunno...i've never run them

i only rely on my gem tests right now;;;","20/Aug/09 12:07;euphoria;It passes unit tests and system tests, or I wouldn't have +1'd :)
I can't necessarily vouch for the tests.;;;","21/Aug/09 04:38;jbellis;committed w/ new unit test to catch endian problems and switching test_server.py to big endian;;;","21/Aug/09 20:34;hudson;Integrated in Cassandra #174 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/174/])
    change LongType to read longs in big endian order to be consistent with network order and the UUID types.
patch by jbellis and Evan Weaver; reviewed by Michael Greene for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hinted Handoff CF contention,CASSANDRA-658,12444353,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,30/Dec/09 10:44,16/Apr/19 17:33,22/Mar/23 14:57,07/Jan/10 13:41,0.6,,,,0,,,,,,"Hinted handoff causes a lot of contention on the HH CF, causing insert speed to massively drop.  Most of the row mutation stage threads end up blocking on each other at Memtable.resolve.  This is because HH sends the hint to the closest node, which will always be the node handling the write.

To reproduce: start a cluster with even InitialTokens, and begin a constant stream of writes to one node, with an even key distribution. (I used 4 nodes and stress.py in random mode.)  Take a node down, and the insert rate begin to drop, eventually settling between 100-300/s and sustaining there.  Bringing the down node back up will restore the original insert rate.","debian lenny amd64 OpenJDK 64-Bit Server VM (build 1.6.0_0-b11, mixed mode)
",david.pan,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Jan/10 12:02;jbellis;ASF.LICENSE.NOT.GRANTED--0001-use-throughput-and-op-count-instead-of-size-and-column.txt;https://issues.apache.org/jira/secure/attachment/12429240/ASF.LICENSE.NOT.GRANTED--0001-use-throughput-and-op-count-instead-of-size-and-column.txt","01/Jan/10 12:02;jbellis;ASF.LICENSE.NOT.GRANTED--0002-replace-sharded-row-locks-with-column-level-locking.txt;https://issues.apache.org/jira/secure/attachment/12429241/ASF.LICENSE.NOT.GRANTED--0002-replace-sharded-row-locks-with-column-level-locking.txt","01/Jan/10 12:02;jbellis;ASF.LICENSE.NOT.GRANTED--0003-r-m-unused-code.txt;https://issues.apache.org/jira/secure/attachment/12429242/ASF.LICENSE.NOT.GRANTED--0003-r-m-unused-code.txt",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19806,,,Fri Jan 08 12:38:36 UTC 2010,,,,,,,,,,"0|i0g0af:",91477,,,,,Normal,,,,,,,,,,,,,,,,,"01/Jan/10 03:44;stuhood;Brilliant... I really like this change.

I think the main reason that we had an Atomic variable for size() in CF and SC was to perform this calculation, so perhaps that code should be considered dead, and removed. I noticed a bunch of subtle bugs in it last time I was looking anyway. The size() method can remain, and calculate the size for each call?;;;","01/Jan/10 05:18;jbellis;We still want to know size for when we're serializing, so I've left that code alone for now.;;;","01/Jan/10 05:57;stuhood;Removes the atomic size calculation from SuperColumn, which was not being updated for remove() anyway.;;;","01/Jan/10 10:22;jbellis;this actually has a race for simple columns: if a value is preset for a given column name, and two threads (A and B) run the putIfAbsent part at the same time, then A goes into the synchronized block and changes the value, thread C could attempt addColumn and sync on the value put by A, while B syncs on the old value.

This should be fixable by using replace(), not put().;;;","01/Jan/10 12:04;jbellis;new version 2 fixes the races and adds a similar fine-grained approach to SuperColumn (which is not really more expensive, since we're paying the price of using a Concurrent map implementation already).  This includes Stu's size-removal patch.  03 does more cleanup.;;;","07/Jan/10 13:15;brandon.williams;+1, I replicated the exact scenario originally outlined and insert speed does not dramatically drop.;;;","07/Jan/10 13:41;jbellis;committed;;;","07/Jan/10 21:08;hudson;Integrated in Cassandra #316 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/316/])
    replace sharded row locks with column-level locking
patch by jbellis; tested by Brandon Williams for 
use throughput and op count instead of size and column count to determine when to flush, greatly reducing the amount of synchronization required to insert
patch by jbellis; tested by Brandon Williams for 
;;;","08/Jan/10 20:38;hudson;Integrated in Cassandra #317 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/317/])
    update release notes for config changes

Patch by eevans for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix regression in CL.ALL read,CASSANDRA-2094,12497408,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,kelvin,kelvin,kelvin,02/Feb/11 09:14,16/Apr/19 17:33,22/Mar/23 14:57,02/Feb/11 22:33,0.7.1,,,,0,,,,,,"regression:
- digest message object re-used across multiple hosts.

problem:
- shared message id, so the first digest response received will remove the callback for all others.",,cburroughs,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-2038,,,,,,"02/Feb/11 09:16;kelvin;0001-fix-bug-in-2038-via-4826e8c8.patch;https://issues.apache.org/jira/secure/attachment/12470007/0001-fix-bug-in-2038-via-4826e8c8.patch",,,,,,,,,,,,,,1.0,kelvin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20438,,,Wed Feb 02 14:52:10 UTC 2011,,,,,,,,,,"0|i0g99z:",92933,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"02/Feb/11 09:15;kelvin;caught by distributed testing.;;;","02/Feb/11 09:16;kelvin;do not re-use digest message object.;;;","02/Feb/11 09:31;kelvin;future reference: please test w/ RF=3, which is a more interesting case for CL.ALL.;;;","02/Feb/11 22:33;jbellis;Thanks for catching that, Kelvin!  committed.

(FWIW this is a regression from CASSANDRA-1959.);;;","02/Feb/11 22:52;hudson;Integrated in Cassandra-0.7 #234 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/234/])
    remove digestMessage reuse to fix regression from #1959
patch by Kelvin Kakugawa; reviewed by jbellis for CASSANDRA-2094
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
missing imports in CQL Python driver,CASSANDRA-2508,12504694,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,urandom,urandom,20/Apr/11 01:44,16/Apr/19 17:33,22/Mar/23 14:57,26/Apr/11 01:56,0.8.0 beta 2,,Legacy/Tools,,0,cql,,,,,"Try:

bq. cd drivers/py && python -c 'from cql import DateFromTicks; DateFromTicks(1)'

Also:
{{cql.connection}} is missing an import of {{AuthenticationRequest}} from {{ttypes}}, and the exceptions {{NotSupportedError}}, and {{InternalError}}.

Also:
{{marshal.unmarshal_long}} has a NameError waiting to happen in the form of ""unpack""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Apr/11 04:13;jbellis;2508.txt;https://issues.apache.org/jira/secure/attachment/12476770/2508.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20662,,,Tue Apr 19 21:28:03 UTC 2011,,,,,,,,,,"0|i0gbrz:",93338,,thobbs,,thobbs,Normal,,,,,,,,,,,,,,,,,"20/Apr/11 04:13;jbellis;bq. python -c 'from cql import DateFromTicks; DateFromTicks(1)'

fixed

bq. cql.connection is missing an import of AuthenticationRequest from ttypes

fixed

bq. NotSupportedError

changed to builtin NotImplementedError

bq. InternalError

ttypes InternalError should be internal errors on the server; change this to a no-op (in the case of repeated closes) and ValueError (in the case of operation-on-closed-handle), which match the behavior of the file class.

bq. marshal.unmarshal_long has a NameError waiting to happen in the form of ""unpack""

fixed, and also pulled the if out of the unmarshal definition to only execute once.;;;","20/Apr/11 04:28;thobbs;Looks good with the exception of changing the exceptions that are raised.

Internal error refers to cql.InternalError here, and PEP 249 outlines its usage in a way that closely matches the way it was used.
{noformat}
InternalError 
                      
Exception raised when the database encounters an internal
error, e.g. the cursor is not valid anymore, the
transaction is out of sync, etc.  It must be a subclass of
DatabaseError.
{noformat}

PEP 249 also mentions using NotSupportedError explicitly in reference to rollback():
{noformat}
NotSupportedError
          
Exception raised in case a method or database API was used
which is not supported by the database, e.g. requesting a
.rollback() on a connection that does not support
transaction or has transactions turned off.  It must be a
subclass of DatabaseError.
{noformat}
I think it should be kept here.;;;","20/Apr/11 04:40;jbellis;You're right re NSE. Fixed.

InternalError is for _database_ errors not driver errors. Left re-close() as ok; changed cursor()-from-closed-conn to ProgrammingError, matching the sqlite3 behavior.

committed w/ above changes.;;;","20/Apr/11 05:28;hudson;Integrated in Cassandra-0.8 #23 (See [https://hudson.apache.org/hudson/job/Cassandra-0.8/23/])
    fix imports in python cql driver
patch by jbellis; reviewed by thobbs for CASSANDRA-2508
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Javadoc for thrift interface is not generated,CASSANDRA-997,12462357,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,hannes@helma.at,hannes@helma.at,hannes@helma.at,18/Apr/10 23:07,16/Apr/19 17:33,22/Mar/23 14:57,25/May/10 06:34,0.6.3,,Legacy/Documentation and Website,,0,,,,,,In both 0.6 and svn trunk no javadoc is generated for thrift-generated classes like org.apache.cassandra.thrift.Cassandra. The problem is that the wrong directory is included in the javadoc ant target (interface/thrift instead of interface/thrift/gen-java). ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Apr/10 23:09;hannes@helma.at;ASF.LICENSE.NOT.GRANTED--thrift-javadoc.diff;https://issues.apache.org/jira/secure/attachment/12442111/ASF.LICENSE.NOT.GRANTED--thrift-javadoc.diff",,,,,,,,,,,,,,1.0,hannes@helma.at,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19948,,,Mon May 24 22:34:37 UTC 2010,,,,,,,,,,"0|i0g2db:",91814,,,,,Low,,,,,,,,,,,,,,,,,"18/Apr/10 23:09;hannes@helma.at;Simple patch to use proper directory for thrift-generated classes in javadoc task.;;;","25/May/10 06:34;urandom;committed; thanks Hannes.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get_slice_from forces iterating all columns and leaks file handlers with exception ,CASSANDRA-201,12426517,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,junrao,junrao,junrao,28/May/09 05:27,16/Apr/19 17:33,22/Mar/23 14:57,28/May/09 05:41,0.4,,,,0,,,,,,"There are 2 bugs in the get_slice_from code in CFS.java. 
1. The following 2 lines forces all columns to be iterated in each iterator, which is inefficient.
            List<IColumn> L = new ArrayList();
            CollectionUtils.addAll(L, collated);
2. If any exception occurs, the opened file handlers are not closed.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/May/09 05:29;junrao;issue201.patchv1;https://issues.apache.org/jira/secure/attachment/12409212/issue201.patchv1",,,,,,,,,,,,,,1.0,junrao,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19592,,,Thu May 28 12:34:53 UTC 2009,,,,,,,,,,"0|i0fxhj:",91023,,,,,Normal,,,,,,,,,,,,,,,,,"28/May/09 05:29;junrao;Attach a fix.;;;","28/May/09 05:41;jbellis;committed, with extra catch block in case close() throws (so we still release the lock in that case).;;;","28/May/09 20:34;hudson;Integrated in Cassandra #90 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/90/])
    remove unnecessary List creation in getSliceFrom and move close() cleanup for to the finally block.  patch by Jun Rao; reviewed by jbellis for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mapreduce support is broken,CASSANDRA-1700,12478945,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,stuhood,jeromatron,jeromatron,03/Nov/10 09:16,16/Apr/19 17:33,22/Mar/23 14:57,16/Nov/10 23:16,0.6.9,0.7.0 rc 1,,,0,,,,,,Running from a vanilla download of beta3 src.  Tried the word count example and it's broken.  Attaching the stack trace.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Nov/10 13:08;stuhood;0001-Add-testcase-for-wrapping-range-on-member-token.patch;https://issues.apache.org/jira/secure/attachment/12458713/0001-Add-testcase-for-wrapping-range-on-member-token.patch","03/Nov/10 11:36;jeromatron;server_log.txt;https://issues.apache.org/jira/secure/attachment/12458706/server_log.txt","03/Nov/10 09:16;jeromatron;stacktrace.txt;https://issues.apache.org/jira/secure/attachment/12458699/stacktrace.txt",,,,,,,,,,,,3.0,stuhood,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20265,,,Tue Nov 16 16:05:59 UTC 2010,,,,,,,,,,"0|i0g6un:",92540,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"03/Nov/10 11:23;jbellis;The server stacktrace would be more useful than the client one.;;;","03/Nov/10 11:36;jeromatron;Attaching server-side errors.;;;","03/Nov/10 11:55;jbellis;possibly caused by CASSANDRA-1442 which happened during beta3.

looks like the assert
{code}
        assert range instanceof Bounds
               || (!((Range)range).isWrapAround() || range.right.equals(StorageService.getPartitioner().getMinimumToken()))
               : range;
{code}

is failing.  it may no longer be a valid assert.;;;","03/Nov/10 13:08;stuhood;Somehow in all those testcases on 1442 I didn't test the one case that Hadoop uses: a wrapping range centered on a member token. Attaching a patch to add a testcase and fix.;;;","03/Nov/10 22:09;jbellis;committed, thanks!;;;","16/Nov/10 23:05;jbellis;also broken on 0.6 since we backported https://issues.apache.org/jira/browse/CASSANDRA-1722;;;","16/Nov/10 23:16;jbellis;backported fix in r1035656;;;","17/Nov/10 00:05;hudson;Integrated in Cassandra-0.6 #5 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/5/])
    backport CASSANDRA-1700
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Read repair causes tremendous GC pressure,CASSANDRA-2069,12496983,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,28/Jan/11 05:14,16/Apr/19 17:33,22/Mar/23 14:57,16/Feb/11 01:15,0.7.3,,,,0,,,,,,"To reproduce: start a three node cluster, insert 1M rows with stress.java and rf=2.  Take one down, delete its data, then bring it back up and issue 1M reads against it.  After the run is done you will see at least 1 STW long enough to mark the node as dead, often 4 or 5.",,mdennis,,,,,,,,,,,,,,,,,,,,,57600,57600,,0%,57600,57600,,,,,,,,,,,,,,,,,,,,"15/Feb/11 06:44;jbellis;2069-v10.txt;https://issues.apache.org/jira/secure/attachment/12471030/2069-v10.txt","04/Feb/11 14:04;jbellis;2069-v2.txt;https://issues.apache.org/jira/secure/attachment/12470216/2069-v2.txt","04/Feb/11 23:38;jbellis;2069-v3.txt;https://issues.apache.org/jira/secure/attachment/12470239/2069-v3.txt","05/Feb/11 03:37;jbellis;2069-v4.txt;https://issues.apache.org/jira/secure/attachment/12470271/2069-v4.txt","05/Feb/11 05:22;jbellis;2069-v5.txt;https://issues.apache.org/jira/secure/attachment/12470280/2069-v5.txt","08/Feb/11 01:09;jbellis;2069-v6.txt;https://issues.apache.org/jira/secure/attachment/12470472/2069-v6.txt","08/Feb/11 07:48;jbellis;2069-v7.txt;https://issues.apache.org/jira/secure/attachment/12470525/2069-v7.txt","08/Feb/11 13:02;jbellis;2069-v8.txt;https://issues.apache.org/jira/secure/attachment/12470544/2069-v8.txt","09/Feb/11 03:11;jbellis;2069-v9.txt;https://issues.apache.org/jira/secure/attachment/12470615/2069-v9.txt","04/Feb/11 10:14;jbellis;2069.txt;https://issues.apache.org/jira/secure/attachment/12470212/2069.txt",,,,,10.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20425,,,Tue Feb 15 21:43:50 UTC 2011,,,,,,,,,,"0|i0g94v:",92910,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"28/Jan/11 06:08;jbellis;is this also true for 0.6?;;;","28/Jan/11 06:31;brandon.williams;No, only 0.7;;;","29/Jan/11 00:16;jbellis;In the log snippets I saw the heap was legitimately nearly-full during the repair process.  What does MAT show is using all the memory?;;;","04/Feb/11 06:35;brandon.williams;MAT indicates there are millions of pending futuretasks in StorageProxy.repairExecutor.;;;","04/Feb/11 08:05;brandon.williams;Bisecting this indicates two problems.  First, the tremendous GC pressure doesn't exhibit until we increase the newgen size.  If that is ignored and we only observe whether stress.java moves forward (it has the 'keep trying' rather than 'keep going' behavior) then the culprit is r1053245.

Both cases show that RR requests can grow unbounded, which is probably ok, but if it's the same request over and over we still queue it as many times as it's requested.;;;","04/Feb/11 08:44;mdennis;I've observed similar issues with large a large newgen, largish memtables and writing at RF>1 at CL.Q/CL.ALL though it's not as immediate as it is with RR.

We should look into a way at bounding the number of outstanding requests (RR or ""send to replica""), or more aggressively timing out the older requests once we hit some configurable threshold of outstanding requests.
;;;","04/Feb/11 10:14;jbellis;Attached patch does two things:

- allows the repair executor to use threads = # of cores
- stops sending repair requests when the executor queue gets too large.  (""dropped"" requests will be logged by MessagingService along with the other overload-scenario dropping.);;;","04/Feb/11 10:36;jbellis;Working on another approach that should let us throw away repair handlers in the expected case that everyone responds quickly.  This will be kinder to new gen gc since we won't have nearly as many survivors.;;;","04/Feb/11 14:04;jbellis;v2 adds a READ_REPAIR stage and	does resolve of digests that were not checked for the client result on that stage as soon as response() collects all the replies.	If there is a mismatch and we do a re-read of full	resultset, we also check those results on the RR stage based on response() (in AsyncRepairRunner, now in ReadCallback.)

I preserved the	feature	from v1	of not doing repairs if	the RR stage is	full.

Most of the code changes are about getting the right information into ReadCallback (e.g. endpoints) and some ceremony to make static typing happy (IReadCallback).

A side benefit is that StorageProxy.fetchRows is significantly cleaner (no more commandEndpoints or  repairs collections).
;;;","04/Feb/11 23:38;jbellis;bq. I preserved the feature from v1 of not doing repairs if the RR stage is full.

Removed this for v3. It's complexity we don't need to solve a non-problem, when we're not hanging on to each callback until RPC_TIMEOUT (as demonstrated by 0.6).;;;","05/Feb/11 03:37;jbellis;v4 fixes build;;;","05/Feb/11 05:22;jbellis;v5 fixes a confusion of endpoints and handler.endpoints;;;","05/Feb/11 07:20;brandon.williams;No GC pressure now, however RR does not appear to actually occur.;;;","05/Feb/11 09:20;jbellis;what does debug log show?;;;","05/Feb/11 09:27;brandon.williams;Nothing interesting, afaict.  Just a lot of this:
{noformat}
DEBUG 23:19:42,094 get_slice
DEBUG 23:19:42,094 get_slice
DEBUG 23:19:42,240 ReadCallback blocking for 1 responses
DEBUG 23:19:42,240 ReadCallback blocking for 1 responses
DEBUG 23:19:42,093 ReadCallback blocking for 1 responses
DEBUG 23:19:42,092 reading data for SliceFromReadCommand(table='Keyspace1', key='30343939323034', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5) locally
DEBUG 23:19:42,091 Read: 21 ms.
DEBUG 23:19:42,091 LocalReadRunnable reading SliceFromReadCommand(table='Keyspace1', key='30353032333935', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5)
DEBUG 23:19:42,091 Read: 17 ms.
DEBUG 23:19:42,241 Read: 159 ms.
DEBUG 23:19:42,091 Read: 12 ms.
DEBUG 23:19:42,091 reading data for SliceFromReadCommand(table='Keyspace1', key='30333636313035', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5) locally
DEBUG 23:19:42,091 LocalReadRunnable reading SliceFromReadCommand(table='Keyspace1', key='30343831383238', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5)
DEBUG 23:19:42,091 get_slice
DEBUG 23:19:42,242 Read: 161 ms.
DEBUG 23:19:42,242 ReadCallback blocking for 1 responses
DEBUG 23:19:42,091 get_slice
DEBUG 23:19:42,090 Read: 17 ms.
DEBUG 23:19:42,243 ReadCallback blocking for 1 responses
DEBUG 23:19:42,090 Read: 19 ms.
DEBUG 23:19:42,090 reading data for SliceFromReadCommand(table='Keyspace1', key='30353131363631', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5) locally
DEBUG 23:19:42,089 ReadCallback blocking for 1 responses
DEBUG 23:19:42,243 reading data for SliceFromReadCommand(table='Keyspace1', key='30353532343833', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5) locally
DEBUG 23:19:42,244 LocalReadRunnable reading SliceFromReadCommand(table='Keyspace1', key='30353532343833', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5)
DEBUG 23:19:42,244 Read: 1 ms.
DEBUG 23:19:42,243 reading data for SliceFromReadCommand(table='Keyspace1', key='30363137363033', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5) locally
DEBUG 23:19:42,242 reading data for SliceFromReadCommand(table='Keyspace1', key='30363234303332', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5) locally
DEBUG 23:19:42,242 LocalReadRunnable reading SliceFromReadCommand(table='Keyspace1', key='30333636313035', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5)
DEBUG 23:19:42,241 LocalReadRunnable reading SliceFromReadCommand(table='Keyspace1', key='30343939323034', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5)
DEBUG 23:19:42,245 Read: 3 ms.
{noformat};;;","08/Feb/11 01:09;jbellis;v6 adds the new repair callbacks to DES latency tracking.;;;","08/Feb/11 07:48;jbellis;v7 rebases post-1530.;;;","08/Feb/11 13:02;jbellis;bq. RR does not appear to actually occur

No digest reads are being sent but I don't know why.  v8 adds better debug logging around ReadCallback.endpoints creation which governs that.;;;","09/Feb/11 03:11;jbellis;v9 fixes ReadCallback construction and adds more asserts around AsyncRepairRunner.;;;","11/Feb/11 04:35;jbellis;v10 fixes a race where a quick repair would remove the data needed for the response to client.  it also splits repair and digest-processing resolvers into different classes.;;;","15/Feb/11 06:44;jbellis;rebased v10;;;","15/Feb/11 07:45;brandon.williams;+1, RR works, GC pressure is gone.;;;","16/Feb/11 01:15;jbellis;committed;;;","16/Feb/11 05:43;hudson;Integrated in Cassandra-0.7 #280 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/280/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in StorageService when cluster is first being created,CASSANDRA-1640,12477943,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,davew,davew,21/Oct/10 08:41,16/Apr/19 17:33,22/Mar/23 14:57,21/Oct/10 10:11,,,Legacy/Tools,,0,,,,,,"Saw this exception on the 0.7.0-beta2 version of cassandra right after bringing up a cluster and trying to get the number of live nodes.

java.lang.NullPointerException
        at org.apache.cassandra.service.StorageService.stringify(StorageService.java:1151)
        at org.apache.cassandra.service.StorageService.getLiveNodes(StorageService.java:1138)
        at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
        at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:65)
        at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:216)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:666)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:638)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1404)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
        at javax.management.remote.rmi.RMIConnectionImpl.getAttribute(RMIConnectionImpl.java:600)
        at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
        at sun.rmi.transport.Transport$1.run(Transport.java:159)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)


I fixed this by adding some null checks to the stringify methods 

    private Set<String> stringify(Collection<InetAddress> endpoints)
    {
        Set<String> stringEndpoints = new HashSet<String>();
        for (InetAddress ep : endpoints)
        {
        	if(ep != null) {
        		stringEndpoints.add(ep.getHostAddress());
        	}
        }
        return stringEndpoints;
    }

    private List<String> stringify(List<InetAddress> endpoints)
    {
        List<String> stringEndpoints = new ArrayList<String>();
        for (InetAddress ep : endpoints)
        {
        	if(ep != null) {
        		stringEndpoints.add(ep.getHostAddress());
        	}
        }
        return stringEndpoints;
    }

After adding those checks, then I got more reasonable/realistic errors from a different part of the code since the service wasn't up yet as the cluster was still initializing:

Caused by: javax.management.InstanceNotFoundException: org.apache.cassandra.service:type=StorageService
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1094)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:662)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:638)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1404)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
        at javax.management.remote.rmi.RMIConnectionImpl.getAttribute(RMIConnectionImpl.java:600)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
        at sun.rmi.transport.Transport$1.run(Transport.java:159)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
        at sun.rmi.transport.StreamRemoteCall.exceptionReceivedFromServer(StreamRemoteCall.java:255)
        at sun.rmi.transport.StreamRemoteCall.executeCall(StreamRemoteCall.java:233)
        at sun.rmi.server.UnicastRef.invoke(UnicastRef.java:142)
        at com.sun.jmx.remote.internal.PRef.invoke(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl_Stub.getAttribute(Unknown Source)
        at javax.management.remote.rmi.RMIConnector$RemoteMBeanServerConnection.getAttribute(RMIConnector.java:878)
        at javax.management.MBeanServerInvocationHandler.invoke(MBeanServerInvocationHandler.java:263)
",Windows XP,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20231,,,Thu Oct 21 02:11:58 UTC 2010,,,,,,,,,,"0|i0g6h3:",92479,,,,,Normal,,,,,,,,,,,,,,,,,"21/Oct/10 10:11;jbellis;see CASSANDRA-1639;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
weakreadremote has high response return (>100ms) ,CASSANDRA-219,12427238,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,lenn0x,lenn0x,06/Jun/09 02:31,16/Apr/19 17:33,22/Mar/23 14:57,06/Jun/09 02:52,0.3,,,,0,,,,,,"When a client makes a request to a node where data does not live, it tries to fetch from a remote node. We noticed that in some JDK's the select() has very strange bugs. Right now under:

src/java/org/apache/cassandra/net/SelectorManager.java

run() {
 ..
 select(100) should be changed to select(1);
}",,hammer,herchu,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19598,,,Fri Jun 05 18:43:04 UTC 2009,,,,,,,,,,"0|i0fxlj:",91041,,,,,Normal,,,,,,,,,,,,,,,,,"06/Jun/09 02:43;jbellis;committed to 0.3 and trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update py_stress to reflect udpated KsDef params,CASSANDRA-1352,12470747,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,rnirmal,rnirmal,rnirmal,04/Aug/10 02:11,16/Apr/19 17:33,22/Mar/23 14:57,04/Aug/10 04:41,0.7 beta 1,,,,0,,,,,,"The following occurs while running stress.py. 

Traceback (most recent call last):
  File ""stress.py"", line 387, in <module>
    make_keyspaces()
  File ""stress.py"", line 160, in make_keyspaces
    client.system_add_keyspace(keyspace)
  File ""/Users/nirmal.ranganathan/Documents/workspace/cassandra.git/interface/thrift/gen-py/cassandra/Cassandra.py"", line 1288, in system_add_keyspace
    self.send_system_add_keyspace(ks_def)
  File ""/Users/nirmal.ranganathan/Documents/workspace/cassandra.git/interface/thrift/gen-py/cassandra/Cassandra.py"", line 1295, in send_system_add_keyspace
    args.write(self._oprot)
  File ""/Users/nirmal.ranganathan/Documents/workspace/cassandra.git/interface/thrift/gen-py/cassandra/Cassandra.py"", line 5745, in write
    oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
SystemError: Objects/dictobject.c:1562: bad argument to internal function

KsDef added an extra parameter and needs to be updated here.",,brandon.williams,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Aug/10 04:20;rnirmal;1352-v2.patch;https://issues.apache.org/jira/secure/attachment/12451152/1352-v2.patch","04/Aug/10 02:22;rnirmal;1352.patch;https://issues.apache.org/jira/secure/attachment/12451142/1352.patch",,,,,,,,,,,,,2.0,rnirmal,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20098,,,Wed Aug 04 13:25:34 UTC 2010,,,,,,,,,,"0|i0g4j3:",92164,,,,,Low,,,,,,,,,,,,,,,,,"04/Aug/10 03:37;brandon.williams;This is the second time we've had to fix this.  Can we change the ks/cf definitions to use kwargs as much as possible to avoid any future problems?;;;","04/Aug/10 04:20;rnirmal;Good thought, I've updated v2 to use kwargs.;;;","04/Aug/10 04:41;brandon.williams;Thanks!  Committed.;;;","04/Aug/10 21:25;hudson;Integrated in Cassandra #509 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/509/])
    Use keyword args for CfDef/KsDef to futureproof.  Patch by  Nirmal Ranganathan, reviewed by brandonwilliams for CASSANDRA-1352
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamingService.StreamDestinations never empties,CASSANDRA-1076,12464261,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,stuhood,gdusbabek,gdusbabek,12/May/10 00:41,13/Aug/20 07:02,22/Mar/23 14:57,15/May/10 01:58,0.6.2,0.7 beta 1,,,0,,,,,,"The problem is that StreamOutManager.streamManagers never has anything removed from it.  In order for StreamingService.getDestinations() to work properly, we either need to track hosts differently, or remove from StreamOutManager.streamManagers when we are no longer streaming to a node.

I lean towards the former, as any time we call StreamOutManager.get(), we're back in the same boat.",,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-956,,,,,,"15/May/10 00:33;gdusbabek;0001-remove-from-streamManagers-when-finished.patch;https://issues.apache.org/jira/secure/attachment/12444509/0001-remove-from-streamManagers-when-finished.patch","15/May/10 00:33;gdusbabek;0002-a-better-StreamingServcice.getStatus.patch;https://issues.apache.org/jira/secure/attachment/12444507/0002-a-better-StreamingServcice.getStatus.patch","15/May/10 00:33;gdusbabek;0003-nix-StreamFile.patch;https://issues.apache.org/jira/secure/attachment/12444508/0003-nix-StreamFile.patch",,,,,,,,,,,,3.0,stuhood,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19984,,,Fri May 14 16:58:50 UTC 2010,,,,,,,,,,"0|i0g2un:",91892,,,,,Low,,,,,,,,,,,,,,,,,"12/May/10 04:25;gdusbabek;Patch is against trunk.;;;","14/May/10 22:25;gdusbabek;Stu, I noticed you assigned this issue to yourself.  Do you still intend to review the patches?;;;","14/May/10 23:56;stuhood;* The synchronized methods don't seem necessary: all structures are concurrent, and I don't think we need any operations to compose
* getPendingFiles has a race between 'streamManagers.containsKey(host)' and 'get(host)': perhaps just perform a single get, and check for null

Other than that, looks good to me.
;;;","15/May/10 00:33;gdusbabek;fixed those.;;;","15/May/10 00:58;stuhood;+1 Looks good.
Thanks Gary!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multiple migrations might run at once,CASSANDRA-1292,12469498,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,gdusbabek,stuhood,stuhood,17/Jul/10 05:58,16/Apr/19 17:33,22/Mar/23 14:57,29/Jul/10 02:02,0.7 beta 1,,,,0,,,,,,"The service.MigrationManager class manages a MIGRATION_STAGE where nodes should execute db.migration.Migration instances.

The problem is that the node that a client connects to via Thrift or Avro initiates the migration in their client thread (calls migration.apply). Instead, the Thrift and Avro clients should ensure that the migration occurs in MIGRATION_STAGE, and should block until the migration is applied by the stage.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Jul/10 00:12;gdusbabek;0001-run-thrift-and-jmx-migrations-on-to-migration-stage.patch;https://issues.apache.org/jira/secure/attachment/12450713/0001-run-thrift-and-jmx-migrations-on-to-migration-stage.patch",,,,,,,,,,,,,,1.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20063,,,Thu Jul 29 13:14:13 UTC 2010,,,,,,,,,,"0|i0g467:",92106,,,,,Critical,,,,,,,,,,,,,,,,,"28/Jul/10 04:56;gdusbabek;same thing goes for loadSchemaFromYaml();;;","29/Jul/10 01:23;stuhood;+1;;;","29/Jul/10 21:14;hudson;Integrated in Cassandra #503 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/503/])
    run thrift and jmx migrations on migration stage. patch by gdusbabek, reviewed by stuhood. CASSANDRA-1292
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
read repair on CL.ONE regression,CASSANDRA-1985,12495562,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,kelvin,kelvin,14/Jan/11 08:32,16/Apr/19 17:33,22/Mar/23 14:57,16/Jan/11 03:37,0.7.1,,,,0,,,,,,"read repair w/ CL.ONE had a regression.

The RepairCallback was dropped (in the background for CL.ONE), so ReadResponseResolver : resolve() was never called.",,johanoskarsson,stuhood,,,,,,,,,,,,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,CASSANDRA-982,,,,,,"15/Jan/11 04:33;jbellis;1985-v2.txt;https://issues.apache.org/jira/secure/attachment/12468403/1985-v2.txt","14/Jan/11 08:45;kelvin;CASSANDRA-1985-0001-fix-CL.ONE-read-repair-regression.patch;https://issues.apache.org/jira/secure/attachment/12468326/CASSANDRA-1985-0001-fix-CL.ONE-read-repair-regression.patch",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19352,,,Sat Jan 15 19:53:24 UTC 2011,,,,,,,,,,"0|i0g8lz:",92825,,tjake,,tjake,Normal,,,,,,,,,,,,,,,,,"14/Jan/11 08:33;kelvin;This regression occurred, here.;;;","14/Jan/11 08:45;kelvin;ensure RR happens in the background.;;;","14/Jan/11 12:10;jbellis;It's possible that we missed something, but we we did test read repair post-982.  This is the part that does the resolve:

                if (repairs.contains(command))
                    repairExecutor.schedule(new RepairRunner(readCallback.resolver, command, endpoints), DatabaseDescriptor.getRpcTimeout(), TimeUnit.MILLISECONDS);
;;;","15/Jan/11 02:00;kelvin;Yes, you're right, that does schedule it, once.

The process for CL.ONE is:
1) schedule RR for data+digest and watch for a DigestMismatchException,
2) catch DME and call repair() to do a RR for data-only.

However, the handler for the second RR (that repair() returns) is never used.  So, even though it's collecting all the data repair messages, the RRR's resolve() never gets called.
;;;","15/Jan/11 02:50;jbellis;The ""second RR"" (that is, the second read request, for performing repair when a mismatch was detected by the digest read) is this one:

{code}
                RepairCallback<Row> handler = repair(command, endpoints);
...
                repairResponseHandlers.add(handler);
...
            for (RepairCallback<Row> handler : repairResponseHandlers)
            {
                try
                {
                    Row row = handler.get();
{code};;;","15/Jan/11 03:44;kelvin;Yes, that's correct for read CL > ONE.  A quorum / all read goes through that path.  However, the CL.ONE case does not go through that code path.

The branch in the code is in fetchRows(...) when it checks for randomlyReadRepair(...).  If the targets > handler.blockfor, it does a background repair via RepairRunner in service.StorageProxy.  i.e. it won't go through the block of code you pasted, because a DigestMismatchException won't be thrown for CL.ONE.

Now, let's look at RepairRunner : runMayThrow.  It calls repair(command, endpoints), but the RepairCallback<row> that is returned by repair(...) is dropped on the floor.  So, resolve is never called on that RepairCallback's ReadResponseResolver.

The above error was found via my own set of distributed tests.;;;","15/Jan/11 03:48;jbellis;I get it now: the callback from the repair() call in RepairRunner is the one that we don't resolve.;;;","15/Jan/11 04:33;jbellis;v2 keeps the resolve off the response stage, which we want to keep very low latency.;;;","15/Jan/11 06:34;tjake;+1;;;","16/Jan/11 03:37;jbellis;committed;;;","16/Jan/11 03:53;hudson;Integrated in Cassandra-0.7 #162 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/162/])
    fix read repair on CL.ONE regression
patch by jbellis; reviewed by tjake for CASSANDRA-1985
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Invalid host/port parameters to cassandra-cli leaves system in unrecoverable state,CASSANDRA-867,12458579,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,mwn,mwn,mwn,10/Mar/10 04:44,16/Apr/19 17:33,22/Mar/23 14:57,10/Mar/10 05:03,0.6,,Legacy/Tools,,0,,,,,,"bin\cassandra-cl.bati -host localhost -port 8880  (cassandra not running localhost/8880 ;) ) 

Starting Cassandra Client
Exception connecting to localhost/8880 - java.net.ConnectException: Connection refused: connect
Welcome to cassandra CLI.

Type 'help' or '?' for help. Type 'quit' or 'exit' to quit.
cassandra> j
Exception null
cassandra> quit
Exception null

Problem is that main does not ensure that cliClient_ is set to an instanse of CliClient.",WinXP / java 1.6 / svn @ 921110 trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/10 04:47;mwn;CASSANDRA-867.patch;https://issues.apache.org/jira/secure/attachment/12438326/CASSANDRA-867.patch",,,,,,,,,,,,,,1.0,mwn,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19897,,,Tue Mar 09 21:03:58 UTC 2010,,,,,,,,,,"0|i0g1kf:",91684,,,,,Low,,,,,,,,,,,,,,,,,"10/Mar/10 04:47;mwn;Small patch to solve above issue.;;;","10/Mar/10 05:03;jbellis;committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix Findbugs: Static initializers,CASSANDRA-614,12442762,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,stuhood,stuhood,09/Dec/09 03:04,16/Apr/19 17:33,22/Mar/23 14:57,15/Dec/09 04:00,0.6,,,,0,,,,,,"We have a few singleton classes that are not doing their lazy initialization correctly (we really need a generic solution to this problem, sigh). In most cases, we are missing 'volatile' on the static field. See attached.

In order to find unused method parameters for CASSANDRA-608, I ran FindBugs against Cassandra, and found a few interesting issues we ought to explore (but not the unused method params, oi.)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Dec/09 03:42;jbellis;614.patch;https://issues.apache.org/jira/secure/attachment/12427377/614.patch","09/Dec/09 03:04;stuhood;lazy-init.txt;https://issues.apache.org/jira/secure/attachment/12427368/lazy-init.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19782,,,Thu Dec 17 22:03:02 UTC 2009,,,,,,,,,,"0|i0g00n:",91433,,,,,Normal,,,,,,,,,,,,,,,,,"09/Dec/09 03:42;jbellis;fixes attached;;;","10/Dec/09 11:19;stuhood;Looks good to me. +1;;;","15/Dec/09 04:00;jbellis;rebased & committed;;;","18/Dec/09 06:03;hudson;Integrated in Cassandra #291 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/291/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[multi_]get_count should take a SlicePredicate,CASSANDRA-744,12446705,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,jbellis,jbellis,27/Jan/10 01:53,16/Apr/19 17:33,22/Mar/23 14:57,20/Apr/10 22:53,0.7 beta 1,,,,0,,,,,,"both to make it more flexible, and to emphasize that counting ""everything"" is as bad as slicing it",,bendiken,hammer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/10 22:55;slebresne;ASF.LICENSE.NOT.GRANTED--0001-Add-SlicePredicate-to-get_count.patch;https://issues.apache.org/jira/secure/attachment/12442050/ASF.LICENSE.NOT.GRANTED--0001-Add-SlicePredicate-to-get_count.patch","17/Apr/10 22:55;slebresne;ASF.LICENSE.NOT.GRANTED--0002-Add-mutliget_count.patch;https://issues.apache.org/jira/secure/attachment/12442051/ASF.LICENSE.NOT.GRANTED--0002-Add-mutliget_count.patch",,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19849,,,Tue May 11 10:25:57 UTC 2010,,,,,,,,,,"0|i0g0t3:",91561,,,,,Low,,,,,,,,,,,,,,,,,"17/Apr/10 22:55;slebresne;Got some time to kill, so I'm proposing two (quite trivial) patches.
First one add the SlicePredicate to get_count, second one add
a multiget_count.;;;","20/Apr/10 22:49;gdusbabek;+1;;;","20/Apr/10 22:53;gdusbabek;committed.  Thanks Sylvain!;;;","09/May/10 07:10;bendiken;Would it be possible to specify multiget_count in terms of i64 return values instead of i32, pretty please?;;;","09/May/10 08:19;jbellis;that's a little silly, isn't it?  surely counting over 2B columns isn't a good idea in the first place...;;;","11/May/10 04:43;bendiken;Is it? I apologize if I have too high expectations for Cassandra, but we've already used rows with up to one hundred million columns. Problems at that scale have been mostly GC churn and compaction related, and since it appears that such issues are being worked on (CASSANDRA-1014, CASSANDRA-16, and the like), a mere one order of magnitude more doesn't seem like *too* much of a stretch for Cassandra to eventually handle on sufficiently big iron.;;;","11/May/10 16:56;slebresne;That's not really the problem of having row with more than MAX_INTEGER columns. What is silly
is counting those. Counting all the columns in a row is the same than reading the whole row. Excepted
that you don't send all those columns over the network. So a call to count will never need a i64 as any 
call that would need an i64 will very likely timeout. 
That's the ""raison d'être"" of this patch. You can count a huge row by paging (but it's probably a better
idea to not count those huge row at all if you can afford it as even with paging this is expensive).;;;","11/May/10 17:54;bendiken;I guess it depends on your use case. We have one where each Cassandra row represents a very large set, each column name being a 20-byte SHA-1 binary hash identifying an object in that set and each such column's value being simply the empty string. As I mentioned, we've stored up to a hundred million columns per row in this manner. As each SHA-1 column takes 35.5 bytes of space in the SStables, that's a total of less than 4 gigs of disk storage for a row with 100 million columns. On the big iron we've run this on, these are not _inherently_ infeasible numbers. The limiting factor is Cassandra's implementation, not the hardware.

Counting the number of objects in a given set (i.e. the number of columns in a given row) is an important operation for us. It's fine for the count to take a while, as it is still vastly (many, many orders of magnitude) faster than the infeasible alternative of directly counting the source data (also stored in Cassandra, but apart) that the set data is derived from, which would (prior to your multi_get_count patch, which does alleviate it a little) involve performing an individual get_count operation for each of hundreds of millions (soon to be billions) of distinct source rows.

Now, given existing GC and SStable compaction issues that we've run into with Cassandra 0.6, we're in practice now manually sharding the larger sets into multiple rows of a size that Cassandra has less issues dealing with (on our hardware, up to 15-20 million columns per row is performing very well).

t expect that as Cassandra evolves and issues are fixed, we can keep upping this, and I don't see anything inherently ridiculous about rows of the size I've mentioned. It seems a little shortsighted to place incidental limits on the protocol, but then again I suppose the protocol will have broken backwards compatibility a couple of times by the time I get around to testing 2 billion columns with some future Cassandra 1.x version - so perhaps we can revisit this in a year or two ;-)
;;;","11/May/10 18:25;slebresne;But then again, I do not even contest the fact that it could be useful to have row with billions of columns. As you mentioned, 
there is a few limitations to that today but they will hopefully be lifted soon enough. I still think that, despite these current limitations,  
the sharding  of the row you already do is useful at some point (but maybe this point is 1 billion columns in your case) if only for the 
sake of load distribution. But that's not my point at all.

My point is that the time it takes to perform one given individual get_count() operation that count n columns is as long as the time it takes 
to read those n columns (from server-side at least, the only advantage of get_count() over get_slice() is that you don't send those n columns 
over the network). So, if n > 2 billion, it will takes a bit of time to perform this one get_count() operation, even in a year or two, even with
super duper SSD drives and even if each column is quite small. I haven't tried (I don't have a super duper SSD drive and I don't live one 
or two year from now) and it's always dangerous to make assumption in the future, but I bet it will take far time than any reasonable 
timeout you would want to set for your Cassandra operations. 

Hence, the right way to count a row with 2 billions+ columns is to do multiple get_count() operations using a predicate to limit the number 
of counted columns by each individual get_count() operation and sum all those results client side. But then only the sum needs be a 64 bit 
integer, not the result of get_count().   ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
support deletes in counters,CASSANDRA-2101,12497514,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,kelvin,kelvin,kelvin,03/Feb/11 06:05,16/Apr/19 17:33,22/Mar/23 14:57,05/Feb/11 13:13,0.8 beta 1,,,,0,,,,,,Obey timestampOfLastDelete during reconciliation.,,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-1936,CASSANDRA-1072,"04/Feb/11 04:55;kelvin;0001-CASSANDRA-2101-obey-tsOfLastDelete-remove-irrelevant.patch;https://issues.apache.org/jira/secure/attachment/12470183/0001-CASSANDRA-2101-obey-tsOfLastDelete-remove-irrelevant.patch",,,,,,,,,,,,,,1.0,kelvin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20443,,,Sat Feb 05 06:42:05 UTC 2011,,,,,,,,,,"0|i0g9bj:",92940,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"03/Feb/11 06:06;kelvin;timestampOfLastDelete was added and 90% implemented.  Missed this last logic.;;;","03/Feb/11 06:07;kelvin;add logic to reconcile; test logic.;;;","03/Feb/11 06:22;kelvin;modified patch w/o dependencies on expiring counter column code.;;;","03/Feb/11 21:08;slebresne;Before commenting on the patch itself, I want to use this ticket to recall that counter deletes are intrinsically broken. It has been said already but I'll use this comment to explain in more depth how so and keep a trace of this for the record.

First, I'll use the following notation:
{noformat}
  c(x, 3)@[4, 2] - for a counter column of name x, value 3, timestamp 4 and timestampOfLastDelete 2 (I'll use -1 as the min timestampOfLastDelete).
{noformat}
and
{noformat}
  d(x)@[5] - for a tombstone of name x and timestamp 5
{noformat}

And now suppose that the following inserts are done (in that order):
{noformat}
   c(x, 1)@[1, -1]
   d(x)@[2]
   c(x, 1)@[3, -1]
{noformat}

If these inserts are resolved in that order, everything is fine:
{noformat}
   c(x, 1)@[1, -1]
 + d(x)@[2]
=> d(x)@[2]
 + c(x, 1)@[3, -1]
=> c(x, 1)@[3, 2]
{noformat}

However, some reordering don't work. Namely, if you merge the two counts together, before you merge one of the count with the delete:
{noformat}
   c(x, 1)@[1, -1]
 + c(x, 1)@[3, -1]
=> c(x, 2)@[3, -1]
 + d(x)@[2]
=> c(x, 2)@[3, 2]
{noformat}

The problem is, the resolve operation is not commutative when you consider counter columns and tombstones. But Cassandra rely heavily on resolve being commutative (as a side note, I never understood the reason of the CommutativeType terminology in the code. It suggest that regular columns are not commutative, while they are as far as resolve is concerned. Resolve is not idempotent on counters however).

Not only is there no guarantee on which order the insert will be received by each node, but even if they are in the right order, there is no guarantee that (minor) compaction won't screw up this.

Hence I think that there is not much guarantee we can give on deletes. The only one I can think of is that when on issue a delete, you must wait to issue any following update that the delete have reach all the nodes and all of them have been fully compacted.

That being said, we can keep counter deletes. It's at least useful for cases where you know that you won't reuse a counter ever and want to get rid of the disk space. But I would add a very strong warning to its documentation.

Lastly, the deletion of full counter rows or super columns suffers the same problem for the same reason.
;;;","03/Feb/11 21:08;slebresne;+1 on the patch in the spirit of ""let's make delete work as often as we can"".

However, I realized that the first 'if' of CounterColumn.reconcile() is dead-code since a CounterColumn is never markedForDelete(). It would be nice to remove it while submitting this
;;;","04/Feb/11 04:55;kelvin;Good point, Sylvain!

Attached a new patch w/ the irrelevant code removed.;;;","04/Feb/11 20:10;slebresne;So +1 on the new patch;;;","05/Feb/11 13:13;jbellis;committed;;;","05/Feb/11 14:42;hudson;Integrated in Cassandra #709 (See [https://hudson.apache.org/hudson/job/Cassandra/709/])
    add delete support for counters
patch by Kelvin Kakugawa; reviewed by slebresne for CASSANDRA-2101
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Read Repair behavior thwarts DynamicEndpointSnitch at CL.ONE,CASSANDRA-1873,12493477,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,17/Dec/10 08:23,16/Apr/19 17:33,22/Mar/23 14:57,22/Dec/10 05:23,0.6.9,0.7.0 rc 3,,,0,,,,,,"When doing a CL.ONE read, the coordinator node selects the data node from the list of replicas via snitch sortByProximity.  The data node (_not_ the coordinator) then sends digest requests to the remaining replicas, and compares their answers to its own (in ConsistencyChecker).

This means that, in a multi-datacenter situation, for any given range R with replicas X in dc1 and Y in dc2, the only node with latency information for Y will be X.  Since DES falls back to subsnitch (static) order when latency information is missing for any replica it is asked to sort, DES will be unable to direct requests to Y no matter how overwhelmed X becomes.

To fix this, we should move the digest-checking code into the coordinator node (probably starting with the 0.7 ConsistencyChecker, which represents a cleanup of the 0.6 one).",,brandon.williams,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Dec/10 01:58;jbellis;1873.txt;https://issues.apache.org/jira/secure/attachment/12466542/1873.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20353,,,Tue Dec 21 21:23:24 UTC 2010,,,,,,,,,,"0|i0g7xj:",92715,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"17/Dec/10 08:24;jbellis;Note: IMO it is okay to break RR temporarily when upgrading a cluster piecemeal -- that is, it's okay for RR to not happen; it's not okay to generate internal errors.;;;","19/Dec/10 01:58;jbellis;the only tricky part was getting handles to both a command object and the address of the data read efficiently.  ended up handling the former w/ a Map in StorageProxy.weakRead, and the latter by adding a field to AsyncResult.

RR vs local reads continues to be handled by weakReadCallable.  that part required relatively little change.;;;","19/Dec/10 01:58;jbellis;(patch is against 0.6);;;","22/Dec/10 02:37;brandon.williams;+1, no internal errors generated during a rolling restart.;;;","22/Dec/10 04:58;hudson;Integrated in Cassandra-0.6 #31 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/31/])
    manage read repair in coordinator instead of data source, to provide latency information to dynamic snitch
patch by jbellis; reviewed by brandonwilliams for CASSANDRA-1873
;;;","22/Dec/10 05:23;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
storage-conf.xml reformatting,CASSANDRA-373,12433363,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,urandom,urandom,urandom,19/Aug/09 02:34,16/Apr/19 17:33,22/Mar/23 14:57,21/Aug/09 03:48,0.4,,,,0,,,,,,"Our sample config (conf/storage-conf.xml) is the canonical source of configuration documentation. As such readability should be a priority, and it should serve as the best possible basis for customization.

I propose the following:

1. Wherever possible, lines should wrap at 75 chars. The file will be edited post-installation by operational personnel, who are often confined to standard 80 character terminals.

2. Indention of 2 spaces to make maximum use of horizontal real estate.

3. More distinctive multi-line comments.

4. Path locations that conform to the FHS.

Patch to follow...",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Aug/09 02:42;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-373-re-format-factor-conf-storage-conf.xml.txt;https://issues.apache.org/jira/secure/attachment/12416901/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-373-re-format-factor-conf-storage-conf.xml.txt","19/Aug/09 02:47;urandom;ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-373-re-format-factor-conf-storage-conf.xml.txt;https://issues.apache.org/jira/secure/attachment/12416902/ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-373-re-format-factor-conf-storage-conf.xml.txt","20/Aug/09 05:56;urandom;ASF.LICENSE.NOT.GRANTED--v3-0001-CASSANDRA-373-re-format-factor-conf-storage-conf.xml.txt;https://issues.apache.org/jira/secure/attachment/12417060/ASF.LICENSE.NOT.GRANTED--v3-0001-CASSANDRA-373-re-format-factor-conf-storage-conf.xml.txt","20/Aug/09 11:07;urandom;ASF.LICENSE.NOT.GRANTED--v4-0001-CASSANDRA-373-re-format-factor-conf-storage-conf.xml.txt;https://issues.apache.org/jira/secure/attachment/12417093/ASF.LICENSE.NOT.GRANTED--v4-0001-CASSANDRA-373-re-format-factor-conf-storage-conf.xml.txt",,,,,,,,,,,4.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19658,,,Fri Aug 21 12:34:39 UTC 2009,,,,,,,,,,"0|i0fyjj:",91194,,,,,Normal,,,,,,,,,,,,,,,,,"19/Aug/09 02:48;urandom;whoops, v1 had an error, see v2 instead.;;;","19/Aug/09 03:57;euphoria;In general, looks good.  While reviewing, noticed an existing internal conflict between microseconds and milliseconds for CommitLogSyncBatchWindowInMS that should be corrected.;;;","20/Aug/09 04:54;dehora;{quote}
1. Wherever possible, lines should wrap at 75 chars.
{quote}

is this not best done by reducing the comment noise? 

Also verbose comments tend to be annoying in practice operationally - hey get in the way of the the actual elements  you need to set/read when you need to see/read them on a console - tomcat is an example offender and a system i'm tired of stripping comments from.

{quote}
4. Path locations that conform to the FHS.
{quote}

Not for us to mandate surely.


 * The tilde's at the beginning of the comment lines impair reading.  

* I don't like putting advice or good rules of thumb in the config - would we do this kind of thing in the thrift? 

 * CommitLogSyncDelay : ""in millis"" but other elements have the InFoo quantity pattern. Inconsistent.

 * MemtableObjectCountInMillions: asking people to type in floats is error prone, why not MemtableObjectCount?

* CommitLogSync: doesn't document its legal values

* Thrift*: should be wrapped in an holding element. Because, and I'm guessing here, Thrift will not remain the only, or even the default, transport/api.

* InitialToken: doesn't document what happens when empty

* Seed, ListenAddress, ThriftAddress: don't document whether they take Ipv6 addresses

;;;","20/Aug/09 04:59;jbellis;> 1. Wherever possible, lines should wrap at 75 chars.

Who seriously uses a vt100 rather than remoting in these days?  You can always use a larger terminal and/or an editor that handles wrapped lines sanely.
;;;","20/Aug/09 05:06;jbellis;> verbose comments tend to be annoying in practice operationally

That's the lesser of evils compared to out-of-date documentation of the options.;;;","20/Aug/09 05:14;dehora;{quote}
That's the lesser of evils compared to out-of-date documentation of the options. 
{quote}

Agreed!;;;","20/Aug/09 05:31;urandom;>Who seriously uses a vt100 rather than remoting in these days? You can always use a larger terminal and/or an editor that handles wrapped lines sanely.

You have to wrap somewhere; the point of 80 chars is that it is the minimum you could expect to see in use.

This is pretty much best practice among projects with hand edited configuration files. A quick survey of the files under /etc on any *nix box should confirm this.;;;","20/Aug/09 05:50;euphoria;There are a few errors in the CommitLog area that Eric and I discussed on IRC that will need to be fixed (including the legal values Bill mentions).

The path locations are just examples -- we have to choose something to default to, and the best thing to default to would be a common standard.  Users are still, of course, able to specify whatever writable paths they desire.;;;","20/Aug/09 05:57;urandom;v3-0001-CASSANDRA-373-re-format-factor-conf-storage-conf.xml.txt brings the file up-to-date with the commitlog options in trunk.;;;","20/Aug/09 06:02;urandom;>  The tilde's at the beginning of the comment lines impair reading.

Ok. The intention was to make it more readable by marking the left-most margin of the comment block. I did this with a tilde only because that was already in use for the copyright header, and I thought that looked ok. Suggestions welcome.

>  * CommitLogSyncDelay : ""in millis"" but other elements have the InFoo quantity pattern. Inconsistent.
> 
>  * MemtableObjectCountInMillions: asking people to type in floats is error prone, why not MemtableObjectCount?
> 
> * CommitLogSync: doesn't document its legal values
> 
> * Thrift*: should be wrapped in an holding element. Because, and I'm guessing here, Thrift will not remain the only, or even the default, transport/api.
> 
> * InitialToken: doesn't document what happens when empty
> 
> * Seed, ListenAddress, ThriftAddress: don't document whether they take Ipv6 addresses 

I had intended the scope of this ticket to be formatting/styling. Could you open a separate ticket for these?;;;","20/Aug/09 07:39;jbellis;Can we at least wrap things symmetrically?

      <ColumnFamily ColumnType=""Super"" 
                                    CompareWith=""UTF8Type""
                                    CompareSubcolumnsWith=""UTF8Type""
                                    Name=""Super1""/>

instead of

+      <ColumnFamily ColumnType=""Super"" CompareWith=""UTF8Type""
+              CompareSubcolumnsWith=""UTF8Type"" Name=""Super1""/>
;;;","20/Aug/09 11:09;urandom;> Can we at least wrap things symmetrically?

Sure. See v4 patch.
;;;","20/Aug/09 11:19;jbellis;+1;;;","20/Aug/09 11:26;euphoria;Still says 'microseconds' but +1 if microseconds is changed to milliseconds for the commit log batch delay;;;","21/Aug/09 03:48;urandom;committed.;;;","21/Aug/09 08:07;dehora;Please revert this and remove the verbose comments and the leading tildes. All I'm going to do with them is strip them out.;;;","21/Aug/09 08:25;jbellis;> remove the verbose comments

not going to happen.  see above re lesser of evils.

> and the leading tildes

if you have a better suggestion, you are welcome to submit a patch -- Eric said above that he was open to suggestions to improve this but none were forthcoming.

> All I'm going to do with them is strip them out. 

That's fine, but because you don't need them doesn't mean they are not useful for others.;;;","21/Aug/09 20:34;hudson;Integrated in Cassandra #174 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/174/])
     re{format,factor} conf/storage-conf.xml
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"get_range_slices always returns super columns that's been removed/restored, regardless of count value in slicerange",CASSANDRA-1591,12476722,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,hujn,hujn,07/Oct/10 08:06,16/Apr/19 17:33,22/Mar/23 14:57,23/Nov/10 03:29,0.6.9,0.7.0 rc 1,,,0,,,,,,"I'm seeing cases where the count in slicerange predicate is not respected. This is only happening for super columns. I'm running Cassandra 0.6.4 in a single node.

Steps to reproduce, using the Keyspace1.Super1 CF:
* insert three super columns, bar1 bar 2, and bar3, under the same key
* delete bar1
* insert bar1 again
* run a get_range_slices on Super1, with start=bar1, finish=bar3, and count=1
* I expected only bar1 to be returned, but both both bar1 and bar2 are returned. bar3 isn't, though. so count is somewhat respected.

perl code to reproduce is attached
when I tried the same test on a standard CF it worked. only super CF seem to have this problem.","CentOS 5.4, single Cassandra node ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Oct/10 05:25;thobbs;1591-0.6-reproduce.txt;https://issues.apache.org/jira/secure/attachment/12457299/1591-0.6-reproduce.txt","23/Nov/10 02:43;jbellis;1591.txt;https://issues.apache.org/jira/secure/attachment/12460194/1591.txt","07/Oct/10 08:10;hujn;test.pl;https://issues.apache.org/jira/secure/attachment/12456559/test.pl",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19368,,,Mon Nov 22 22:01:02 UTC 2010,,,,,,,,,,"0|i0g65z:",92429,,tjake,,tjake,Low,,,,,,,,,,,,,,,,,"16/Oct/10 05:25;thobbs;Attached patch at least partially reproduces using system tests.;;;","23/Nov/10 02:43;jbellis;the deleted supercolumn w/ a live subcolumn wasn't being included in the count.  this patch adds an isLive method to fix this:

+     * For a simple column, live == !isMarkedForDelete.
+     * For a supercolumn, live means it has at least one subcolumn whose timestamp is greater than the
+     * supercolumn deleted-at time.
;;;","23/Nov/10 03:18;tjake;LGTM +1;;;","23/Nov/10 06:01;hudson;Integrated in Cassandra-0.6 #11 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/11/])
    fix live-column-count of slice ranges including tombstoned supercolumn with live subcolumn
patch by jbellis and Tyler Hobbs; reviewed by tjake for CASSANDRA-1591
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compaction can echo data which breaks upon sstable format changes,CASSANDRA-2216,12499332,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,slebresne,slebresne,slebresne,22/Feb/11 20:17,16/Apr/19 17:33,22/Mar/23 14:57,23/Feb/11 01:32,0.7.3,,,,0,compaction,,,,,"While compaction, if for a row we have only 1 sstable holding data, we echo this data. This breaks when we change the data format, creating mixed (corrupted) sstable.

(I suspect this is the cause of CASSANDRA-2195, but opening a new ticket until we can confirm that hunch)",,cburroughs,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,"22/Feb/11 20:24;slebresne;0001-Don-t-echo-data-during-compaction.patch;https://issues.apache.org/jira/secure/attachment/12471609/0001-Don-t-echo-data-during-compaction.patch","22/Feb/11 23:35;slebresne;2216_v2.patch;https://issues.apache.org/jira/secure/attachment/12471625/2216_v2.patch",,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20512,,,Tue Feb 22 18:57:55 UTC 2011,,,,,,,,,,"0|i0ga13:",93055,,jbellis,,jbellis,Critical,,,,,,,,,,,,,,,,,"22/Feb/11 20:24;slebresne;Attached patch completely remove the echoing of data when we have only one row. We could easily, as in CASSANDRA-2211, echo data if the sstable we are echoing from is at the last version.

However, not doing so will allow potentially corrupted sstable to get repaired by compaction (in the case of corruption from the bloom filter change).

We could add back the echoing optimisation later on.;;;","22/Feb/11 23:15;slebresne;Attaching v2 that does the optimisation of checking for last version. Implies we repair the inconsistencies introduced outside of compaction.;;;","23/Feb/11 00:40;jbellis;bq. We could easily echo data if the sstable we are echoing from is at the last version

Let's do that, and introduce a separate command to force deserialization.  (Telling people ""compact to fix it"" is not something we want to do since that leaves you with One Big SSTable and all the problems associated w/ that.);;;","23/Feb/11 01:32;jbellis;committed v2;;;","23/Feb/11 02:57;hudson;Integrated in Cassandra-0.7 #304 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/304/])
    fix compaction echoing old-style data into new sstable version
patch by slebresne; reviewed by jbellis for CASSANDRA-2216
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ensure compaction thresholds are sane,CASSANDRA-1527,12474746,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jhermes,jbellis,jbellis,22/Sep/10 03:45,16/Apr/19 17:33,22/Mar/23 14:57,28/Sep/10 06:10,0.7 beta 2,,Legacy/CQL,,0,,,,,,"make sure min <= max and neither is negative.

also make sure that min=max=0 works (this is ""no compaction"")",,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Sep/10 05:07;jhermes;1527-unoopsed.txt;https://issues.apache.org/jira/secure/attachment/12455751/1527-unoopsed.txt","28/Sep/10 04:29;jhermes;1527.txt;https://issues.apache.org/jira/secure/attachment/12455748/1527.txt",,,,,,,,,,,,,2.0,jhermes,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20179,,,Tue Sep 28 13:31:31 UTC 2010,,,,,,,,,,"0|i0g5rb:",92363,,,,,Low,,,,,,,,,,,,,,,,,"23/Sep/10 06:33;jhermes;Because there are now too many ways to invalidate a metadata, I'm going to enumerate them.

1) Creation:
- First and foremost, catching these boundaries on the constructor fixes the case where the config is invalid. The config is read and DD.readTablesFromYaml() creates a CFMD for each CF.
- This also catches 'schematool import', which hits SS.loadSchemasFromYaml() which hits DD.readTablesFromYaml() in the same way.
- This also catches creation of an avro CfDef and loading it (generally, this is because we serialized it incorrectly). Calling CFMD.inflate(avro cfdef) will hit the constructor as well.

2) Modification:
- Throwing a runtime exception on the JMX CFS.setMin/MaxCompactionThreshold() methods will suffice to stop this invalidation point. This could be done a bit more kindly.
- Catching these boundaries on CFMD.apply(avro/thrift def) limits the case where someone makes a def and then pushes it in. This is ALSO caught by the constructor.

Now, bad things. A constructor throwing a ConfigurationException is not the nicest thing in the world. These would be internal errors, as thrift/avro aren't always involved.;;;","25/Sep/10 05:19;jhermes;Can't deal with the constructor throwing an exception, it's just too ugly.

Current patch:
# Adding validation to readTablesFromYaml()/loadSchemas() is pretty, and catches the erroneous config. This is necessarily different than the same validation for add/update API calls, as this is parsed and deflated immediately.
# Explode on invalid JMX poking. To counter this, made disableAutoCompaction() pokeable (previously internal). This should help avoid the case where one goes to disable via JMX, sets max = 0 < min, and kills the node.
# Leave inflate alone. If we deflate it incorrectly, then 1) was wrong.
# CFMetaData has a validateMinMaxCTs() for both avro and thrift. When updating a CF, the method is called in CFMetaData.apply(). When adding a CF, the method is called in CassandraServer.convertToCFMetaData(). (This is duplicated based on protocol because there is no interface for ""a CfDef"" to deal with both avro and thrift at the same time, much in same way all the other methods in CFMetaData are duplicated.)
# Check that apply validates correctly in DefsTest.;;;","25/Sep/10 05:33;jhermes;Now, for min=max=0, estimating the compactions _will currently_ explode if min == max.

Estimated compactions are the sum of all (1 + size_i / thresholdrange) for i in SSTables with size > min. What do we want the estimated compactions to be when min == max (or thresholdrange=0)?
Furthermore, do we want to be estimating compactions when minor compactions are disabled?;;;","28/Sep/10 04:06;jbellis;bq. Estimated compactions are the sum of all (1 + size_i / thresholdrange) for i in SSTables with size > min

that looks buggy to me.  doesn't estimate of

                n += Math.ceil((double)sstables.size() / maxct);

make more sense?

bq. do we want to be estimating compactions when minor compactions are disabled

no;;;","28/Sep/10 04:29;jhermes;min=max is fine, min=0 || max=0 is fine.;;;","28/Sep/10 05:07;jhermes;Patch error caused loss of code. That was unfortunate.;;;","28/Sep/10 05:09;jbellis;can you add unit or system tests demonstrating rejection of invalid configurations?;;;","28/Sep/10 05:12;jbellis;nvm, I see it in DefsTest.

what is the commented-out code for in DD?  is that a todo that should be cleaned up?;;;","28/Sep/10 05:24;jhermes;Oh, missed one!
The comments should be removed, the rest is valid.;;;","28/Sep/10 06:10;jbellis;committed;;;","28/Sep/10 21:31;hudson;Integrated in Cassandra #549 (See [https://hudson.apache.org/hudson/job/Cassandra/549/])
    sanity checks for compaction thresholds.  
patch by jhermes; reviewed by jbellis for CASSANDRA-1527
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OutOfBoundException in StorageService.getAllRanges ,CASSANDRA-933,12460730,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,riffraff,riffraff,riffraff,31/Mar/10 03:07,16/Apr/19 17:33,22/Mar/23 14:57,02/Apr/10 21:20,0.6.1,,,,0,,,,,,"this was seen on 0.6-beta3 but it appears to be in trunk too, given the same code for getAllRanges. The problem appeared while accessing a bootstraping node via nodetool, giving the following stacktrace

Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: -1
        at java.util.ArrayList.get(ArrayList.java:324)
        at java.util.Collections$UnmodifiableList.get(Collections.java:1154)
        at org.apache.cassandra.service.StorageService.getAllRanges(StorageService.java:1133)
        at org.apache.cassandra.service.StorageService.getRangeToAddressMap(StorageService.java:440)
        at org.apache.cassandra.service.StorageService.getRangeToEndPointMap(StorageService.java:431)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1426)
...


Basically, no test is performed for non-emptyness of the input token list. If such list should never be empty, I guess this should be explicit in the interface/javadoc, otherwise I'm attaching a patch & testcase (pretty silly code, but the test passes :) ) ",,,,,,,,,,,,,,,,,,,,,,,600,600,,0%,600,600,,,,,,,,,,,,,,,,,,,,"31/Mar/10 03:09;riffraff;CASSANDRA-933.patch;https://issues.apache.org/jira/secure/attachment/12440265/CASSANDRA-933.patch",,,,,,,,,,,,,,1.0,riffraff,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19923,,,Fri Apr 02 13:20:11 UTC 2010,,,,,,,,,,"0|i0g1z3:",91750,,,,,Low,,,,,,,,,,,,,,,,,"31/Mar/10 03:09;riffraff;simply return empty output on empty input. ;;;","02/Apr/10 21:20;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EstimatedHistogram.max is buggy,CASSANDRA-1413,12472062,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,20/Aug/10 14:56,16/Apr/19 17:33,22/Mar/23 14:57,26/Aug/10 04:36,0.7 beta 2,,,,0,,,,,,"EH.max returns the largest bucket floor, which will will be LESS than the largest value added to the histogram, which is not the usual behavior expected of a method called max.",,brandon.williams,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Aug/10 06:13;brandon.williams;0002_adjust_EH_sizes.txt;https://issues.apache.org/jira/secure/attachment/12452866/0002_adjust_EH_sizes.txt","20/Aug/10 14:58;jbellis;1413.txt;https://issues.apache.org/jira/secure/attachment/12452610/1413.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20126,,,Wed Aug 25 20:36:16 UTC 2010,,,,,,,,,,"0|i0g4wn:",92225,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"21/Aug/10 02:46;brandon.williams;Isn't the ISE being thrown here going to get raised in CFS when cfstats is called?;;;","21/Aug/10 03:03;jbellis;Dunno, that's why I wanted you to review.  You sized those EHes. :)

Isn't the ""right"" solution just to make them large enough?

Open to other ideas.;;;","24/Aug/10 06:13;brandon.williams;I think we need to adjust the sizes a bit.  Bump the column count EH to 114, giving us a max of about 2.4B columns (113 allows for 1996099046), and bump the row size EH to 150, allowing for almost a 1.7T row.  It's theoretically possible to make a row larger than that, but I think having cfstats throw an exception will be the least of the user's problems at that point.;;;","26/Aug/10 04:36;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-cli doesn't allow hyphens in hostnames,CASSANDRA-677,12444836,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,rschildmeijer,urandom,urandom,07/Jan/10 04:40,16/Apr/19 17:33,22/Mar/23 14:57,25/Mar/10 02:43,0.6,,Legacy/Tools,,2,,,,,,"It's not possible to use a hostname that contains a hyphen with the ""connect"" command interactively, (the parser does not accept hostnames that contain hyphens).

Note: It is still possible to connect to such hosts by passing it on the command line using -host.",Any,rschildmeijer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-914,,,,,,,,,,,,"19/Jan/10 04:08;rschildmeijer;CASSANDRA-677.patch;https://issues.apache.org/jira/secure/attachment/12430667/CASSANDRA-677.patch","18/Jan/10 00:13;rschildmeijer;CASSANDRA-677.patch;https://issues.apache.org/jira/secure/attachment/12430560/CASSANDRA-677.patch",,,,,,,,,,,,,2.0,rschildmeijer,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19814,,,Wed Mar 24 18:43:17 UTC 2010,,,,,,,,,,"0|i0g0ef:",91495,,,,,Low,,,,,,,,,,,,,,,,,"19/Jan/10 01:00;urandom;Thanks Roger. But I'm afraid this won't work for a couple of reasons.

1. Because `Identifier' is used for column family names, it permits the use of hyphens there, which as things currently stand will cause problems (hint: on disk, CFs use hypen delimited filenames).

2. Something other than `Identifier' should be used since that would allow underscores in hostnames, which should not be permitted.;;;","19/Jan/10 04:07;rschildmeijer;Improvements made after Eric's comment about the previous patch (Identifier is now left untouched).;;;","20/Jan/10 06:52;urandom;Sorry Roger, I did some quick testing and this patch seem to have issues as well. It does allow hyphens in hostnames, but only when there are no numbers involved.

cassandra> connect cass-1.lab/9160
line 1:13 mismatched input '1' expecting Identifier
Exception Cannot open null host.;;;","25/Mar/10 02:43;urandom;This has been applied. See CASSANDRA-914 for some additional background.

Thanks Roger.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OrderPreservingPartitioner with type validated indexed columns causes ClassCastException,CASSANDRA-1373,12471158,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,thobbs,thobbs,10/Aug/10 09:28,16/Apr/19 17:33,22/Mar/23 14:57,18/Aug/10 01:06,0.7 beta 2,,,,0,,,,,,"If OrderPreservingPartitioner is used and you have an indexed column with a type validator, using batch_mutate to insert column values (like pycassa does) on the same key and indexed column causes a ClassCastException to be thrown the *second* time you execute it.  That is, the first batch_mutate succeeds, but the following ones fail.  CollatedOrderPreservingPartitioner seems to avoid this problem.  Also, it appears that the row key is being compared to the column value at some point using the validator's Comparator class (such as LongType) which is where the actual exception is thrown.

Stack trace below:
{noformat}
java.lang.RuntimeException: java.lang.ClassCastException: java.lang.String cannot be cast to [B
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.ClassCastException: java.lang.String cannot be cast to [B
	at org.apache.cassandra.db.marshal.LongType.compare(LongType.java:27)
	at org.apache.cassandra.dht.LocalToken.compareTo(LocalToken.java:45)
	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:82)
	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:37)
	at java.util.concurrent.ConcurrentSkipListMap.doPut(ConcurrentSkipListMap.java:878)
	at java.util.concurrent.ConcurrentSkipListMap.putIfAbsent(ConcurrentSkipListMap.java:1893)
	at org.apache.cassandra.db.Memtable.resolve(Memtable.java:127)
	at org.apache.cassandra.db.Memtable.put(Memtable.java:119)
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:508)
	at org.apache.cassandra.db.Table.applyCF(Table.java:452)
	at org.apache.cassandra.db.Table.apply(Table.java:409)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:196)
	at org.apache.cassandra.service.StorageProxy$2.runMayThrow(StorageProxy.java:276)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more
{noformat}",Cassandra Trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Aug/10 06:35;jbellis;1373.txt;https://issues.apache.org/jira/secure/attachment/12452221/1373.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20108,,,Wed Aug 18 13:13:48 UTC 2010,,,,,,,,,,"0|i0g4nr:",92185,,thobbs,,thobbs,Normal,,,,,,,,,,,,,,,,,"17/Aug/10 06:35;jbellis;correct patch attached.;;;","17/Aug/10 07:01;thobbs;New stacktrace with patch 1373.txt (2010-08-16 06:35 PM) applied:

\\
{noformat}
java.lang.RuntimeException: java.lang.ClassCastException: java.lang.String cannot be cast to [B
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.ClassCastException: java.lang.String cannot be cast to [B
	at org.apache.cassandra.db.marshal.LongType.compare(LongType.java:28)
	at org.apache.cassandra.dht.LocalToken.compareTo(LocalToken.java:45)
	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:82)
	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:37)
	at java.util.concurrent.ConcurrentSkipListMap.findPredecessor(ConcurrentSkipListMap.java:685)
	at java.util.concurrent.ConcurrentSkipListMap.doPut(ConcurrentSkipListMap.java:864)
	at java.util.concurrent.ConcurrentSkipListMap.putIfAbsent(ConcurrentSkipListMap.java:1893)
	at org.apache.cassandra.db.Memtable.resolve(Memtable.java:127)
	at org.apache.cassandra.db.Memtable.put(Memtable.java:119)
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:508)
	at org.apache.cassandra.db.Table.applyCF(Table.java:452)
	at org.apache.cassandra.db.Table.apply(Table.java:409)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:196)
	at org.apache.cassandra.service.StorageProxy$1.runMayThrow(StorageProxy.java:194)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more
{noformat};;;","18/Aug/10 01:03;thobbs;Patch was not exercised correctly last time.  The patch does fix the issue, now.;;;","18/Aug/10 01:06;jbellis;committed;;;","18/Aug/10 21:13;hudson;Integrated in Cassandra #517 (See [https://hudson.apache.org/hudson/job/Cassandra/517/])
    fix updating index when value is changed.  patch by jbellis; tested by Tyler Hobbs for CASSANDRA-1373
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use of ByteBuffer limit() must account for arrayOffset(),CASSANDRA-1661,12478239,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,tjake,tjake,tjake,25/Oct/10 23:06,16/Apr/19 17:33,22/Mar/23 14:57,26/Oct/10 11:14,0.7 beta 3,,,,0,,,,,,"There are a few places in the code where it loops across a byte buffers backing array wrong:


        for (int i=bytes.position()+bytes.arrayOffset(); i<bytes.limit(); i++)

This is incorrect as the limit() does not account for arrayOffset()

              for (int i=bytes.position()+bytes.arrayOffset(); i<bytes.limit()+bytes.arrayOffset(); i++)

is the correct code.

There is also a few places where the unit tests would fail if we used non wrapped byte arrays.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Oct/10 23:07;tjake;1661_v1.txt;https://issues.apache.org/jira/secure/attachment/12457974/1661_v1.txt","26/Oct/10 02:31;tjake;1661_v2.txt;https://issues.apache.org/jira/secure/attachment/12457989/1661_v2.txt",,,,,,,,,,,,,2.0,tjake,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20245,,,Tue Oct 26 03:14:43 UTC 2010,,,,,,,,,,"0|i0g6lz:",92501,,stuhood,,stuhood,Normal,,,,,,,,,,,,,,,,,"25/Oct/10 23:09;stuhood;Regarding failing unit tests: a utility function that calls the allocator from 1651 would probably shorten the code and catch a lot of these.;;;","25/Oct/10 23:21;tjake;good idea, I'll address it with that ticket;;;","26/Oct/10 00:24;stuhood;* A few of the loops involving direct array access (in particular the one in RandomPartitioner) could be simplified with judicious use of mark() and get()
* Adding the follow function and using it in the tests would clarify things a lot: CFSTest looks like it exploded.
{code:java}public static ByteBuffer bytes(String s) { return ByteBuffer.wrap(s.getBytes(UTF_8)); }{code}
* Converting BytesToken to ByteBuffer storage would clean up a ton of special casing (should probably be tackled in a separate issue though)
* ByteBufferTest is literally a test of ByteBuffers: I don't think it should be in C*

Thanks!;;;","26/Oct/10 00:29;jbellis;bq. simplified with judicious use of mark() and get()

I worry that we're going to introduce very very tricky bugs if we don't treat position as immutable.  If BB/Thrift were sane about using offset to represent the start of what we're allowed to mess with, that would be another story, but it's not -- Thrift uses wrap to generate the BB it gives us, so position is the only indication we have of what a given BB is supposed to cover.

bq. ByteBufferTest is literally a test of ByteBuffers: I don't think it should be in C*

OTOH ByteBuffer is a tricky enough API (see: this ticket :) that I don't mind having BBT as an illustration of what is going on if not exactly a test.

Maybe docstring in BBUtil instead of actual tests?;;;","26/Oct/10 00:30;jbellis;bq. Converting BytesToken to ByteBuffer storage

As background, this is an artifact of MerkleTree requiring serializable Tokens.  Using byte[] there was the easiest way to get that.  (Not necessarily the best, I agree.);;;","26/Oct/10 00:32;tjake;  * A few of the loops involving direct array access (in particular the one in RandomPartitioner) could be simplified with judicious use of mark() and get()
   *A previous version of the BB patch did exactly this, but Johnathan thought it would be better to treat ByteBuffers as immutable so avoid changing position and mark.*


  * Adding the follow function and using it in the tests would clarify things a lot: CFSTest looks like it exploded.
     *I agree this is a mess, if you think it should be fixed in this patch ok, but it seems like a small issue.*

  * Converting BytesToken to ByteBuffer storage would clean up a ton of special casing (should probably be tackled in a separate issue though)
     *I had also done this in a previous patch but it broke MerkleTree serialization*

  * ByteBufferTest is literally a test of ByteBuffers: I don't think it should be in C
     *I put it in there incase others have questions about how ByteBuffers work.*

 ;;;","26/Oct/10 00:40;jbellis;bq. Adding the follow function and using it in the tests would clarify things a lot

+1 doing this.;;;","26/Oct/10 00:43;stuhood;> better to treat ByteBuffers as immutable so avoid changing position and mark
In cases where the buffer should be immutable, you'd need to duplicate() it first.

> should be fixed in this patch ok, but it seems like a small issue
It's a small function: waiting any longer to add it isn't worth it, imo.

> I put it in there incase others have questions about how ByteBuffers work.
I'm out-numbered on this one I guess.;;;","26/Oct/10 02:31;tjake;New version:

  * Puts BB test in BBUtil javadoc
  * Cleans up tests with new BBUtil call
  * Fixes Pig and word count;;;","26/Oct/10 02:43;stuhood;+1 All that is left is nitpicks, which you can choose to ignore:

> Cleans up tests with new BBUtil call
A static import might be the last bit of sugar to make this a real win.

Code style dictates that there should be spaces around operators.;;;","26/Oct/10 11:14;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Consisteny Level of ZERO blocks for ack on Commit Log,CASSANDRA-399,12434368,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,lenn0x,lenn0x,lenn0x,30/Aug/09 06:23,16/Apr/19 17:33,22/Mar/23 14:57,30/Aug/09 06:47,0.4,,,,0,,,,,,"If consistency level is set to ZERO and endpoint is local, clients must wait for a write to the commit log. We need to remove this special case, and just send through MessagingService.getMessagingInstance().sendOneWay.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Aug/09 06:31;lenn0x;0001-CASSANDRA-399-If-consistency-level-is-set-to-ZERO-an.patch;https://issues.apache.org/jira/secure/attachment/12418076/0001-CASSANDRA-399-If-consistency-level-is-set-to-ZERO-an.patch",,,,,,,,,,,,,,1.0,lenn0x,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19671,,,Sun Aug 30 12:35:36 UTC 2009,,,,,,,,,,"0|i0fyp3:",91219,,,,,Low,,,,,,,,,,,,,,,,,"30/Aug/09 06:33;sandeep_tata;I remember we put this in to get limited read-your-writes consistency for a session. (Log + rm.apply()  before ack.)
See CASSANDRA-132.

There are cases when this is not the right thing to do (eg skip logging during a load operation), but for a normal client, read-your-writes is way easier to deal with than pure eventual consistency. 





;;;","30/Aug/09 06:47;jbellis;Committed before I saw Sandeep's objection.  But insert -- the method that handles ConsistencyLevel.ZERO -- is the wrong place to do this.  If you want any blocking you need to use ONE or higher, that's how it's supposed to work.;;;","30/Aug/09 06:47;jbellis;(which goes to the separate insertBlocking method.);;;","30/Aug/09 06:49;jbellis;to clarify: I am +1 on adding a CASSANDRA-132 special case for insertBlocking, but -1 on having it in insert.  My bad for not catching that in review originally.  Should we re-open 132?;;;","30/Aug/09 06:51;lenn0x;I agree that I think this special case should live in insertBlocking. ;;;","30/Aug/09 07:05;sandeep_tata;I'm +1 on the patch, but we should re-open 132. 
Current insertBlocking=1 is not the same as session level read-your-writes.
I guess we could just special-case this write for insertBlocking=1. I'd rather not introduce a new ConsistencyLevel.


ConsistencyLevel.ZERO should help stress test MessagingService... I remember with the old codepath writes would get queued up in the MessagingService and we ended up with a few lost writes. Should be interesting to see if those problems pop up again.



;;;","30/Aug/09 20:35;hudson;Integrated in Cassandra #182 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/182/])
    r/m special case of local destination when writing with ConsistencyLevel.ZERO, since it causes blocking for commitlog.  (MessagingService still optimizes out the network write/read.)  Patch by Chris Goffinet; reviewed by jbellis for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ByteBufferUtil.clone shouldn't mutate the passed bytebuffer,CASSANDRA-1847,12493038,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,tjake,tjake,tjake,13/Dec/10 09:57,16/Apr/19 17:33,22/Mar/23 14:57,14/Dec/10 00:56,0.7.0 rc 3,,,,0,,,,,,"Currently ByteBufferUtil.clone uses .mark and .reset on the passed ByteBuffer.

This is fine when using thrift because no two ByteBuffer are being accessed at the same time, but this could be thread-unsafe if the same BB was passed concurrently.

Solandra is having problems with this (Solandra shares JVM with Cassandra).",,tcn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Dec/10 10:13;jbellis;1847-v2.txt;https://issues.apache.org/jira/secure/attachment/12466113/1847-v2.txt","13/Dec/10 23:04;tjake;1847-v3.txt;https://issues.apache.org/jira/secure/attachment/12466137/1847-v3.txt","14/Dec/10 00:44;jbellis;1847-v4.txt;https://issues.apache.org/jira/secure/attachment/12466146/1847-v4.txt","13/Dec/10 10:00;tjake;1847_v1.txt;https://issues.apache.org/jira/secure/attachment/12466111/1847_v1.txt",,,,,,,,,,,4.0,tjake,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20342,,,Mon Dec 13 17:14:37 UTC 2010,,,,,,,,,,"0|i0g7rz:",92690,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"13/Dec/10 10:13;jbellis;v2 uses arraycopy instead of manual loop and enforces non-null;;;","13/Dec/10 16:59;slebresne;+1 on v2;;;","13/Dec/10 22:00;tjake;Right, thats faster but I was thinking ahead for CASSANDRA-1714 we'd need to use this for putting a row into the cache and .array() only works with HeapByteBuffers.  ;;;","13/Dec/10 22:49;jbellis;should we add an instanceof check for HeapByteBuffer then? seems like it's probably worth it. ;;;","13/Dec/10 23:04;tjake;v3 checks if buffer isDirect();;;","14/Dec/10 00:44;jbellis;BB.get(i) adds arrayoffset to the given index, so I think the loop in v3 is buggy.  v4 attached;;;","14/Dec/10 00:52;tjake;ok took ""absolute"" too literally then :)
+1 v4;;;","14/Dec/10 00:56;jbellis;committed;;;","14/Dec/10 01:14;hudson;Integrated in Cassandra-0.7 #72 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/72/])
    make ByteBufferUtil.clone thread-safe
patch by tjake and jbellis for CASSANDRA-1847
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed compactation doesn't delete temporary files causing hd to fill,CASSANDRA-2424,12503497,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Duplicate,,tbritz,tbritz,06/Apr/11 17:45,16/Apr/19 17:33,22/Mar/23 14:57,29/Apr/11 21:30,0.8.1,,,,0,,,,,,"Hi,

We seem to have a few incorrect keys in one of our columns (Seems to be a memory bit flip). Cassandra will try to compact and compact those tables again but won't delete any temporary files.



 INFO [CompactionExecutor:1] 2011-04-05 17:52:45,494 SSTableWriter.java (line 108) Last written key : DecoratedKey(dsearch_1300960988716456<398199492444161_1301855754714_4564397053271578441, 647365617263685f313330303936303938383731363435363c3339383139393439323434343136315f313330313835353735343731345f34353634333937303533323731353738343431)
 INFO [CompactionExecutor:1] 2011-04-05 17:52:45,521 SSTableWriter.java (line 109) Current key : DecoratedKey(dsearch_13009609887164564398199492444161_1301855754760_4564395937046043595, 647365617263685f31333030393630393838373136343536343339383139393439323434343136315f313330313835353735343736305f34353634333935393337303436303433353935)
 INFO [CompactionExecutor:1] 2011-04-05 17:52:45,521 SSTableWriter.java (line 110) Writing into file /cassandra/data/table_userentries/table_userentries-tmp-f-3550-Data.db
ERROR [CompactionExecutor:1] 2011-04-05 17:52:45,522 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.io.IOException: Keys must be written in ascending order.
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:111)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:128)
        at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:452)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:124)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:94)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)

		

ls -l /cassandra/data/table_userentries/table_userentries-tmp*
-rw-r--r-- 1 root root  13G 2011-04-05 16:09 /cassandra/data/table_userentries/table_userentries-tmp-f-3507-Data.db
-rw-r--r-- 1 root root 512M 2011-04-05 16:09 /cassandra/data/table_userentries/table_userentries-tmp-f-3507-Index.db
-rw-r--r-- 1 root root  13G 2011-04-05 16:21 /cassandra/data/table_userentries/table_userentries-tmp-f-3512-Data.db
-rw-r--r-- 1 root root 512M 2011-04-05 16:21 /cassandra/data/table_userentries/table_userentries-tmp-f-3512-Index.db
-rw-r--r-- 1 root root  13G 2011-04-05 16:33 /cassandra/data/table_userentries/table_userentries-tmp-f-3517-Data.db
-rw-r--r-- 1 root root 512M 2011-04-05 16:33 /cassandra/data/table_userentries/table_userentries-tmp-f-3517-Index.db
-rw-r--r-- 1 root root  13G 2011-04-05 16:44 /cassandra/data/table_userentries/table_userentries-tmp-f-3521-Data.db
-rw-r--r-- 1 root root 512M 2011-04-05 16:44 /cassandra/data/table_userentries/table_userentries-tmp-f-3521-Index.db
-rw-r--r-- 1 root root  13G 2011-04-05 16:55 /cassandra/data/table_userentries/table_userentries-tmp-f-3527-Data.db
-rw-r--r-- 1 root root 512M 2011-04-05 16:55 /cassandra/data/table_userentries/table_userentries-tmp-f-3527-Index.db
-rw-r--r-- 1 root root  13G 2011-04-05 17:05 /cassandra/data/table_userentries/table_userentries-tmp-f-3531-Data.db
-rw-r--r-- 1 root root 512M 2011-04-05 17:06 /cassandra/data/table_userentries/table_userentries-tmp-f-3531-Index.db
-rw-r--r-- 1 root root  13G 2011-04-05 17:16 /cassandra/data/table_userentries/table_userentries-tmp-f-3535-Data.db
-rw-r--r-- 1 root root 512M 2011-04-05 17:16 /cassandra/data/table_userentries/table_userentries-tmp-f-3535-Index.db
-rw-r--r-- 1 root root  13G 2011-04-05 17:28 /cassandra/data/table_userentries/table_userentries-tmp-f-3540-Data.db
-rw-r--r-- 1 root root 512M 2011-04-05 17:28 /cassandra/data/table_userentries/table_userentries-tmp-f-3540-Index.db
-rw-r--r-- 1 root root  13G 2011-04-05 17:39 /cassandra/data/table_userentries/table_userentries-tmp-f-3545-Data.db
-rw-r--r-- 1 root root 512M 2011-04-05 17:40 /cassandra/data/table_userentries/table_userentries-tmp-f-3545-Index.db
-rw-r--r-- 1 root root  13G 2011-04-05 17:52 /cassandra/data/table_userentries/table_userentries-tmp-f-3550-Data.db
-rw-r--r-- 1 root root 512M 2011-04-05 17:52 /cassandra/data/table_userentries/table_userentries-tmp-f-3550-Index.db
-rw-r--r-- 1 root root  13G 2011-04-05 18:04 /cassandra/data/table_userentries/table_userentries-tmp-f-3556-Data.db
-rw-r--r-- 1 root root 512M 2011-04-05 18:04 /cassandra/data/table_userentries/table_userentries-tmp-f-3556-Index.db
-rw-r--r-- 1 root root  13G 2011-04-05 18:15 /cassandra/data/table_userentries/table_userentries-tmp-f-3562-Data.db
-rw-r--r-- 1 root root 512M 2011-04-05 18:15 /cassandra/data/table_userentries/table_userentries-tmp-f-3562-Index.db
-rw-r--r-- 1 root root  13G 2011-04-05 18:28 /cassandra/data/table_userentries/table_userentries-tmp-f-3566-Data.db
-rw-r--r-- 1 root root 512M 2011-04-05 18:28 /cassandra/data/table_userentries/table_userentries-tmp-f-3566-Index.db
-rw-r--r-- 1 root root  13G 2011-04-05 18:40 /cassandra/data/table_userentries/table_userentries-tmp-f-3572-Data.db
-rw-r--r-- 1 root root 512M 2011-04-05 18:40 /cassandra/data/table_userentries/table_userentries-tmp-f-3572-Index.db
-rw-r--r-- 1 root root  13G 2011-04-05 18:53 /cassandra/data/table_userentries/table_userentries-tmp-f-3577-Data.db
-rw-r--r-- 1 root root 512M 2011-04-05 18:53 /cassandra/data/table_userentries/table_userentries-tmp-f-3577-Index.db
-rw-r--r-- 1 root root  13G 2011-04-05 19:06 /cassandra/data/table_userentries/table_userentries-tmp-f-3582-Data.db
-rw-r--r-- 1 root root 512M 2011-04-05 19:07 /cassandra/data/table_userentries/table_userentries-tmp-f-3582-Index.db
-rw-r--r-- 1 root root  19G 2011-04-05 19:29 /cassandra/data/table_userentries/table_userentries-tmp-f-3592-Data.db
-rw-r--r-- 1 root root 736M 2011-04-05 19:29 /cassandra/data/table_userentries/table_userentries-tmp-f-3592-Index.db
-rw-r--r-- 1 root root  19G 2011-04-05 19:42 /cassandra/data/table_userentries/table_userentries-tmp-f-3598-Data.db
-rw-r--r-- 1 root root 736M 2011-04-05 19:42 /cassandra/data/table_userentries/table_userentries-tmp-f-3598-Index.db
-rw-r--r-- 1 root root  19G 2011-04-05 19:56 /cassandra/data/table_userentries/table_userentries-tmp-f-3602-Data.db
-rw-r--r-- 1 root root 736M 2011-04-05 19:56 /cassandra/data/table_userentries/table_userentries-tmp-f-3602-Index.db
-rw-r--r-- 1 root root  19G 2011-04-05 20:12 /cassandra/data/table_userentries/table_userentries-tmp-f-3606-Data.db
-rw-r--r-- 1 root root 736M 2011-04-05 20:12 /cassandra/data/table_userentries/table_userentries-tmp-f-3606-Index.db
-rw-r--r-- 1 root root  19G 2011-04-05 20:28 /cassandra/data/table_userentries/table_userentries-tmp-f-3612-Data.db
-rw-r--r-- 1 root root 736M 2011-04-05 20:28 /cassandra/data/table_userentries/table_userentries-tmp-f-3612-Index.db
-rw-r--r-- 1 root root  19G 2011-04-05 20:44 /cassandra/data/table_userentries/table_userentries-tmp-f-3617-Data.db
-rw-r--r-- 1 root root 736M 2011-04-05 20:44 /cassandra/data/table_userentries/table_userentries-tmp-f-3617-Index.db
-rw-r--r-- 1 root root  19G 2011-04-05 21:01 /cassandra/data/table_userentries/table_userentries-tmp-f-3622-Data.db
-rw-r--r-- 1 root root 736M 2011-04-05 21:01 /cassandra/data/table_userentries/table_userentries-tmp-f-3622-Index.db
-rw-r--r-- 1 root root  19G 2011-04-05 21:23 /cassandra/data/table_userentries/table_userentries-tmp-f-3627-Data.db
-rw-r--r-- 1 root root 736M 2011-04-05 21:23 /cassandra/data/table_userentries/table_userentries-tmp-f-3627-Index.db
-rw-r--r-- 1 root root  19G 2011-04-05 21:48 /cassandra/data/table_userentries/table_userentries-tmp-f-3635-Data.db
-rw-r--r-- 1 root root 736M 2011-04-05 21:48 /cassandra/data/table_userentries/table_userentries-tmp-f-3635-Index.db
-rw-r--r-- 1 root root  19G 2011-04-05 22:16 /cassandra/data/table_userentries/table_userentries-tmp-f-3646-Data.db
-rw-r--r-- 1 root root 736M 2011-04-05 22:16 /cassandra/data/table_userentries/table_userentries-tmp-f-3646-Index.db
-rw-r--r-- 1 root root  19G 2011-04-05 22:48 /cassandra/data/table_userentries/table_userentries-tmp-f-3658-Data.db
-rw-r--r-- 1 root root 736M 2011-04-05 22:48 /cassandra/data/table_userentries/table_userentries-tmp-f-3658-Index.db
-rw-r--r-- 1 root root  19G 2011-04-05 23:22 /cassandra/data/table_userentries/table_userentries-tmp-f-3672-Data.db
-rw-r--r-- 1 root root 736M 2011-04-05 23:21 /cassandra/data/table_userentries/table_userentries-tmp-f-3672-Index.db
-rw-r--r-- 1 root root  19G 2011-04-05 23:53 /cassandra/data/table_userentries/table_userentries-tmp-f-3685-Data.db
-rw-r--r-- 1 root root 736M 2011-04-05 23:52 /cassandra/data/table_userentries/table_userentries-tmp-f-3685-Index.db
-rw-r--r-- 1 root root  19G 2011-04-06 00:29 /cassandra/data/table_userentries/table_userentries-tmp-f-3698-Data.db
-rw-r--r-- 1 root root 736M 2011-04-06 00:28 /cassandra/data/table_userentries/table_userentries-tmp-f-3698-Index.db
-rw-r--r-- 1 root root  19G 2011-04-06 01:01 /cassandra/data/table_userentries/table_userentries-tmp-f-3712-Data.db
-rw-r--r-- 1 root root 736M 2011-04-06 01:01 /cassandra/data/table_userentries/table_userentries-tmp-f-3712-Index.db
-rw-r--r-- 1 root root  19G 2011-04-06 01:36 /cassandra/data/table_userentries/table_userentries-tmp-f-3723-Data.db
-rw-r--r-- 1 root root 736M 2011-04-06 01:36 /cassandra/data/table_userentries/table_userentries-tmp-f-3723-Index.db
-rw-r--r-- 1 root root  19G 2011-04-06 02:08 /cassandra/data/table_userentries/table_userentries-tmp-f-3735-Data.db
-rw-r--r-- 1 root root 736M 2011-04-06 02:08 /cassandra/data/table_userentries/table_userentries-tmp-f-3735-Index.db
-rw-r--r-- 1 root root  13G 2011-04-06 02:32 /cassandra/data/table_userentries/table_userentries-tmp-f-3742-Data.db
-rw-r--r-- 1 root root 512M 2011-04-06 02:32 /cassandra/data/table_userentries/table_userentries-tmp-f-3742-Index.db
-rw-r--r-- 1 root root 4.5G 2011-04-06 02:42 /cassandra/data/table_userentries/table_userentries-tmp-f-3748-Data.db
-rw-r--r-- 1 root root 168M 2011-04-06 02:42 /cassandra/data/table_userentries/table_userentries-tmp-f-3748-Index.db


	",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20617,,,Fri Apr 29 13:30:48 UTC 2011,,,,,,,,,,"0|i0gban:",93260,,,,,Normal,,,,,,,,,,,,,,,,,"29/Apr/11 21:30;slebresne;dupe of CASSANDRA-2468;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cfstats is broken,CASSANDRA-1540,12474972,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,24/Sep/10 03:36,16/Apr/19 17:33,22/Mar/23 14:57,24/Sep/10 05:23,0.7 beta 2,,,,0,,,,,,"There appears to be a problem reading the cache stats:

bin/nodetool -h cassandra-6 cfstats
Keyspace: org.apache.cassandra.db.Table@620a3d3b
        Read Count: 9
        Read Latency: 2.563222222222222 ms.
        Write Count: 11
        Write Latency: 0.1572727272727273 ms.
        Pending Tasks: 0
                Column Family: LocationInfo
                SSTable count: 1
                Space used (live): 4886
                Space used (total): 4886
                Memtable Columns Count: 6
                Memtable Data Size: 179
                Memtable Switch Count: 1
                Read Count: 4
                Read Latency: 5.369 ms.
                Write Count: 8
                Write Latency: 0.210 ms.
                Pending Tasks: 0
Exception in thread ""main"" java.lang.reflect.UndeclaredThrowableException
        at $Proxy5.getCapacity(Unknown Source)
        at org.apache.cassandra.tools.NodeCmd.printColumnFamilyStats(NodeCmd.java:326)
        at org.apache.cassandra.tools.NodeCmd.main(NodeCmd.java:439)
Caused by: javax.management.InstanceNotFoundException: org.apache.cassandra.db:type=Caches,keyspace=org.apache.cassandra.db.Table@620a3d3b,cache=LocationInfoKeyCache
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1118)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:679)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:672)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1426)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:90)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1284)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1382)
        at javax.management.remote.rmi.RMIConnectionImpl.getAttribute(RMIConnectionImpl.java:619)
        at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
        at sun.rmi.transport.Transport$1.run(Transport.java:177)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
        at sun.rmi.transport.StreamRemoteCall.exceptionReceivedFromServer(StreamRemoteCall.java:273)
        at sun.rmi.transport.StreamRemoteCall.executeCall(StreamRemoteCall.java:251)
        at sun.rmi.server.UnicastRef.invoke(UnicastRef.java:160)
        at com.sun.jmx.remote.internal.PRef.invoke(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl_Stub.getAttribute(Unknown Source)
        at javax.management.remote.rmi.RMIConnector$RemoteMBeanServerConnection.getAttribute(RMIConnector.java:885)
        at javax.management.MBeanServerInvocationHandler.invoke(MBeanServerInvocationHandler.java:280)
        ... 3 more
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Sep/10 04:34;brandon.williams;1540.txt;https://issues.apache.org/jira/secure/attachment/12455412/1540.txt",,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20187,,,Fri Sep 24 12:47:04 UTC 2010,,,,,,,,,,"0|i0g5uv:",92379,,,,,Normal,,,,,,,,,,,,,,,,,"24/Sep/10 04:34;brandon.williams;Patch to register the mbean name correctly.;;;","24/Sep/10 05:17;jbellis;+1;;;","24/Sep/10 05:23;brandon.williams;Committed.;;;","24/Sep/10 20:47;hudson;Integrated in Cassandra #545 (See [https://hudson.apache.org/hudson/job/Cassandra/545/])
    Register CFS Mbean with the correct keyspace name.  Patch by brandonwilliams, reviewed by jbellis for CASSANDRA-1540
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Races between schema changes and StorageService operations,CASSANDRA-2350,12501718,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,paladin8,paladin8,18/Mar/11 02:30,16/Apr/19 17:33,22/Mar/23 14:57,18/Mar/11 21:20,0.7.5,,,,0,,,,,,"I only tested this on 0.7.0, but it judging by the 0.7.3 code (latest I've looked at) the same thing should happen.

The case in particular that I ran into is this: I force a compaction for all CFs in a keyspace, and while the compaction is happening I add another CF to the keyspace. I get the following exception because the underlying set of CFs has changed while being iterated over.

{noformat}
java.util.ConcurrentModificationException
        at java.util.HashMap$HashIterator.nextEntry(Unknown Source)
        at java.util.HashMap$ValueIterator.next(Unknown Source)
        at java.util.Collections$UnmodifiableCollection$1.next(Unknown Source)
        at org.apache.cassandra.service.StorageService.forceTableCompaction(StorageService.java:1140)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(Unknown Source)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(Unknown Source)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(Unknown Source)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(Unknown Source)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(Unknown Source)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(Unknown Source)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor84.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at sun.rmi.server.UnicastServerRef.dispatch(Unknown Source)
        at sun.rmi.transport.Transport$1.run(Unknown Source)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Unknown Source)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(Unknown Source)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(Unknown Source)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source) 
{noformat}

The problem is a little more fundamental than that, though, as I believe any schema change of CFs in the keyspace during one of these operations (e.g. flush, compaction, etc) has the potential to cause a race. I'm not sure what would happen if the set of CFs to compact was acquired and one of them was dropped before it had been compacted.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Mar/11 12:15;jbellis;2350.txt;https://issues.apache.org/jira/secure/attachment/12473969/2350.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20573,,,Mon Mar 21 22:15:38 UTC 2011,,,,,,,,,,"0|i0gavb:",93191,,gdusbabek,,gdusbabek,Low,,,,,,,,,,,,,,,,,"18/Mar/11 12:15;jbellis;make Table.columnFamilyStores a ConcurrentHashMap (and make it private);;;","18/Mar/11 20:43;gdusbabek;+1;;;","18/Mar/11 21:20;jbellis;committed;;;","22/Mar/11 06:15;hudson;Integrated in Cassandra-0.7 #397 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/397/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SystemTable is not persisted across reboots,CASSANDRA-362,12433009,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,sammy.yu,sammy.yu,sammy.yu,14/Aug/09 01:03,16/Apr/19 17:33,22/Mar/23 14:57,14/Aug/09 06:16,0.4,,,,0,,,,,,"If you set InititalToken to """" and restart cassandra it should generate a initialtoken and store it in the SystemTable so that next time it is not regenerated.  However, this is not the case as a new inititaltoken is generated every time.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Aug/09 05:52;sammy.yu;0001-Based-on-jbellis-original-patch-3-and-what-I-had-lo.patch;https://issues.apache.org/jira/secure/attachment/12416499/0001-Based-on-jbellis-original-patch-3-and-what-I-had-lo.patch","14/Aug/09 05:56;sammy.yu;0002-Based-on-jbellis-original-patch-3-and-what-I-had-lo.patch;https://issues.apache.org/jira/secure/attachment/12416500/0002-Based-on-jbellis-original-patch-3-and-what-I-had-lo.patch","14/Aug/09 05:03;jbellis;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-362-rename-underscores-away.txt;https://issues.apache.org/jira/secure/attachment/12416484/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-362-rename-underscores-away.txt","14/Aug/09 05:03;jbellis;ASF.LICENSE.NOT.GRANTED--0002-rename-getInitialToken-to-getToken-it-doesn-t-actuall.txt;https://issues.apache.org/jira/secure/attachment/12416485/ASF.LICENSE.NOT.GRANTED--0002-rename-getInitialToken-to-getToken-it-doesn-t-actuall.txt","14/Aug/09 05:03;jbellis;ASF.LICENSE.NOT.GRANTED--0003-fix-incompletely-configured-system-table-and-query-for.txt;https://issues.apache.org/jira/secure/attachment/12416486/ASF.LICENSE.NOT.GRANTED--0003-fix-incompletely-configured-system-table-and-query-for.txt",,,,,,,,,,5.0,sammy.yu,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19652,,,Fri Aug 14 14:06:51 UTC 2009,,,,,,,,,,"0|i0fyh3:",91183,,,,,Normal,,,,,,,,,,,,,,,,,"14/Aug/09 04:55;markr;Sounds pretty serious - but easy to resolve.;;;","14/Aug/09 05:07;jbellis;01 and 02 are renaming and cleanup.

03 has fixes for 4 separate bugs that colluded to break this. :)

+            tableKeysCachedFractions_.put(""system"", 0.01);

this is required to flush now, so we need to generate it for the system table (keyspace).

-                    /* TODO: Remove this to not process Hints */

for some reason FB decided not to replay writes to the system tables.  the comment indicates that this was an optimization for hinted handoff, but if there isn't much hinted handoff data, then replaying isn't a Big Deal, and if there _is_ a lot then you probably don't want to throw it away.

this was biting us here since we moved the token/generation info into the system table instead of a special-cased file.

+        QueryFilter filter = new IdentityQueryFilter(LOCATION_KEY, new QueryPath(LOCATION_CF));

the old query only fetched generation.  this is the easiest way to fetch the whole CF.

+        return ColumnFamily.create(getTableName(), getColumnFamilyName());

old code was broken (passing null in several places).  not sure why; didn't investigate too closely since the new code is the more straightforward solution anyway.;;;","14/Aug/09 05:08;jbellis;(note, if you want to restart rapidly you will still need to turn commitlogsync on for the write to actually hit the log and get replayed.);;;","14/Aug/09 05:09;jbellis;(note2: these patches are generated on top of 358, so if they don't apply by themselves, feel free to review that first :);;;","14/Aug/09 05:52;sammy.yu;Enabled system table so that it can be queried
Disabled hinted CF from being replayed (original behavior)
;;;","14/Aug/09 05:56;sammy.yu;Removed hint hand off table check
Ignore My 0001-patch.  This patch is a replacement for jbellis original 03.patch.

This patch applies cleanly against trunk
;;;","14/Aug/09 06:16;jbellis;committed sammy's 0002, minus the table.flush replacing commented-out code.  will open a new ticket for cleanup.;;;","14/Aug/09 22:06;hudson;Integrated in Cassandra #167 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/167/])
    Fixes for saving Token in SystemTable.  patch by jbellis and Sammy Yu for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unit tests log RTE to stderr even though tests still succeed.,CASSANDRA-806,12456582,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,gdusbabek,gdusbabek,gdusbabek,18/Feb/10 04:11,16/Apr/19 17:33,22/Mar/23 14:57,18/Feb/10 05:17,0.6,,,,0,,,,,,"[junit] ------------- Standard Error -----------------
   [junit] ERROR 20:05:29,165 Error in executor futuretask
   [junit] java.util.concurrent.ExecutionException: java.lang.RuntimeException: No replica strategy configured for ltable
   [junit]     at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
   [junit]     at java.util.concurrent.FutureTask.get(FutureTask.java:83)
   [junit]     at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:65)
   [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
   [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
   [junit]     at java.lang.Thread.run(Thread.java:619)
   [junit] Caused by: java.lang.RuntimeException: No replica strategy configured for ltable
   [junit]     at org.apache.cassandra.service.StorageService.getReplicationStrategy(StorageService.java:245)
   [junit]     at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:1150)
   [junit]     at org.apache.cassandra.service.AntiEntropyService.getNeighbors(AntiEntropyService.java:149)
   [junit]     at org.apache.cassandra.service.AntiEntropyService.access$100(AntiEntropyService.java:88)
   [junit]     at org.apache.cassandra.service.AntiEntropyService$Validator.call(AntiEntropyService.java:487)
   [junit]     at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
   [junit]     at java.util.concurrent.FutureTask.run(FutureTask.java:138)
   [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
   [junit]     ... 2 more
   [junit] ERROR 20:05:31,949 Error in executor futuretask
   [junit] java.util.concurrent.ExecutionException: java.lang.RuntimeException: No replica strategy configured for rtable
   [junit]     at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
   [junit]     at java.util.concurrent.FutureTask.get(FutureTask.java:83)
   [junit]     at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:65)
   [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
   [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
   [junit]     at java.lang.Thread.run(Thread.java:619)
   [junit] Caused by: java.lang.RuntimeException: No replica strategy configured for rtable
   [junit]     at org.apache.cassandra.service.StorageService.getReplicationStrategy(StorageService.java:245)
   [junit]     at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:1150)
   [junit]     at org.apache.cassandra.service.AntiEntropyService.getNeighbors(AntiEntropyService.java:149)
   [junit]     at org.apache.cassandra.service.AntiEntropyService.access$100(AntiEntropyService.java:88)
   [junit]     at org.apache.cassandra.service.AntiEntropyService$Validator.call(AntiEntropyService.java:487)
   [junit]     at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
   [junit]     at java.util.concurrent.FutureTask.run(FutureTask.java:138)
   [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
   [junit]     ... 2 more
   [junit] ------------- ---------------- ---------------",,johanoskarsson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Feb/10 04:50;gdusbabek;0001-use-real-keyspace-names-so-that-real-replication-str.patch;https://issues.apache.org/jira/secure/attachment/12436126/0001-use-real-keyspace-names-so-that-real-replication-str.patch",,,,,,,,,,,,,,1.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19873,,,Wed Feb 17 21:17:15 UTC 2010,,,,,,,,,,"0|i0g16v:",91623,,,,,Low,,,,,,,,,,,,,,,,,"18/Feb/10 04:52;gdusbabek;This was introduced in 620 when each table was assigned a specific replication strategy, rather than relying on a global one.;;;","18/Feb/10 05:06;stuhood;+1 Looks good to me.;;;","18/Feb/10 05:17;gdusbabek;r911178;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Get Range Slices is broken,CASSANDRA-1442,12472820,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,stuhood,molezam,molezam,30/Aug/10 04:18,16/Apr/19 17:33,22/Mar/23 14:57,09/Oct/10 09:37,0.6.6,0.7 beta 3,,,0,,,,,,"HI,
We just recently tried to use 0.6.4 and 0.6.5 in our production environment and
had some serious problem.
The getRangeSlices functionality is broken.
We have a cluster of 5 machines.
We use getRangeSlices to iterate over all of the keys in a cf (2062 keys total).
We are using OrderPreservingPartitioner.
We use getRangeSlices with KeyRange using keys (not tokens).
If we set the requestBlockCount (aka: KeyRange.setCount()) to a number
greater than 2062 we get all keys in one shot (all is good).
If we try to fetch the keys in smaller blocks (requestBlockCount=100)
we get BAD RESULTS.
We get only 800 unique keys back.
We start with (startKey="""" and endKey="""") then, after each iteration, we use the lastKey to set the startKey for the next page.
Except on first page, we always skip the first item of the page (knowing that it is a repeat, the last one, of the prior page).
To get the lastKey we tried two strategies: [1] set the lastKey to the last item in the page, and [2] use String.compareTo to get the largest ley. Neither strategy worked.
Our keys are strings (obviously the only option in 0.6) that represent numbers.
Some Sample keys are: (in correct lexi order)
-1
11113
11457
6831
7035
8060
8839
------
This code (without any changes) was working correctly under 0.6.3 (we
got same response from getRangeSlices if using requestBlockCounts of
10,000 or 100).
We tried it under 0.6.4 and 0.6.5 and it stopped working.
We reverted back to 0.6.3 and (again, without changing the code) it
started working again.
------
I tried inserting all the keys into a test cluster of one (1 machine) and it worked fine.
So this must be related to how the page is build in a cluster of more than 1 nodes.
We have a cluster of 5 nodes with replication factor of 3.",Linux - CentOs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-1505,,,,,,,,"02/Oct/10 15:14;stuhood;0001-Add-tests-for-StorageProxy.getRestrictedRanges.patch;https://issues.apache.org/jira/secure/attachment/12456179/0001-Add-tests-for-StorageProxy.getRestrictedRanges.patch","02/Oct/10 15:14;stuhood;0002-Allow-ringIterator-to-include-the-minimum-token-and-.patch;https://issues.apache.org/jira/secure/attachment/12456180/0002-Allow-ringIterator-to-include-the-minimum-token-and-.patch","02/Oct/10 15:14;stuhood;0003-Split-the-queryRange-by-ring-and-minimum-tokens.patch;https://issues.apache.org/jira/secure/attachment/12456181/0003-Split-the-queryRange-by-ring-and-minimum-tokens.patch","02/Oct/10 15:17;stuhood;0004-Remove-restrictTo-and-unwrap.patch;https://issues.apache.org/jira/secure/attachment/12456182/0004-Remove-restrictTo-and-unwrap.patch","09/Oct/10 03:56;stuhood;for-trunk-0001-Add-tests-for-StorageProxy.getRestrictedRanges.patch;https://issues.apache.org/jira/secure/attachment/12456723/for-trunk-0001-Add-tests-for-StorageProxy.getRestrictedRanges.patch","09/Oct/10 03:56;stuhood;for-trunk-0002-Allow-ringIterator-to-include-the-minimum-token-and-.patch;https://issues.apache.org/jira/secure/attachment/12456724/for-trunk-0002-Allow-ringIterator-to-include-the-minimum-token-and-.patch","09/Oct/10 03:56;stuhood;for-trunk-0003-Split-the-queryRange-by-ring-and-minimum-tokens.patch;https://issues.apache.org/jira/secure/attachment/12456725/for-trunk-0003-Split-the-queryRange-by-ring-and-minimum-tokens.patch","09/Oct/10 03:57;stuhood;for-trunk-0004-Remove-AbstractBounds.restrictTo.patch;https://issues.apache.org/jira/secure/attachment/12456726/for-trunk-0004-Remove-AbstractBounds.restrictTo.patch",,,,,,,8.0,stuhood,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20140,,,Sat Oct 09 01:37:06 UTC 2010,,,,,,,,,,"0|i0g533:",92254,,jbellis,,jbellis,Critical,,,,,,,,,,,,,,,,,"02/Oct/10 15:17;stuhood;0001 Adds tons of unit tests for StorageProxy.getRestrictedRanges
0002 Adds a flag to TokenMetadata.ringIterator to include the minimum token, to allow for cleaner iteration over split points
0003 Implement getRestrictedRanges using ringIterator and a new AbstractBounds.split method
0004 Remove old implementations;;;","02/Oct/10 15:19;stuhood;Patch applies to 0.6, but the tests should be forward ported.;;;","04/Oct/10 23:42;jbellis;02 seems like it shouldn't be necessary for a correct solution.  thoughts?;;;","04/Oct/10 23:54;stuhood;> 02 seems like it shouldn't be necessary for a correct solution. thoughts?
I don't see a cleaner alternative... finding the correct place to insert the minimum token without the support of the iterator implementation would mean peeking on it to see whether the next token wrapped. This solution is much shorter, and likely to have applications elsewhere in the codebase.;;;","09/Oct/10 03:57;stuhood;Attaching a port of this patch to trunk.;;;","09/Oct/10 09:37;jbellis;committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra deb should depend on adduser,CASSANDRA-1816,12492012,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thepaul,thepaul,thepaul,04/Dec/10 02:04,16/Apr/19 17:33,22/Mar/23 14:57,04/Dec/10 09:51,0.7.0 rc 2,,Packaging,,0,,,,,,"The cassandra debian package uses the addgroup and adduser commands in its postinst script, which are found in the 'adduser' package, but the cassandra debian package does not depend on adduser.   When a user does not already have adduser installed, this will lead to an error like:

{noformat}
Setting up cassandra (0.7.0~rc1) ...
/var/lib/dpkg/info/cassandra.postinst: 50: addgroup: not found
dpkg: error processing cassandra (--configure):
 subprocess installed post-installation script returned error exit status 127
{noformat}

Yes, this won't happen much, because systems without adduser installed are rare.  But adduser is not ""Essential: yes"", so it will happen sometimes in bare-bones VMs or development environments.",Debian squeeze,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Dec/10 02:05;thepaul;cass-1816.txt;https://issues.apache.org/jira/secure/attachment/12465251/cass-1816.txt",,,,,,,,,,,,,,1.0,thepaul,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20326,,,Tue Dec 07 20:30:52 UTC 2010,,,,,,,,,,"0|i0g7l3:",92659,,urandom,,urandom,Low,,,,,,,,,,,,,,,,,"04/Dec/10 09:51;urandom;committed; thanks!;;;","08/Dec/10 04:30;hudson;Integrated in Cassandra #615 (See [https://hudson.apache.org/hudson/job/Cassandra/615/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLI doesn't handle inserting negative integers,CASSANDRA-2358,12501895,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,thobbs,thobbs,20/Mar/11 14:36,16/Apr/19 17:33,22/Mar/23 14:57,01/Apr/11 02:41,0.7.5,,Legacy/Tools,,0,,,,,,"The CLI raises a syntax error when trying to insert negative integers:

{noformat}
[default@Keyspace1] set StandardInteger['key'][-12] = 'val';
Syntax error at position 28: mismatched character '1' expecting '-'
{noformat}",,,,,,,,,,,,,,,,,,,,,,,1800,1800,,0%,1800,1800,,,,,,,,,,,,,,,,,,,,"29/Mar/11 18:51;xedin;CASSANDRA-2358-trunk.patch;https://issues.apache.org/jira/secure/attachment/12474866/CASSANDRA-2358-trunk.patch","20/Mar/11 23:12;xedin;CASSANDRA-2358.patch;https://issues.apache.org/jira/secure/attachment/12474116/CASSANDRA-2358.patch",,,,,,,,,,,,,2.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20578,,,Thu Mar 31 19:56:40 UTC 2011,,,,,,,,,,"0|i0gawv:",93198,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"20/Mar/11 22:37;xedin;I will wait until CASSANDRA-2341 is committed, it brings notation of the positive/negative integers into CLI grammar.;;;","20/Mar/11 22:42;jbellis;We should fix negative ints in 0.7 unless it's a huge pain.  (Counters will stay 0.8 only.);;;","20/Mar/11 22:43;xedin;Gotcha! No, this won't be a pain at all.;;;","24/Mar/11 02:56;jbellis;committed;;;","24/Mar/11 03:32;hudson;Integrated in Cassandra-0.7 #402 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/402/])
    allow negative numbers in the cli
patch by Pavel Yaskevich; reviewed by jbellis for CASSANDRA-2358
;;;","29/Mar/11 05:21;jbellis;merged to trunk but CliTest fails.  Can you fix?;;;","29/Mar/11 05:29;xedin;Can you remove it from trunk for now? I will create a separate version of this patch for the trunk.;;;","29/Mar/11 05:58;jbellis;removed;;;","29/Mar/11 06:01;xedin;Thanks! ;;;","29/Mar/11 18:51;xedin;branch: trunk (latest commit e6c5a28da940a086d0e786f1ad0288c0b0efa27d) ;;;","01/Apr/11 02:41;jbellis;committed trunk patch;;;","01/Apr/11 03:56;hudson;Integrated in Cassandra #822 (See [https://hudson.apache.org/hudson/job/Cassandra/822/])
    add negative number support to cli, trunk version
patch by Pavel Yaskevich for CASSANDRA-2358
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DatacenterShardStrategyTest.testProperties fails,CASSANDRA-1107,12464898,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jbellis,johanoskarsson,johanoskarsson,19/May/10 22:58,16/Apr/19 17:33,22/Mar/23 14:57,19/May/10 23:21,,,,,0,,,,,,"Stacktrace
junit.framework.AssertionFailedError
	at org.apache.cassandra.locator.DatacenterShardStrategyTest.testProperties(DatacenterShardStrategyTest.java:36)

Standard Error
ERROR 12:39:35,968 Could not find end point information for 127.0.0.1, will use default.

http://hudson.zones.apache.org/hudson/job/Cassandra/440/testReport/junit/org.apache.cassandra.locator/DatacenterShardStrategyTest/testProperties/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19995,,,Wed May 19 15:21:01 UTC 2010,,,,,,,,,,"0|i0g31j:",91923,,,,,Critical,,,,,,,,,,,,,,,,,"19/May/10 23:21;jbellis;already fixed in r946192;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Thrift client forwarding the null keys to the servers,CASSANDRA-308,12430972,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,vijay2win@yahoo.com,vijay2win@yahoo.com,21/Jul/09 07:54,16/Apr/19 17:33,22/Mar/23 14:57,03/Sep/09 23:59,0.4,,,,0,,,,,,"Thrift client is suppose to validate the input before it actually sends it to the server but it did not.... 

client logs are like the below: (Java client lib)
org.apache.thrift.TApplicationException: Internal error processing get_slice
        at org.apache.thrift.TApplicationException.read(TApplicationException.java:107)
        at org.apache.cassandra.service.Cassandra$Client.recv_get_slice(Cassandra.java:178)
        at org.apache.cassandra.service.Cassandra$Client.get_slice(Cassandra.java:154)
        at com.webex.dms.datastore.DataStoreRead.readObject(DataStoreRead.java:163)
        at com.webex.dms.repository.ReadDocument.load(ReadDocument.java:87)
        at com.webex.dms.repository.Document.readBasic(Document.java:307)

Server Logs are as below:
DEBUG [pool-1-thread-448] 2009-07-20 09:27:50,831 CassandraServer.java (line 172) get_slice_from
ERROR [pool-1-thread-448] 2009-07-20 09:27:50,837 Cassandra.java (line 844) Internal error processing get_slice
java.lang.NullPointerException
        at org.apache.cassandra.service.ThriftValidation.validateKey(ThriftValidation.java:18)
        at org.apache.cassandra.service.CassandraServer.readColumnFamily(CassandraServer.java:75)
        at org.apache.cassandra.service.CassandraServer.get_slice(CassandraServer.java:181)
        at org.apache.cassandra.service.Cassandra$Processor$get_slice.process(Cassandra.java:838)
        at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:796)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:252)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)

","Centos 3.0, cassendra trunk, JVM 1.6, tomcat 1.6",sbtourist,,,,,,,,,,,,,,,,,,,,,,,,,,,,,THRIFT-562,CASSANDRA-387,,,,,,,,,,,,,,,,"03/Sep/09 05:58;jbellis;308-v2.patch;https://issues.apache.org/jira/secure/attachment/12418442/308-v2.patch","28/Aug/09 05:06;jbellis;308.patch;https://issues.apache.org/jira/secure/attachment/12417933/308.patch",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19628,,,Thu Sep 03 15:59:03 UTC 2009,,,,,,,,,,"0|i0fy53:",91129,,,,,Low,,,,,,,,,,,,,,,,,"14/Aug/09 23:39;jbellis;The real fix is for the server to do the validation; we don't want to rely on the client since there are so many implementations of such widely varying maturity.

I've attached a patch for this to THRIFT-562.;;;","15/Aug/09 20:50;sbtourist;Jonathan,

so should it, in your opinion, be resolved by patching the CassandraServer?;;;","15/Aug/09 21:36;jbellis;No.;;;","15/Aug/09 21:36;jbellis;To clarify: it should be resolved by having the Cassandra.java code generated by Thrift do the checking, not the hand-coded CassandraServer.java.;;;","28/Aug/09 05:06;jbellis;Making ""not optional"" imply ""required"" was apparently too much of a leap for dreiss of Thrift.  (See THRIFT-562, THRIFT-455.)  This patch adds `required` boilerplate to all such fields.;;;","28/Aug/09 23:08;hudson;Integrated in Cassandra #180 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/180/])
    update to thrift trunk and rename jar after the svn revision (806014).  inline our map typedef to work around regression introduced in THRIFT-144.  add slf4j dependencies (required since THRIFT-558).  regenerate thrift structs with new version.
patch by jbellis for , CASSANDRA-387
;;;","29/Aug/09 01:56;urandom;+1;;;","29/Aug/09 03:03;jbellis;committed;;;","29/Aug/09 20:35;hudson;Integrated in Cassandra #181 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/181/])
    add ""required"" to non-optional fields. Thrift sucks.
patch by jbellis; reviewed by Eric Evans for 
;;;","03/Sep/09 05:57;jbellis;still not null checking method parameters;;;","03/Sep/09 05:58;jbellis;v2 adds null-checking to method parameters, as generated by the patch to THRIFT-575.;;;","03/Sep/09 23:25;sammy.yu;+1
;;;","03/Sep/09 23:59;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError in MessagingService.receive for READ_REPAIR verb,CASSANDRA-1493,12473820,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,johanoskarsson,johanoskarsson,johanoskarsson,11/Sep/10 00:11,16/Apr/19 17:33,22/Mar/23 14:57,12/Sep/10 21:58,0.7 beta 2,,,,0,,,,,,"Read repair messages are causing an assertion error in MessagingService. Looks like the enum introduced in CASSANDRA-1465 is missing a verb?

Added two lines of debug output, so lines are a bit off:
DEBUG [pool-1-thread-1] 2010-09-10 15:39:23,555 MessagingService.java (line 373) Verb: READ_REPAIR
DEBUG [pool-1-thread-1] 2010-09-10 15:39:23,555 MessagingService.java (line 374) MessageType: null
ERROR [pool-1-thread-1] 2010-09-10 15:39:23,555 Cassandra.java (line 1744) Internal error processing get
java.lang.AssertionError
        at org.apache.cassandra.net.MessagingService.receive(MessagingService.java:376)
        at org.apache.cassandra.net.MessagingService.sendOneWay(MessagingService.java:285)
        at org.apache.cassandra.service.ReadResponseResolver.maybeScheduleRepairs(ReadResponseResolver.java:163)
        at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:116)
        at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:43)
        at org.apache.cassandra.service.QuorumResponseHandler.get(QuorumResponseHandler.java:89)
        at org.apache.cassandra.service.StorageProxy.strongRead(StorageProxy.java:430)
        at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:266)
        at org.apache.cassandra.thrift.CassandraServer.readColumnFamily(CassandraServer.java:113)
        at org.apache.cassandra.thrift.CassandraServer.get(CassandraServer.java:317)
        at org.apache.cassandra.thrift.Cassandra$Processor$get.process(Cassandra.java:1734)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:1634)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Sep/10 05:53;johanoskarsson;CASSANDRA-1493.patch;https://issues.apache.org/jira/secure/attachment/12454338/CASSANDRA-1493.patch",,,,,,,,,,,,,,1.0,johanoskarsson,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20165,,,Sun Sep 12 13:58:16 UTC 2010,,,,,,,,,,"0|i0g5ef:",92305,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"11/Sep/10 05:53;johanoskarsson;Adding the read repair verb seems to have fixed it.;;;","11/Sep/10 07:48;jbellis;+1;;;","12/Sep/10 21:58;johanoskarsson;Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
describe_keyspace fails on the system table,CASSANDRA-481,12437635,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,brandon.williams,brandon.williams,09/Oct/09 02:43,16/Apr/19 17:33,22/Mar/23 14:57,09/Oct/09 06:31,0.5,,,,0,,,,,,"When the thrift call describe_keyspace is called on the system table, an NPE is raised:

ERROR - Error occurred during processing of message.
java.lang.NullPointerException
        at org.apache.thrift.protocol.TBinaryProtocol.writeString(TBinaryProtocol.java:174)
        at org.apache.cassandra.service.Cassandra$describe_keyspace_result.write(Cassandra.java:11109)
        at org.apache.cassandra.service.Cassandra$Processor$describe_keyspace.process(Cassandra.java:959)
        at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:627)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
","debian lenny amd64, openjdk 6b11-9.1",hammer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Oct/09 03:40;jbellis;481.patch;https://issues.apache.org/jira/secure/attachment/12421653/481.patch","09/Oct/09 02:46;brandon.williams;stub_system_keyspace.patch;https://issues.apache.org/jira/secure/attachment/12421648/stub_system_keyspace.patch","09/Oct/09 02:44;brandon.williams;test_describe_keyspace.patch;https://issues.apache.org/jira/secure/attachment/12421647/test_describe_keyspace.patch",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,628,,,Fri Oct 09 12:35:07 UTC 2009,,,,,,,,,,"0|i0fz73:",91300,,,,,Low,,,,,,,,,,,,,,,,,"09/Oct/09 02:43;brandon.williams;Patch to test describe_keyspace;;;","09/Oct/09 02:46;brandon.williams;Patch that stubs out the system table with fake values to prevent the NPE.  I do not know what the real values should be.;;;","09/Oct/09 03:40;jbellis;this patch incorporates Brandon's test and replaces the undocumented CF attribute aliases with a single human-readable ""comment"" attribute.;;;","09/Oct/09 05:24;euphoria;jbellis's latest patch looks good to me and works + handy feature.  Why the commenting of the assertions? They both make sense and pass for me.;;;","09/Oct/09 05:27;jbellis;oops, assertions should have been left in.;;;","09/Oct/09 05:33;euphoria;k, +1 on commit with assertions;;;","09/Oct/09 06:31;jbellis;committed w/ asserts restored;;;","09/Oct/09 20:35;hudson;Integrated in Cassandra #222 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/222/])
    r/m undocumented term aliases in favor of a single 'comment' field.
patch by jbellis and Brandon Williams; reviewed by Michael Greene for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra doesn't startup on single core boxes.,CASSANDRA-2182,12498911,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,tjake,tjake,tjake,18/Feb/11 00:13,16/Apr/19 17:33,22/Mar/23 14:57,18/Feb/11 00:31,0.7.3,,,,0,,,,,,"I happened to run cassandra in a VM and got the following error, caused by the single core:

ERROR 10:47:30,304 Exception encountered during startup.
java.lang.AssertionError: multi-threaded stages must have at least 2 threads
        at org.apache.cassandra.concurrent.StageManager.multiThreadedStage(StageManager.java:60)
        at org.apache.cassandra.concurrent.StageManager.<clinit>(StageManager.java:53)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:303)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:159)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:175)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)",,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,"18/Feb/11 00:15;tjake;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-2182-rr-pool-needs-at-least-two-threads.txt;https://issues.apache.org/jira/secure/attachment/12471285/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-2182-rr-pool-needs-at-least-two-threads.txt",,,,,,,,,,,,,,1.0,tjake,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20485,,,Thu Feb 17 17:23:07 UTC 2011,,,,,,,,,,"0|i0g9tj:",93021,,,,,Low,,,,,,,,,,,,,,,,,"18/Feb/11 00:18;jbellis;+1 (this was from CASSANDRA-2069 which was committed after 0.7.2);;;","18/Feb/11 01:23;hudson;Integrated in Cassandra-0.7 #287 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/287/])
    read repair stage requires a minimum of 2 threads
patch by tjake for CASSANDRA-2182
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra-cli doesn't use framed transport by default,CASSANDRA-1290,12469491,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jhermes,jhermes,jhermes,17/Jul/10 04:54,16/Apr/19 17:33,22/Mar/23 14:57,17/Jul/10 05:56,0.7 beta 1,,,,0,,,,,,"Spawned by CASSANDRA-475 .
The cli uses non-framed transport, which causes errors on connection that look like:

cli:{noformat}$ bin/cassandra-cli 
Welcome to cassandra CLI.

Type 'help' or '?' for help. Type 'quit' or 'exit' to quit.
[default@unknown] connect 127.0.0.1/9160
Exception retrieving information about the cassandra node, check you have connected to the thrift port.{noformat}

cass:{noformat}ERROR 15:48:12,523 Thrift error occurred during processing of message.
org.apache.thrift.TException: Message length exceeded: 1684370275
	at org.apache.thrift.protocol.TBinaryProtocol.checkReadLength(TBinaryProtocol.java:384)
	at org.apache.thrift.protocol.TBinaryProtocol.readStringBody(TBinaryProtocol.java:350)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:213)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2519)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619){noformat}

Changing it to use framed transport fixes this, and it should be using framed transport by default regardless.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jul/10 05:42;jhermes;TRUNK-1290.txt;https://issues.apache.org/jira/secure/attachment/12449708/TRUNK-1290.txt",,,,,,,,,,,,,,1.0,jhermes,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20061,,,Mon Jul 19 14:45:04 UTC 2010,,,,,,,,,,"0|i0g45r:",92104,,,,,Low,,,,,,,,,,,,,,,,,"17/Jul/10 04:57;jbellis;there is a --framed option

should be switched to --unframed;;;","17/Jul/10 04:58;jhermes;This is a one-line (line 32 to be exact) patch with some minor spatial formatting.;;;","17/Jul/10 05:42;jhermes;Yeah, good idea.
Changed the arg to unframed.;;;","17/Jul/10 05:56;jbellis;committed;;;","19/Jul/10 22:45;hudson;Integrated in Cassandra #494 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/494/])
    make cli framed by default.  patch by Jon Hermes; reviewed by jbellis for CASSANDRA-1290
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception auto-bootstrapping two nodes nodes at the same time,CASSANDRA-1011,12462730,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,gdusbabek,erickt,erickt,22/Apr/10 13:09,16/Apr/19 17:33,22/Mar/23 14:57,14/Jul/10 20:11,0.7 beta 1,,,,0,,,,,,"I've got a small cluster of 3 machines, and after starting the first node (which is the seed), I brought up the other two nodes at the same time. This exception then gets raised on the seed node. Looks like the seed node is assigning the same token to the subnodes at the same time:

ERROR 21:46:49,417 Error in ThreadPoolExecutor
java.lang.RuntimeException: Bootstrap Token collision between /10.0.0.2 and /10.0.0.3 (token Token(bytes[4c617374204d6967726174696f6e])
	at org.apache.cassandra.locator.TokenMetadata.addBootstrapToken(TokenMetadata.java:130)
	at org.apache.cassandra.service.StorageService.handleStateBootstrap(StorageService.java:548)
	at org.apache.cassandra.service.StorageService.onChange(StorageService.java:511)
	at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:705)
	at org.apache.cassandra.gms.Gossiper.applyApplicationStateLocally(Gossiper.java:670)
	at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:624)
	at org.apache.cassandra.gms.Gossiper$GossipDigestAck2VerbHandler.doVerb(Gossiper.java:1016)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:41)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
ERROR 21:46:49,418 Fatal exception in thread Thread[GMFD:1,5,main]
java.lang.RuntimeException: Bootstrap Token collision between /10.0.0.2 and /10.0.0.3 (token Token(bytes[4c617374204d6967726174696f6e])
	at org.apache.cassandra.locator.TokenMetadata.addBootstrapToken(TokenMetadata.java:130)
	at org.apache.cassandra.service.StorageService.handleStateBootstrap(StorageService.java:548)
	at org.apache.cassandra.service.StorageService.onChange(StorageService.java:511)
	at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:705)
	at org.apache.cassandra.gms.Gossiper.applyApplicationStateLocally(Gossiper.java:670)
	at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:624)
	at org.apache.cassandra.gms.Gossiper$GossipDigestAck2VerbHandler.doVerb(Gossiper.java:1016)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:41)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
",,johanoskarsson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Jul/10 02:10;gdusbabek;0001-fail-bootstrap-if-all-nodes-are-bootstrapping.patch;https://issues.apache.org/jira/secure/attachment/12449372/0001-fail-bootstrap-if-all-nodes-are-bootstrapping.patch",,,,,,,,,,,,,,1.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19955,,,Wed Jul 14 13:56:22 UTC 2010,,,,,,,,,,"0|i0g2gf:",91828,,,,,Low,,,,,,,,,,,,,,,,,"25/Jun/10 02:31;gdusbabek;There is still plenty of window for this to happen, but this should narrow it down some.;;;","25/Jun/10 02:44;jbellis;i don't think the window between when the bootstrapping machine chooses a machine to ask for a token, and when it actually does so (sub ms under any scenario i can think of) is large enough to add the extra check in getBalancedToken.  either way the main danger is that some node has already chosen the same token but we haven't heard about it yet over gossip.

it would be worth checking though in getBootstrapSource that if sorted node zero has a bootstrap target already (i.e., _every_ node already has one), then we should fail the bootstrap (and tell the user to specify an initialtoken manually, or wait for one of the current bootstraps to finish).;;;","13/Jul/10 05:12;gdusbabek;> it would be worth checking though in getBootstrapSource that if sorted node zero has a bootstrap target already (i.e., every node already has one), then we should fail the bootstrap (and tell the user to specify an initialtoken manually, or wait for one of the current bootstraps to finish).

I think I'm missing something.  We don't keep track of whether or not node zero has a bootstrap target, do we?  Nothing I can see in TMD indicates this.;;;","13/Jul/10 06:25;jbellis;metadata.pendingRangeChanges = bootstrap targets, more or less;;;","14/Jul/10 11:45;jbellis;+1;;;","14/Jul/10 21:56;hudson;Integrated in Cassandra #491 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/491/])
    fail bootstrap if all nodes are bootstrapping. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1011
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bootstrapping does not work properly using multiple DataFileDirectory,CASSANDRA-716,12445889,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,gdusbabek,david.pan,david.pan,19/Jan/10 14:55,16/Apr/19 17:33,22/Mar/23 14:57,22/Jan/10 07:13,0.5,,,,0,,,,,,"I was adding a new machine A which has 2 DataFileDirectories into the ring. The A will throw exception while bootstrapping.
DEBUG [MESSAGING-SERVICE-POOL:4] 2010-01-19 11:43:32,837 ContentStreamState.java (line 88) Removing stream context /home/store0/data/pic/raw_data-tmp-1-Data.db:209833142
 WARN [MESSAGING-SERVICE-POOL:4] 2010-01-19 11:43:32,837 TcpConnection.java (line 484) Problem reading from socket connected to : java.nio.channels.SocketChannel[connected local
=/10.81.37.65:7000 remote=/10.81.42.26:10418]
 WARN [MESSAGING-SERVICE-POOL:4] 2010-01-19 11:43:32,837 TcpConnection.java (line 485) Exception was generated at : 01/19/2010 11:43:32 on thread MESSAGING-SERVICE-POOL:4
java.io.IOException: rename failed of /home/store0/data/pic/raw_data-1-Filter.db
java.io.IOError: java.io.IOException: rename failed of /home/store0/data/pic/raw_data-1-Filter.db
        at org.apache.cassandra.io.SSTableWriter.rename(SSTableWriter.java:154)
        at org.apache.cassandra.io.SSTableWriter.renameAndOpen(SSTableWriter.java:162)
        at org.apache.cassandra.io.Streaming$StreamCompletionHandler.onStreamCompletion(Streaming.java:284)
        at org.apache.cassandra.net.io.ContentStreamState.handleStreamCompletion(ContentStreamState.java:108)
        at org.apache.cassandra.net.io.ContentStreamState.read(ContentStreamState.java:90)
        at org.apache.cassandra.net.io.TcpReader.read(TcpReader.java:96)
        at org.apache.cassandra.net.TcpConnection$ReadWorkItem.run(TcpConnection.java:445)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.IOException: rename failed of /home/store0/data/pic/raw_data-1-Filter.db
        at org.apache.cassandra.utils.FBUtilities.renameWithConfirm(FBUtilities.java:306)
        at org.apache.cassandra.io.SSTableWriter.rename(SSTableWriter.java:150)
        ... 9 more


I traced the exception and maybe found the reason.
StreamInitiateVerbHandler::doVerb() will create 3 temporary files(index, filter, data) for each ssTable. The name for each file is generated by getNewFileNameFromOldContextAndNames(). This method will generate a file name and a path for each ssTable, but the path is generated with DatabaseDescriptor.getDataFileLocationForTable() which will return different path for ech call when we configure multi-DataFileDirectory. 
eg: the ssTable raw_data-1 may have 3 temporary files : 
/home/store0/data/pic/raw_data-tmp-1-Index.db
/home/store1/data/pic/raw_data-tmp-1-Filter.db
/home/store0/data/pic/raw_data-tmp-1-Data.db

After receiving all data, StreamCompletionHandler::onStreamCompletion() will rename all temporary files and this method think all ssTable files will have the same path as data.db file. 
            if (streamContext.getTargetFile().contains(""-Data.db""))
            {
               ......
                try
                {
                    SSTableReader sstable = SSTableWriter.renameAndOpen(streamContext.getTargetFile());
                    ......
                }
                ......
            }
Then the renameAndOpen() will throw that exception.



","storage-conf.xml:
  <CommitLogDirectory>/home/store1/commitlog</CommitLogDirectory>
  <DataFileDirectories>
      <DataFileDirectory>/home/store0/data</DataFileDirectory>
      <DataFileDirectory>/home/store1/data</DataFileDirectory>
  </DataFileDirectories>
  <CalloutLocation>/home/store1/cassandra/callouts</CalloutLocation>
  <StagingFileDirectory>/home/store1/cassandra/staging</StagingFileDirectory>",david.pan,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,"22/Jan/10 06:08;gdusbabek;0001-rename-DD.getDAtaFileLocationForTable-to-something-m.patch;https://issues.apache.org/jira/secure/attachment/12431070/0001-rename-DD.getDAtaFileLocationForTable-to-something-m.patch","22/Jan/10 06:08;gdusbabek;0002-ensure-all-files-for-an-sstable-are-streamed-to-the-.patch;https://issues.apache.org/jira/secure/attachment/12431071/0002-ensure-all-files-for-an-sstable-are-streamed-to-the-.patch",,,,,,,,,,,,,2.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19832,,,Thu Jan 21 23:13:04 UTC 2010,,,,,,,,,,"0|i0g0mv:",91533,,,,,Normal,,,,,,,,,,,,,,,,,"19/Jan/10 15:04;david.pan;sorry, my version is 0.5-rc1.;;;","21/Jan/10 21:36;jbellis;(CASSANDRA-730 has another example.);;;","22/Jan/10 06:08;gdusbabek;ensures that all files for a single sstable end up in the same directory.  The same round-robin approach is used in the case of multiple sstables/keyspaces.;;;","22/Jan/10 06:14;jbellis;+1;;;","22/Jan/10 07:13;gdusbabek;r901902 (0.5), r901915 (trunk);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hinted handoff null pointer exception,CASSANDRA-585,12441733,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,dispalt,dispalt,26/Nov/09 10:37,16/Apr/19 17:33,22/Mar/23 14:57,26/Nov/09 22:58,,,,,0,,,,,,"During the course of running the cluster I have now run into this error

2009-11-26_02:28:08.99076 ERROR - Error in executor futuretask
2009-11-26_02:28:08.99076 java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.NullPointerException
2009-11-26_02:28:08.99076       at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
2009-11-26_02:28:08.99076       at java.util.concurrent.FutureTask.get(FutureTask.java:111)
2009-11-26_02:28:08.99076       at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:112)
2009-11-26_02:28:08.99076       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1118)
2009-11-26_02:28:08.99076       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
2009-11-26_02:28:08.99076       at java.lang.Thread.run(Thread.java:636)
2009-11-26_02:28:08.99076 Caused by: java.lang.RuntimeException: java.lang.NullPointerException
2009-11-26_02:28:08.99076       at org.apache.cassandra.db.HintedHandOffManager$3.run(HintedHandOffManager.java:281)
2009-11-26_02:28:08.99076       at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
2009-11-26_02:28:08.99076       at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
2009-11-26_02:28:08.99076       at java.util.concurrent.FutureTask.run(FutureTask.java:166)
2009-11-26_02:28:08.99076       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
2009-11-26_02:28:08.99076       ... 2 more
2009-11-26_02:28:08.99076 Caused by: java.lang.NullPointerException
2009-11-26_02:28:08.99076       at org.apache.cassandra.db.RowMutation.add(RowMutation.java:119)
2009-11-26_02:28:08.99076       at org.apache.cassandra.db.HintedHandOffManager.sendMessage(HintedHandOffManager.java:115)
2009-11-26_02:28:08.99076       at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:219)
2009-11-26_02:28:08.99076       at org.apache.cassandra.db.HintedHandOffManager.access$200(HintedHandOffManager.java:75)
2009-11-26_02:28:08.99076       at org.apache.cassandra.db.HintedHandOffManager$3.run(HintedHandOffManager.java:277)
2009-11-26_02:28:08.99076       ... 6 more

","r884373
3 node cluster",hammer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Nov/09 11:48;jbellis;585.patch;https://issues.apache.org/jira/secure/attachment/12426181/585.patch",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19766,,,Thu Nov 26 14:58:39 UTC 2009,,,,,,,,,,"0|i0fzu7:",91404,,,,,Normal,,,,,,,,,,,,,,,,,"26/Nov/09 11:48;jbellis;this should fix it;;;","26/Nov/09 13:47;dispalt;seems to work well, no exceptions yet...;;;","26/Nov/09 22:58;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
incompatibility w/ 0.7 schemas,CASSANDRA-2450,12503968,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,12/Apr/11 03:32,16/Apr/19 17:33,22/Mar/23 14:57,15/Apr/11 22:38,0.8 beta 1,,,,0,,,,,,"If you create a SimpleStrategy keyspace under 0.7, then switch to 0.8, you will get this error on startup:

{noformat}
ERROR 14:31:41,725 Exception encountered during startup.
java.lang.RuntimeException: org.apache.cassandra.config.ConfigurationException: SimpleStrategy requires a replication_factor strategy option.
	at org.apache.cassandra.db.Table.<init>(Table.java:277)
	at org.apache.cassandra.db.Table.open(Table.java:109)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:160)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:314)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)
Caused by: org.apache.cassandra.config.ConfigurationException: SimpleStrategy requires a replication_factor strategy option.
	at org.apache.cassandra.locator.SimpleStrategy.validateOptions(SimpleStrategy.java:75)
	at org.apache.cassandra.locator.AbstractReplicationStrategy.createReplicationStrategy(AbstractReplicationStrategy.java:262)
	at org.apache.cassandra.db.Table.createReplicationStrategy(Table.java:327)
	at org.apache.cassandra.db.Table.<init>(Table.java:273)
	... 4 more
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Apr/11 21:58;jbellis;2450.txt;https://issues.apache.org/jira/secure/attachment/12476455/2450.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20631,,,Fri Apr 15 14:38:15 UTC 2011,,,,,,,,,,"0|i0gbfz:",93284,,gdusbabek,,gdusbabek,Normal,,,,,,,,,,,,,,,,,"15/Apr/11 21:58;jbellis;patch add back replication_factor to the avro class as union { int, null } so we can upgrade old-style schema information seamlessly in KSMetaData.;;;","15/Apr/11 22:24;gdusbabek;+1;;;","15/Apr/11 22:38;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Index predicate values used in get_indexed_slice() are not validated,CASSANDRA-2328,12501429,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,amorton,amorton,amorton,15/Mar/11 14:53,16/Apr/19 17:33,22/Mar/23 14:57,17/Mar/11 10:55,0.7.5,,,,0,,,,,,"If a client makes a get_indexed_slice() request with malformed predicate values we get an assertion failing rather than InvalidRequestException.

{noformat}
ERROR 14:47:56,842 Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.IndexOutOfBoundsException: 6
        at org.apache.cassandra.service.IndexScanVerbHandler.doVerb(IndexScanVer
bHandler.java:51)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.
java:72)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExec
utor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor
.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.IndexOutOfBoundsException: 6
        at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:121)
        at org.apache.cassandra.db.marshal.TimeUUIDType.compareTimestampBytes(Ti
meUUIDType.java:56)
        at org.apache.cassandra.db.marshal.TimeUUIDType.compare(TimeUUIDType.jav
a:45)
        at org.apache.cassandra.db.marshal.TimeUUIDType.compare(TimeUUIDType.jav
a:29)
        at org.apache.cassandra.db.ColumnFamilyStore.satisfies(ColumnFamilyStore
.java:1608)
        at org.apache.cassandra.db.ColumnFamilyStore.scan(ColumnFamilyStore.java
:1552)
        at org.apache.cassandra.service.IndexScanVerbHandler.doVerb(IndexScanVer
bHandler.java:42)
        ... 4 more
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Mar/11 08:12;amorton;0001-validate-index-predicate-name-and-value.patch;https://issues.apache.org/jira/secure/attachment/12473753/0001-validate-index-predicate-name-and-value.patch",,,,,,,,,,,,,,1.0,amorton,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20565,,,Fri Mar 18 04:06:55 UTC 2011,,,,,,,,,,"0|i0gaqf:",93169,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"16/Mar/11 08:12;amorton;Attached patch validates the expression column name and value for get_indexed_slice(). 

Also adds a regression test in the (thrift) system tests.;;;","17/Mar/11 10:55;jbellis;committed, thanks!;;;","18/Mar/11 12:06;hudson;Integrated in Cassandra-0.7 #391 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/391/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in get_slice quorum read,CASSANDRA-1883,12493608,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,kmueller,kmueller,19/Dec/10 05:16,16/Apr/19 17:33,22/Mar/23 14:57,22/Dec/10 05:37,0.6.9,0.7.0 rc 3,,,0,,,,,,"Getting this NPE as of the 2010-12-17 0.7 trunk.  Some data may be corrupt somewhere on a node.  It could be a null key somewhere.

ERROR [pool-1-thread-28] 2010-12-18 12:53:20,411 Cassandra.java (line 2707) Internal error processing get_slice
java.lang.NullPointerException
        at org.apache.cassandra.service.DigestMismatchException.<init>(DigestMismatchException.java:30)
        at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:92)
        at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:43)
        at org.apache.cassandra.service.QuorumResponseHandler.get(QuorumResponseHandler.java:91)
        at org.apache.cassandra.service.StorageProxy.strongRead(StorageProxy.java:362)
        at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:229)
        at org.apache.cassandra.thrift.CassandraServer.readColumnFamily(CassandraServer.java:128)
        at org.apache.cassandra.thrift.CassandraServer.getSlice(CassandraServer.java:225)
        at org.apache.cassandra.thrift.CassandraServer.multigetSliceInternal(CassandraServer.java:301)
        at org.apache.cassandra.thrift.CassandraServer.get_slice(CassandraServer.java:263)
        at org.apache.cassandra.thrift.Cassandra$Processor$get_slice.process(Cassandra.java:2699)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2555)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)",Linux Fedora 12 x86_64,mdennis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Dec/10 13:23;jbellis;1883.txt;https://issues.apache.org/jira/secure/attachment/12466603/1883.txt","19/Dec/10 10:09;kmueller;digestmismatch-debug.txt;https://issues.apache.org/jira/secure/attachment/12466556/digestmismatch-debug.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19360,,,Tue Dec 21 21:37:43 UTC 2010,,,,,,,,,,"0|i0g7zr:",92725,,tjake,,tjake,Low,,,,,,,,,,,,,,,,,"19/Dec/10 05:26;kmueller;Additional information: one of the SSD raid0s went bad recently.  This may have produced weird data for one cassandra node.   ;;;","19/Dec/10 10:09;kmueller;this is a debug output from a node with this NPE happening around the same time.  If you need more from the log, I have the rest of it available;;;","19/Dec/10 11:50;jbellis;Does this reproduce whenever you query a certain key?;;;","20/Dec/10 11:54;brandon.williams;Note that there was data here inserted from RC2, and then CASSANDRA-1847 was encountered causing the upgrade to trunk, so this may just be fallout from previous corruption.;;;","20/Dec/10 13:23;jbellis;patch to fix NPE when two different digests arrive before the data read does;;;","20/Dec/10 13:59;mdennis;+1;;;","21/Dec/10 01:33;tjake;+1 this was a regression from CASSANDRA-1830;;;","22/Dec/10 05:37;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
batch_mutate Deletion with column family type mismatch causes RuntimeException,CASSANDRA-1139,12465613,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,mdennis,tholzer,tholzer,28/May/10 10:29,16/Apr/19 17:33,22/Mar/23 14:57,04/Jun/10 10:58,0.6.3,,,,0,batch_mutate,Deletion,,,,"When specifying a super column family name inside a Deletion and a standard column family name in the mutations dictionary, we get a RuntimeException in the server and a TimedOutException on the client:

{noformat}
ERROR 14:22:29,757 Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.UnsupportedOperationException: This operation is not supported for Super Columns.
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.UnsupportedOperationException: This operation is not supported for Super Columns.
        at org.apache.cassandra.db.SuperColumn.timestamp(SuperColumn.java:137)
        at org.apache.cassandra.db.ColumnSerializer.serialize(ColumnSerializer.java:65)
        at org.apache.cassandra.db.ColumnSerializer.serialize(ColumnSerializer.java:29)
        at org.apache.cassandra.db.ColumnFamilySerializer.serializeForSSTable(ColumnFamilySerializer.java:87)
        at org.apache.cassandra.db.ColumnFamilySerializer.serialize(ColumnFamilySerializer.java:73)
        at org.apache.cassandra.db.RowMutationSerializer.freezeTheMaps(RowMutation.java:337)
        at org.apache.cassandra.db.RowMutationSerializer.serialize(RowMutation.java:349)
        at org.apache.cassandra.db.RowMutationSerializer.serialize(RowMutation.java:322)
        at org.apache.cassandra.db.RowMutation.getSerializedBuffer(RowMutation.java:275)
        at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:200)
        at org.apache.cassandra.service.StorageProxy$3.runMayThrow(StorageProxy.java:310)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
ERROR 14:22:29,757 Fatal exception in thread Thread[ROW-MUTATION-STAGE:39,5,main]
{noformat}

{noformat}
Traceback (most recent call last):
  File ""./test.py"", line 15, in <module>
    client.batch_mutate(""Keyspace1"", mutations, ConsistencyLevel.QUORUM)
  File ""cassandra/Cassandra.py"", line 771, in batch_mutate
    self.recv_batch_mutate()
  File ""cassandra/Cassandra.py"", line 798, in recv_batch_mutate
    raise result.te
cassandra.ttypes.TimedOutException: TimedOutException()
{noformat}

To reproduce:

{noformat}
from thrift.transport.TSocket import TSocket
from thrift.protocol.TBinaryProtocol import TBinaryProtocol
from cassandra.Cassandra import Client
from cassandra.ttypes import Deletion, Mutation, ConsistencyLevel

if __name__ == ""__main__"":
    tsocket = TSocket('localhost', 9160)
    tsocket.open()
    tprotocol = TBinaryProtocol(tsocket)
    client = Client(tprotocol)
    deletion = Deletion(1, 'supercolumn', None)
    mutation = Mutation(deletion=deletion)
    mutations = { 'key' : { 'Standard1' : [ mutation ] } }
    client.batch_mutate(""Keyspace1"", mutations, ConsistencyLevel.QUORUM)
{noformat}

",Linux & Python 2.6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Jun/10 02:58;mdennis;CASSANDRA-0_6-1139.patch;https://issues.apache.org/jira/secure/attachment/12446043/CASSANDRA-0_6-1139.patch","02/Jun/10 02:40;mdennis;CASSANDRA-1139.patch;https://issues.apache.org/jira/secure/attachment/12446039/CASSANDRA-1139.patch",,,,,,,,,,,,,2.0,mdennis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20005,,,Fri Jun 04 02:58:27 UTC 2010,,,,,,,,,,"0|i0g38f:",91954,,,,,Low,,,,,,,,,,,,,,,,,"29/May/10 01:13;jbellis;Looks like converting the given test to a system test (in test_bad_calls) and updating ThriftValidation to catch this is fairly straightforward.;;;","02/Jun/10 02:41;mdennis;patch against trunk r950099;;;","02/Jun/10 02:58;mdennis;0.6 patch against cassandra-0.6 r950198;;;","04/Jun/10 10:58;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointer during a get_range_slice,CASSANDRA-623,12443008,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,dispalt,dispalt,11/Dec/09 02:03,16/Apr/19 17:33,22/Mar/23 14:57,11/Dec/09 11:58,0.5,,,,0,,,,,,"I keep running into this exception on a get_range_slice command.  It seems to be a regression because it didn't happen in r886010 but does happen in r888913

2009-12-10_17:17:22.56200 ERROR - Error in ThreadPoolExecutor
2009-12-10_17:17:22.56200 java.lang.RuntimeException: java.lang.NullPointerException
2009-12-10_17:17:22.56200       at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:53)
2009-12-10_17:17:22.56200       at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)
2009-12-10_17:17:22.56200       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
2009-12-10_17:17:22.56200       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
2009-12-10_17:17:22.56200       at java.lang.Thread.run(Thread.java:636)
2009-12-10_17:17:22.56200 Caused by: java.lang.NullPointerException
2009-12-10_17:17:22.56200       at org.apache.cassandra.dht.OrderPreservingPartitioner$1.compare(OrderPreservingPartitioner.java:45)
2009-12-10_17:17:22.56200       at org.apache.cassandra.dht.OrderPreservingPartitioner$1.compare(OrderPreservingPartitioner.java:42)
2009-12-10_17:17:22.56200       at org.apache.cassandra.db.ColumnFamilyStore.getKeyRangeRaw(ColumnFamilyStore.java:1465)
2009-12-10_17:17:22.56200       at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1506)
2009-12-10_17:17:22.56200       at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:39)
2009-12-10_17:17:22.56200       ... 4 more
2009-12-10_17:17:22.57200 ERROR - Fatal exception in thread Thread[ROW-READ-STAGE:96,5,main]
2009-12-10_17:17:22.57200 java.lang.RuntimeException: java.lang.NullPointerException
2009-12-10_17:17:22.57200       at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:53)
2009-12-10_17:17:22.57200       at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)
2009-12-10_17:17:22.57200       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
2009-12-10_17:17:22.57200       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
2009-12-10_17:17:22.57200       at java.lang.Thread.run(Thread.java:636)
2009-12-10_17:17:22.57200 Caused by: java.lang.NullPointerException
2009-12-10_17:17:22.57200       at org.apache.cassandra.dht.OrderPreservingPartitioner$1.compare(OrderPreservingPartitioner.java:45)
2009-12-10_17:17:22.57200       at org.apache.cassandra.dht.OrderPreservingPartitioner$1.compare(OrderPreservingPartitioner.java:42)
2009-12-10_17:17:22.57200       at org.apache.cassandra.db.ColumnFamilyStore.getKeyRangeRaw(ColumnFamilyStore.java:1465)
2009-12-10_17:17:22.57200       at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1506)
2009-12-10_17:17:22.57200       at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:39)
2009-12-10_17:17:22.57200       ... 4 more",r888913  RF=2 3 node cluster,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Dec/09 06:25;jbellis;623.patch;https://issues.apache.org/jira/secure/attachment/12427645/623.patch",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19785,,,Fri Dec 11 03:58:47 UTC 2009,,,,,,,,,,"0|i0g02n:",91442,,,,,Normal,,,,,,,,,,,,,,,,,"11/Dec/09 06:39;stuhood;+1
Looks good to me.;;;","11/Dec/09 08:57;dispalt;Works great.  Thank you Jonathan!;;;","11/Dec/09 11:58;jbellis;committed to 0.5 and trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
can't write with consistency level of one after some nodes fail,CASSANDRA-524,12439463,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,edmond,edmond,30/Oct/09 06:00,16/Apr/19 17:33,22/Mar/23 14:57,31/Oct/09 02:27,0.5,,,,0,,,,,,"Start a 3 node cluster with a replication factor of 2.  Then take down two nodes.

If I write with a consistency level of ONE on any key, I get an InvalidRequestException:

ERROR [pool-1-thread-45] 2009-10-29 21:27:10,120 StorageProxy.java
(line 183) error writing key 1
InvalidRequestException(why:Cannot block for less than one replica)
       at org.apache.cassandra.service.QuorumResponseHandler.<init>(QuorumResponseHandler.java:52)
       at org.apache.cassandra.locator.AbstractReplicationStrategy.getResponseHandler(AbstractReplicationStrategy.java:64)
       at org.apache.cassandra.service.StorageService.getResponseHandler(StorageService.java:869)
       at org.apache.cassandra.service.StorageProxy.insertBlocking(StorageProxy.java:162)
       at org.apache.cassandra.service.CassandraServer.doInsert(CassandraServer.java:473)
       at org.apache.cassandra.service.CassandraServer.insert(CassandraServer.java:424)
       at org.apache.cassandra.service.Cassandra$Processor$insert.process(Cassandra.java:819)
       at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:624)
       at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:619)

Oddly, a write with a consistency level of QUORUM succeeds for certain
keys (but fails with others) even though I only have one live node.",trunk@r830776,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Oct/09 11:13;jbellis;524.patch;https://issues.apache.org/jira/secure/attachment/12423654/524.patch",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19737,,,Sat Oct 31 12:34:32 UTC 2009,,,,,,,,,,"0|i0fzgn:",91343,,,,,Normal,,,,,,,,,,,,,,,,,"31/Oct/09 01:43;edmond;Patched and verified the fix.;;;","31/Oct/09 02:27;jbellis;committed;;;","31/Oct/09 20:34;hudson;Integrated in Cassandra #244 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/244/])
    fix hinted handoff map computation
patch by jbellis; tested by Edmond Lau for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Illegal file mode when saving caches,CASSANDRA-2131,12497914,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,amorton,amorton,amorton,08/Feb/11 06:01,16/Apr/19 17:33,22/Mar/23 14:57,08/Feb/11 07:29,0.7.1,,,,0,,,,,,"The following error is logged when trying to save caches


DEBUG [CompactionExecutor:1] 2011-02-08 07:30:03,647 CacheWriter.java (line 45) Saving /var/lib/cassandra/saved_caches/Keyspace1-ascii-KeyCache
ERROR [CompactionExecutor:1] 2011-02-08 07:30:03,725 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.RuntimeException: java.lang.IllegalArgumentException: Illegal mode ""w"" must be one of ""r"", ""rw"", ""rws"", or ""rwd""
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
Caused by: java.lang.IllegalArgumentException: Illegal mode ""w"" must be one of ""r"", ""rw"", ""rws"", or ""rwd""
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:197)
	at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:116)
	at org.apache.cassandra.io.sstable.CacheWriter.saveCache(CacheWriter.java:48)
	at org.apache.cassandra.db.CompactionManager$9.runMayThrow(CompactionManager.java:746)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 6 more
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,amorton,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20458,,,Mon Feb 07 23:29:36 UTC 2011,,,,,,,,,,"0|i0g9hz:",92969,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"08/Feb/11 07:29;jbellis;fixed in r1068220;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException in consistency manager after a failed node rejoins,CASSANDRA-124,12424315,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jbellis,markr,markr,01/May/09 00:32,16/Apr/19 17:33,22/Mar/23 14:57,06/May/09 04:07,0.3,,,,0,,,,,,"ERROR [CONSISTENCY-MANAGER:2] 2009-04-30 18:22:38,946 DebuggableThreadPoolExecutor.java (line 89) Error in ThreadPoolExecutor
java.util.concurrent.ExecutionException: java.lang.NullPointerException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:65)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.service.ConsistencyManager.run(ConsistencyManager.java:168)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        ... 2 more

Plus other similar ones.

Config:

    <ReplicationFactor>2</ReplicationFactor>
    <Tables>
        <Table Name=""Messages"">
            <ColumnFamily ColumnSort=""Name"" Name=""base""/>
            <ColumnFamily ColumnSort=""Name"" Name=""extra""/>
            <ColumnFamily ColumnSort=""Time"" Name=""StandardByTime1""/>
            <ColumnFamily ColumnSort=""Time"" Name=""StandardByTime2""/>
            <ColumnFamily ColumnType=""Super"" ColumnSort=""Name"" Name=""Super1""/>
            <ColumnFamily ColumnType=""Super"" ColumnSort=""Name"" Name=""Super2""/>
        </Table>
    </Tables>


I inserted some data using insert method on another node while one node had failed (of three), then brought the failed node back","Centos 5.0 java version ""1.6.0_13""
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19557,,,Tue May 05 20:07:55 UTC 2009,,,,,,,,,,"0|i0fx0v:",90948,,,,,Critical,,,,,,,,,,,,,,,,,"03/May/09 00:26;nk11;Shouldn't ConsistencyManager() constructor contain the following line?
this.replicas_ = replicas_;;;;","04/May/09 22:49;jbellis;FWIW this is not a regression, it's only showing up in the logs now after I added a change that logs exceptions from our executors instead of ignoring them.;;;","04/May/09 22:52;jbellis;Oops, I take it back.  It's a regression from CASSANDRA-95.  nk11 is right, the constructor got broken.;;;","05/May/09 21:43;hudson;Integrated in Cassandra #59 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/59/])
    do not leave variables uninitialized in ConsistencyManager constructor.  fixes regression from #95.  patch by jbellis for 
;;;","06/May/09 04:07;jbellis;looks fixed in my testing;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
gossip throws IllegalStateException,CASSANDRA-1343,12470543,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,gdusbabek,gdusbabek,gdusbabek,31/Jul/10 03:34,16/Apr/19 17:33,22/Mar/23 14:57,04/Aug/10 02:37,0.7 beta 1,,,,0,,,,,,"when starting a second node, gossip throws IllegalStateException when KS with RF>1 defined on an existing 1-node cluster.

we should be able to define keyspaces on a cluster of any size.",,johanoskarsson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-1191,,CASSANDRA-1428,,,,,,"04/Aug/10 02:23;gdusbabek;0001-complain-if-there-aren-t-enough-nodes-to-support-req.patch;https://issues.apache.org/jira/secure/attachment/12451143/0001-complain-if-there-aren-t-enough-nodes-to-support-req.patch",,,,,,,,,,,,,,1.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20090,,,Fri Sep 03 23:28:09 UTC 2010,,,,,,,,,,"0|i0g4h3:",92155,,,,,Low,,,,,,,,,,,,,,,,,"31/Jul/10 03:43;gdusbabek;/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/bin/java -agentlib:jdwp=transport=dt_socket,address=127.0.0.1:54219,suspend=y,server=n -ea -Xms128M -Xmx1G -XX:TargetSurvivorRatio=90 -XX:+AggressiveOpts -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:+HeapDumpOnOutOfMemoryError -XX:SurvivorRatio=128 -XX:MaxTenuringThreshold=0 -Dcom.sun.management.jmxremote.port=8081 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Dcassandra -Dcassandra-foreground=yes -Dlog4j.configuration=log4j-server.properties -Dmx4jport=9081 -Dfile.encoding=MacRoman -classpath /System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/deploy.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/dt.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/javaws.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/jce.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/management-agent.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/plugin.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/sa-jdi.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/../Classes/alt-rt.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/../Classes/charsets.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/../Classes/classes.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/../Classes/dt.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/../Classes/jce.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/../Classes/jconsole.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/../Classes/jsse.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/../Classes/laf.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/../Classes/management-agent.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/../Classes/ui.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/ext/apple_provider.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/ext/dnsns.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/ext/localedata.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/ext/sunjce_provider.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/ext/sunpkcs11.jar:/Users/gary.dusbabek/codes/apache/git-trunk/out/production/conf1:/Users/gary.dusbabek/codes/apache/git-trunk/out/production/core:/Users/gary.dusbabek/codes/apache/git-trunk/lib/jackson-mapper-asl-1.4.0.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/log4j-1.2.14.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/antlr-3.1.3.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/clhm-production.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/commons-cli-1.1.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/jetty-6.1.21.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/jackson-core-asl-1.4.0.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/slf4j-log4j12-1.5.8.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/hadoop-core-0.20.1.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/jug-2.0.0.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/slf4j-api-1.5.8.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/jetty-util-6.1.21.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/commons-collections-3.2.1.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/jline-0.9.94.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/servlet-api-2.5-20081211.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/high-scale-lib.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/commons-codec-1.2.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/commons-lang-2.4.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/libthrift-r959516.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/json-simple-1.1.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/snakeyaml-1.6.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/avro-1.3.3~cust2.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/guava-r05.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/avro-1.3.3-sources~cust1.jar:/Users/gary.dusbabek/codes/apache/git-trunk/build/lib/jars/junit-4.6.jar:/Users/gary.dusbabek/codes/apache/git-trunk/out/production/thrift:/Users/gary.dusbabek/codes/apache/git-trunk/out/production/avro:/Users/gary.dusbabek/codes/apache/git-trunk/out/production/ext:/Applications/IntelliJ IDEA 9.0.1.app/lib/idea_rt.jar org.apache.cassandra.thrift.CassandraDaemon
Connected to the target VM, address: '127.0.0.1:54219', transport: 'socket'
 INFO 13:42:52,352 [main] Loading settings from /Users/gary.dusbabek/codes/apache/git-trunk/out/production/conf1/cassandra.yaml
DEBUG 13:42:52,613 [main] Syncing log with a period of 10000
 INFO 13:42:52,613 [main] DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
DEBUG 13:42:52,628 [main] setting auto_bootstrap to true
DEBUG 13:42:52,768 [main] Starting CFS Statistics
DEBUG 13:42:52,786 [main] Starting CFS Schema
DEBUG 13:42:52,788 [main] Starting CFS Migrations
DEBUG 13:42:52,800 [main] Starting CFS LocationInfo
DEBUG 13:42:52,844 [main] Starting CFS HintsColumnFamily
 INFO 13:42:52,891 [main] Couldn't detect any schema definitions in local storage.
 INFO 13:42:52,891 [main] Found table data in data directories. Consider using JMX to call org.apache.cassandra.service.StorageService.loadSchemaFromYaml().
 INFO 13:42:52,900 [main] Replaying /Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515326767.log
DEBUG 13:42:52,902 [main] Replaying /Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515326767.log starting at 276
DEBUG 13:42:52,903 [main] Reading mutation at 276
DEBUG 13:42:52,910 [main] replaying mutation for system.[B@5f9299f5: {ColumnFamily(LocationInfo [B:false:1@1280515327074,])}
 INFO 13:42:52,945 [main] Finished reading /Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515326767.log
DEBUG 13:42:52,946 [main] Finished waiting on mutations from recovery
 INFO 13:42:52,947 [main] Creating new commitlog segment /Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515372947.log
 INFO 13:42:52,953 [main] switching in a fresh Memtable for LocationInfo at CommitLogContext(file='/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515372947.log', position=0)
 INFO 13:42:52,966 [main] Enqueuing flush of Memtable-LocationInfo@828432489(17 bytes, 1 operations)
 INFO 13:42:52,968 [FLUSH-WRITER-POOL:1] Writing Memtable-LocationInfo@828432489(17 bytes, 1 operations)
 INFO 13:42:53,050 [FLUSH-WRITER-POOL:1] Completed flushing /Users/gary.dusbabek/cass-configs/trunk/node1/data/system/LocationInfo-e-2-Data.db
DEBUG 13:42:53,095 [CompactionExecutor:1] Checking to see if compaction of LocationInfo would be useful
DEBUG 13:42:53,096 [MEMTABLE-POST-FLUSHER:1] Discarding 1000
DEBUG 13:42:53,098 [COMMIT-LOG-WRITER] discard completed log segments for CommitLogContext(file='/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515372947.log', position=0), column family 1000.
DEBUG 13:42:53,098 [COMMIT-LOG-WRITER] Marking replay position 0 on commit log CommitLogSegment(/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515372947.log)
 INFO 13:42:53,100 [main] Recovery complete
DEBUG 13:42:53,101 [main] Deleting CommitLog-1280515326767.log
 INFO 13:42:53,102 [main] Log replay complete
DEBUG 13:42:53,116 [CompactionExecutor:1] Estimating compactions for HintsColumnFamily
DEBUG 13:42:53,117 [CompactionExecutor:1] Estimating compactions for LocationInfo
DEBUG 13:42:53,117 [CompactionExecutor:1] Estimating compactions for Schema
DEBUG 13:42:53,117 [CompactionExecutor:1] Estimating compactions for Migrations
DEBUG 13:42:53,117 [CompactionExecutor:1] Estimating compactions for Statistics
DEBUG 13:42:53,117 [CompactionExecutor:1] Checking to see if compaction of HintsColumnFamily would be useful
 INFO 13:42:53,117 [main] Cassandra version: 0.7.0-SNAPSHOT
 INFO 13:42:53,117 [main] Thrift API version: 9.0.0
DEBUG 13:42:53,117 [CompactionExecutor:1] Checking to see if compaction of LocationInfo would be useful
DEBUG 13:42:53,118 [CompactionExecutor:1] Checking to see if compaction of Schema would be useful
DEBUG 13:42:53,118 [CompactionExecutor:1] Checking to see if compaction of Migrations would be useful
DEBUG 13:42:53,118 [CompactionExecutor:1] Checking to see if compaction of Statistics would be useful
 INFO 13:42:53,118 [main] Saved Token found: 127492708246848026497487109173721015738
 INFO 13:42:53,119 [main] Saved ClusterName found: Test Cluster
 INFO 13:42:53,119 [main] Saved partitioner not found. Using org.apache.cassandra.dht.RandomPartitioner
 INFO 13:42:53,120 [main] switching in a fresh Memtable for LocationInfo at CommitLogContext(file='/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515372947.log', position=276)
 INFO 13:42:53,120 [main] Enqueuing flush of Memtable-LocationInfo@1780804346(95 bytes, 2 operations)
 INFO 13:42:53,120 [FLUSH-WRITER-POOL:1] Writing Memtable-LocationInfo@1780804346(95 bytes, 2 operations)
 INFO 13:42:53,282 [FLUSH-WRITER-POOL:1] Completed flushing /Users/gary.dusbabek/cass-configs/trunk/node1/data/system/LocationInfo-e-3-Data.db
DEBUG 13:42:53,283 [MEMTABLE-POST-FLUSHER:1] Discarding 1000
DEBUG 13:42:53,306 [COMMIT-LOG-WRITER] discard completed log segments for CommitLogContext(file='/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515372947.log', position=276), column family 1000.
DEBUG 13:42:53,306 [COMMIT-LOG-WRITER] Marking replay position 276 on commit log CommitLogSegment(/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515372947.log)
DEBUG 13:42:53,307 [CompactionExecutor:1] Checking to see if compaction of LocationInfo would be useful
 INFO 13:42:53,335 [main] Starting up server gossip
DEBUG 13:42:53,381 [main] clearing cached endpoints
DEBUG 13:42:53,590 [main] Will try to load mx4j now, if it's in the classpath
 INFO 13:42:53,591 [main] Will not load MX4J, mx4j-tools.jar is not in the classpath
DEBUG 13:42:54,344 [GC inspection] GC for ParNew: 13 ms, 224224 reclaimed leaving 92832088 used; max is 1211826176
DEBUG 13:42:54,345 [GC inspection] GC for ConcurrentMarkSweep: 71 ms, 67203872 reclaimed leaving 25628216 used; max is 1211826176
DEBUG 13:42:55,373 [Timer-1] Disseminating load info ...
DEBUG 13:43:20,282 [MIGRATION-STAGE:1] Applying migration 5300795b-9c0a-11df-8af1-e700f669bcfc
 INFO 13:43:20,284 [MIGRATION-STAGE:1] switching in a fresh Memtable for Migrations at CommitLogContext(file='/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515372947.log', position=11790)
 INFO 13:43:20,284 [MIGRATION-STAGE:1] Enqueuing flush of Memtable-Migrations@19614086(5585 bytes, 1 operations)
 INFO 13:43:20,284 [FLUSH-WRITER-POOL:1] Writing Memtable-Migrations@19614086(5585 bytes, 1 operations)
 INFO 13:43:20,284 [MIGRATION-STAGE:1] switching in a fresh Memtable for Schema at CommitLogContext(file='/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515372947.log', position=11790)
 INFO 13:43:20,285 [MIGRATION-STAGE:1] Enqueuing flush of Memtable-Schema@1088945411(2768 bytes, 3 operations)
DEBUG 13:43:20,419 [GC inspection] GC for ParNew: 13 ms, 147440 reclaimed leaving 105343544 used; max is 1211826176
DEBUG 13:43:20,427 [GC inspection] GC for ConcurrentMarkSweep: 72 ms, 75573104 reclaimed leaving 29770440 used; max is 1211826176
 INFO 13:43:20,432 [FLUSH-WRITER-POOL:1] Completed flushing /Users/gary.dusbabek/cass-configs/trunk/node1/data/system/Migrations-e-1-Data.db
DEBUG 13:43:20,433 [CompactionExecutor:1] Checking to see if compaction of Migrations would be useful
 INFO 13:43:20,433 [FLUSH-WRITER-POOL:1] Writing Memtable-Schema@1088945411(2768 bytes, 3 operations)
DEBUG 13:43:20,433 [MEMTABLE-POST-FLUSHER:1] Discarding 1002
DEBUG 13:43:20,433 [COMMIT-LOG-WRITER] discard completed log segments for CommitLogContext(file='/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515372947.log', position=11790), column family 1002.
DEBUG 13:43:20,433 [COMMIT-LOG-WRITER] Marking replay position 11790 on commit log CommitLogSegment(/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515372947.log)
 INFO 13:43:20,595 [FLUSH-WRITER-POOL:1] Completed flushing /Users/gary.dusbabek/cass-configs/trunk/node1/data/system/Schema-e-1-Data.db
DEBUG 13:43:20,602 [MEMTABLE-POST-FLUSHER:1] Discarding 1003
DEBUG 13:43:20,602 [COMMIT-LOG-WRITER] discard completed log segments for CommitLogContext(file='/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515372947.log', position=11790), column family 1003.
DEBUG 13:43:20,602 [COMMIT-LOG-WRITER] Marking replay position 11790 on commit log CommitLogSegment(/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515372947.log)
DEBUG 13:43:20,608 [MIGRATION-STAGE:1] Starting CFS Super1
DEBUG 13:43:20,609 [MIGRATION-STAGE:1] Starting CFS Standard2
DEBUG 13:43:20,610 [MIGRATION-STAGE:1] Starting CFS Super2
DEBUG 13:43:20,611 [MIGRATION-STAGE:1] Starting CFS Standard1
DEBUG 13:43:20,612 [MIGRATION-STAGE:1] Starting CFS Super3
DEBUG 13:43:20,612 [MIGRATION-STAGE:1] Starting CFS StandardByUUID1
DEBUG 13:43:20,613 [CompactionExecutor:1] Checking to see if compaction of Schema would be useful
 INFO 13:43:20,626 [COMMIT-LOG-WRITER] Creating new commitlog segment /Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515400626.log
DEBUG 13:43:20,657 [MIGRATION-STAGE:1] Applying migration 53580f3c-9c0a-11df-8af1-e700f669bcfc
 INFO 13:43:20,660 [MIGRATION-STAGE:1] switching in a fresh Memtable for Migrations at CommitLogContext(file='/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515400626.log', position=12779)
 INFO 13:43:20,660 [MIGRATION-STAGE:1] Enqueuing flush of Memtable-Migrations@193189276(6998 bytes, 1 operations)
 INFO 13:43:20,660 [MIGRATION-STAGE:1] switching in a fresh Memtable for Schema at CommitLogContext(file='/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515400626.log', position=12779)
 INFO 13:43:20,661 [MIGRATION-STAGE:1] Enqueuing flush of Memtable-Schema@2084371115(4181 bytes, 4 operations)
 INFO 13:43:20,661 [FLUSH-WRITER-POOL:1] Writing Memtable-Migrations@193189276(6998 bytes, 1 operations)
 INFO 13:43:20,815 [FLUSH-WRITER-POOL:1] Completed flushing /Users/gary.dusbabek/cass-configs/trunk/node1/data/system/Migrations-e-2-Data.db
 INFO 13:43:20,816 [FLUSH-WRITER-POOL:1] Writing Memtable-Schema@2084371115(4181 bytes, 4 operations)
DEBUG 13:43:20,855 [CompactionExecutor:1] Checking to see if compaction of Migrations would be useful
DEBUG 13:43:20,856 [MEMTABLE-POST-FLUSHER:1] Discarding 1002
DEBUG 13:43:20,864 [COMMIT-LOG-WRITER] discard completed log segments for CommitLogContext(file='/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515400626.log', position=12779), column family 1002.
DEBUG 13:43:20,864 [COMMIT-LOG-WRITER] Not safe to delete commit log CommitLogSegment(/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515372947.log); dirty is 1000, 1003, 
DEBUG 13:43:20,864 [COMMIT-LOG-WRITER] Marking replay position 12779 on commit log CommitLogSegment(/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515400626.log)
 INFO 13:43:21,040 [FLUSH-WRITER-POOL:1] Completed flushing /Users/gary.dusbabek/cass-configs/trunk/node1/data/system/Schema-e-2-Data.db
DEBUG 13:43:21,041 [CompactionExecutor:1] Checking to see if compaction of Schema would be useful
DEBUG 13:43:21,041 [MEMTABLE-POST-FLUSHER:1] Discarding 1003
DEBUG 13:43:21,041 [COMMIT-LOG-WRITER] discard completed log segments for CommitLogContext(file='/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515400626.log', position=12779), column family 1003.
DEBUG 13:43:21,041 [COMMIT-LOG-WRITER] Not safe to delete commit log CommitLogSegment(/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515372947.log); dirty is 1000, 
DEBUG 13:43:21,042 [COMMIT-LOG-WRITER] Marking replay position 12779 on commit log CommitLogSegment(/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515400626.log)
DEBUG 13:43:21,044 [MIGRATION-STAGE:1] Starting CFS Super1
DEBUG 13:43:21,050 [MIGRATION-STAGE:1] Starting CFS Standard2
DEBUG 13:43:21,051 [MIGRATION-STAGE:1] Starting CFS Super2
DEBUG 13:43:21,052 [MIGRATION-STAGE:1] Starting CFS Standard1
DEBUG 13:43:21,053 [MIGRATION-STAGE:1] Starting CFS Super3
DEBUG 13:43:21,058 [MIGRATION-STAGE:1] Starting CFS StandardByUUID1
 INFO 13:43:21,061 [COMMIT-LOG-WRITER] Creating new commitlog segment /Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515401061.log
 INFO 13:43:21,073 [RMI TCP Connection(2)-10.6.34.56] switching in a fresh Memtable for LocationInfo at CommitLogContext(file='/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515401061.log', position=5708)
 INFO 13:43:21,081 [RMI TCP Connection(2)-10.6.34.56] Enqueuing flush of Memtable-LocationInfo@300139206(17 bytes, 1 operations)
 INFO 13:43:21,082 [FLUSH-WRITER-POOL:1] Writing Memtable-LocationInfo@300139206(17 bytes, 1 operations)
 INFO 13:43:21,093 [RMI TCP Connection(2)-10.6.34.56] switching in a fresh Memtable for Schema at CommitLogContext(file='/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515401061.log', position=5708)
 INFO 13:43:21,100 [RMI TCP Connection(2)-10.6.34.56] Enqueuing flush of Memtable-Schema@972791731(4181 bytes, 4 operations)
 INFO 13:43:21,235 [FLUSH-WRITER-POOL:1] Completed flushing /Users/gary.dusbabek/cass-configs/trunk/node1/data/system/LocationInfo-e-4-Data.db
 INFO 13:43:21,235 [FLUSH-WRITER-POOL:1] Writing Memtable-Schema@972791731(4181 bytes, 4 operations)
DEBUG 13:43:21,239 [MEMTABLE-POST-FLUSHER:1] Discarding 1000
DEBUG 13:43:21,240 [COMMIT-LOG-WRITER] discard completed log segments for CommitLogContext(file='/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515401061.log', position=5708), column family 1000.
 INFO 13:43:21,240 [COMMIT-LOG-WRITER] Discarding obsolete commit log:CommitLogSegment(/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515372947.log)
DEBUG 13:43:21,243 [CompactionExecutor:1] Checking to see if compaction of LocationInfo would be useful
DEBUG 13:43:21,439 [GC inspection] GC for ParNew: 7 ms, 350704 reclaimed leaving 105776848 used; max is 1211826176
DEBUG 13:43:21,439 [GC inspection] GC for ConcurrentMarkSweep: 114 ms, 75637088 reclaimed leaving 30139760 used; max is 1211826176
 INFO 13:43:21,450 [CompactionExecutor:1] Compacting [org.apache.cassandra.io.sstable.SSTableReader(path='/Users/gary.dusbabek/cass-configs/trunk/node1/data/system/LocationInfo-e-1-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='/Users/gary.dusbabek/cass-configs/trunk/node1/data/system/LocationInfo-e-2-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='/Users/gary.dusbabek/cass-configs/trunk/node1/data/system/LocationInfo-e-3-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='/Users/gary.dusbabek/cass-configs/trunk/node1/data/system/LocationInfo-e-4-Data.db')]
DEBUG 13:43:21,465 [CompactionExecutor:1] Expected bloom filter size : 1024
DEBUG 13:43:21,466 [COMMIT-LOG-WRITER] Not safe to delete commit log CommitLogSegment(/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515400626.log); dirty is 1003, 1002, 
DEBUG 13:43:21,469 [COMMIT-LOG-WRITER] Marking replay position 5708 on commit log CommitLogSegment(/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515401061.log)
DEBUG 13:43:21,471 [FILEUTILS-DELETE-POOL:1] Deleting CommitLog-1280515372947.log.header
DEBUG 13:43:21,473 [FILEUTILS-DELETE-POOL:1] Deleting CommitLog-1280515372947.log
 INFO 13:43:21,484 [FLUSH-WRITER-POOL:1] Completed flushing /Users/gary.dusbabek/cass-configs/trunk/node1/data/system/Schema-e-3-Data.db
DEBUG 13:43:21,486 [MEMTABLE-POST-FLUSHER:1] Discarding 1003
DEBUG 13:43:21,489 [COMMIT-LOG-WRITER] discard completed log segments for CommitLogContext(file='/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515401061.log', position=5708), column family 1003.
DEBUG 13:43:21,495 [COMMIT-LOG-WRITER] Not safe to delete commit log CommitLogSegment(/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515400626.log); dirty is 1002, 
DEBUG 13:43:21,496 [COMMIT-LOG-WRITER] Marking replay position 5708 on commit log CommitLogSegment(/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515401061.log)
 INFO 13:43:22,016 [CompactionExecutor:1] Compacted to /Users/gary.dusbabek/cass-configs/trunk/node1/data/system/LocationInfo-tmp-e-5-Data.db.  913 to 492 (~53% of original) bytes for 2 keys.  Time: 554ms.
DEBUG 13:43:22,016 [CompactionExecutor:1] Checking to see if compaction of Schema would be useful
DEBUG 13:43:22,016 [CompactionExecutor:1] Checking to see if compaction of LocationInfo would be useful
DEBUG 13:43:22,439 [GC inspection] GC for ParNew: 11 ms, 1461000 reclaimed leaving 109998456 used; max is 1211826176
DEBUG 13:43:22,439 [GC inspection] GC for ConcurrentMarkSweep: 87 ms, 75576552 reclaimed leaving 34421904 used; max is 1211826176
DEBUG 13:43:55,373 [Timer-1] Disseminating load info ...
DEBUG 13:44:55,373 [Timer-1] Disseminating load info ...
DEBUG 13:45:55,373 [Timer-1] Disseminating load info ...
DEBUG 13:46:55,373 [Timer-1] Disseminating load info ...
DEBUG 13:47:55,373 [Timer-1] Disseminating load info ...
DEBUG 13:48:55,373 [Timer-1] Disseminating load info ...
DEBUG 13:49:55,373 [Timer-1] Disseminating load info ...
DEBUG 13:50:55,373 [Timer-1] Disseminating load info ...
DEBUG 13:51:55,373 [Timer-1] Disseminating load info ...
DEBUG 13:52:55,373 [Timer-1] Disseminating load info ...
DEBUG 13:53:55,374 [Timer-1] Disseminating load info ...
DEBUG 13:54:55,373 [Timer-1] Disseminating load info ...
DEBUG 13:55:55,373 [Timer-1] Disseminating load info ...
DEBUG 13:56:55,373 [Timer-1] Disseminating load info ...
DEBUG 13:57:55,373 [Timer-1] Disseminating load info ...
DEBUG 13:58:55,373 [Timer-1] Disseminating load info ...
DEBUG 13:59:55,374 [Timer-1] Disseminating load info ...
DEBUG 14:00:55,379 [Timer-1] Disseminating load info ...
DEBUG 14:01:55,380 [Timer-1] Disseminating load info ...
DEBUG 14:02:55,381 [Timer-1] Disseminating load info ...
DEBUG 14:03:55,382 [Timer-1] Disseminating load info ...
DEBUG 14:04:55,383 [Timer-1] Disseminating load info ...
DEBUG 14:05:55,383 [Timer-1] Disseminating load info ...
DEBUG 14:06:55,383 [Timer-1] Disseminating load info ...
DEBUG 14:07:55,383 [Timer-1] Disseminating load info ...
DEBUG 14:08:55,383 [Timer-1] Disseminating load info ...
DEBUG 14:09:36,781 [GC inspection] GC for ParNew: 1 ms, 21480096 reclaimed leaving 22127704 used; max is 1211826176
DEBUG 14:09:55,383 [Timer-1] Disseminating load info ...
DEBUG 14:10:55,384 [Timer-1] Disseminating load info ...
DEBUG 14:11:55,384 [Timer-1] Disseminating load info ...
DEBUG 14:12:55,385 [Timer-1] Disseminating load info ...
DEBUG 14:13:55,385 [Timer-1] Disseminating load info ...
DEBUG 14:14:55,385 [Timer-1] Disseminating load info ...
DEBUG 14:15:55,384 [Timer-1] Disseminating load info ...
DEBUG 14:16:55,384 [Timer-1] Disseminating load info ...
DEBUG 14:17:55,384 [Timer-1] Disseminating load info ...
DEBUG 14:18:55,384 [Timer-1] Disseminating load info ...
DEBUG 14:19:55,385 [Timer-1] Disseminating load info ...
DEBUG 14:20:55,385 [Timer-1] Disseminating load info ...
DEBUG 14:21:55,386 [Timer-1] Disseminating load info ...
DEBUG 14:22:06,144 [ROW-READ-STAGE:3] Their data definitions are old. Sending updates since 00000000-0000-1000-0000-000000000000
DEBUG 14:22:06,166 [ROW-READ-STAGE:3] collecting 0 of 1000: 5300795b-9c0a-11df-8af1-e700f669bcfc:false:5554@1280515400281
DEBUG 14:22:06,166 [ROW-READ-STAGE:3] collecting 1 of 1000: 53580f3c-9c0a-11df-8af1-e700f669bcfc:false:6967@1280515400638
DEBUG 14:22:06,169 [WRITE-/127.0.0.2] attempting to connect to /127.0.0.2
DEBUG 14:22:06,992 [WRITE-/127.0.0.2] attempting to connect to /127.0.0.2
 INFO 14:22:07,349 [GOSSIP_STAGE:1] Node /127.0.0.2 is now part of the cluster
DEBUG 14:22:07,349 [GOSSIP_STAGE:1] Resetting pool for /127.0.0.2
DEBUG 14:22:07,893 [WRITE-/127.0.0.2] attempting to connect to /127.0.0.2
 INFO 14:22:07,980 [HINTED-HANDOFF-POOL:1] Started hinted handoff for endpoint /127.0.0.2
 INFO 14:22:07,980 [GOSSIP_STAGE:1] InetAddress /127.0.0.2 is now UP
 INFO 14:22:07,985 [HINTED-HANDOFF-POOL:1] Finished hinted handoff of 0 rows to endpoint /127.0.0.2
DEBUG 14:22:55,386 [Timer-1] Disseminating load info ...
DEBUG 14:23:35,990 [MESSAGE-DESERIALIZER-POOL:1] Running  on default stage
DEBUG 14:23:36,913 [GOSSIP_STAGE:1] Node /127.0.0.2 state bootstrapping, token 61078635599166706937511052402724559481
ERROR 14:23:36,915 [GOSSIP_STAGE:1] Error in ThreadPoolExecutor
java.lang.IllegalStateException: replication factor (3) exceeds number of endpoints (1)
	at org.apache.cassandra.locator.RackUnawareStrategy.calculateNaturalEndpoints(RackUnawareStrategy.java:61)
	at org.apache.cassandra.locator.AbstractReplicationStrategy.getAddressRanges(AbstractReplicationStrategy.java:180)
	at org.apache.cassandra.locator.AbstractReplicationStrategy.getAddressRanges(AbstractReplicationStrategy.java:207)
	at org.apache.cassandra.service.StorageService.calculatePendingRanges(StorageService.java:786)
	at org.apache.cassandra.service.StorageService.calculatePendingRanges(StorageService.java:767)
	at org.apache.cassandra.service.StorageService.handleStateBootstrap(StorageService.java:607)
	at org.apache.cassandra.service.StorageService.onChange(StorageService.java:569)
	at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:721)
	at org.apache.cassandra.gms.Gossiper.applyApplicationStateLocally(Gossiper.java:686)
	at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:640)
	at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:61)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:41)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:637)
ERROR 14:23:36,916 [GOSSIP_STAGE:1] Uncaught exception in thread Thread[GOSSIP_STAGE:1,5,main]
java.lang.IllegalStateException: replication factor (3) exceeds number of endpoints (1)
	at org.apache.cassandra.locator.RackUnawareStrategy.calculateNaturalEndpoints(RackUnawareStrategy.java:61)
	at org.apache.cassandra.locator.AbstractReplicationStrategy.getAddressRanges(AbstractReplicationStrategy.java:180)
	at org.apache.cassandra.locator.AbstractReplicationStrategy.getAddressRanges(AbstractReplicationStrategy.java:207)
	at org.apache.cassandra.service.StorageService.calculatePendingRanges(StorageService.java:786)
	at org.apache.cassandra.service.StorageService.calculatePendingRanges(StorageService.java:767)
	at org.apache.cassandra.service.StorageService.handleStateBootstrap(StorageService.java:607)
	at org.apache.cassandra.service.StorageService.onChange(StorageService.java:569)
	at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:721)
	at org.apache.cassandra.gms.Gossiper.applyApplicationStateLocally(Gossiper.java:686)
	at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:640)
	at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:61)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:41)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:637)
DEBUG 14:23:55,387 [Timer-1] Disseminating load info ...
 INFO 14:24:07,913 [WRITE-/127.0.0.2] error writing to /127.0.0.2
DEBUG 14:24:08,912 [WRITE-/127.0.0.2] attempting to connect to /127.0.0.2
 INFO 14:24:12,913 [Timer-0] InetAddress /127.0.0.2 is now dead.
DEBUG 14:24:12,914 [Timer-0] Resetting pool for /127.0.0.2
DEBUG 14:24:19,913 [WRITE-/127.0.0.2] attempting to connect to /127.0.0.2
DEBUG 14:24:30,913 [WRITE-/127.0.0.2] attempting to connect to /127.0.0.2
DEBUG 14:24:41,925 [WRITE-/127.0.0.2] attempting to connect to /127.0.0.2
Disconnected from the target VM, address: '127.0.0.1:54219', transport: 'socket'

Process finished with exit code 255
;;;","31/Jul/10 06:48;gdusbabek;solution: bring all nodes up, then load schema.;;;","31/Jul/10 06:50;gdusbabek;On second thought, I think the right solution is to disallow KS creation when the number of live nodes cannot support the replication factor.;;;","04/Aug/10 02:33;jbellis;+1;;;","04/Aug/10 21:25;hudson;Integrated in Cassandra #509 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/509/])
    complain if there aren't enough nodes to support requested RF. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1343
;;;","04/Sep/10 07:28;blanquer;the same or something similar is still happening in beta1
I've opened a ticket that results in the same error message, however I believe it is different since it doesn't happen when trying to create a KS but when bringing up a new node:
https://issues.apache.org/jira/browse/CASSANDRA-1467;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Abort bootstrap if our IP is already in the token ring,CASSANDRA-663,12444552,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,04/Jan/10 09:31,16/Apr/19 17:33,22/Mar/23 14:57,06/Jan/10 01:16,0.5,,,,0,,,,,,Trying to bootstrap a node w/ the same IP as one that is Down but not removed from the ring should give an error.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Jan/10 01:07;jbellis;663.txt;https://issues.apache.org/jira/secure/attachment/12429355/663.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19809,,,Tue Jan 05 17:11:50 UTC 2010,,,,,,,,,,"0|i0g0bj:",91482,,,,,Normal,,,,,,,,,,,,,,,,,"05/Jan/10 01:12;urandom;+1;;;","05/Jan/10 07:32;jaakko;IMHO this check should be done before starting the bootstrap process (immediately after ""... got load info""). At that point we've already slept for RING_DELAY, so waiting for another RING_DELAY in startBootstrap is probably not useful. If we do the check at its current place, we've already started to gossip our bootsrap token, which will cause the other rightful IP owner to be removed from token metadata.
;;;","05/Jan/10 07:46;jbellis;I'd like to have this as a sanity check on all move operations, not just the initial bootstrap.  What about putting the check at the beginning of SS.startBootstrap?;;;","05/Jan/10 08:52;jaakko;As long as it is before gossip, it does not make so big difference.

However, I don't know what this check would achieve in a move operation. If the node is up and running in order to do a move operation, it must be in token metadata as well. Move does not change IP address, so this check for duplicate IP address is relevant only in first bootstrap.
;;;","06/Jan/10 01:11;jbellis;you're right,  this makes the most sense after ""got load info.""  committed with that change.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConcurrentModificationException,CASSANDRA-853,12458265,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,riffraff,btoddb,btoddb,06/Mar/10 01:11,16/Apr/19 17:33,22/Mar/23 14:57,06/Mar/10 04:12,0.6,,,,0,,,,,,"i'm seeing a lot of these ... any idea?

2010-03-04 18:53:21,455 ERROR [MEMTABLE-POST-FLUSHER:1] [DebuggableThreadPoolExecutor.java:94] Error in executor futuretask
java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.util.ConcurrentModificationException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:86)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.util.ConcurrentModificationException
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        ... 2 more
Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.util.ConcurrentModificationException
        at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegments(CommitLog.java:357)
        at org.apache.cassandra.db.ColumnFamilyStore$2.runMayThrow(ColumnFamilyStore.java:392)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 6 more
Caused by: java.util.concurrent.ExecutionException: java.util.ConcurrentModificationException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegments(CommitLog.java:349)
        ... 8 more
Caused by: java.util.ConcurrentModificationException
        at java.util.ArrayDeque$DeqIterator.next(ArrayDeque.java:605)
        at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegmentsInternal(CommitLog.java:385)
        at org.apache.cassandra.db.commitlog.CommitLog.access$300(CommitLog.java:71)
        at org.apache.cassandra.db.commitlog.CommitLog$6.call(CommitLog.java:343)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at org.apache.cassandra.db.commitlog.CommitLogExecutorService.process(CommitLogExecutorService.java:113)
        at org.apache.cassandra.db.commitlog.CommitLogExecutorService.access$200(CommitLogExecutorService.java:35)
        at org.apache.cassandra.db.commitlog.CommitLogExecutorService$1.runMayThrow(CommitLogExecutorService.java:67)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 1 more
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Mar/10 01:30;riffraff;CASSANDRA-853.patch;https://issues.apache.org/jira/secure/attachment/12438025/CASSANDRA-853.patch",,,,,,,,,,,,,,1.0,riffraff,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19890,,,Fri Mar 05 20:12:50 UTC 2010,,,,,,,,,,"0|i0g1hb:",91670,,,,,Normal,,,,,,,,,,,,,,,,,"06/Mar/10 01:29;riffraff;this seems just a mis-use of the foreach loop, attaching patch that should fix it but AFAICT there are no tests for discardCompletedSegments at all? 
;;;","06/Mar/10 03:48;riffraff;avoids collection modification while looping;;;","06/Mar/10 04:12;jbellis;Committed to 0.6 and trunk, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InstanceAlreadyExistsException,CASSANDRA-650,12444066,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,lenn0x,lenn0x,23/Dec/09 14:18,16/Apr/19 17:33,22/Mar/23 14:57,23/Dec/09 23:02,0.5,,,,0,,,,,,"Was testing out 05 last night, and got this error, can't boot up a node because of it. I'll be happy to debug if more info is needed, it's quite late here.

ERROR [main] 2009-12-22 21:52:14,167 StorageService.java (line 129) Exception was generated at : 12/22/2009 21:52:14 on thread main
javax.management.InstanceAlreadyExistsException: org.apache.cassandra.concurrent:type=CONSISTENCY-MANAGER
java.lang.RuntimeException: javax.management.InstanceAlreadyExistsException: org.apache.cassandra.concurrent:type=CONSISTENCY-MANAGER
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.<init>(DebuggableThreadPoolExecutor.java:63)
        at org.apache.cassandra.service.StorageService.<init>(StorageService.java:151)
        at org.apache.cassandra.service.StorageService.instance(StorageService.java:125)
        at org.apache.cassandra.locator.RackAwareStrategy.<init>(RackAwareStrategy.java:47)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
        at org.apache.cassandra.service.StorageService.getReplicationStrategy(StorageService.java:244)
        at org.apache.cassandra.service.StorageService.<init>(StorageService.java:233)
        at org.apache.cassandra.service.StorageService.instance(StorageService.java:125)
        at org.apache.cassandra.service.CassandraServer.<init>(CassandraServer.java:58)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:93)
        at org.apache.cassandra.service.CassandraDaemon.init(CassandraDaemon.java:135)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:160)
Caused by: javax.management.InstanceAlreadyExistsException: org.apache.cassandra.concurrent:type=CONSISTENCY-MANAGER
        at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:453)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.internal_addObject(DefaultMBeanServerInterceptor.java:1484)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:963)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:917)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:312)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:482)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.<init>(DebuggableThreadPoolExecutor.java:59)
        ... 18 more",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19802,,,Wed Dec 23 15:02:36 UTC 2009,,,,,,,,,,"0|i0g08n:",91469,,,,,Normal,,,,,,,,,,,,,,,,,"23/Dec/09 23:02;jbellis;fixed in r893539.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RowWarningThreshold doesn't allow values more than about 2GB,CASSANDRA-882,12458946,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,12/Mar/10 23:08,16/Apr/19 17:33,22/Mar/23 14:57,13/Mar/10 04:34,0.6,,,,0,,,,,,"Using values bigger than 2048 for RowWarningThreshold makes 
Cassandra not start with the message: 
  Fatal error: Row warning threshold must be a positive integer",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Mar/10 23:10;slebresne;882-fix_rowWarningThresholdLimit.diff;https://issues.apache.org/jira/secure/attachment/12438617/882-fix_rowWarningThresholdLimit.diff",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19900,,,Fri Mar 12 20:29:59 UTC 2010,,,,,,,,,,"0|i0g1nr:",91699,,,,,Low,,,,,,,,,,,,,,,,,"13/Mar/10 04:29;gdusbabek;+1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compaction can't find files,CASSANDRA-606,12442556,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,lenn0x,lenn0x,lenn0x,06/Dec/09 07:34,16/Apr/19 17:33,22/Mar/23 14:57,06/Dec/09 09:46,0.5,,,,0,,,,,,"We have been seeing issues with compaction running very often. We ran into this case when I found out in one of our CFs we were inserting a high volume of columns. Our threshold for memtable size flushes is 64MB but the MemtableObjectCountInMillions is 0.1 (we know this is low and will increase). On average we are writing so much data that compaction kicks off reguarly. And when we start trying to lookup data, we get lots of errors during compaction and get_slice (since sometimes files don't get cleaned up). In every event that this has occurred, a -Filter file was left behind. Never Data or Index.

ERROR [COMPACTION-POOL:1] 2009-12-05 15:04:47,412 DebuggableThreadPoolExecutor.java (line 120) Error in executor futuretask
java.util.concurrent.ExecutionException: java.io.FileNotFoundException: /mnt/var/cassandra/data/Digg/UserActivity-1243-Data.db (No such file or directory)
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:112)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.FileNotFoundException: /mnt/var/cassandra/data/Digg/UserActivity-1243-Data.db (No such file or directory)
        at java.io.RandomAccessFile.open(Native Method)
        at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
        at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
        at org.apache.cassandra.io.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)
        at org.apache.cassandra.io.SSTableScanner.<init>(SSTableScanner.java:47)
        at org.apache.cassandra.io.SSTableReader.getScanner(SSTableReader.java:386)
        at org.apache.cassandra.io.CompactionIterator.getCollatingIterator(CompactionIterator.java:65)
        at org.apache.cassandra.io.CompactionIterator.<init>(CompactionIterator.java:48)
        at org.apache.cassandra.db.ColumnFamilyStore.doFileCompaction(ColumnFamilyStore.java:902)
        at org.apache.cassandra.db.ColumnFamilyStore.doFileCompaction(ColumnFamilyStore.java:861)
        at org.apache.cassandra.db.ColumnFamilyStore.doCompaction(ColumnFamilyStore.java:663)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:180)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:177)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        ... 2 more
 INFO [main] 2009-12-05 15:08:54,384 SSTable.java (line 156) Deleted /mnt/var/cassandra/data/Digg/UserActivity-1130-Data.db

",,hammer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Dec/09 09:29;lenn0x;0001-ColumnFamily.onStart-was-not-checking-files-related-.patch;https://issues.apache.org/jira/secure/attachment/12427082/0001-ColumnFamily.onStart-was-not-checking-files-related-.patch","06/Dec/09 09:39;lenn0x;0001-v2-ColumnFamily.onStart-was-not-checking-files-related-.patch;https://issues.apache.org/jira/secure/attachment/12427083/0001-v2-ColumnFamily.onStart-was-not-checking-files-related-.patch",,,,,,,,,,,,,2.0,lenn0x,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19777,,,Sun Dec 06 12:35:08 UTC 2009,,,,,,,,,,"0|i0fzyv:",91425,,,,,Normal,,,,,,,,,,,,,,,,,"06/Dec/09 08:20;lenn0x;Looks like ColumnFamilyStore.onStart() is not restricting itself to it's own CF files only. After CF A onstart finishes CF B onstart could remove orphans.;;;","06/Dec/09 09:31;jbellis;the filename.contains(columnFamily_)) and cfName.equals(columnFamily_) checks are redundant too w/ this;;;","06/Dec/09 09:44;jbellis;+1 v2;;;","06/Dec/09 20:35;hudson;Integrated in Cassandra #279 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/279/])
    ColumnFamily.onStart() was not checking files related to only it's column family. It was possible to remove files from other CFs thinking they were orphans. Skip files not apart of the CF. patch by goffinet; reviewed by jbellis for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
disallow column family names containing hyphens,CASSANDRA-915,12460046,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,urandom,urandom,24/Mar/10 05:33,16/Apr/19 17:33,22/Mar/23 14:57,20/Apr/10 05:58,0.7 beta 1,,,,0,,,,,,"You cannot use use hyphens in column family names because hyphens are used as delimiters in sstable filenames (which are derived from the CF name). 

It should be an error to configure such a column family name.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-44,,,"24/Mar/10 06:35;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-915-don-t-allow-hyphens-in-column-family-nam.txt;https://issues.apache.org/jira/secure/attachment/12439616/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-915-don-t-allow-hyphens-in-column-family-nam.txt",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19917,,,Wed Mar 24 12:40:18 UTC 2010,,,,,,,,,,"0|i0g1v3:",91732,,,,,Low,,,,,,,,,,,,,,,,,"24/Mar/10 06:37;urandom;The attached patch should take care of 0.6 (and trunk for the time being), but this ticket will need to be added as a dependency to CASSANDRA-44 in order to make sure that the new-shiny handles this case as well.;;;","24/Mar/10 06:45;jbellis;+1;;;","24/Mar/10 07:00;urandom;committed to 0.6 and trunk;;;","24/Mar/10 20:40;hudson;Integrated in Cassandra #389 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/389/])
    don't allow hyphens in column family names

Patch by eevans; reviewed by jbellis for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flush creates empty SSTables if nothing exists in that CF,CASSANDRA-532,12440045,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,lenn0x,lenn0x,lenn0x,07/Nov/09 01:45,16/Apr/19 17:33,22/Mar/23 14:57,10/Nov/09 13:20,0.5,,,,0,,,,,,"When calling flush() through nodeprobe, we see SSTables being created that are empty for CFs:

 INFO [COMPACTION-POOL:1] 2009-11-05 22:58:09,515 ColumnFamilyStore.java (line 850) Compacting [org.apache.cassandra.io.SSTableReader(path='/mnt/cassandra/data/Digg/Buries-9-Data.db'),org.apache.cassandra.io.SSTableReader(path='/mnt/cassandra/data/Digg/Buries-10-Data.db'),org.apache.cassandra.io.SSTableReader(path='/mnt/cassandra/data/Digg/Buries-7-Data.db'),org.apache.cassandra.io.SSTableReader(path='/mnt/cassandra/data/Digg/Buries-11-Data.db'),org.apache.cassandra.io.SSTableReader(path='/mnt/cassandra/data/Digg/Buries-8-Data.db'),org.apache.cassandra.io.SSTableReader(path='/mnt/cassandra/data/Digg/Buries-5-Data.db'),org.apache.cassandra.io.SSTableReader(path='/mnt/cassandra/data/Digg/Buries-6-Data.db')]
ERROR [COMPACTION-POOL:1] 2009-11-05 22:58:09,516 DebuggableThreadPoolExecutor.java (line 120) Error in executor futuretask
java.util.concurrent.ExecutionException: java.lang.NullPointerException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:112)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.io.SSTableReader.getApproximateKeyCount(SSTableReader.java:102)
        at org.apache.cassandra.db.ColumnFamilyStore.doFileCompaction(ColumnFamilyStore.java:866)
        at org.apache.cassandra.db.ColumnFamilyStore.doFileCompaction(ColumnFamilyStore.java:830)
        at org.apache.cassandra.db.ColumnFamilyStore.doMajorCompactionInternal(ColumnFamilyStore.java:673)
        at org.apache.cassandra.db.ColumnFamilyStore.doMajorCompaction(ColumnFamilyStore.java:645)
        at org.apache.cassandra.db.CompactionManager$OnDemandCompactor.run(CompactionManager.java:123)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        ... 2 more
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Nov/09 12:50;lenn0x;0001-forceFlushBinary-was-calling-memtable-not-binaryMemt.patch;https://issues.apache.org/jira/secure/attachment/12424436/0001-forceFlushBinary-was-calling-memtable-not-binaryMemt.patch","07/Nov/09 01:51;jbellis;532.patch;https://issues.apache.org/jira/secure/attachment/12424211/532.patch",,,,,,,,,,,,,2.0,lenn0x,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19740,,,Tue Nov 10 05:20:43 UTC 2009,,,,,,,,,,"0|i0fzif:",91351,,,,,Normal,,,,,,,,,,,,,,,,,"07/Nov/09 01:52;lenn0x;+1 looks good;;;","07/Nov/09 01:57;jbellis;committed;;;","07/Nov/09 20:33;hudson;Integrated in Cassandra #251 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/251/])
    avoid flushing empty binarymemtable
patch by jbellis; reviewed by goffinet for 
;;;","10/Nov/09 12:50;lenn0x;forceFlushBinary() was calling memtable not binaryMemtable_;;;","10/Nov/09 13:04;jbellis;+1

(apply to 0.4 first, then merge to trunk pls);;;","10/Nov/09 13:20;lenn0x;commited;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JNA Native check can throw a NoSuchMethodError,CASSANDRA-1760,12480451,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,tjake,tjake,tjake,20/Nov/10 05:33,16/Apr/19 17:33,22/Mar/23 14:57,20/Nov/10 05:49,0.6.9,0.7.0 rc 1,,,0,,,,,,"Looks like older versions of JNA have a different Native.register() method

From IRC:
hi, i'm having trouble starting cassandra up...the error is very bizarre: java.lang.NoSuchMethodError: com.sun.jna.Native.register(Ljava/lang/String;)V",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Nov/10 05:34;tjake;1760_v1.txt;https://issues.apache.org/jira/secure/attachment/12460055/1760_v1.txt",,,,,,,,,,,,,,1.0,tjake,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20297,,,Fri Nov 19 21:49:34 UTC 2010,,,,,,,,,,"0|i0g77z:",92600,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"20/Nov/10 05:43;tjake;it was jruby :(;;;","20/Nov/10 05:49;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""maven install"" does not find libthrift-r808609.jar",CASSANDRA-624,12443048,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,cpierret,cpierret,11/Dec/09 07:56,16/Apr/19 17:33,22/Mar/23 14:57,31/Dec/09 00:55,0.5,0.6,Legacy/Tools,,0,,,,,,"Do a fresh svn checkout of subversion trunk HEAD (at revision 89436 when error occurred)
run: maven -e install
It fails, not finding dependency libthrift.jar

pom.xml references ${basedir}/lib/libthrift-r808609.jar
It should be ${basedir}/lib/libthrift-r820831.jar

",svn trunk r889436,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,THRIFT-363,,,,,,"11/Dec/09 08:21;cpierret;pom.xml.patch;https://issues.apache.org/jira/secure/attachment/12427663/pom.xml.patch",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19786,,,Mon Apr 25 13:59:09 UTC 2011,,,,,,,,,,"0|i0g02v:",91443,,,,,Low,,,,,,,,,,,,,,,,,"11/Dec/09 07:57;cpierret;Index: pom.xml
===================================================================
--- pom.xml	(revision 889422)
+++ pom.xml	(working copy)
@@ -187,7 +187,7 @@
       <artifactId>libthrift</artifactId>
       <version>UNKNOWN</version>
       <scope>system</scope>
-      <systemPath>${basedir}/lib/libthrift-r808609.jar</systemPath>
+      <systemPath>${basedir}/lib/libthrift-r820831.jar</systemPath>
     </dependency>
     <dependency>
       <groupId>reardencommerce</groupId>
;;;","31/Dec/09 00:55;urandom;This was fixed in http://svn.apache.org/viewvc?revision=892351&view=revision (I wasn't aware there was an issue open on it).;;;","25/Apr/11 21:59;jfarrell;Libthrift and libfb303 now available in the apache repo (http://repo1.maven.org/maven2)

<dependency>
  <groupId>org.apache.thrift</groupId>
  <artifactId>libthrift</artifactId>
  <version>[0.6.1,)</version>
</dependency>;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StorageProxy throws an InvalidRequestException in readProtocol during bootstrap,CASSANDRA-1862,12493250,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,zznate,zznate,zznate,15/Dec/10 06:40,16/Apr/19 17:33,22/Mar/23 14:57,17/Dec/10 03:51,0.7.0 rc 3,,,,0,,,,,,"Though the error message provides details, IRE is supposed to signify poorly formed API requests. In the context of a client request, an UnavailableException is more appropriate. This would allow the client to take action like removing the node from its host list. ",,doubleday,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Dec/10 06:42;zznate;1862.txt;https://issues.apache.org/jira/secure/attachment/12466266/1862.txt","16/Dec/10 00:32;zznate;1862_0.6.txt;https://issues.apache.org/jira/secure/attachment/12466327/1862_0.6.txt",,,,,,,,,,,,,2.0,zznate,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20348,,,Wed Apr 20 15:40:47 UTC 2011,,,,,,,,,,"0|i0g7v3:",92704,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"15/Dec/10 06:42;zznate;Changes exception type to UnavailalbeException;;;","15/Dec/10 23:10;jbellis;committed, thanks!;;;","15/Dec/10 23:24;hudson;Integrated in Cassandra-0.7 #83 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/83/])
    change exceptionfor readrequests duringbootstrap from InvalidRequest to Unavailable
patch by Nate McCall; reviewed by jbellis for CASSANDRA-1862
;;;","16/Dec/10 00:32;zznate;Backport to 0.6;;;","16/Dec/10 00:50;jbellis;I'm a little leery of breaking 0.6 compatibility for something fairly minor.  Thoughts?;;;","16/Dec/10 01:38;zznate;I think this is extremely minor (the exception declaration of the method did not change) and brings us in line with the API as specified in cassandra.thrift: 

IRE:
""Invalid request could mean keyspace or column family does not exist, required parameters are missing, or a parameter is malformed...""

vs. UE:
""Not all the replicas required could be created and/or read."";;;","16/Dec/10 01:43;jbellis;bq. the exception declaration of the method did not change

i'll buy that.  committed;;;","16/Dec/10 02:14;hudson;Integrated in Cassandra-0.6 #24 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/24/])
    backport CASSANDRA-1862 from 0.7
;;;","20/Apr/11 23:40;doubleday;Don't know wether commenting a closed jira makes sense but I think that this made matters worse because now bootstrapping is not distinguishable anymore.
I thought that UnavailableException would signal that a read cannot be served due to CL. And a bootstrapping coordinator does not imply this right? ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CFMetaData id gets out of sync,CASSANDRA-1403,12471843,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,gdusbabek,gdusbabek,gdusbabek,18/Aug/10 04:32,16/Apr/19 17:33,22/Mar/23 14:57,18/Aug/10 05:44,0.7 beta 2,,,,0,,,,,,"stand up two nodes.
load a KS + cf on A.
add another CF on A.
Let the cluster quiesce.
add a CF on B.

You get the out of sync error.  I'm pretty sure this is because AddColumnFamily doesn't CFM.fixMaxId() like AddKeyspace does.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Aug/10 04:58;gdusbabek;ASF.LICENSE.NOT.GRANTED--v0-0001-fix-max-id-after-adding-a-column-family.txt;https://issues.apache.org/jira/secure/attachment/12452322/ASF.LICENSE.NOT.GRANTED--v0-0001-fix-max-id-after-adding-a-column-family.txt",,,,,,,,,,,,,,1.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20120,,,Wed Aug 18 13:13:50 UTC 2010,,,,,,,,,,"0|i0g4uf:",92215,,,,,Normal,,,,,,,,,,,,,,,,,"18/Aug/10 05:20;stuhood;+1
I expect that cfId generation is going to need to change significantly to allow for concurrent schema changes.;;;","18/Aug/10 05:40;gdusbabek;It will need to act like a distributed counter.;;;","18/Aug/10 21:13;hudson;Integrated in Cassandra #517 (See [https://hudson.apache.org/hudson/job/Cassandra/517/])
    fix max id after adding a column family. patch by gdusbabek, reviewed by stuhood. CASSANDRA-1403
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-env.sh pattern matching for OpenJDK broken in some cases,CASSANDRA-2499,12504595,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,cywjackson,thobbs,thobbs,19/Apr/11 04:37,16/Apr/19 17:33,22/Mar/23 14:57,21/Apr/11 03:16,0.8.0 beta 2,,Packaging,,0,,,,,,"With bash version 4.1.5, the section of cassandra-env that tries to match the JDK distribution seems to have some kind of syntax error.  I get the following message when running bin/cassandra:

{noformat}
bin/../conf/cassandra-env.sh: 99: [[: not found
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,cywjackson,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20657,,,Wed Apr 20 20:02:24 UTC 2011,,,,,,,,,,"0|i0gbpz:",93329,,thobbs,,thobbs,Low,,,,,,,,,,,,,,,,,"20/Apr/11 09:11;cywjackson;chances are the default /bin/sh is linked to dash, please run a ls -al /bin/sh to confirm

a minor fix could be remove the [[ and add """" around the variables and values.

-if [[ $java_version != \*OpenJDK\* ]]
+if [ ""$java_version"" != ""\*OpenJDK\*"" ]

a more drastic fix is to update all the /bin/sh with /bin/bash (if desired to only support on bash)

see ref: https://wiki.ubuntu.com/DashAsBinSh

suggest to be reviewed to make a decision.;;;","21/Apr/11 00:27;thobbs;/bin/sh was a link to dash, and the current script seems to work fine with bash.
{noformat}
[ ""$java_version"" != ""*OpenJDK*"" ]
{noformat} works for me in both dash and bash.;;;","21/Apr/11 00:56;thobbs;Correction: the suggested replacement does *not* seem to detect OpenJDK.;;;","21/Apr/11 01:46;cywjackson;try this:

-java_version=`java -version 2>&1`
-if [[ $java_version != *OpenJDK* ]]
+check_openjdk=$(java -version 2>&1 | awk '{if (NR == 2) {print $1}}')
+if [ ""$check_openjdk"" != ""OpenJDK"" ]

{noformat}$ bash /tmp/testjdk java
version: Java(TM)
not OpenJDK
$ dash /tmp/testjdk /usr/bin/java
version: OpenJDK
yes OpenJDK
$ cat /tmp/testjdk
check_openjdk=$($1 -version 2>&1 | awk '{if (NR ==2) {print $1}}')
echo ""version: $check_openjdk""
if [ ""$check_openjdk"" != ""OpenJDK"" ]
then
    echo ""not OpenJDK""
    JVM_OPTS=""$JVM_OPTS -javaagent:$CASSANDRA_HOME/lib/jamm-0.2.1.jar""
else
    echo ""yes OpenJDK""
fi
{noformat};;;","21/Apr/11 03:16;jbellis;committed;;;","21/Apr/11 04:02;hudson;Integrated in Cassandra-0.8 #30 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/30/])
    fix jdk verison check for sh/dash
patch by Jackson Chung; reviewed by thobbs and jbellis for CASSANDRA-2499
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CommitLogHeader raises an AssertionError during  startup,CASSANDRA-1435,12472560,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,amorton,amorton,26/Aug/10 14:52,16/Apr/19 17:33,22/Mar/23 14:57,31/Aug/10 22:58,0.7 beta 2,,,,0,,,,,,"On a cluster that was pretty sick due to CASSANDRA-1416 and CASSANDRA-1432 I got the error below when starting up a node. The node failed to start.

After retrying the node started. 
ERROR [main] 2010-08-26 14:59:22,315 AbstractCassandraDaemon.java (line 107) Exception encountered during startup.
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
        at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:549)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:339)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:174)
        at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:120)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:90)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:545)
        ... 5 more
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
        at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegments(CommitLog.java:408)
        at org.apache.cassandra.db.ColumnFamilyStore$2.runMayThrow(ColumnFamilyStore.java:445)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 6 more
Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegments(CommitLog.java:400)
        ... 8 more
Caused by: java.lang.AssertionError
        at org.apache.cassandra.db.commitlog.CommitLogHeader$CommitLogHeaderSerializer.serialize(CommitLogHeader.java:157)
        at org.apache.cassandra.db.commitlog.CommitLogHeader.writeCommitLogHeader(CommitLogHeader.java:124)
        at org.apache.cassandra.db.commitlog.CommitLogSegment.writeHeader(CommitLogSegment.java:70)
        at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegmentsInternal(CommitLog.java:450)
        at org.apache.cassandra.db.commitlog.CommitLog.access$300(CommitLog.java:75)
        at org.apache.cassandra.db.commitlog.CommitLog$6.call(CommitLog.java:394)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:52)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 1 more
",,johanoskarsson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-1376,,,,,,,,,,,"31/Aug/10 06:30;jbellis;ASF.LICENSE.NOT.GRANTED--0001-avoid-attempting-to-keep-CL-header-constant-size-schem.txt;https://issues.apache.org/jira/secure/attachment/12453471/ASF.LICENSE.NOT.GRANTED--0001-avoid-attempting-to-keep-CL-header-constant-size-schem.txt","31/Aug/10 06:30;jbellis;ASF.LICENSE.NOT.GRANTED--0002-r-m-forceNewSegment.txt;https://issues.apache.org/jira/secure/attachment/12453472/ASF.LICENSE.NOT.GRANTED--0002-r-m-forceNewSegment.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20137,,,Sun Sep 12 19:39:05 UTC 2010,,,,,,,,,,"0|i0g51j:",92247,,gdusbabek,,gdusbabek,Normal,,,,,,,,,,,,,,,,,"27/Aug/10 06:08;jbellis;a write during a schema change could cause adding an entry to the CLH dirty map, which the assert was dutifully noticing.

attached solution is to simply allow the map to grow; we can do this because we previously moved the header to a separate file, so restricting it to the same amount of bytes is no longer important.

does this mean we can drop all the forcenewsegment calls in the migration code?;;;","27/Aug/10 21:31;gdusbabek;>does this mean we can drop all the forcenewsegment calls in the migration code?
If the CLH is no longer bounded, yes.;;;","27/Aug/10 21:59;jbellis;-2 removes forcenewsegment.  (shouldn't be necessary for drain, either);;;","31/Aug/10 21:58;gdusbabek;+1;;;","31/Aug/10 22:55;jbellis;committed;;;","13/Sep/10 03:39;hudson;Integrated in Cassandra #533 (See [https://hudson.apache.org/hudson/job/Cassandra/533/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New node is attempting to bootstrap to itself,CASSANDRA-536,12440313,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,rays,rays,11/Nov/09 02:38,16/Apr/19 17:33,22/Mar/23 14:57,12/Nov/09 11:37,0.5,,,,0,,,,,,"I took an existing testing node and deleted the data, and commitlog directories and enabled AutoBootstrap in the config file and restarted cassandra and saw the following error in the log file which indicates to me that its attempting to bootstrap off of itself and failing. All nodes are running the same version of trunk.

DEBUG - Beginning bootstrap process
DEBUG [Thread-7] 2009-11-10 13:20:31,915 BootStrapper.java (line 98) Beginning bootstrap process
DEBUG - Binding thrift service to /0.0.0.0:9160
DEBUG [main] 2009-11-10 13:20:31,915 CassandraDaemon.java (line 101) Binding thrift service to /0.0.0.0:9160
 INFO [main] 2009-11-10 13:20:31,915 CassandraDaemon.java (line 141) Cassandra starting up...
DEBUG [Thread-7] 2009-11-10 13:20:31,966 BootStrapper.java (line 104) Sending BootstrapMetadataMessage to /10.2.4.110 for (36566327313263876483692170869705586748,68939025851256836916907001051563673941]
 WARN [Thread-7] 2009-11-10 13:20:31,966 MessagingService.java (line 468) Running on default stage - beware
DEBUG [Thread-7] 2009-11-10 13:20:31,966 StorageService.java (line 153) Added /10.2.4.110 as a bootstrap source
DEBUG [MESSAGE-DESERIALIZER-POOL:2] 2009-11-10 13:20:31,966 BootstrapMetadataVerbHandler.java (line 52) Received a BootstrapMetadataMessage from /10.2.4.110
ERROR [MESSAGE-DESERIALIZER-POOL:2] 2009-11-10 13:20:31,966 CassandraDaemon.java (line 71) Fatal exception in thread Thread[MESSAGE-DESERIALIZER-POOL:2,5,main]
java.lang.AssertionError
	at org.apache.cassandra.dht.BootstrapMetadataVerbHandler.doVerb(BootstrapMetadataVerbHandler.java:55)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
	at java.lang.Thread.run(Thread.java:619)
","FreeBSD 7.2-RELEASE amd64
Diablo Java HotSpot(TM) 64-Bit Server VM (build 10.0-b23, mixed mode)",rays,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Nov/09 07:21;jbellis;536.patch;https://issues.apache.org/jira/secure/attachment/12424671/536.patch",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19743,,,Thu Nov 12 12:34:56 UTC 2009,,,,,,,,,,"0|i0fzjb:",91355,,,,,Normal,,,,,,,,,,,,,,,,,"12/Nov/09 07:21;jbellis;fixes attached.;;;","12/Nov/09 11:28;jaakko;+1;;;","12/Nov/09 11:37;jbellis;committed;;;","12/Nov/09 20:34;hudson;Integrated in Cassandra #256 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/256/])
    avoid making local node part of the token ring until bootstrap completes; fix other buglets
patch by jbellis; reviewed by Jaakko Laine for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Corruption in CommitLog,CASSANDRA-605,12442555,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,lenn0x,lenn0x,06/Dec/09 07:30,16/Apr/19 17:33,22/Mar/23 14:57,11/Dec/09 23:32,0.5,,,,0,,,,,,"We are seeing corruption in commit log files when cassandra gets shutdown sometimes. I can attach the commitlogs privately. Here is a stacktrace:

 INFO [main] 2009-12-05 14:59:25,946 RecoveryManager.java (line 64) Replaying /mnt/var/cassandra/commitlog/CommitLog-1259972135241.log, /mnt/var/cassandra/commitlog/CommitLog-1260050162791.log, /mnt/var/cassandra/commitlog/CommitLog-1260047770958.log, /mnt/var/cassandra/commitlog/CommitLog-1260052237605.log
ERROR [COMPACTION-POOL:1] 2009-12-05 14:59:37,436 DebuggableThreadPoolExecutor.java (line 120) Error in executor futuretask
java.util.concurrent.ExecutionException: java.lang.NumberFormatException: For input string: ""nan""
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:112)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.NumberFormatException: For input string: ""nan""
        at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1224)
        at java.lang.Float.valueOf(Float.java:388)
        at java.lang.Float.<init>(Float.java:489)
        at com.digg.cassandra.db.marshal.FloatStringType.compare(FloatStringType.java:72)
        at com.digg.cassandra.db.marshal.FloatStringType.compare(FloatStringType.java:19)
        at java.util.concurrent.ConcurrentSkipListMap$ComparableUsingComparator.compareTo(ConcurrentSkipListMap.java:606)
        at java.util.concurrent.ConcurrentSkipListMap.doGet(ConcurrentSkipListMap.java:797)
        at java.util.concurrent.ConcurrentSkipListMap.get(ConcurrentSkipListMap.java:1640)
        at org.apache.cassandra.db.ColumnFamilyStore.removeDeletedStandard(ColumnFamilyStore.java:530)
        at org.apache.cassandra.db.ColumnFamilyStore.removeDeleted(ColumnFamilyStore.java:515)
        at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:111)
        at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:38)
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:73)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:135)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:130)
        at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
        at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
        at org.apache.cassandra.db.ColumnFamilyStore.doFileCompaction(ColumnFamilyStore.java:919)
        at org.apache.cassandra.db.ColumnFamilyStore.doFileCompaction(ColumnFamilyStore.java:861)
        at org.apache.cassandra.db.ColumnFamilyStore.doCompaction(ColumnFamilyStore.java:663)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:180)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:177)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        ... 2 more",,hammer,johanoskarsson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Dec/09 12:41;jbellis;605.patch;https://issues.apache.org/jira/secure/attachment/12427675/605.patch",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19776,,,Fri Dec 11 15:32:22 UTC 2009,,,,,,,,,,"0|i0fzyn:",91424,,,,,Normal,,,,,,,,,,,,,,,,,"08/Dec/09 01:24;jbellis;Chris indicated in IRC that this does look like it only occurs near the end of the commitlog, where fsync hasn't yet guaranteed that data is safe.;;;","10/Dec/09 13:40;jbellis;adding a CRC to each log record should fix this.;;;","11/Dec/09 12:41;jbellis;patch to add CRC;;;","11/Dec/09 13:52;jbellis;From IRC:

junrao: if that doesn't add much overhead during writes, then it's ok
driftx: if there's a [performance] difference, it's negligible;;;","11/Dec/09 23:32;jbellis;committed to 0.5 and trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fat client example cannot find schema,CASSANDRA-1002,12462441,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,gdusbabek,johanoskarsson,johanoskarsson,19/Apr/10 22:59,16/Apr/19 17:33,22/Mar/23 14:57,27/Apr/10 06:04,0.7 beta 1,,,,0,,,,,,"Running the client example in contrib shows that it cannot find the schema, possibly caused by CASSANDRA-44.

Throws this error:
Exception in thread ""main"" java.lang.IllegalArgumentException: Unknown ColumnFamily Standard1 in keyspace Keyspace1
	at org.apache.cassandra.config.DatabaseDescriptor.getComparator(DatabaseDescriptor.java:1123)
	at org.apache.cassandra.db.ColumnFamily.getComparatorFor(ColumnFamily.java:437)
	at ClientOnlyExample.testWriting(ClientOnlyExample.java:52)
	at ClientOnlyExample.main(ClientOnlyExample.java:169)
",,jeromatron,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Apr/10 04:48;gdusbabek;0001-modify-migrations-to-respect-client-only-mode.patch;https://issues.apache.org/jira/secure/attachment/12442892/0001-modify-migrations-to-respect-client-only-mode.patch",,,,,,,,,,,,,,1.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19949,,,Mon Apr 26 21:45:33 UTC 2010,,,,,,,,,,"0|i0g2ef:",91819,,,,,Normal,,,,,,,,,,,,,,,,,"27/Apr/10 04:49;gdusbabek;I dislike all of the 'if-clientMode' stuff, but Migrations weren't designed to separate schema-state from schema-storage.;;;","27/Apr/10 05:11;jbellis;so this makes clientmode not make changes to local storage, is there anything else i should be noticing?;;;","27/Apr/10 05:20;gdusbabek;No, that's basically it.;;;","27/Apr/10 05:45;jbellis;+1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InternalException on system_update_column_family if column_metadata is not assigned,CASSANDRA-2096,12497417,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,lenn0x,lenn0x,lenn0x,02/Feb/11 11:44,16/Apr/19 17:33,22/Mar/23 14:57,08/Feb/11 13:06,0.7.1,,,,0,,,,,,"Steps to reproduce:

Execute system_update_column_family without passing in column_metadata in CfDef object.

Error:


java.lang.NullPointerException
	at org.apache.cassandra.config.CFMetaData.convertToAvro(CFMetaData.java:827)
	at org.apache.cassandra.thrift.CassandraServer.system_update_column_family(CassandraServer.java:882)
	at org.apache.cassandra.thrift.Cassandra$Processor$system_update_column_family.process(Cassandra.java:4518)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:3227)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
",,gdusbabek,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Feb/11 11:48;lenn0x;0001-Fix-internal-exception-when-not-passing-in-column_me.patch;https://issues.apache.org/jira/secure/attachment/12470015/0001-Fix-internal-exception-when-not-passing-in-column_me.patch",,,,,,,,,,,,,,1.0,lenn0x,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20440,,,Tue Feb 08 04:41:17 UTC 2011,,,,,,,,,,"0|i0g9af:",92935,,gdusbabek,,gdusbabek,Low,,,,,,,,,,,,,,,,,"03/Feb/11 22:21;gdusbabek;I think this needs a rebase. The patch doesn't apply to trunk or 0.7.;;;","04/Feb/11 01:54;gdusbabek;+1;;;","08/Feb/11 02:21;hudson;Integrated in Cassandra-0.7 #254 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/254/])
    Fix internal exception when not passing in column_metadata over Thrift patch by goffinet; reviewed by gdusbabek for CASSANDRA-2096
;;;","08/Feb/11 12:30;jbellis;needs to be merged to trunk [there are conflicts]: svn merge https://svn.apache.org/repos/asf/cassandra/branches/cassandra-0.7 -c r1068028;;;","08/Feb/11 12:41;lenn0x;Merged from 0.7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception starting up cluster with ByteOrderedPartitioner without schema set,CASSANDRA-1006,12462502,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,stuhood,erickt,erickt,20/Apr/10 06:35,16/Apr/19 17:33,22/Mar/23 14:57,21/Apr/10 06:04,0.7 beta 1,,,,0,,,,,,"Testing out the new ByteOrderedPartitioner, I ran into this exception on the tip:

java.lang.NumberFormatException: For input string: ""To""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:481)
	at org.apache.cassandra.utils.FBUtilities.hexToBytes(FBUtilities.java:361)
	at org.apache.cassandra.dht.AbstractByteOrderedPartitioner$1.fromString(AbstractByteOrderedPartitioner.java:133)
	at org.apache.cassandra.dht.BootStrapper$BootstrapTokenCallback.response(BootStrapper.java:246)
	at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:36)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:41)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
ERROR 16:31:21,737 Fatal exception in thread Thread[RESPONSE-STAGE:1,5,main]

It works fine with the RandomPartitioner, however.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Apr/10 10:35;stuhood;0001-Serialize-Tokens-using-TokenFactory.patch;https://issues.apache.org/jira/secure/attachment/12442256/0001-Serialize-Tokens-using-TokenFactory.patch",,,,,,,,,,,,,,1.0,stuhood,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19951,,,Tue Apr 20 22:04:29 UTC 2010,,,,,,,,,,"0|i0g2fb:",91823,,,,,Normal,,,,,,,,,,,,,,,,,"20/Apr/10 06:48;erickt;I hacked up my code a bit, and it turns out the line ByteOrderedPartitioner is trying to parse is the string ""Token(bytes[4695b973940d36dce35be8d73832f848])"", which obviously is not a hex string.;;;","20/Apr/10 08:00;erickt;Fyi, turns out this is happening whether or not I've set up the storage configuration.;;;","20/Apr/10 10:35;stuhood;BootStrapper was serializing tokens using toString rather than TokenFactory.toString.;;;","21/Apr/10 06:03;gdusbabek;+1;;;","21/Apr/10 06:04;gdusbabek;committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig example script no longer working,CASSANDRA-2487,12504385,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jeromatron,jeromatron,jeromatron,16/Apr/11 00:34,16/Apr/19 17:33,22/Mar/23 14:57,30/Apr/11 03:28,0.7.6,,,,0,hadoop,pig,,,,"There is a strange error given when trying to run the example-script.pig.

java.io.IOException: Type mismatch in key from map: expected org.apache.pig.impl.io.NullableBytesWritable, recieved org.apache.pig.impl.io.NullableText
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:870)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:573)
	at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.collect(PigMapReduce.java:116)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:238)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:231)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:53)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:646)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:322)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:210)

Looks like it has to do with PIG-919 and PIG-1277.  For now we can just cast the var as a chararray and it works though.  Will attach a patch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Apr/11 00:36;jeromatron;2487.txt;https://issues.apache.org/jira/secure/attachment/12476461/2487.txt",,,,,,,,,,,,,,1.0,jeromatron,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20649,,,Fri Apr 29 19:45:49 UTC 2011,,,,,,,,,,"0|i0gbnj:",93318,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"16/Apr/11 00:36;jeromatron;Did the workaround they use in PIG-919 by casting as a chararray for now.  Also put the schema in the load.  Made the name of the keyspace and column family not like the old 0.6 stuff.  Also updated the readme a bit and included an example of setting env vars for running locally since a FAQ.;;;","30/Apr/11 03:28;brandon.williams;Thanks, committed.;;;","30/Apr/11 03:45;hudson;Integrated in Cassandra-0.7 #465 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/465/])
    Update pig example script to work again.
Patch by Jeremy Hanna, reviewed by brandonwilliams for CASSANDRA-2487
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
system_drop_keyspace can cause a node to be unstartable,CASSANDRA-1203,12467270,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,gdusbabek,mbryant,mbryant,18/Jun/10 05:28,16/Apr/19 17:33,22/Mar/23 14:57,02/Jul/10 00:52,0.7 beta 1,,,,0,,,,,,"calling thriftClient_.system_drop_keyspace(keyspaceName) on a newly created keyspace, then stopping the node renders the node unstartable. Results in the following stacktrace:

10/06/17 14:23:16 ERROR thrift.CassandraDaemon: Fatal exception during initialization
java.io.EOFException
	at java.io.DataInputStream.readFully(DataInputStream.java:180)
	at java.io.DataInputStream.readUTF(DataInputStream.java:592)
	at java.io.DataInputStream.readUTF(DataInputStream.java:547)
	at org.apache.cassandra.config.KSMetaData.deserialize(KSMetaData.java:92)
	at org.apache.cassandra.db.DefsTable.loadFromStorage(DefsTable.java:75)
	at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:422)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:103)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:90)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:221)

my repro:

start new node with empty data directory
create a new keyspace
drop the keyspace
attempt to restart the node, notice that it fails to start.
",Mac OS X 10.6.3 (10D573) / Darwin 10.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jun/10 06:00;gdusbabek;0001-handle-schema-loading-when-a-node-has-had-all-keyspa.patch;https://issues.apache.org/jira/secure/attachment/12447752/0001-handle-schema-loading-when-a-node-has-had-all-keyspa.patch",,,,,,,,,,,,,,1.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20033,,,Fri Jul 02 12:47:06 UTC 2010,,,,,,,,,,"0|i0g3mn:",92018,,,,,Normal,,,,,,,,,,,,,,,,,"02/Jul/10 00:04;jbellis;+1

(s/No tables where/No schema definitions were/);;;","02/Jul/10 20:47;hudson;Integrated in Cassandra #483 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/483/])
    handle schema loading when a node has had all keyspaces dropped. Patch by gdusbabek, reviewed by jbellis. CASSANDRA-1203.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gossiper misses first updates when restarting a node,CASSANDRA-515,12439113,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,27/Oct/09 05:36,16/Apr/19 17:33,22/Mar/23 14:57,27/Oct/09 23:29,0.5,,,,0,,,,,,"Easy way to reproduce:

Start node A.
Start node B, with autobootstrap=false.
Kill B, wipe data dir, and restart (still w/ autobootstrap=false).

A will show B as down, with its old token.  (B will see both nodes correctly.)

This appears to be because when you wipe data dir, generation restarts at 1.  (This is not just operator error; besides during testing, this could arise if a node dies completely and has to be replaced.)  Then gossip state is ignored until the new heartbeat is larger than the one previously reached.

It appears that initializing the generation to seconds-since-epoch would fix this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Oct/09 06:23;jbellis;515.patch;https://issues.apache.org/jira/secure/attachment/12423258/515.patch",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19731,,,Tue Oct 27 15:29:40 UTC 2009,,,,,,,,,,"0|i0fzen:",91334,,,,,Low,,,,,,,,,,,,,,,,,"27/Oct/09 06:23;jbellis;as described.;;;","27/Oct/09 06:37;urandom;Looks good. +1;;;","27/Oct/09 20:34;hudson;Integrated in Cassandra #240 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/240/])
    initialize generation to seconds-since-epoch, a value much more likely to be unique than ""1""
patch by jbellis; reviewed by eevans for 
;;;","27/Oct/09 23:29;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming Old Format Data Fails in 0.7.3 after upgrade from 0.6.8,CASSANDRA-2283,12500677,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,eonnen,eonnen,08/Mar/11 05:00,16/Apr/19 17:33,22/Mar/23 14:57,19/Apr/11 05:58,0.7.5,,,,0,,,,,,"After successfully upgrading a 0.6.8 ring to 0.7.3, we needed to bootstrap in a new node relatively quickly. When starting the new node with an assigned token in auto bootstrap mode, we see the following exceptions on the new node:

INFO [main] 2011-03-07 10:37:32,671 StorageService.java (line 505) Joining: sleeping 30000 ms for pending range setup
 INFO [main] 2011-03-07 10:38:02,679 StorageService.java (line 505) Bootstrapping
 INFO [HintedHandoff:1] 2011-03-07 10:38:02,899 HintedHandOffManager.java (line 304) Started hinted handoff for endpoint /10.211.14.200
 INFO [HintedHandoff:1] 2011-03-07 10:38:02,900 HintedHandOffManager.java (line 360) Finished hinted handoff of 0 rows to endpoint /10.211.14.200
 INFO [CompactionExecutor:1] 2011-03-07 10:38:04,924 SSTableReader.java (line 154) Opening /mnt/services/cassandra/var/data/0.7.3/data/Stuff/stuff-f-1
 INFO [CompactionExecutor:1] 2011-03-07 10:38:05,390 SSTableReader.java (line 154) Opening /mnt/services/cassandra/var/data/0.7.3/data/Stuff/stuff-f-2
 INFO [CompactionExecutor:1] 2011-03-07 10:38:05,768 SSTableReader.java (line 154) Opening /mnt/services/cassandra/var/data/0.7.3/data/Stuff/stuffid-f-1
 INFO [CompactionExecutor:1] 2011-03-07 10:38:06,389 SSTableReader.java (line 154) Opening /mnt/services/cassandra/var/data/0.7.3/data/Stuff/stuffid-f-2
 INFO [CompactionExecutor:1] 2011-03-07 10:38:06,581 SSTableReader.java (line 154) Opening /mnt/services/cassandra/var/data/0.7.3/data/Stuff/stuffid-f-3
ERROR [CompactionExecutor:1] 2011-03-07 10:38:07,056 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.io.EOFException
        at org.apache.cassandra.io.sstable.IndexHelper.skipIndex(IndexHelper.java:65)
        at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:303)
        at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:923)
        at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:916)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
 INFO [CompactionExecutor:1] 2011-03-07 10:38:08,480 SSTableReader.java (line 154) Opening /mnt/services/cassandra/var/data/0.7.3/data/Stuff/stuffid-f-5
 INFO [CompactionExecutor:1] 2011-03-07 10:38:08,582 SSTableReader.java (line 154) Opening /mnt/services/cassandra/var/data/0.7.3/data/Stuff/stuffid_reg_idx-f-1
ERROR [CompactionExecutor:1] 2011-03-07 10:38:08,635 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.io.EOFException
        at org.apache.cassandra.io.sstable.IndexHelper.skipIndex(IndexHelper.java:65)
        at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:303)
        at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:923)
        at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:916)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
ERROR [CompactionExecutor:1] 2011-03-07 10:38:08,666 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.io.EOFException
        at org.apache.cassandra.io.sstable.IndexHelper.skipIndex(IndexHelper.java:65)
        at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:303)
        at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:923)
        at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:916)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
 INFO [CompactionExecutor:1] 2011-03-07 10:38:08,855 SSTableReader.java (line 154) Opening /mnt/services/cassandra/var/data/0.7.3/data/Stuff/stuffid_reg_idx-f-4

Two attempts to bootstrap in the new node both exhibited this behavior. On the node owning the tokens being migrated, stream activity is visible but doesn't update any progress I think due to the issues on the receiving host.







Lastly, just case it's relevant, we had an EC2 node die underneath us during the upgrade so not all nodes were drained. This didn't affect the upgrade but I wanted to note it her to be thorough.","0.7.3 upgraded from 0.6.8, Linux Java HotSpot(TM) 64-Bit Server VM (build 17.1-b03, mixed mode)",eonnen,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Mar/11 22:36;jbellis;2283.txt;https://issues.apache.org/jira/secure/attachment/12474779/2283.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20539,,,Mon Apr 18 22:21:41 UTC 2011,,,,,,,,,,"0|i0gagf:",93124,,stuhood,,stuhood,Normal,,,,,,,,,,,,,,,,,"09/Mar/11 22:52;jbellis;CASSANDRA-2296 is going to cause streaming trouble too. In other words: scrub won't fix this unless run w/ a build that has 2296 applied.;;;","10/Mar/11 01:10;jbellis;bq. scrub won't fix this

that is, for sstables containing expired tombstones;;;","13/Mar/11 04:31;stuhood;While not ideal, this is actually supposed to throw an exception in SSTableWriter.createBuilder, but that is dependent on a correctly versioned Descriptor being created on the destination side. I expect that streaming is dropping the source version when it creates the destination descriptor.;;;","25/Mar/11 04:54;eonnen;Ok, I can confirm that after upgrading to 0.7.4 where 2296 was applied, and after performing a scrub, we were able to bootstrap in new nodes again.;;;","26/Mar/11 03:03;jbellis;IMO the right thing to do here is to deserialize enough of the data sent during stream to (a) rewrite it to latest format and (b) write bloom filter and row index -- currently this is done in a second pass post-stream.;;;","26/Mar/11 17:51;stuhood;> (a) rewrite it to latest format and (b) write bloom filter and row index
I was hoping that we could get away with just doing (b) in order to avoid having to re-write the data file, but it certainly simplifies things to re-write in the current format.

EDIT: CASSANDRA-2336 took a step toward allowing rebuilding and index writing to be version specific, in order to implement (b). I'm most of the way through an implementation of CASSANDRA-2319 on top of it, but I don't see a clear answer for a/b/a+b.;;;","28/Mar/11 22:34;jbellis;You're right, making it one-pass isn't feasible without writing the streamed row out to a temporary file first, since we don't have a way to rebuild the row-level bloom filter + block index.  In other words, not really any more one-pass than the existing approach.;;;","28/Mar/11 22:36;jbellis;Patch to preserve version across streams. Also removes obsolete component field from PendingFile (we only stream Component.DATA).;;;","13/Apr/11 06:13;stuhood;Looks good: only comment is that BootstrapTest should probably purposely use an old version and check that it is preserved.;;;","19/Apr/11 05:58;jbellis;added version check to BootstrapTest.

committed w/ just the version changes -- left PendingFile alone. (CASSANDRA-2438 suggests we might want to stream fully-formed sstables for bulk load.);;;","19/Apr/11 06:21;hudson;Integrated in Cassandra-0.7 #442 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/442/])
    preserve version when streaming data from old sstables
patch by jbellis; reviewed by Stu Hood for CASSANDRA-2283
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception during batch_mutate,CASSANDRA-834,12457340,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,brandon.williams,brandon.williams,25/Feb/10 04:34,16/Apr/19 17:33,22/Mar/23 14:57,23/Mar/10 23:10,0.6,,,,0,,,,,,"If a batch mutation is sent with deletions referring to a SCF but no SC is specified in the Deletion object, the following traceback is generated:

ERROR 15:28:16,746 Fatal exception in thread Thread[ROW-MUTATION-STAGE:22,5,main]
java.lang.RuntimeException: java.lang.ClassCastException: org.apache.cassandra.db.Column cannot be cast to org.apache.cassandra.db.SuperColumn
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.ClassCastException: org.apache.cassandra.db.Column cannot be cast to org.apache.cassandra.db.SuperColumn
        at org.apache.cassandra.db.SuperColumnSerializer.serialize(SuperColumn.java:300)
        at org.apache.cassandra.db.SuperColumnSerializer.serialize(SuperColumn.java:284)
        at org.apache.cassandra.db.ColumnFamilySerializer.serializeForSSTable(ColumnFamilySerializer.java:87)
        at org.apache.cassandra.db.ColumnFamilySerializer.serialize(ColumnFamilySerializer.java:73)
        at org.apache.cassandra.db.RowMutationSerializer.freezeTheMaps(RowMutation.java:329)
        at org.apache.cassandra.db.RowMutationSerializer.serialize(RowMutation.java:341)
        at org.apache.cassandra.db.RowMutationSerializer.serialize(RowMutation.java:314)
        at org.apache.cassandra.db.RowMutation.getSerializedBuffer(RowMutation.java:270)
        at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:200)
        at org.apache.cassandra.service.StorageProxy$3.runMayThrow(StorageProxy.java:282)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
","debian lenny amd64 OpenJDK 64-Bit Server VM (build 1.6.0_0-b11, mixed mode)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Feb/10 07:20;brandon.williams;834-test.patch;https://issues.apache.org/jira/secure/attachment/12436924/834-test.patch","23/Mar/10 06:08;jbellis;834-v2.txt;https://issues.apache.org/jira/secure/attachment/12439515/834-v2.txt","02/Mar/10 05:19;brandon.williams;834.patch;https://issues.apache.org/jira/secure/attachment/12437520/834.patch",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19885,,,Tue Mar 23 15:10:12 UTC 2010,,,,,,,,,,"0|i0g1d3:",91651,,,,,Low,,,,,,,,,,,,,,,,,"25/Feb/10 07:20;brandon.williams;System test to reproduce.;;;","02/Mar/10 05:19;brandon.williams;Patch to validate column paths in a slice predicate.;;;","02/Mar/10 06:24;jbellis;hmm...

shouldn't supercolumn==null mean ""apply this predicate to top level supercolumns?""  that is what we do in get_slice for instance.

this will be important when we add deletion of ranges, since there's no other way to specify ""a range of supercolumns"" than in the predicate.;;;","23/Mar/10 06:08;jbellis;Patch taking the 2nd approach, of promoting columns to supercolumns when SC is null in a super CF.;;;","23/Mar/10 21:59;brandon.williams;+1;;;","23/Mar/10 23:10;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When loading an AbstractType that does not include an instance field, an unhelpful exception is raised making diagnosis difficult",CASSANDRA-1242,12468314,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,eonnen,eonnen,eonnen,01/Jul/10 09:21,16/Apr/19 17:33,22/Mar/23 14:57,02/Jul/10 06:21,0.7 beta 1,,,,0,,,,,,"0.7.0 changes the contract for creating AbstractTypes. A custom AbstractType defined against 0.6.x will be incompatible and the error messaging around why the comparator is invalid is obtuse and non-obvious. Specifically, when porting a valid AbstractType from 0.6.x to 0.7 that does not include a public static instance field, the thrift system_add_column_family call will throw an exception whose only message is:

InvalidRequestException(why:instance)

No log messages are generated from the server as to the issue so the root cause is non obvious to developers.

I marked as Major because types defined against 0.6.x did not require an ""instance"" field so at a minimum migration of AbstractTypes to 0.7 should document the change in what is expected of AbstractTypes.

Patch attached for better logging and to create a more helpful exception for better communication to the client as to the issue.
",,eonnen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Jul/10 09:23;eonnen;CASSANDRA-1242.patch;https://issues.apache.org/jira/secure/attachment/12448465/CASSANDRA-1242.patch",,,,,,,,,,,,,,1.0,eonnen,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20048,,,Fri Jul 02 13:01:50 UTC 2010,,,,,,,,,,"0|i0g3v3:",92056,,,,,Normal,,,,,,,,,,,,,,,,,"02/Jul/10 06:21;jbellis;committed, thanks!

(I note in our defense that NEWS.txt has mentioned the AbstractType change for several weeks now though :);;;","02/Jul/10 20:47;hudson;Integrated in Cassandra #483 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/483/])
    add friendlier ConfigurationException for malformed AbstractTypes.  patch by Erik Onnen; reviewed by jbellis for CASSANDRA-1242
;;;","02/Jul/10 21:01;jeromatron;Good call Erik - it needed some good feedback like this when people get bitten by the change.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-cli doesn't work with system allowed column family names,CASSANDRA-1005,12462487,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,urandom,kingjamm,kingjamm,20/Apr/10 04:34,16/Apr/19 17:33,22/Mar/23 14:57,25/May/10 00:04,0.6.2,,,,0,cassandra-cli,,,,,"Given the following definitions for columns:

<Keyspaces>

<Keyspace Name=""NGram"">

<KeysCachedFraction>0.01</KeysCachedFraction>

<ColumnFamily CompareWith=""UTF8Type"" Name=""1GramR""/>

<ColumnFamily CompareWith=""UTF8Type"" Name=""1GramL""/>

</Keyspaces>

The appropriate keyspaces are created an persisteted on startup. When executing a query or a set operation in the cassandra-cli, you end up with the following error:

******************************************************

cassandra> get NGram.1GramR['hte']

line 1:10 extraneous input '1' expecting Identifier

No such column family: GramR

******************************************************


Following the syntax of the grammer we can see the following:

setStmt
: K_SET columnFamilyExpr '=' value -> ^(NODE_THRIFT_SET columnFamilyExpr value)
;

...

columnFamilyExpr
: table DOT columnFamily '[' rowKey ']'
( '[' a+=columnOrSuperColumn ']'
('[' a+=columnOrSuperColumn ']')?
)?
-> ^(NODE_COLUMN_ACCESS table columnFamily rowKey ($a+)?)
;
...

// syntactic Elements
Identifier
: Letter ( Alnum | '_' )*
;

There is a mismatch on what is appropriate values for this in the system. So either the restriction needs to be lifted in the cli, or the system must have a way of honoring the names.",Windows XP 32 bit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/May/10 05:50;urandom;0001-support-all-legal-keyspace-and-column-names-in-cli.patch;https://issues.apache.org/jira/secure/attachment/12445206/0001-support-all-legal-keyspace-and-column-names-in-cli.patch","22/May/10 05:50;urandom;cli.sh;https://issues.apache.org/jira/secure/attachment/12445207/cli.sh",,,,,,,,,,,,,2.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19950,,,Mon May 24 16:04:41 UTC 2010,,,,,,,,,,"0|i0g2f3:",91822,,,,,Low,,,,,,,,,,,,,,,,,"20/Apr/10 04:36;kingjamm;changed from major to minor as we have a current workaround.;;;","06/May/10 11:21;jbellis;is this another ""abandon all hope"" antlr thing?;;;","22/May/10 05:50;urandom;The attached patch seems to do it.

Also attached is the cassandra-cli script I used to test. To run it, first create keyspaces named {{1Space}} and {{0000}}, with column families named {{2Family}} and {{1111}} respectively, then run:

{noformat}
$ cassandra-cli < cli.sh
{noformat};;;","24/May/10 23:57;jbellis;+1;;;","25/May/10 00:04;urandom;committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition leads to FileNotFoundException on startup,CASSANDRA-1382,12471371,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,gdusbabek,rzotter,rzotter,12/Aug/10 12:08,16/Apr/19 17:33,22/Mar/23 14:57,18/Aug/10 02:12,0.7 beta 2,,,,0,,,,,,"On startup LocationInfo file is deleted then attempted to be read from.

Steps to reproduce: Kill then quickly restart

Switching to ParallelGC to avoid CMS/CompressedOops incompatibility
INFO 17:05:08,680 DiskAccessMode isstandard, indexAccessMode is mmap
 INFO 17:05:08,786 Sampling index for /var/lib/cassandra/data/system/Schema-e-1-<>
 INFO 17:05:08,797 Sampling index for /var/lib/cassandra/data/system/Schema-e-2-<>
 INFO 17:05:08,807 Sampling index for /var/lib/cassandra/data/system/Migrations-e-1-<>
 INFO 17:05:08,833 Sampling index for /var/lib/cassandra/data/system/Schema-e-1-<>
 INFO 17:05:08,834 Sampling index for /var/lib/cassandra/data/system/Schema-e-2-<>
 INFO 17:05:08,839 Sampling index for /var/lib/cassandra/data/system/Migrations-e-1-<>
 INFO 17:05:08,862 Sampling index for /var/lib/cassandra/data/system/Schema-e-1-<>
 INFO 17:05:08,864 Sampling index for /var/lib/cassandra/data/system/Schema-e-2-<>
 INFO 17:05:08,876 Sampling index for /var/lib/cassandra/data/system/Migrations-e-1-<>
 INFO 17:05:08,885 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-17-<>
 INFO 17:05:08,892 Sampling index for /var/lib/cassandra/data/system/Schema-e-1-<>
 INFO 17:05:08,893 Sampling index for /var/lib/cassandra/data/system/Schema-e-2-<>
 INFO 17:05:08,897 Sampling index for /var/lib/cassandra/data/system/Migrations-e-1-<>
 INFO 17:05:08,901 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-17-<>
 INFO 17:05:08,906 Sampling index for /var/lib/cassandra/data/system/Schema-e-1-<>
 INFO 17:05:08,909 Sampling index for /var/lib/cassandra/data/system/Schema-e-2-<>
 INFO 17:05:08,918 Sampling index for /var/lib/cassandra/data/system/Migrations-e-1-<>
 INFO 17:05:08,922 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-17-<>
 INFO 17:05:08,928 Creating new commitlog segment /var/lib/cassandra/commitlog/CommitLog-1281571508928.log
 INFO 17:05:08,933 Deleted /var/lib/cassandra/data/system/LocationInfo-e-16-Data.db
 INFO 17:05:08,936 Deleted /var/lib/cassandra/data/system/LocationInfo-e-15-Data.db
 INFO 17:05:08,936 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-16-<>
ERROR 17:05:08,937 Corrupt file /var/lib/cassandra/data/system/LocationInfo-e-16-Data.db; skipped
java.io.FileNotFoundException: /var/lib/cassandra/data/system/LocationInfo-e-16-Index.db (No such file or directory)
    at java.io.RandomAccessFile.open(Native Method)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:137)
    at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:289)
    at org.apache.cassandra.io.sstable.SSTableReader.internalOpen(SSTableReader.java:197)
    at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:176)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:208)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:196)
    at org.apache.cassandra.db.StatisticsTable.deleteSSTableStatistics(StatisticsTable.java:81)
    at org.apache.cassandra.io.sstable.SSTable.deleteIfCompacted(SSTable.java:136)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:202)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:196)
    at org.apache.cassandra.db.StatisticsTable.deleteSSTableStatistics(StatisticsTable.java:81)
    at org.apache.cassandra.io.sstable.SSTable.deleteIfCompacted(SSTable.java:136)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:202)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:121)
    at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:93)
    at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:90)
    at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
 INFO 17:05:08,947 Deleted /var/lib/cassandra/data/system/LocationInfo-e-14-Data.db
 INFO 17:05:08,947 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-17-<>
 INFO 17:05:08,948 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-15-<>
ERROR 17:05:08,948 Corrupt file /var/lib/cassandra/data/system/LocationInfo-e-15-Data.db; skipped
java.io.FileNotFoundException: /var/lib/cassandra/data/system/LocationInfo-e-15-Index.db (No such file or directory)
    at java.io.RandomAccessFile.open(Native Method)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:137)
    at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:289)
    at org.apache.cassandra.io.sstable.SSTableReader.internalOpen(SSTableReader.java:197)
    at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:176)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:208)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:196)
    at org.apache.cassandra.db.StatisticsTable.deleteSSTableStatistics(StatisticsTable.java:81)
    at org.apache.cassandra.io.sstable.SSTable.deleteIfCompacted(SSTable.java:136)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:202)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:121)
    at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:93)
    at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:90)
    at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
 INFO 17:05:08,950 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-16-<>
ERROR 17:05:08,951 Corrupt file /var/lib/cassandra/data/system/LocationInfo-e-16-Data.db; skipped
java.io.FileNotFoundException: /var/lib/cassandra/data/system/LocationInfo-e-16-Index.db (No such file or directory)
    at java.io.RandomAccessFile.open(Native Method)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:137)
    at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:289)
    at org.apache.cassandra.io.sstable.SSTableReader.internalOpen(SSTableReader.java:197)
    at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:176)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:208)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:196)
    at org.apache.cassandra.db.StatisticsTable.deleteSSTableStatistics(StatisticsTable.java:81)
    at org.apache.cassandra.io.sstable.SSTable.deleteIfCompacted(SSTable.java:136)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:202)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:121)
    at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:93)
    at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:90)
    at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
 INFO 17:05:08,970 Deleted /var/lib/cassandra/data/system/LocationInfo-e-13-Data.db
 INFO 17:05:08,971 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-14-<>
ERROR 17:05:08,971 Corrupt file /var/lib/cassandra/data/system/LocationInfo-e-14-Data.db; skipped
java.io.FileNotFoundException: /var/lib/cassandra/data/system/LocationInfo-e-14-Index.db (No such file or directory)
    at java.io.RandomAccessFile.open(Native Method)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:137)
    at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:289)
    at org.apache.cassandra.io.sstable.SSTableReader.internalOpen(SSTableReader.java:197)
    at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:176)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:208)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:121)
    at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:93)
    at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:90)
    at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
 INFO 17:05:08,972 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-17-<>
 INFO 17:05:08,973 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-15-<>
ERROR 17:05:08,973 Corrupt file /var/lib/cassandra/data/system/LocationInfo-e-15-Data.db; skipped
java.io.FileNotFoundException: /var/lib/cassandra/data/system/LocationInfo-e-15-Index.db (No such file or directory)
    at java.io.RandomAccessFile.open(Native Method)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:137)
    at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:289)
    at org.apache.cassandra.io.sstable.SSTableReader.internalOpen(SSTableReader.java:197)
    at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:176)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:208)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:121)
    at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:93)
    at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:90)
    at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
 INFO 17:05:08,974 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-16-<>
ERROR 17:05:08,974 Corrupt file /var/lib/cassandra/data/system/LocationInfo-e-16-Data.db; skipped
java.io.FileNotFoundException: /var/lib/cassandra/data/system/LocationInfo-e-16-Index.db (No such file or directory)
    at java.io.RandomAccessFile.open(Native Method)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:137)
    at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:289)
    at org.apache.cassandra.io.sstable.SSTableReader.internalOpen(SSTableReader.java:197)
    at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:176)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:208)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:121)
    at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:93)
    at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:90)
    at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
 INFO 17:05:08,996 Loading schema version acc5646a-a59d-11df-83fb-e700f669bcfc
 WARN 17:05:09,158 Schema definitions were defined both locally and in cassandra.yaml. Definitions in cassandra.yaml were ignored.
 INFO 17:05:09,164 Replaying /var/lib/cassandra/commitlog/CommitLog-1281571453475.log, /var/lib/cassandra/commitlog/CommitLog-1281571508928.log
 INFO 17:05:09,172 Finished reading /var/lib/cassandra/commitlog/CommitLog-1281571453475.log
 INFO 17:05:09,172 Finished reading /var/lib/cassandra/commitlog/CommitLog-1281571508928.log
 INFO 17:05:09,173 switching in a fresh Memtable for LocationInfo at CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1281571508928.log', position=592)
 INFO 17:05:09,183 Enqueuing flush of Memtable-LocationInfo@137493297(17 bytes, 1 operations)
 INFO 17:05:09,183 Writing Memtable-LocationInfo@137493297(17 bytes, 1 operations)
 INFO 17:05:09,184 switching in a fresh Memtable for Statistics at CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1281571508928.log', position=592)
 INFO 17:05:09,184 Enqueuing flush of Memtable-Statistics@86823325(0 bytes, 0 operations)
 INFO 17:05:09,265 Completed flushing /var/lib/cassandra/data/system/LocationInfo-e-18-Data.db
 INFO 17:05:09,273 Writing Memtable-Statistics@86823325(0 bytes, 0 operations)
 INFO 17:05:09,352 Completed flushing /var/lib/cassandra/data/system/Statistics-e-1-Data.db
 INFO 17:05:09,353 Recovery complete ",,johanoskarsson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Aug/10 04:48;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-separate-CFS-dir-cleanup-from-CFS-instantiation.txt;https://issues.apache.org/jira/secure/attachment/12451948/ASF.LICENSE.NOT.GRANTED--v1-0001-separate-CFS-dir-cleanup-from-CFS-instantiation.txt",,,,,,,,,,,,,,1.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20112,,,Wed Aug 18 13:13:49 UTC 2010,,,,,,,,,,"0|i0g4pr:",92194,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"13/Aug/10 02:13;gdusbabek;When there are compacted files to delete, initializing the system table tries to delete its statistics, which results in trying to initialize the system table.  The statistics deletion needs to happen outside of the CFS initialization.;;;","13/Aug/10 04:49;gdusbabek;Another approach to this fix would have been us have an initialization stage and make sure that CFS creation and statistics clean up happen on it. The stats cleanup would be submitted at the end of the stage while a CFS was being initialized.;;;","14/Aug/10 00:51;jbellis;it looks like this patch says we only delete statistics for sstables that had .compacted files left hanging around on the next restart, which won't be the case if they got cleaned up by the GC hook;;;","14/Aug/10 01:52;gdusbabek;It was the same way before.  SSTable.deleteIfCompacted() is the only method that calls StatisticsTable.deleteSSTableStatistics().  The only place SST.deleteIfCompacted() gets called is from the CFS constructor which is called during init.;;;","18/Aug/10 00:34;jbellis;+1;;;","18/Aug/10 21:13;hudson;Integrated in Cassandra #517 (See [https://hudson.apache.org/hudson/job/Cassandra/517/])
    missed CHANGES.txt for CASSANDRA-1382
separate CFS dir cleanup from CFS instantiation. fixes race condition. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1382
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OPP makes HH unhappy,CASSANDRA-1439,12472648,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,jbellis,jbellis,27/Aug/10 05:47,16/Apr/19 17:33,22/Mar/23 14:57,28/Aug/10 03:42,0.7 beta 2,,,,0,,,,,,"as reported multiple times on the mailing list and IRC:

ERROR [HINTED-HANDOFF-POOL:1] 2010-08-26 15:58:20,310 CassandraDaemon.java (line 82) Uncaught exception in thread Thread[HINTED-HANDOFF-POOL:1,5,main]
java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:87)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        ... 2 more
Caused by: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:169)
        at org.apache.cassandra.dht.OrderPreservingPartitioner.decorateKey(OrderPreservingPartitioner.java:41)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:199)
        at org.apache.cassandra.db.HintedHandOffManager.access$000(HintedHandOffManager.java:78)
        at org.apache.cassandra.db.HintedHandOffManager$1.runMayThrow(HintedHandOffManager.java:296)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 6 more
Caused by: java.nio.charset.MalformedInputException: Input length = 1
        at java.nio.charset.CoderResult.throwException(CoderResult.java:260)
        at java.nio.charset.CharsetDecoder.decode(CharsetDecoder.java:781)
        at org.apache.cassandra.utils.FBUtilities.decodeToUTF8(FBUtilities.java:483)
        at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:165)
        ... 11 more",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Aug/10 03:04;brandon.williams;1439.txt;https://issues.apache.org/jira/secure/attachment/12453261/1439.txt",,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20139,,,Fri Aug 27 19:42:34 UTC 2010,,,,,,,,,,"0|i0g52f:",92251,,,,,Normal,,,,,,,,,,,,,,,,,"28/Aug/10 03:04;brandon.williams;Patch to always use UTF-8 for hint keys.;;;","28/Aug/10 03:27;jbellis;+1 (w/ update to CHANGES);;;","28/Aug/10 03:42;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expected both token and generation columns,CASSANDRA-576,12441535,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,dispalt,dispalt,24/Nov/09 09:14,16/Apr/19 17:33,22/Mar/23 14:57,01/Dec/09 04:00,0.5,,,,0,,,,,,"I restarted cassandra and this happened, and kept repeating for a couple hours, seemingly never finishing.

2009-11-24_00:16:49.23400 INFO - Compacting [org.apache.cassandra.io.SSTableReader(path='/var/lib/cassandra/data/MonitorApp/Rollup20m-414-Data.db'),org.apache.cassandra.io.SSTableReader(path='/var/lib/cassandra/data/MonitorApp/Rollup20m-415-Data.db'),org.apache.cassandra.io.SSTableReader(path='/var/lib/cassandra/data/MonitorApp/Rollup20m-416-Data.db'),org.apache.cassandra.io.SSTableReader(path='/var/lib/cassandra/data/MonitorApp/Rollup20m-417-Data.db')]
2009-11-24_00:16:49.98396 ERROR - Exception encountered during startup.
2009-11-24_00:16:49.98396 java.lang.RuntimeException: Expected both token and generation columns; found ColumnFamily(LocationInfo [Token,])
2009-11-24_00:16:49.98396       at org.apache.cassandra.db.SystemTable.initMetadata(SystemTable.java:154)
2009-11-24_00:16:49.98396       at org.apache.cassandra.service.StorageService.start(StorageService.java:257)
2009-11-24_00:16:49.98396       at org.apache.cassandra.service.CassandraServer.start(CassandraServer.java:70)
2009-11-24_00:16:49.98396       at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:94)
2009-11-24_00:16:49.98396       at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:166)
2009-11-24_00:16:49.99396 Exception encountered during startup.
2009-11-24_00:16:49.99396 java.lang.RuntimeException: Expected both token and generation columns; found ColumnFamily(LocationInfo [Token,])
2009-11-24_00:16:49.99396       at org.apache.cassandra.db.SystemTable.initMetadata(SystemTable.java:154)
2009-11-24_00:16:49.99396       at org.apache.cassandra.service.StorageService.start(StorageService.java:257)
2009-11-24_00:16:49.99396       at org.apache.cassandra.service.CassandraServer.start(CassandraServer.java:70)
2009-11-24_00:16:49.99396       at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:94)
2009-11-24_00:16:49.99396       at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:166)
2009-11-24_00:16:50.83392 Listening for transport dt_socket at address: 8888",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Nov/09 10:37;jbellis;576.patch;https://issues.apache.org/jira/secure/attachment/12425927/576.patch",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19762,,,Mon Nov 30 20:00:05 UTC 2009,,,,,,,,,,"0|i0fzs7:",91395,,,,,Normal,,,,,,,,,,,,,,,,,"24/Nov/09 09:19;jbellis;dispalt: I restarted cassandra, after using cleanup

that's the culprit, cleanup cleaned out the system table row; the token setting was in the commit log and got replayed so it was still there, but the generation was not.;;;","24/Nov/09 10:37;jbellis;skip the system table (keyspace) for cleanup, it's local-only;;;","24/Nov/09 10:55;stuhood;Looks good to me.;;;","01/Dec/09 04:00;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stress.java doesn't actually read its data,CASSANDRA-1915,12494256,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,brandon.williams,brandon.williams,30/Dec/10 00:58,16/Apr/19 17:33,22/Mar/23 14:57,30/Dec/10 03:38,0.7.1,,,,0,,,,,,"stress.java doesn't actually read back the keys it inserts, but also reports no errors.  This is evident on larger (1M) runs where the read request rate is equal to what the bloom filter can do.  Stress.py also cannot find the rows that stress.java inserts.",,,,,,,,,,,,,,,,,,,,,,,14400,14400,,0%,14400,14400,,,,,,,,,,,,,,,,,,,,"30/Dec/10 03:27;xedin;CASSANDRA-1915.patch;https://issues.apache.org/jira/secure/attachment/12467131/CASSANDRA-1915.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20368,,,Wed Dec 29 20:21:56 UTC 2010,,,,,,,,,,"0|i0g873:",92758,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"30/Dec/10 03:38;brandon.williams;Committed, thanks!;;;","30/Dec/10 04:21;hudson;Integrated in Cassandra-0.7 #133 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/133/])
    Fix for stress.java using wrong key names and not detecting empty keys.
Patch by Pavel Yaskevich, reviewed by brandonwilliams for CASSANDRA-1915
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LegacySSTableTest breaks when run from a svn checkout,CASSANDRA-1309,12469940,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,stuhood,jbellis,jbellis,23/Jul/10 02:47,16/Apr/19 17:33,22/Mar/23 14:57,27/Jul/10 21:25,0.7 beta 1,,,,0,,,,,,"Works fine under git where there is no .svn turd.

    [junit] ------------- Standard Error -----------------
    [junit] Failed to read .svn
    [junit] java.io.FileNotFoundException: /Users/jonathan/projects/cassandra/svn-trunk/test/data/legacy-sstables/.svn/Keyspace1/Standard1-.svn-0-Index.db (No such file or directory)
    [junit] 	at java.io.RandomAccessFile.open(Native Method)
    [junit] 	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
    [junit] 	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
    [junit] 	at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)
    [junit] 	at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:137)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:256)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.internalOpen(SSTableReader.java:187)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:170)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:150)
    [junit] 	at org.apache.cassandra.io.sstable.LegacySSTableTest.testVersion(LegacySSTableTest.java:102)
    [junit] 	at org.apache.cassandra.io.sstable.LegacySSTableTest.testVersions(LegacySSTableTest.java:95)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit] 	at java.lang.reflect.Method.invoke(Method.java:597)
    [junit] 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    [junit] 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit] 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    [junit] 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit] 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
    [junit] 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:44)
    [junit] 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:180)
    [junit] 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:41)
    [junit] 	at org.junit.runners.ParentRunner$1.evaluate(ParentRunner.java:173)
    [junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit] 	at org.junit.runners.ParentRunner.run(ParentRunner.java:220)
    [junit] 	at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:768)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-1249,,,,"23/Jul/10 05:26;stuhood;0001-Only-test-valid-version-strings.patch;https://issues.apache.org/jira/secure/attachment/12450221/0001-Only-test-valid-version-strings.patch",,,,,,,,,,,,,,1.0,stuhood,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20073,,,Tue Jul 27 13:25:08 UTC 2010,,,,,,,,,,"0|i0g49z:",92123,,,,,Low,,,,,,,,,,,,,,,,,"23/Jul/10 05:26;stuhood;Skips invalid version subdirectories.

Also, it looks like we're missing a few versions in test/data/legacy-sstables: would a committer mind following the instructions in LegacySSTableTest to generate sstables for versions 'c' and 'd'? In the past, I've posted them as git 'data diffs', but I think patch might have dropped them silently.;;;","27/Jul/10 21:25;gdusbabek;+1 committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Thrift interface uses reserved keyword ""end""",CASSANDRA-247,12428493,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,stephenjudkins,stephenjudkins,stephenjudkins,21/Jun/09 06:53,16/Apr/19 17:33,22/Mar/23 14:57,22/Jun/09 22:48,0.4,,,,0,,,,,,"The definition for get_slice_by_name_range has an argument named ""end"".

According to https://issues.apache.org/jira/browse/THRIFT-434, this will soon become an illegal keyword in Thrift definitions due to its use as a reserved keyword in Ruby.

Currently, attempting to use the Ruby interface results in syntax errors unless the interface definition is changed prior to generation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jun/09 06:58;stephenjudkins;CASSANDRA-247.patch;https://issues.apache.org/jira/secure/attachment/12411324/CASSANDRA-247.patch",,,,,,,,,,,,,,1.0,stephenjudkins,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19608,,,Tue Jun 23 12:34:55 UTC 2009,,,,,,,,,,"0|i0fxrr:",91069,,,,,Normal,,,,,,,,,,,,,,,,,"21/Jun/09 06:58;stephenjudkins;This patch changes argument for get_slice_by_name_range from ""end"" to ""finish"" in interface definition.

I'm fairly certain this won't break much of anything? Correct me if I'm wrong.;;;","22/Jun/09 22:48;jbellis;Committed, thanks!

(Accidentally attributed patch to Stu Hood in the commit message.  Sorry, must be Monday. :);;;","23/Jun/09 20:34;hudson;Integrated in Cassandra #117 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/117/])
    rename ""end"" parameter to ""finish"" to avoid conflict w/ ruby keyword.  patch by Stu Hood; reviewed by jbellis for 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError: discard at CommitLogContext(file=...) is not after last flush at  ...,CASSANDRA-1330,12470364,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,vilda,vilda,29/Jul/10 01:26,16/Apr/19 17:33,22/Mar/23 14:57,02/Oct/10 13:05,0.6.6,0.7 beta 3,,,0,,,,,,"Looks related to CASSANDRA-936?

ERROR [MEMTABLE-POST-FLUSHER:1] 2010-07-28 11:39:36,909 CassandraDaemon.java (line 83) Uncaught exception in thread Thread[MEMTABLE-POST-FLUSHER:1,5,main]
java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: discard at CommitLogContext(file='/srv/cassandra/commitlog/CommitLog-1280331567364.log', position=181) is not after last flush at 563
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
        at java.util.concurrent.FutureTask.get(FutureTask.java:111)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:86)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1118)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: discard at CommitLogContext(file='/srv/cassandra/commitlog/CommitLog-1280331567364.log', position=181) is not after last flush at 563
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        ... 2 more
Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: discard at CommitLogContext(file='/srv/cassandra/commitlog/CommitLog-1280331567364.log', position=181) is not after last flush at 563
        at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegments(CommitLog.java:373)
        at org.apache.cassandra.db.ColumnFamilyStore$1.runMayThrow(ColumnFamilyStore.java:371)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 6 more
Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError: discard at CommitLogContext(file='/srv/cassandra/commitlog/CommitLog-1280331567364.log', position=181) is not after last flush at 563
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
        at java.util.concurrent.FutureTask.get(FutureTask.java:111)
        at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegments(CommitLog.java:365)
        ... 8 more
Caused by: java.lang.AssertionError: discard at CommitLogContext(file='/srv/cassandra/commitlog/CommitLog-1280331567364.log', position=181) is not after last flush at 563
        at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegmentsInternal(CommitLog.java:394)
        at org.apache.cassandra.db.commitlog.CommitLog.access$300(CommitLog.java:70)
        at org.apache.cassandra.db.commitlog.CommitLog$6.call(CommitLog.java:359)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:52)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 1 more
",Java 1.6 / Linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Oct/10 01:25;jbellis;1330.txt;https://issues.apache.org/jira/secure/attachment/12456141/1330.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20083,,,Sat Oct 02 05:05:47 UTC 2010,,,,,,,,,,"0|i0g4ef:",92143,,mdennis,,mdennis,Normal,,,,,,,,,,,,,,,,,"02/Oct/10 01:25;jbellis;This really is essentially the same scenario as CASSANDRA-936.  (See my 3rd-from-the-bottom comment for background and a diagram.)

The part my analysis in 936 is missing is that the CL holds mutations from many columnfamilies, so the position of the first turnOn in a given CF may be arbitrarily high depending on how many mutations happened to other CFs first.  (Similarly, the flush context may also be arbitrarily high, although it's more likely to be low because this scenario can't occur once each CF has been flushed once in the new segment.)

Given the turnOn-for-first-write-to-CF-in-new-segment behavior (which is what keeps us from having to replay the entirety of any not-completely-flushed segement), I don't think we can usefully assert anything about the flush context vs the dirty position.  This patch removes it -- and changes CLS.lastFlushedAt to CLS.cfDirtiedAt (which is what it has already been renamed to in 0.7) to make it more clear that flushing isn't the only thing that affects the header positions.;;;","02/Oct/10 10:55;mdennis;+1;;;","02/Oct/10 13:05;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LongCompactionSpeedTest fails,CASSANDRA-2461,12504066,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,jbellis,jbellis,12/Apr/11 23:02,16/Apr/19 17:33,22/Mar/23 14:57,15/Apr/11 02:20,0.8 beta 1,,,,0,,,,,,"ant long-test -Dtest.name=LongCompactionSpeedTest fails.

There are several errors. Here is the first:

{noformat}
    [junit] java.lang.IllegalArgumentException
    [junit] 	at java.nio.ByteBuffer.allocate(ByteBuffer.java:311)
    [junit] 	at org.apache.cassandra.db.context.CounterContext.clearAllDelta(CounterContext.java:444)
    [junit] 	at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:100)
    [junit] 	at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:36)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.next(SSTableIdentityIterator.java:158)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.next(SSTableIdentityIterator.java:41)
    [junit] 	at org.apache.commons.collections.iterators.CollatingIterator.set(CollatingIterator.java:284)
    [junit] 	at org.apache.commons.collections.iterators.CollatingIterator.least(CollatingIterator.java:326)
    [junit] 	at org.apache.commons.collections.iterators.CollatingIterator.next(CollatingIterator.java:230)
    [junit] 	at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:69)
    [junit] 	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
    [junit] 	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
    [junit] 	at com.google.common.collect.Iterators$7.computeNext(Iterators.java:614)
    [junit] 	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
    [junit] 	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
    [junit] 	at org.apache.cassandra.db.ColumnIndexer.serializeInternal(ColumnIndexer.java:76)
    [junit] 	at org.apache.cassandra.db.ColumnIndexer.serialize(ColumnIndexer.java:50)
    [junit] 	at org.apache.cassandra.io.LazilyCompactedRow.<init>(LazilyCompactedRow.java:87)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableWriter$CommutativeRowIndexer.doIndexing(SSTableWriter.java:462)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.index(SSTableWriter.java:364)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:317)
    [junit] 	at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:1089)
    [junit] 	at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:1080)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Apr/11 18:10;slebresne;0001-Fix-LongCompactionSpeedTest.patch;https://issues.apache.org/jira/secure/attachment/12476227/0001-Fix-LongCompactionSpeedTest.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20636,,,Thu Apr 14 18:20:22 UTC 2011,,,,,,,,,,"0|i0gbif:",93295,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"13/Apr/11 18:10;slebresne;That was not updated correctly with CASSANDRA-1938, sorry. Patch attached.;;;","15/Apr/11 02:20;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.util.NoSuchElementException when returning a node to the cluster,CASSANDRA-1432,12472526,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,amorton,amorton,26/Aug/10 05:40,16/Apr/19 17:33,22/Mar/23 14:57,27/Aug/10 06:14,0.6.6,0.7 beta 2,,,0,,,,,,"I'm running the v0.7-beta1 in a 4 nodes cluster and just doing some simple testing. One of the nodes had been down (machine off, unclean shutdown) for an hour or so not sure how many writes were going on, when I bought it back up this message appears in the other 3 nodes...


INFO [GOSSIP_STAGE:1] 2010-08-25 19:29:51,199 Gossiper.java (line 584) Node /192.168.34.27 has restarted, now UP again
 INFO [HINTED-HANDOFF-POOL:1] 2010-08-25 19:29:51,200 HintedHandOffManager.java (line 191) Started hinted handoff for endpoint /192.168.34.27
 INFO [GOSSIP_STAGE:1] 2010-08-25 19:29:51,201 StorageService.java (line 636) Node /192.168.34.27 state jump to normal
 INFO [GOSSIP_STAGE:1] 2010-08-25 19:29:51,201 StorageService.java (line 643) Will not change my token ownership to /192.168.34.27
ERROR [HINTED-HANDOFF-POOL:1] 2010-08-25 19:29:51,640 CassandraDaemon.java (line 82) Uncaught exception in thread Thread[HINTED-HANDOFF-POOL:1,5,main]
java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.util.NoSuchElementException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:87)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.RuntimeException: java.util.NoSuchElementException
        at orgapache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        ... 2 more
Caused by: java.util.NoSuchElementException
        at java.util.concurrent.ConcurrentSkipListMap.lastKey(ConcurrentSkipListMap.java:1981)
        at java.util.concurrent.ConcurrentSkipListMap$KeySet.last(ConcurrentSkipListMap.java:2331)
        at org.apache.cassandra.db.HintedHandOffManager.sendMessage(HintedHandOffManager.java:121)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:218)
        at org.apache.cassandra.db.HintedHandOffManager.access$000(HintedHandOffManager.java:78)
        at org.apache.cassandra.db.HintedHandOffManager$1.runMayThrow(HintedHandOffManager.java:296) not sure how many writes were going on
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 6 more

On the machine that was off (34.27) there are no errors in the logs, and here are the entries for around the same time...

 INFO [main] 2010-08-25 19:29:50,679 CommitLog.java (line 340) Recovery complete
 INFO [main] 2010-08-25 19:29:50,769 CommitLog.java (line 180) Log replay complete
 INFO [main] 2010-08-25 19:29:50,797 StorageService.java (line 342) Cassandra version: 0.7.0-beta1-SNAPSHOT
 INFO [main] 2010-08-25 19:29:50,797 StorageService.java (line 343) Thrift API version: 10.0.0
 INFO [main] 2010-08-25 19:29:50,813 SystemTable.java (line 240) Saved Token found: 85070591730234615865843651857942052864
 INFO [main] 2010-08-25 19:29:50,813 SystemTable.java (line 257) Saved ClusterName found: FOO
 INFO [main] 2010-08-25 19:29:50,813 SystemTable.java (line 272) Saved partitioner not found. Using org.apache.cassandra.dht.RandomPartitioner
 INFO [main] 2010-08-25 19:29:50,814 ColumnFamilyStore.java (line 422) switching in a fresh Memtable for LocationInfo at CommitLogContext(file='/local1/junkbox/cassandra/commitlog/CommitLog-12827213897
70.log', position=41336)
 INFO [main] 2010-08-25 19:29:50,814 ColumnFamilyStore.java (line 706) Enqueuing flush of Memtable-LocationInfo@916236367(95 bytes, 2 operations)
 INFO [FLUSH-WRITER-POOL:1] 2010-08-25 19:29:50,815 Memtable.java (line 150) Writing Memtable-LocationInfo@916236367(95 bytes, 2 operations)
 INFO [FLUSH-WRITER-POOL:1] 2010-08-25 19:29:50,873 Memtable.java (line 157) Completed flushing /local1/junkbox/cassandra/data/system/LocationInfo-e-6-Data.db
 INFO [main] 2010-08-25 19:29:50,917 StorageService.java (line 374) Starting up server gossip
 INFO [main] 2010-08-25 19:29:51,093 ColumnFamilyStore.java (line 1239) Loaded 0 rows into the Super2 cache
 INFO [main] 2010-08-25 19:29:51,170 CassandraDaemon.java (line 153) Binding thrift service to /0.0.0.0:9160
 INFO [main] 2010-08-25 19:29:51,174 CassandraDaemon.java (line 167) Using TFramedTransport with a max frame size of 15728640 bytes.
 INFO [GOSSIP_STAGE:1] 2010-08-25 19:29:51,198 Gossiper.java (line 578) Node /192.168.34.28 is now part of the cluster
 INFO [GOSSIP_STAGE:1] 2010-08-25 19:29:51,199 Gossiper.java (line 578) Node /192.168.34.29 is now part of the cluster
 INFO [GOSSIP_STAGE:1] 2010-08-25 19:29:51,199 Gossiper.java (line 578) Node /192.168.34.26 is now part of the cluster
 INFO [main] 2010-08-25 19:29:51,204 CassandraDaemon.java (line 208) Listening for thrift clients...
 INFO [main] 2010-08-25 19:29:51,210 Mx4jTool.java (line 73) Will not load MX4J, mx4j-tools.jar is not in the classpath
 INFO [HINTED-HANDOFF-POOL:1] 2010-08-25 19:29:51,417 HintedHandOffManager.java (line 191) Started hinted handoff for endpoint /192.168.34.28
 INFO [GOSSIP_STAGE:1] 2010-08-25 19:29:51,417 Gossiper.java (line 570) InetAddress /192.168.34.28 is now UP
 INFO [HINTED-HANDOFF-POOL:1] 2010-08-25 19:29:51,418 HintedHandOffManager.java (line 247) Finished hinted handoff of 0 rows to endpoint /192.168.34.28
 INFO [HINTED-HANDOFF-POOL:1] 2010-08-25 19:29:51,855 HintedHandOffManager.java (line 191) Started hinted handoff for endpoint /192.168.34.29
 INFO [GOSSIP_STAGE:1] 2010-08-25 19:29:51,855 Gossiper.java (line 570) InetAddress /192.168.34.29 is now UP
 INFO [HINTED-HANDOFF-POOL:1] 2010-08-25 19:29:51,860 HintedHandOffManager.java (line 247) Finished hinted handoff of 0 rows to endpoint /192.168.34.29
 INFO [HINTED-HANDOFF-POOL:1] 2010-08-25 19:29:52,930 HintedHandOffManager.java (line 191) Started hinted handoff for endpoint /192.168.34.26
 INFO [GOSSIP_STAGE:1] 2010-08-25 19:29:52,930 Gossiper.java (line 570) InetAddress /192.168.34.26 is now UP
 INFO [HINTED-HANDOFF-POOL:1] 2010-08-25 19:29:52,930 HintedHandOffManager.java (line 247) Finished hinted handoff of 0 rows to endpoint /192.168.34.26

I ran a repair on all the nodes and this was all that they each logged 
 INFO [manual-repair-fe7c5abb-bb0a-4415-aa75-0d72ba4e7f1b] 2010-08-25 19:49:24,194 AntiEntropyService.java (line 803) Waiting for repair requests to: []

The cluster seemed OK and kept on working.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Aug/10 05:36;jbellis;1432-06.txt;https://issues.apache.org/jira/secure/attachment/12453173/1432-06.txt","27/Aug/10 05:37;jbellis;1432-trunk.txt;https://issues.apache.org/jira/secure/attachment/12453174/1432-trunk.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20134,,,Thu Aug 26 22:14:56 UTC 2010,,,,,,,,,,"0|i0g50v:",92244,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"26/Aug/10 14:41;amorton;I think this is stopping the HH and AE from running. 

I did the following:
- stopped a node
- deleted commit log and data dir for the keyspace 
- turned the node on 
- ran repair 

Then saw similar messages in the logs, with the repairing node logging the Waiting for repair requests to: [] message. No data was sent to the node. 

So my last comment about ""cluster seemed ok"" is probably wrong. 
;;;","26/Aug/10 15:52;stuhood;> Waiting for repair requests to: [] message.
This would imply that that node thinks the replication factor is 1, and that it therefore doesn't need to request trees.;;;","27/Aug/10 05:39;jbellis;the HH paging code doesn't handle zero columns found for the row, which could happen if cleanup removes the rows before they are handed off or if they are tombstoned and aged out during compaction.

patches attached for 0.6 and trunk;;;","27/Aug/10 06:05;brandon.williams;+1;;;","27/Aug/10 06:14;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Discard Commitlog Exception,CASSANDRA-936,12460748,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,dispalt,dispalt,31/Mar/10 05:47,16/Apr/19 17:33,22/Mar/23 14:57,06/Apr/10 03:54,0.6.1,,,,0,,,,,,"2010-03-30_21:19:02.31041 java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: discard called on obsolete context CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1269983937410.log', position=8780)
2010-03-30_21:19:02.31041 	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
2010-03-30_21:19:02.31041 	at java.util.concurrent.FutureTask.get(FutureTask.java:111)
2010-03-30_21:19:02.31041 	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:86)
2010-03-30_21:19:02.31041 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1118)
2010-03-30_21:19:02.31041 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
2010-03-30_21:19:02.31041 	at java.lang.Thread.run(Thread.java:636)
2010-03-30_21:19:02.31041 Caused by: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: discard called on obsolete context CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1269983937410.log', position=8780)
2010-03-30_21:19:02.31041 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
2010-03-30_21:19:02.31041 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
2010-03-30_21:19:02.31041 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
2010-03-30_21:19:02.31041 	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
2010-03-30_21:19:02.31041 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
2010-03-30_21:19:02.31041 	... 2 more
2010-03-30_21:19:02.31041 Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: discard called on obsolete context CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1269983937410.log', position=8780)
2010-03-30_21:19:02.31041 	at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegments(CommitLog.java:358)
2010-03-30_21:19:02.31041 	at org.apache.cassandra.db.ColumnFamilyStore$1.runMayThrow(ColumnFamilyStore.java:371)
2010-03-30_21:19:02.31041 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
2010-03-30_21:19:02.31041 	... 6 more
2010-03-30_21:19:02.31041 Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError: discard called on obsolete context CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1269983937410.log', position=8780)
2010-03-30_21:19:02.31041 	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
2010-03-30_21:19:02.31041 	at java.util.concurrent.FutureTask.get(FutureTask.java:111)
2010-03-30_21:19:02.31041 	at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegments(CommitLog.java:350)
2010-03-30_21:19:02.31041 	... 8 more
2010-03-30_21:19:02.31041 Caused by: java.lang.AssertionError: discard called on obsolete context CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1269983937410.log', position=8780)
2010-03-30_21:19:02.31041 	at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegmentsInternal(CommitLog.java:378)
2010-03-30_21:19:02.31041 	at org.apache.cassandra.db.commitlog.CommitLog.access$300(CommitLog.java:72)
2010-03-30_21:19:02.31041 	at org.apache.cassandra.db.commitlog.CommitLog$6.call(CommitLog.java:344)
2010-03-30_21:19:02.31041 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
2010-03-30_21:19:02.31041 	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
2010-03-30_21:19:02.31041 	at org.apache.cassandra.db.commitlog.CommitLogExecutorService.process(CommitLogExecutorService.java:113)
2010-03-30_21:19:02.31041 	at org.apache.cassandra.db.commitlog.CommitLogExecutorService.access$200(CommitLogExecutorService.java:35)
2010-03-30_21:19:02.31041 	at org.apache.cassandra.db.commitlog.CommitLogExecutorService$1.runMayThrow(CommitLogExecutorService.java:67)
2010-03-30_21:19:02.31041 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
2010-03-30_21:19:02.31041 	... 1 more",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Apr/10 03:20;jbellis;936.txt;https://issues.apache.org/jira/secure/attachment/12440784/936.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19926,,,Mon Apr 05 19:54:29 UTC 2010,,,,,,,,,,"0|i0g1zr:",91753,,,,,Normal,,,,,,,,,,,,,,,,,"02/Apr/10 00:35;jbellis;Thought I had this figured out but I was wrong.

Can you turn on debug logging for the commitlog and include the log up to the next exception?

You can do this w/ JConsole under the service.StorageService MBean, Operations group, setLog4jLevel(org.apache.cassandra.db.commitlog, DEBUG)
;;;","03/Apr/10 02:02;dispalt;2010-04-02_04:12:48.85690 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270178196299.log); dirty is 1, 
2010-04-02_04:12:48.87690 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270178421319.log); dirty is 1, 
2010-04-02_04:12:48.87690 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270178736305.log); dirty is 1, 5, 15, 
2010-04-02_04:12:48.88690 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270178971505.log); dirty is 1, 5, 15, 
2010-04-02_04:12:48.88690 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270179198585.log); dirty is 1, 5, 15, 
2010-04-02_04:12:48.99689 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270179421825.log); dirty is 1, 5, 7, 9, 15, 2010-04-02_04:12:49.01689 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270179734722.log); dirty is 1, 5, 9, 15, 
2010-04-02_04:12:49.04689 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270179957332.log); dirty is 1, 5, 9, 15, 
2010-04-02_04:12:49.05689 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270180177252.log); dirty is 1, 5, 9, 15, 
2010-04-02_04:12:49.16688 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270180484668.log); dirty is 1, 9, 15, 
2010-04-02_04:12:49.21688 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270180714597.log); dirty is 1, 6, 9, 12, 15, 16, 
2010-04-02_04:12:49.23688 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270180931705.log); dirty is 1, 3, 5, 6, 9, 12, 14, 15, 16, 
2010-04-02_04:12:49.28688 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270181242081.log); dirty is 1, 3, 4, 5, 6, 9, 11, 12, 15, 16, 
2010-04-02_04:12:49.29688 DEBUG - Marking replay position 67674133 on commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270181459011.log)2010-04-02_04:14:33.32220 INFO - Rollup4h has reached its threshold; switching in a fresh Memtable at CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1270181459011.log', position=117622522)
2010-04-02_04:14:33.33219 INFO - Enqueuing flush of Memtable(Rollup4h)@1686682813
2010-04-02_04:14:33.34219 INFO - Writing Memtable(Rollup4h)@1686682813
2010-04-02_04:14:36.60205 INFO - Completed flushing /var/lib/cassandra/data/MonitorApp/Rollup4h-2483-Data.db2010-04-02_04:14:36.60205 DEBUG - discard completed log segments for CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1270181459011.log', position=117622522), column family 16. CFIDs are {system: TableMetadata({LocationInfo: 0, HintsColumnFamily: 1, }), MonitorApp: TableMetadata({AppCounter: 2, Rollup5m: 3, Rollup20m: 4, TextChangeLog: 5, Rollup12h: 6, CheckDetails: 7, TextArchive: 8, StatusArchive: 9,
 NumericArchive: 10, Rollup30m: 11, Rollup1d: 12, StatusChangeLog: 15, ChangeLog: 13, MetricSummary: 14, Rollup60m: 17, Rollup4h: 16, RollupBookeeper: 18, }), }
2010-04-02_04:14:36.60205 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270083637654.log); dirty is 0, 2010-04-02_04:14:36.67204 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270159986913.log); dirty is 10, 2010-04-02_04:14:36.72204 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270178196299.log); dirty is 1, 
2010-04-02_04:14:36.74204 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270178421319.log); dirty is 1, 
2010-04-02_04:14:36.80204 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270178736305.log); dirty is 1, 5, 15, 2010-04-02_04:14:36.83204 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270178971505.log); dirty is 1, 5, 15, 
2010-04-02_04:14:36.88204 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270179198585.log); dirty is 1, 5, 15, 
2010-04-02_04:14:36.91203 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270179421825.log); dirty is 1, 5, 7, 9, 15, 
2010-04-02_04:14:36.96203 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270179734722.log); dirty is 1, 5, 9, 15, 
2010-04-02_04:14:37.01203 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270179957332.log); dirty is 1, 5, 9, 15, 
2010-04-02_04:14:37.05203 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270180177252.log); dirty is 1, 5, 9, 15, 
2010-04-02_04:14:37.18202 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270180484668.log); dirty is 1, 9, 15, 
2010-04-02_04:14:37.32202 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270180714597.log); dirty is 1, 6, 9, 12, 15, 2010-04-02_04:14:37.35201 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270180931705.log); dirty is 1, 3, 5, 6, 9, 12, 14, 15, 
2010-04-02_04:14:37.38201 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270181242081.log); dirty is 1, 3, 4, 5, 6, 9, 11, 12, 15, 
2010-04-02_04:14:37.40201 DEBUG - Marking replay position 117622522 on commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270181459011.log)
2010-04-02_04:14:37.69200 INFO - Rollup12h has reached its threshold; switching in a fresh Memtable at CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1270181459011.log', position=130228458)
2010-04-02_04:14:37.70200 INFO - Enqueuing flush of Memtable(Rollup12h)@298933940
2010-04-02_04:14:37.70200 INFO - Writing Memtable(Rollup12h)@298933940
2010-04-02_04:14:39.40192 INFO - Creating new commitlog segment /var/lib/cassandra/commitlog/CommitLog-1270181679401.log
2010-04-02_04:14:39.44192 INFO - Rollup1d has reached its threshold; switching in a fresh Memtable at CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1270181679401.log', position=169)2010-04-02_04:14:39.45192 INFO - Enqueuing flush of Memtable(Rollup1d)@46303223
2010-04-02_04:14:40.10189 INFO - Completed flushing /var/lib/cassandra/data/MonitorApp/Rollup12h-2511-Data.db
2010-04-02_04:14:40.10189 INFO - Writing Memtable(Rollup1d)@46303223
2010-04-02_04:14:40.10189 DEBUG - discard completed log segments for CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1270181459011.log', position=130228458), column family 6. CFIDs are {system: T
ableMetadata({LocationInfo: 0, HintsColumnFamily: 1, }), MonitorApp: TableMetadata({AppCounter: 2, Rollup5m: 3, Rollup20m: 4, TextChangeLog: 5, Rollup12h: 6, CheckDetails: 7, TextArchive: 8, StatusArchive: 9, 
NumericArchive: 10, Rollup30m: 11, Rollup1d: 12, StatusChangeLog: 15, ChangeLog: 13, MetricSummary: 14, Rollup60m: 17, Rollup4h: 16, RollupBookeeper: 18, }), }2010-04-02_04:14:40.10189 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270083637654.log); dirty is 0, 
2010-04-02_04:14:40.20189 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270159986913.log); dirty is 10, 
2010-04-02_04:14:40.25188 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270178196299.log); dirty is 1, 
2010-04-02_04:14:40.27188 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270178421319.log); dirty is 1, 
2010-04-02_04:14:40.30188 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270178736305.log); dirty is 1, 5, 15, 
2010-04-02_04:14:40.40188 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270178971505.log); dirty is 1, 5, 15, 
2010-04-02_04:14:40.44187 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270179198585.log); dirty is 1, 5, 15, 
2010-04-02_04:14:40.47187 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270179421825.log); dirty is 1, 5, 7, 9, 15, 
2010-04-02_04:14:40.50187 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270179734722.log); dirty is 1, 5, 9, 15, 
2010-04-02_04:14:40.52187 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270179957332.log); dirty is 1, 5, 9, 15, 
2010-04-02_04:14:40.54187 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270180177252.log); dirty is 1, 5, 9, 15, 
2010-04-02_04:14:40.56187 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270180484668.log); dirty is 1, 9, 15, 
2010-04-02_04:14:40.67186 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270180714597.log); dirty is 1, 9, 12, 15, 
2010-04-02_04:14:40.68186 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270180931705.log); dirty is 1, 3, 5, 9, 12, 14, 15, 
2010-04-02_04:14:40.76186 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270181242081.log); dirty is 1, 3, 4, 5, 9, 11, 12, 15, 
2010-04-02_04:14:40.76186 DEBUG - Marking replay position 130228458 on commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270181459011.log)
2010-04-02_04:14:43.51174 INFO - Completed flushing /var/lib/cassandra/data/MonitorApp/Rollup1d-2568-Data.db
2010-04-02_04:14:43.51174 DEBUG - discard completed log segments for CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1270181679401.log', position=169), column family 12. CFIDs are {system: TableMetadata({LocationInfo: 0, HintsColumnFamily: 1, }), MonitorApp: TableMetadata({AppCounter: 2, Rollup5m: 3, Rollup20m: 4, TextChangeLog: 5, Rollup12h: 6, CheckDetails: 7, TextArchive: 8, StatusArchive: 9, NumericArchive: 10, Rollup30m: 11, Rollup1d: 12, StatusChangeLog: 15, ChangeLog: 13, MetricSummary: 14, Rollup60m: 17, Rollup4h: 16, RollupBookeeper: 18, }), }
2010-04-02_04:14:43.52174 ERROR - Error in executor futuretask
2010-04-02_04:14:43.52174 java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: discard called on obsolete context CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1270181679401.log', position=169)
2010-04-02_04:14:43.52174       at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
2010-04-02_04:14:43.52174       at java.util.concurrent.FutureTask.get(FutureTask.java:111)
2010-04-02_04:14:43.52174       at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:86)
2010-04-02_04:14:43.52174       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1118)
2010-04-02_04:14:43.52174       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
2010-04-02_04:14:43.52174       at java.lang.Thread.run(Thread.java:636)
2010-04-02_04:14:43.52174 Caused by: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: discard called on obsolete context CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1270181679401.log', position=169)
2010-04-02_04:14:43.52174       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
2010-04-02_04:14:43.52174       at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
2010-04-02_04:14:43.52174       at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
2010-04-02_04:14:43.52174       at java.util.concurrent.FutureTask.run(FutureTask.java:166)
2010-04-02_04:14:43.52174       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
2010-04-02_04:14:43.52174       ... 2 more
2010-04-02_04:14:43.52174 Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: discard called on obsolete context CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1270181679401.log', position=169)
2010-04-02_04:14:43.52174       at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegments(CommitLog.java:358)
2010-04-02_04:14:43.52174       at org.apache.cassandra.db.ColumnFamilyStore$1.runMayThrow(ColumnFamilyStore.java:371)
2010-04-02_04:14:43.52174       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
2010-04-02_04:14:43.52174       ... 6 more
2010-04-02_04:14:43.52174 Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError: discard called on obsolete context CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1270181679401.log', position=169)
2010-04-02_04:14:43.52174       at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
2010-04-02_04:14:43.52174       at java.util.concurrent.FutureTask.get(FutureTask.java:111)
2010-04-02_04:14:43.52174       at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegments(CommitLog.java:350)
2010-04-02_04:14:43.52174       ... 8 more
2010-04-02_04:14:43.52174 Caused by: java.lang.AssertionError: discard called on obsolete context CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1270181679401.log', position=169)
2010-04-02_04:14:43.52174       at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegmentsInternal(CommitLog.java:378)
2010-04-02_04:14:43.52174       at org.apache.cassandra.db.commitlog.CommitLog.access$300(CommitLog.java:72)
2010-04-02_04:14:43.52174       at org.apache.cassandra.db.commitlog.CommitLog$6.call(CommitLog.java:344)
2010-04-02_04:14:43.52174       at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
2010-04-02_04:14:43.52174       at java.util.concurrent.FutureTask.run(FutureTask.java:166)
2010-04-02_04:14:43.52174       at org.apache.cassandra.db.commitlog.CommitLogExecutorService.process(CommitLogExecutorService.java:113)
2010-04-02_04:14:43.52174       at org.apache.cassandra.db.commitlog.CommitLogExecutorService.access$200(CommitLogExecutorService.java:35)
2010-04-02_04:14:43.52174       at org.apache.cassandra.db.commitlog.CommitLogExecutorService$1.runMayThrow(CommitLogExecutorService.java:67)
2010-04-02_04:14:43.52174       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
2010-04-02_04:14:43.52174       ... 1 more
;;;","03/Apr/10 02:04;dispalt;Those are the log lines before the exception with the commitlog debugging on full.;;;","06/Apr/10 03:20;jbellis;Okay, here's what's happening.

The assert in question is
        assert context.position > context.getSegment().getHeader().getPosition(id) : ""discard called on obsolete context "" + context;

The first argument, context.position, is ""the position in the current commitlog segment at the time the currently-finishing flush began.""  The second argument is ""the position we wrote in the commitlog segment header showing where the position was for last-completed flush.""  If the current flush position is less than the last flush position, then the last flush potentially deleted segments whose data wasn't turned into sstables yet, which would be Very Bad.  If the positions are equal, that would mean we're flushing a CF that hasn't had any new data since the last one, which isn't supposed to happen either.

But, there's a discontinuity in the position measuring, and that's when a new commitlog context is swapped in.  Let me give a diagram here of the CL segement's contents:

{code}
XXXXXXXXXXXXXXYYYYYYYYYYYYYYZZZZZZZZZZZZZ
0             H             F            etc
{code}

The first H bytes are the header.  From H to position F represents the first mutation written to the fresh segment.

Normally, if you force a memtable to flush after a new commitlog segment is rolled in, one of two things will happen:

(1) you will flush with a context position of H, and a header lastFlushedPosition of 0 (the initial value) or
(2) you will flush with a context position of F or higher, and a header lastFlushedPosition of H (after some writes occur)

But, if the flush context gets measured at H before any writes happen, THEN A WRITE OCCURS DURING THE FLUSH BEFORE THE DISCARD PHASE, you will have context position == lastFlushedPosition == H, because making a write calls turnOn which edits lastFlushedAt, even though no flushes have occurred, because lastFlushedAt is also semantically startReplayAt, and 0 is not a valid place to start mutation replay.  So in the latest stacktrace, H == 169, and it's failing in exactly this scenario.

The least invasive fix for this in 0.6 is to change the assert from > to >=.
;;;","06/Apr/10 03:49;gdusbabek;+1;;;","06/Apr/10 03:54;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in StorageService when cluster is first being created,CASSANDRA-1639,12477942,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,davew,davew,21/Oct/10 08:40,16/Apr/19 17:33,22/Mar/23 14:57,22/Oct/10 22:52,0.7 beta 3,,Legacy/Tools,,0,,,,,,"Saw this exception on the 0.7.0-beta2 version of cassandra right after bringing up a cluster and trying to get the number of live nodes.

java.lang.NullPointerException
        at org.apache.cassandra.service.StorageService.stringify(StorageService.java:1151)
        at org.apache.cassandra.service.StorageService.getLiveNodes(StorageService.java:1138)
        at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
        at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:65)
        at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:216)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:666)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:638)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1404)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
        at javax.management.remote.rmi.RMIConnectionImpl.getAttribute(RMIConnectionImpl.java:600)
        at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
        at sun.rmi.transport.Transport$1.run(Transport.java:159)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)


I fixed this by adding some null checks to the stringify methods 

    private Set<String> stringify(Collection<InetAddress> endpoints)
    {
        Set<String> stringEndpoints = new HashSet<String>();
        for (InetAddress ep : endpoints)
        {
        	if(ep != null) {
        		stringEndpoints.add(ep.getHostAddress());
        	}
        }
        return stringEndpoints;
    }

    private List<String> stringify(List<InetAddress> endpoints)
    {
        List<String> stringEndpoints = new ArrayList<String>();
        for (InetAddress ep : endpoints)
        {
        	if(ep != null) {
        		stringEndpoints.add(ep.getHostAddress());
        	}
        }
        return stringEndpoints;
    }

After adding those checks, then I got more reasonable/realistic errors from a different part of the code since the service wasn't up yet as the cluster was still initializing:

Caused by: javax.management.InstanceNotFoundException: org.apache.cassandra.service:type=StorageService
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1094)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:662)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:638)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1404)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
        at javax.management.remote.rmi.RMIConnectionImpl.getAttribute(RMIConnectionImpl.java:600)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
        at sun.rmi.transport.Transport$1.run(Transport.java:159)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
        at sun.rmi.transport.StreamRemoteCall.exceptionReceivedFromServer(StreamRemoteCall.java:255)
        at sun.rmi.transport.StreamRemoteCall.executeCall(StreamRemoteCall.java:233)
        at sun.rmi.server.UnicastRef.invoke(UnicastRef.java:142)
        at com.sun.jmx.remote.internal.PRef.invoke(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl_Stub.getAttribute(Unknown Source)
        at javax.management.remote.rmi.RMIConnector$RemoteMBeanServerConnection.getAttribute(RMIConnector.java:878)
        at javax.management.MBeanServerInvocationHandler.invoke(MBeanServerInvocationHandler.java:263)
",Windows XP,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20230,,,Fri Oct 22 14:52:30 UTC 2010,,,,,,,,,,"0|i0g6gv:",92478,,,,,Low,,,,,,,,,,,,,,,,,"22/Oct/10 22:52;jbellis;looks like this is fixed in the latest nightly -- should not be possible for getLiveNodes to pass a null list to stringify.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
